In this spreadsheet, and then copy and paste the data into a new spreadsheet and then try and find the average number of Facebook friends. Hm, okay, link to Udasion's Facebook friends, equals, I can't type anything. Okay, oh copy and paste the data into your own spreadsheet. How do I do that? Now from Google drive, looks like I have to sign in, I can go to create spreadsheet. Let's call this the same thing that says here, this way I can reference back to this in the future. Okay, so I have to Copy and Paste the data, edit, Copy's not there, I'm going to copy it by brute force. Force command C. Maybe I'll just try copying the data. Command c, command v, yay it worked. What's something else I could've done? File, maybe download as Microsoft Excel. Hey maybe this'll work. It did! I have an exact copy of the data and I can do the exact same thing in Microsoft Excel. Well, I think I'll use Google Docs for now. Do-do-do-do-do. Oh no, I accidentally clicked on it and it's gone, but here it is in my drive. From your Google account, you can see all the options here at the top. And here's my spreadsheet. Let's get to know this spreadsheet. This is cell A1, this is cell A2. Each cell has a name just like you and me, except we usually don't have numbers in our names. There's no comma or special characters in the name of the cell, and it's always the letter first and then the number. What do you think this cell is? Let's put a header on this data so we know what it is. Insert row above. This is the header, and is not part of the data, it just tells us what we're measuring. So that means, the data is from cell A2 to A28. Now I wonder what cool stuff we could do with this data? How about data, sort, range. Oh, data has header row. I almost said data has header row. Sort by number of Facebook friends, indeed. What else could we do? What's this? That looks like sigma sum. Here's a bunch of functions, let's add them. It automatically put the formula here, we could've written that ourselves though and not been so lazy. Actually, let's do some formulas. Let's calculate the average. Equals. Average A2, because that's where our data starts, all the way to A28. Cool. And then we can see here, in the formula bar, what we just typed. Let's find the deviation from the mean. There's no formula for that, you just have to intuitively know what it is. Deviation, meaning how far something differs. From the mean, how far each value differs from the mean. How do we find the difference? By subtracting. And how do we start our formulas? With an equals. A2 minus, and now we always want this value to be constant. So instead of just writing E1, we're going to put a dollar sign in between the letter and the number. That'll keep it constant when we copy and paste this for the other cells. So that makes sense, right? 0 minus 584, blah-blah-blah should be negative 584, blah-blah-blah. So now, there are a few things we can do. We can go to this corner here where there's a plus sign, then drag it down, and then we get each deviation from the mean. In fact, if we click on it, we see in the formula bar this is A10 here minus the mean, which is E1. Another thing we could do is click on this cell, push command c, or, if you're on a PC, Ctrl+C, go to cell A2, push command-down, go to cell B28, push command-shift-up. And then that area is highlighted. And then push command v. What's the sum of the deviations? Equals, sum B2:B28, it's like 0. It should always be about 0. Let's name this column, Deviations. Xi minus x-bar, and x-bar is the mean. Okay, let's do squared deviations. That's pretty self explanatory, it's just the deviation squared, equals B2 squared. Now again, command c, go to cell B2, command down. Go to cell C28, command shift-up, command v. I'm telling you, if you get used to those keyboard shortcuts, you'll be able to do things way fast, especially if you have a list of 500 data values. It might be easier for short data sets to just drag this down, but for large data sets, these keyboard shortcuts will make your life a lot easier. So now we have our squared deviations. Let's find the sum of squares. Equals, sum, C2;C28. Error, I must have typed something wrong. Let's look and see what I did. Oh, there's a comma right there, let's delete it. Now we get the sum of squares, or SS. Let's find the variance. That's simply the sum of squares divided by n, equals C30 divided by, and it looks like there are 27 values. This goes until C28, but there's a header here. So it's not using one of the cells. So there are only 27 values. Cool. So this is the variance. Should probably write this down, just so you remember. Oh, and look, it says all changes saved in drive. Every change you make is automatically saved in drive, that is so awesome. I love Google Docs. Notice that for variance, we divided by n, but if we were treating this as a sample, we would divide by n minus 1. Okay, let's find the standard deviation, equals square root of C31. So here is the standard deviation. Which remember, it's represented by that Greek symbol Sigma. What if someone had a lot of Facebook friends? Like 100,000. How would that change our mean standard deviation and variant? Let's say that instead of having 408 Facebook friends, he has 100,000. Ready for this? Whoa! Everything changed automatically. The deviations are huge, because the mean is huge, and the square deviations are huge, and everything's huge. That's so cool. And then you don't have to do anything different. What if everybody had zero Facebook friends? Then everything's just zero. Are spreadsheets totally awesome or what? Hi and welcome to Inferential Statistics. This course builds on the skills you previously learned in Descriptive Statistics. Now if you haven't taken Descriptive Stats yet, feel free to follow the link in the instructor's notes box to our descriptive stats class. Statistician John Tukey said an approximate answer to the right problem is worth a good deal more than the exact answer to an approximate problem. Now in this course you will learn to apply statistics to find these approximate answers, make predictions about data, as well as run tests to verify the accuracy of a model. And for your final project, you will get the chance to make your own predictions on a data set. So, let's go to Katie Kormanik to get started Welcome to Elementary Statistics. We're really excited to have you on board. My name's Katie and I'm the one who will be explaining most of the concepts throughout this course. I'll also be doing a lot of drawing and a lot of writing to help you understand the content as much as possible. in each of the sixteen lessons in this course, you'll learn new methods to describe and analyze data. Each lesson builds on the concepts you learned before. So as you progress through the course, try to get a good grasp of the material before moving on. But if it turns out you need a review, no big deal. You can easily go back and rewatch the videos and redo the exercises. What do I mean by exercises? Each lesson consists of short instructional videos where I explain cool stat stuff and guide your thinking. But we want you to make breakthroughs on your own. So instead of just telling you a fact or formula, I'll ask a quiz. A quiz or exercise, same thing, is not there for us to judge you or to impact your grade. It's there for your own benefit, to make you think. At Udacity, we believe that the chance to think about something from your own unique perspective is a powerful learning opportunity. Klout score is the single number that, that measure your influence across the Internet primarily the social media. I'm Chief Scientist at Klout, I run the data science team as well as the data infrastructure team oversee all the data driven products that come, you know kind of lead the team behind the score. And you created the whole infrastructure, and the algorithm behind the Klout score, right? I create the infrastructure and the science behind the Klout score. We grade everyone's influence on the Internet from a scale from 1 to 99. It's a single score that you can see as your credit score for you online reputation or you know, a single score that you can measure influence with. Klout score is calculate by incorporating all your data from various social networks, ranging from Twitter to Facebook, to LinkedIn. And for wach network, I would collect about a hundred signals. Number of retweets you get per twwet, number of likes you get per post on Facebook stuff like that are all the signals that we look at overall we consifer four hundred signals per person. And we generally scores for 400 million and more users at everyday. Klout score follows this by model distribution, where you have this very big peak of users who are in this 40 to 60s on this side and also the users who are quality content producers. And you also have this very nice peak of users on the other side who are the consumers of those content. And if you, you know, plot the score, and the number of users with each score buckets from this side to this side. You know, this is zero, and that's 99. And this, you know, the y axis is the number of users who are in this buckets. You can see a very nice by model distribution there. The average cost score is lower than the median because they are proportionally more people with a lower score, so they drag the average down. So in such a distribution, taking a look at the median makes more sense. Being able to beat forty is actually a pretty good sign of being very influential on the social media. So, the first thing we're going to do, is analyze this data. This is the biggest data set you've worked with so far, so don't be intimidated. So, open the spreadsheet of Klout scores and calculate the mean and standard deviation. Treat it as a population rather than a sample. They're our Klout scores. So first let's calculate the mean. Push Cmd down arrow key and then you see that there are 1,048 values. And make sure we don't have a header row, which we don't. So we know that n is 1,048. So anywhere really, we can write equals, average, A1, colon A1048. So we get that the mean is 37.72 approximately. Let's calculate the standard deviation. We calculated our mean in cell D1048. Cell rate in cell B1, equals A1 minus $D $1048, which keeps the value in cell D1048 constant. Okay, the we'll go to cell B1 hit Cmd+C to copy it, go to cell A1, Cmd down arrow key go to cell B1048, Cmd+Shift up arrow key. So all of column B is highlighted, Cmd+B. Now we have all our deviations. And actually what we could do is instead, we can just calculate the square deviations. So let's put parenthesis around the deviation and then square it. Now let's copy this and paste it. Now we have all our squared deviations in column B. So we calculate the variance by taking the average of this, equals average B1:B1048. So that's our variance and then our standard deviation is the square root, equals square root of D1. So, here's the distribution of Klout data and you can see it's bimodal, like the chief scientist described. The mean is about here at 37.72. So now, let's say we're able to take all possible samples of size 35 and calculate the mean for each one of them. And then, graph this distribution of sample means. So, this distribution should be normal, if you remember from previously. What will be the mean of this distribution? The mean should be the same as the population mean. So, about 37.72. What about the standard deviation of this distribution? The standard deviation is the population standard deviation divided by the square root of n, where n is the size of each sample that we're taking. So this is 16.04 divided by the square root of 35, which is about 2.71. Good job. So now which one of these distributions is the correct sampling distribution for the population of klout scores. Is it this one, this one, or this one? Since the mean is supposed to be the same as the population mean, then it will be one of these two. So, which one of these looks to have a standard deviation of about 2.71? For this wider one, the standard deviation looks like it's at least 5. So, this is the correct sampling distribution. So, why is knowing the shape of the sampling distribution important? Well, first of all, let's say we want to increase our klout score. Brans use cloud to reach out to the influencers for word of mass marketing . So for example Audi has this test drive combined and they are offered to influencers in San Francisco. They basically invited all those opinion leaders in different vectors of the industry to come in and test drive one of this newest model, and get a sense of how they feel about this new model. That's what Albie did, and Virgin America for example offered this VIP servers for those, who are even just account user, when they come to the airport terminals, the envidos influences over to their VIP lounge, just for all the pros they have there. Because you know, they think having a car score is a good indicator of you being. Influentia Savio was this social media thing that's coming up. So, and people has different ways of using car scores across different domains. I've also heard of some dating sites using cars score as a signal to match people up. Come on, the other ways that you can think of often top of your head. Some CM malls chip marking offers is Klout to just filter their candidates with. Because a lot of the bigger, more traditional companies, when they try to adapt to their social media era, they're trying to hire someone with social media background. To be able to help with their branding and, and, and the picture of their company. And for those jobs having a Klout is a must. I've heard. I think Barrack Obama, the President, has the highest class score of 99. And San Francisco Giants since just recently after they won the World's Series, they had a share that same score for a month. Wow, and Justin Bieber? Justin Bieber has a very high class score of 92. And a lot of celebrities and media celebrities are a whiz in their range. And but he's one of, definitely one of the highest Klout score. So, influential people have high Klout scores like the president and Justin Bieber. Now, let's say we looked up the Klout scores of 35 people who use the app Bieber Tweeter, which automatically retweets Justin Bieber whenever he tweets anything. If you don't know, a tweet is a message that people write on the social networking site Twitter. And the Klout score is partly based off of people's Twitter activity. So, let's say that the average Klout score of these people is 40. Where does this mean fall on the distribution of sample means for other samples of size 35? So, just to recap, you found that the standard deviation of this distribution is 2.71. So, how many standard deviations above the mean of this distribution does this mean lie? So where does 40 fall on this distribution? Looks like it's above the mean, and the mean is 37.72, so we're finding how many standard deviations 40 is from 37.72. So 40 minus the mean of the sampling distribution, divided by 2.71, which is about 0.84. Good job. You just calculated the z-score for this particular sample, using the mean and standard deviation of the sampling distribution. And remember that the standard deviation of the sampling distribution is called the standard error. So now what's the probability of randomly drawing a sample of size 35 with a mean of at least 40? As a hint, use the z table. We know the z score, so we can find the probability of getting anything less than that z score using our z table. So here's 0.8 and here's the column where it intersects 0.04. So the probability of getting less than a mean of 40 is 0.7995. So the probability of getting greater than a mean of 40 is 1 minus 0.7995, which is only about 0.2. So it's somewhat unlikely to have randomly drawn a sample from the whole Klout population with a mean of 40. If this mean wasn't selected by chance, then it's possible that this app, the Bieber Tweeter, which automatically re-tweets Justin Bieber, could have played a role in increasing these people's Klout scores. But how unlikely is a probability of 0.2? There's no set threshold. But in the next lesson, we're going to show you how statisticians have formally decided whether or not something is likely or unlikely. So, can we say that the Bieber Tweeter caused these higher Klout scores? No, we can't. All we know is that the Bieber Tweeters tend to have higher Klout scores on average. Maybe the Bieber Tweeter caused this, but maybe it's just that users with higher Klout tended to be the ones who used the app. Moving on, what if instead of having a sample of size 35, our sample had 250 people in it. And they also have the mean Klout score of 40. Now, what's the likelihood of randomly selecting from this Klout population? A sample of size 250, whose mean is at least 40. First let's find the standard error. Again, the standard error will be the population standard deviation, divided by the square root of the sample size, which is now 250. So now, the denominator is bigger which means that the standard error will be smaller. And indeed it's 1.01. So, let's write it over here. So, this sampling distribution is going to be a lot skinnier. So, where would a mean of 40 from a sample of size 250 fall on the distribution of means from all other samples of size 250? 40 is still above the mean of sample means, so the z squared will still be positive, but it's going to be a lot more because the standard deviation is less. So, we have 40 minus 37.72, divided by 1.01. Smaller denominator, bigger quotient. So, this gives us 2.26, approximately. So, now that we know the z-score, what's the probability of randomly selecting a sample with a mean of at least 40? Let's bring the z table back again. So now our z score is 2.26. So here's 2.2, and here is where it intersects 0.06. So the probability of getting a mean less than 40 is 0.9881. The probability of getting at least 40 is 1 minus 0.9881, which is a far lower probability. So we have this really skinny sampling distribution and the probability of getting greater than 40 is very small, 0.0119. So getting a sample of size 250 with a mean of 40, is pretty unlikely to occur by chance. In this case, this shows some kind of relationship, probably, between the Bieber Tweeter, and Klout score. Great job so far, this is pretty complicated stuff. To wrap up this lesson we're going to do a little experiment and it's going to be awesome if you guys participate. Together we're all going to generate sampling distributions of size 1, 5, and t0. So here's what we're going to do. Pick a number, any number between 1 and 1048. Click the link to access the Klout score data, and then find the Klout score in the row of the number you picked. So if you picked the number 786, for example, go to row 786, find what Klout score that is, then write it down. Next,pick 5 numbers between 1 and 1048, find the Klout scores in these rows, take the average, and then write this number down. Finally, repeat this process with ten numbers. So you should get three numbers. Write these numbers in the Google form, with the link below. So essentially, we're going to get a bunch of means, depending on how many of you guys participate. So let's say 1000 of you participate. Then we'll have 1000 Klout scores. Which we'll then create a distribution for. And this should look like the population distribution. And we'll also have 1000 means, of samples of size five. And we'll have 1000 means from samples of size 10. Then we'll be able to graph each of these sampling distributions and see if it turns out how we theoretically would have expected. So time for your last quiz of this lesson. Is this going to be awesome? You bet it's going to be awesome. I'm so excited to see how this turns out. In the meantime, feel free to check out the results in that Google form and try to analyze it yourself. See if we get some sampling distributions that look like they should, theoretically. Great job, you guys. Welcome back. In all these cases up to now, we've assumed that we know the population parameters, mu and sigma. But much of the time, we don't. We often only have samples, which we must then use to draw all our conclusions. In the next two lessons, we'll use samples to find out how different a sample mean is from a population, and how different two samples are from each other. Out of all the measures of center, we usually use the mean. Now, the two samples we're comparing in this case, can either be dependent or independent. We'll go over these differences later. In lesson ten, we're going to look at these. And then in lesson 11, we'll look at independent samples. It's going to be fun. When we work with samples, we have to estimate the population standard deviation using the samples standard deviation with Bessel's correction. Remember this from Lesson 4? Normally, to find out how typical or atypical a sample mean is, in what you did before, as we would find its location on the distribution of sample means, the sampling distribution. And we can determine the shape and parameters of this sampling distribution if we know the population parameters. Remember that for any sample mean, we can find where it falls on this distribution by standardizing. In other words, finding the z-score of the sample mean. We find the difference between the sample mean and mu, and then divide by the standard error. But now, the standard error depends on the sample, we can no longer us sigma if we have a sample. Therefore, we end up with a new distribution that is more prone to error. This is called the t distribution. Since it's more prone to error, then it's more spread out and thicker in the tails than a normal distribution. Remember from lesson seven when you learned that larger sample sizes result in skinnier sampling distributions? That same principal applies here. So what do you think happens as n, the sample size, increases? The standard error increases? The t-distribution approaches a normal distribution? The t-distribution gets skinnier tails? And finally, s, the sample center deviation, approaches sigma. This first ones incorrect. As n increase the standard error decreases. With a greater sample size we have less error. The t-distribution does approach a normal distribution. And as it approaches a normal distribution it gets skinnier in the tails. And also, as n increases, then s approaches sigma. Sometimes the t-statistic is called Student's t. This is because in the early 1900's a guy worked for the Guinness Brewery in Dublin, Ireland, and he used the t-test to monitor and compare the qualities of beer. He then published his findings under the pen name Student. t distributions are defined by their degrees of freedom. Degrees of freedom are pretty complicated to explain. So, first we'll start with some real life examples before applying them to these statistical concepts. Let's say you have three homework assignments to do, writing, statistics, and psychology. And each will take you an hour. And you only have three hours to finish them. So here are your three, hour long, time slots. How many options do you have for which homework assignment, you can do for the first time slot? After you choose which homework assignment you'll do. How many options do you have for the second time slot? And finally, how many options do you have for the third? After you choose what homework assignments you'll do for your first two time slots? In the first time slot, you can pick writing, statistics, or psychology. So you have three options. Let's say you pick writing. Then you can either do statistics or psychology. So you have two options. Let's say you do psychology. This last one is forced. You only have one more option, statistics. You had a choice in two of the time slots, the first and the second. You could have done statistics first, then you can choose between writing and psychology. Or you could do writing first and then choose between statistics and psychology, etcetera. But once you choose your first two, the last one is forced. Therefore, in this simple situation, there are two degrees of freedom. Let's try another one. You have to choose n numbers. You can choose any, n numbers. N is any number. It could be 5, 10, 100. You just have to pick that many numbers. How many degrees of freedom are there? This isn't a trick question. There are no restrictions for what you can choose you just have to pick n numbers it doesn't matter what they are. So there are n degrees of freedom. Are you getting it. Let's try another one. You have n numbers that must sum to 10. So your first number plus your second number, plus all the way to the nth number, has to equal 10. You may choose a certain number of values, these xi's, any way you wish. But then a certain number of values are forced, so that they sum to 10. How many degrees of freedom are there? Or in other words, how many values may you choose any way you wish. This is tricky. Let's say n is 4, so we have x1 plus x2 all the way to x4 has to equal 10. Let's choose a number, any number. I like the number 13. What does this mean? That means that x2 plus x3 plus x4 has to equal 10 minus 13, so it has to equal negative 3. Okay, let's pick another number. I like the number 8. So, so far we've chosen that x1 is 13, x2 is 8. This means that x3 plus x4 equals negative 3 minus 8, which is negative 11. We can still choose another value for x3 or x4. I like the number three so lets say x3 is 3 and now what is that mean, x4 must equal negative 11 minus 3 which is negative 14, so x4 has been forced, but I was able to choose these three, there were four numbers to start out with and I have three degrees of freedom. If we have n numbers, we have n minus 1 degrees of freedom. Good job! Lets do one more, this is really to help with your conceptual understanding of degrees of freedom. Have you ever played Sudoku? Well this is kind of like Sudoku, lets say that each row and each column has to add to 9, you can pick any numbers here in these nine entries as long as they meet this restriction. So how many values do you have a choice? How many degrees of freedom? Let's pick any number in here. Let's say 5. Can we choose this number and still have this sum to 9, and still have these columns sum to 9? Yeah. Let's say 3. So together those make 8, and that means this one is forced it has to be 1. Okay, can we pick a value here. Yeah, let's say 8. So those sum to 13, which means that this value is forced. This has to be negative 4, and then can we pick a value here, and have this row and this column still sum to 9? Yeah, let's say 7. Now if this column has to sum to 9, then this entry's forced, it's negative 1. And as you can see, this entry's forced too, this adds to 15, so to add to 9, this should be negative 6. And this entry's forced as well, this has to be 14. Then both this column and this row sum to 9. So in this case there are 4 degrees of freedom. But if we have an n by n table, in this case this is a 3 by 3 table. This is a 4 by 4 table. Then we would be able to chose all of these entries but then these ones would be forced. This number of tiles is n minus 1. And this number of tiles is also n minus 1. So the total number that we can choose is n minus 1 squared. So here in this 3 by 3 table, we were able to choose 2 times 2. In this 4 by 4 table, we were able to choose 3 times 3. So when we have an n by n table, we can choose n minus 1 times n minus 1, or just n minus 1 squared. Let's apply this idea of the degrees of freedom to samples and populations. Let's say these are values in a population. We can pick any n values from this population to have the sample of size n. If n is five, let's say we pick these five. There are five degrees of freedom. If we pick a sample of size n, we have n degrees of freedom. But then we have to calculate the sample standard deviation. And to do this, we need the sample mean. To find the mean, we add up each value in our sample, divide by the number in our sample, and then we get x bar. That means that the sum of all the values in the sample has to be x bar times n, this is just like that example you did before where you have n values that must add to 10. In that case, you had n minus 1 degrees of freedom and the same applies here. In our population, when we need to pick a sample of size n, we now have a restriction that nth value has to ensure that they add to x bar times n. In other words, n minus 1 of the values may vary as long as the nth value results in that same sum to get the same average. N minus one is called the effective sample size. Since the sample standard deviation depends on x bar, there are n minus 1 degrees of freedom. You might ask, well why do we divide by n with the population standard deviation if that also uses x bar? Well, with the population we can't replace certain values with other values, like we can with samples, because we already have all possible values of the population. Degrees of freedom are the number of pieces of information that can be freely varied without violating any given restrictions. Think of degrees of freedom as the number of independent pieces of information available to estimate another piece of information. Only n minus 1 values are independent after we know the mean. As the degrees of freedom increases, the t-distribution better approximates the normal distribution. Since we no longer have a normal distribution, but a t distribution, we have a new table. This is called the t table, unlike the z table, the t table tells us the critical values in the body. And up here, the column rows are the area in the right tail. On the left we have the degrees of freedom. Remember I said that t distributions are defined by their degrees of freedom. And then on the x axis of the t distributions, we have the t values instead of the z values. In this case we only really care about the t critical values. So let's say we have a t distribution and we want to know the t critical value such that. 10% is in the tail here, or a proportion of 0.1. And let's say that our sample size is 10, so our degrees of freedom is 9. So this is a t distribution with 9 degrees of freedom. And we want to find this t critical value. Well here in the tails, we look for 0.1. And with 9 degrees of freedom, we see that the t critical value is 1.383. You're going to use this t table to do pretty much the exact same thing you did with a z table. But instead of finding the z score and then determining if that z score is bigger or less than the z critical value. We find the t statistic and we figure out if the t statistic is greater or less than the t critical value. Let's practice using the t table. There's a link to the t table at the bottom, so use it to find the t-critical value for a one tailed alpha level of 0.05. In other words, a proportion of 0.05 is in one tail, and that's with 12 degrees of freedom. Let's bring up the T table. We have 12 degrees of freedom with 5% or 0.05 in one tail. So, that's 1.782. Let's do another one you have a sample of size 30. What are the T-critical values for a two-tailed test with alpha equals .05. So remember with a two-tailed test we have two critical regions, one in each tail. And since the T distribution is symmetrical our t critical value should be positive and negative what? Well, remember that the degrees of freedom is n minus one, so our degrees of freedom is 29, so if we bring up the t table, we look for degrees of freedom is 29, and we also know that if alpha equals 0.05 for a two tailed test. Then 0.025 is in each tail. So we look for where it says the tail probability is 0.025. At 29 degrees of freedom, we get 2.045 as our t critical value. That means that negative 2.045 is the t critical value that cuts off the lower 0.025. Good job. Let's do one more. You have a sample of size 24, and you get a t statistic of 2.45. So remember, this is just like the z score. So here's your t statistic. The area to the right of the t statistic is between what and what? Enter these values exactly as shown in the t table, and as precise as possible. This one might be kind of tricky but just remember what the t statistic means. And remember that the values in the column headers are the probabilities. So your t statistic is 2.45. And you also know that since the sample size is 24, the degrees of freedom is 23. So here's 23 degrees of freedom, and we'll look for 2.45. Since that's the t statistic we got. That's in between 2.177 and 2.5. The area to the right of this t critical value, 2.177, is 0.02, or 2%. The area to the right of 2.5 is 0.01, 1%. So that means that the area to the right of 2.45 is between 0.02 and 0.01. Not all t tables are the same. But as long as you understand how we got this, that's the important part. Just like with the z-test, when the t-statistic is far from 0 in either direction, in other words, if the sample mean is far from this population mean, we reject the null. The first thing we're going to do is compare the sample mean with the population mean. So in this case, t is the sample mean minus the population mean, mu naught, divided by the standard error. And instead of being stigma divided by root n, we use the sample standard deviation. Since the t-statistic depends on so many factors, let's takes some quizzes to make sure we understand how these factors affect the t-statistic, and therefore, our statistical decision. The larger or smaller the value of x bar, the stronger the evidence that the population mean that the sample comes from is greater than this population mean we're comparing it to. Similarly, the larger or smaller the value of x bar, the stronger the evidence that the population mean that x bar comes from is less than this certain value we are comparing it to. And finally, the further the value of x bar from mu naught in either direction, the stronger or weaker the evidence that mu is not equal to mu naught. If x-bar increases, then the difference between x-bar and mu 0 becomes larger. And therefore the t statistic becomes larger. That means the t statistic will be out here more, to the right. And then that's evidence that the mean of the population that x bar comes from is greater than this mean. If x bar decreases goes the opposite way, then the distance between these will also increase but in the opposite direction. x bar will then be less than this value whatever it is. That means the t statistic will be way out here in the left. And then that's evidence that mu is less than mu 0. And finally the further the value of x bar from mu 0 in either direction. The stronger the evidence that they're not the same. So when we're comparing x bar to mu 0, we center the t distribution at mu 0. And then we find where x bar lies on this distribution. The further out x bar is, the more likely that it comes from a population that has a mean significantly different than mu 0. This statistic here is for a one sample t test. This is just like the z tests you learned in lesson nine. It's a one sample t test because we only have one sample, with mean x bar. For the first part of this lesson, we'll focus on one sample T tests and then later we'll focus on comparing two samples. We want to know if the population that this sample comes from is significantly different than the population with this mean. Therefore, the null hypothesis is that the population mean is equal to some specified value. Mu naught, and the alternative hypothesis can be either that mu is less than mu naught, greater than mu naught, or just not equal to mu naught. When we calculate the t statistic, the numerator is the difference between the sample mean and this value, where the sample mean is the point estimate for the population mean mu, and the denominator measures the amount of difference between the population mean and mu naught That we would expect by chance. Again we have our alpha levels which are written in the column headers of the t table. The t table tells us the t critical values that mark the cut off point for the alpha levels. If our t stastics is greater than these t critical values. Or if we get a negative T statistic that is less than the negative T critical value cutting off that alpha level, then we can reject the null. Now, what will increase the t statistic? All other factors held constant. Check all that apply. If this difference increases then that means that the whole t statistic will increase. If x increases that means that the denominator is increasing and then that will cause t to decrease. A larger n will result in a smaller standard error, in other words a decreased denominator. And that means that the t-statistic will increase. And if the whole standard error increases, then that means the t-statistic will decrease. Going backwards the larger the t-statistic, the higher it is on the t-distribution and the lower the probability of obtaining that t-statistic. And then the larger the difference between means that we would expect. This same concept applies to negative t statistics. Let's go through an example. In the last lessons you may have noticed our examples have been centered around social science. Like social networking, with the cloud example and your engagement in class. But in this lesson and the next we'll be using a lot of examples from the fields of biology and medicine. Since statistics is heavily used in those fields as well. I drew a finch here, and that's because this next example deals with the beak width of finches. Specifically, scientist have looked at finches to make hypotheses about evolution. By mapping a trait over time, like beak width. We can tell if the environment is selecting for a particular size of beak width. Maybe there's a particular beak size that better help this finch survive. But sometimes there are random fluctuation in beak width due to variation in the population. Some finches just have bigger beak widths than others. Therefore to determine whether there is a significant change in beak width within the population over time. We need to use statistics. If the mean beak width does significantly increase or decrease in size. This is referred to as Directional Selection. Then the environment allows the survival and reproduction of finches with bigger beak widths. Let's say we know that the average beak width of all finches is 6.07 millimeters. This includes finches that have been studied over the last few years. But now let's say we're studying a sample of finches from today. We want to know, do finches today have beak widths that are different in size? That means our null hypothesis is that, the population mean for finches today is equal to what they have been. And what should the alternative hypothesis be? That mu is less than 6.07, greater than 6.07, or not equal to 6.07. Since we just want to know if finches have different sized beak widths, but we're not concerned with, if the beaks are greater or less than what they were before, then our alternative hypothesis is this last option. The next thing we want, is the same size and degrees of freedom. Open the link below with the data for our sample, and enter these values. Good. So, you should have found that the sample size is 500, and the degrees of freedom then is 499. Next, we need to find the sample mean, and standard deviation. And remember when we find the standard deviation, we divide the sum of squares by n minus 1. And then we take the square root. Let's do this in the spread sheet. Remember that there are 500 values. So, here anywhere in the blank white cells, we'll write equals average A2:A501. We get that the average is about 6.47. And here we'll calculate each of the squared deviations equals A2 minus D2 squared. [SOUND]. Copy that. Paste it in column B. We'll take the sum of squared deviations. And then we'll divide that by n minus 1. That gives us the variance. And then we'll take the square root. And then we get our sample SD is about 0.4. Let's round these values to the nearest 100's for the rest of this example, now let's calculate the T statistic, remember that the T statistic is x bar or sample mean minus meu the population mean. And really we can compare x bar to anything, but we are choosing to compare it to the population mean to see if things have changed and then we divide by the standard error as divided by the square root of n. t is 6.47 minus 6.07 divided by 0.4 divided by the square root of 500, our sample size. If you use these rounded values, you get about 22.36. But it's also fine if you did it using the values you calculated in the spreadsheet which are more exact. In which case, you would get about 22.56. Now that we know our T statistic, do we reject H0 or fail to reject H0? In other words, do we accept the alternative hypothesis or do we accept the null? We'll reject h not, because our t statistic is huge. We know that this will fall well beyond the t-critical value no matter what significance level we choose, the smallest usually being 0.01. We reject h-not because the probability of getting this T statistic? In other words the probability of getting this sample mean from a population with mean six point zero seven is very, very small. That's the p value. It's the probability of getting this value if the null hypothesis is true. Since the probability is so small, then there must be something else going on. The null hypothesis is probably not true. Finches with this beak width come from an entirely new population. Not equal to 6.07 as the mean. When we do a one-tailed test, the p value is the probability above the t statistic if it's positive, and below the t statistic if it's negative. But for a two-tailed test, the p value is both the probability above that t statistic And the probablity less than the negative value of that T statistic. If we had gotten a negative T statistic to begin with it'd be the same thing. The T value would be the sum of the probablity below that T statistic and above the positive value of that T statistic. We reject the null, when the p-value is less than the alpha level. Let's say we have the following sample. Is this sample mean significantly different from 10 at an alpha level of 0.5? First, calculate the t-statistic. Remember to use the sample standard deviation for s. For x bar, or the sample mean, if we add them all up and divide by 8. We should get 12.625. And for the sample standard deviation, we take the square root of each value, minus the mean x bar, square each value. And sum them up, and then divide by n minus 1. So we should get about 7.596. When we use these values for the t statistic, we get 0.977 approximately. So now, for a 2 tailed test, which image represents the P-value where the P value is the red shaded region, this one, this one, or this one? If we're doing a 2 tailed test. Where we know we're doing a 2 tailed test because of our hypotheses. Then the p value is the probability above this t statistic. And below the negative version of that t statistic. So this image shows our p value. What is the P value? We can't use the T table. We can use the T table for an interval estimate for our P value but we can't use it to find the exact P value. Since our sample has 8 values, we have 7 degrees of freedom. And out T statistic is in between 0.896 and 1.119. That means the upper tail of our P value would be between 0.15 and 0.20. And so, at the lower tail of our p value which means that our total p value would be between 0.3 0.15 plus 0.15, and 0.4. 0.2 plus 0.2. But we don't need to be able to do it by hand or with a table. Because these days, we can use software. There's a website called graph pad. The link is below. And use that to calculate what this p value is, and you should find that it's between 0.3 and 0.4. Navigate to statistical distributions and interpreting p-values, and then based on the fact that we're doing a two-tailed test, and we know our t-test statistic, figure out the rest. What did you get for the two-tailed p value? Is this statistically significant, or not statistically significant? And that's because of which of these options. Therefore, what's our decision? Here's the Graphpad website. We want to calculate P from our T statistics. And again, we want to calculate P from T, so that's this first option. And here we got that our T statistic is 0.977. And our degrees of freedom is 7, because we had 8 values. Compute P, and indeed, we get that our P value is between 0.3 and 0.4, 0.3611. And the website even tells you that by conventional criteria, usually by alpha level equals 0.5. This difference is considered to be not statistically significant. We got 0.3611. This is not statistically significant and that's because the probability is greater than our alpha level. Remember that our alpha level is 0.05. Therefore, we fail to reject the null. We're sticking to the assumption that the population that our sample is from, is not significantly different from 10. Great job. Let's do another example. This is Santa Clara County in California. And let's say that the mean rent for apartments is 1,830 for all types of units. A certain rental company called Rental California owns and rents a lot of apartments. And they want to know if the amount they're renting it for is significantly different than this population mean. So they examine a random sample of their own units of size 25. They find that on average, they rent each unit out for 1,700. And the sample standard deviation was $200. You're going to conduct a one sample t-test for them. Here are the null and alternative hypothesis, and remember, they're concerned with whether or not it's significantly different. Even though they found that it's less, this will still be the alternative hypothesis. And they also want an alpha level of 0.05. First of all, find a t critical values for a two-tailed test at alpha equals 0.05. Put them here and here. You did this before, but I wanted you to do it again just for practice. Your sample size is 25. So there are 24 degrees of freedom. And then if the alpha level is 0.05. And we're doing a 2 tailed test. Then 0.025 is in each tail. Therefore, our t critical value is 2.064 and negative 2.064. Now, let's find the t statistic. Here, we're using M instead of x bar, but they're the same thing. So, go ahead and calculate what this should be. Our t statistic is 1700 minus 1830 divided by 200 divided by the square root of 25. Remember this is our standard error. And this comes out to negative 3.25. Based on our T statistic and our T critical value which do we choose the null or alternative hypothesis. Since our t statistic is out here in one of our critical regions. In this case the left critical region, then we reject the null in favor of the alternative. Rental California charges significantly less for rent than in Santa Clara County as a whole. One common measure of affect size, when comparing means, is Cohen's d. Named after the statistition Jacob Cohen. Cohen's d is a standardized mean difference that measures the distance between 2 means in standard deviation units. In other words, instead of dividing by standard error. We simply divide by the standard deviation of the sample. We can think of it like this, we have our sample, let's just say it's normally distributed. And here's the sample mean, and the standard deviation of our sample is S. Now let's say we have some population mean out here. How many S's fit between x-bar and the mean? The larger Cohen's d is, the further x-bar is from mu-not, in terms of the sample standard deviation. So, go ahead and calculate Cohen's d for this example. We get 1,700 minus 1,830 divided by 200. This is negative 0.65. Good job. In addition to statistically deciding if a sample comes from a new population. By excepting or rejecting the null. We want to calculate an interval, in which the population mean will probably lie. So in this case, we want an interval that most likely contains the true average rent. For all rental California's units. We'll do essentially the same thing you did in lesson 8. But this time using the t statistic since we're using s instead of sigma. Now you've already found the t statistics for 2.5% in each tail. What confidence interval will this give us? Fill this in 2.5% plus 2.5% make a total of 5%, which means 95% is here in the middle. With these t statistics, we can calculate a 95% confidence interval. So, let's do that. Now we're using the t distribution, and our sample mean falls right here in the middle. We want to find the price that corresponds to a t statistic of negative 2.064 and positive 2.064. In other words, what value is 2.064 standard errors above 1700? Where one standard error is s divided by root n. Try and calculate the confidence interval, and this will be a good review of lesson eight as well. And remember, 1700 should be right in the middle of these two values. Let's just first find what this value should be. We have 2.064 standard errors below 1700. So this should be 1700 minus 2.064 standard errors, which is 200 divided by the square root of 25. This calculates to $1,617.44. And then we'll do the same thing with this value on the right of 1700. We have 1700 plus 2.064 standard errors, and this comes out to $1,782.56. And there we have our 95% confidence interval. Before we move on from one-sample t tests, we're going to take two little quizzes dealing with the margin of error. Remember that the margin of error is one-half the width of the confidence interval. So we had our sample mean, and we added the margin of error to get our upper bound. And subtracted the margin of error to get our lower bound. Now which of the following is the margin of error, where t is the t critical value? Recall that we subtracted the number of standard errors away from the mean, and that was the margin of error. Therefore, this is the correct answer choice, where t is the number of standard errors, negative 2.064. And s divided by root n is the standard error. What happens if we increase the sample size to 100? What's our new margin of error for a 95% confidence interval? You're going to have to use the T table. If our sample size is 100, that means we now have 99 degrees of freedom instead of 24. And if we look at our T table, we see that the closest degrees of freedom to 99 is 100. So we'll just use that one. And again we want .025 in each tail. Notice that it also says at the bottom Confidence level C. And here we have 95%. This is the same as how at the top of the column, it has .025% in either tail. That's equivalent to a 95% confidence interval, pretty cool. And we see that the t critical value is 1.984 and negative 1.984. Therefore, the margin of error is 1.984 times s, divided by root n. And this is 39.68. Notice that now the margin of error got a lot smaller than it was before with a sample size of 25. Before the margin of error was 2.064 times 200 divided by the square root of 25, which was 82.56. So when we increase the sample size, we decrease our margin of error, and we're more precise. And remember, when we increase the sample size, we also have more degrees of freedom and therefore, our t distribution goes from wider to skinnier as it approaches normality. Now that you know how to statistically decide, whether a sample is different from a population parameter, the rest of this lesson will be easy. We're going to use these same concepts to compare two samples that are dependent. They're dependent when the same subject takes the test twice. This is a Within subject design. One example, is when each subject, is assigned two conditions in random order. For example, they're in the control condition, and then they get a treatment. Or maybe there are two kinds of treatments that they're trying to test. Another example is if every subject is given a pretest, and then a post-test. And another example is growth over time, otherwise, known as a longitudinal study. And then we'll measure the variable for each subject at one point in time, and then again at another point in time. When we have these within subject designs, we have paired data. This column of xi's would correspond to the variable measurements of one condition or of the pretest, or of some point in time. And then the y values would correspond to a second treatment, or a post-test, or a measurement at another point in time. And then, what we want to do is look at the difference between these values. We'll call this D sub i. And that's just the x values minus the y values. Or it could be the opposite. We're really just concerned with the absolute difference. Now, we follow the same procedure as we did before with the one sample t-test, except we used the values of D. Let's go through an example of a repeated measures design, where every participant gets both treatments. These are two cell phones with different keyboard configurations. Let's say a researcher is interested in the effects of these cell phone configurations on errors made in typing text messages. To study this, 25 participants used each keyboard type. This is QWERTY since it spells Q-W-E-R-T-Y in the upper left and an alphabetical keyboard. And they typed a standardized 20 word text message within 30 seconds. The number of errors for each person using each type of keyboard was recorded. These participants were randomly assigned to which type they used first. In the Google spreadsheet below, you'll find the data for the number of errors each person made using each type of keyboard. Find the mean number of errors for the QWERTY keyboard and for the alphabetical keyboard. Here's our data, and you'll see that the sample size is 25. It says 26 here because we have a header. So let's find the average of the QWERTY errors, equals average A2 to A26, and we get 5.08. And then if we find the average of the errors made on that alphabetical keyboard, we get 7.8. We want to see if there's a significant difference between the number of errors made on each keyboard. Our null hypothesis is that there is no significant difference. The population of errors from this keyboard, if it were extended to everybody, not just the sample, equals the population of errors using this keyboard. And our alternative hypothesis, is that they're not equal. Notice that this is equivalent to saying that mu sub Q minus mu sub A equals 0. If we just subtract mu sub A on both sides of this equation, which basically says that the difference is equivalent to about 0. What's our point estimate for mu sub q minus mu sub a? In other words, the difference between the population of errors? Since our point estimates are based on our samples, our point estimate for the difference will be 5.08 minus 7.8 which is negative 2.72. But we still want to know how this compares to other differences. And so we need to find the standard error of the differences. Now we need to find what s is? And in this case we're finding the standard deviation of each difference. So bring up your spread sheet and in this column calculate the difference. And then find the standard deviation using the denominator n minus 1. And then write what you get for s. Let's put D as the header in this column. D for difference. If you figured this out yourself, great job. We'll put equals A2 minus B2. And there we have our difference. Then we'll copy this cell, 'cuz we'll copy the formula. Go to cell C26. Highlight the whole column C, and then paste. And we get all our differences. Let's calculate the average difference. We get the same thing we got for our point estimate of the differences. Now let's calculate the standard deviation of these. So we have each value, minus the mean. So remember we're subtracting a negative number, and then square it. So these are our squared deviations from the mean. So we'll sum them up and then divide by n minus 1, which is 24. Finally, we'll take the square root. We get about 3.69. If the sample standard deviation of the differences is about 3.69, then calculate the t statistic. You already found the numerator We divide the numerator by 3.69 divided by the square root of 25. This turns out to be about negative 3.69. Funny that, that turns out to just be the negative value of s. But that usually won't happen. Now, is there a significant difference between these two population means? For an alpha level of 0.05, what are the t critical values? If you have this memorized, then you should be able to do this really fast. For an alpha level of 0.05 with 0.025 in each tail, the t critical values are positive and negative 2.064. Now that we have our t statistic and we know our t critical values, do we fail to reject the null or do we reject the null? Since our t statistic is past this critical value, out here in the critical region, we'll reject the null. This means that participants made significantly fewer errors, using the QWERTY keyboard compared to the alphabetical keyboard. Since this was an experimental design, we can make causal statements. So we can say that they type of keyboard had a causal effect on the number of errors. Let's now calculate the effect size measure Cohen's d. Just like before with the one sample t test. Cohen's d is the mean difference divided by the standard deviation for the differences. Remember that the mean differences was negative 2.72. And the standard deviation was 3.69. Calculate Cohen's d? Just like before with the one sample t test. Cohen's D is about negative 0.74. Before we move on from this example, let's calculate a confidence interval for the mean population difference. This is exactly the same as what you've learned before except this time we use the mean difference. I won't go through it step by step this time. Try and figure it out on your own. Our mean difference as you found earlier is negative 2.72. For the lower bound we'll subtract the T critical value times the standard error, we found before that the T critical value is 2.064 when there are 24 degrees of freedom. And then we'll multiply this by S the standard deviation of the differences. divide by the square root of 25. This gives us about negative 4.24, then we'll do the same for the upper bound of this confidence interval. Negative 2.72 plus 2.064 times 3.69 divided by the square root of 25. If we round to the hundreths place, this gives us negative 1.20. We can conclude then, by saying that on average, users will make around four to one fewer errors on the QWERTY keyboard than on the alphabeticle keyboard. I want to make one more note before finishing with this lesson. Remember how I said before that we could write this as muss of Q minus muss of A equals 0 for the hypothesis test. And likewise for the alternative hypothesis we can say that the difference between them is not equal to 0. Instead of writing this as a difference. We can just write muss of d, which means the difference between these two population means. The null assumes that this difference will be 0, and the alternative assumes that they'll not be 0. Then instead of the numerator we had before, we'll have the difference between the sample means. And then subtract 0. We're testing to see if this difference between the sample means, is significantly different from 0. However, we could see if this difference between the sample means, is significantly different from any value, not just 0. Maybe we want to test if the difference between sample means is 6. Usually though, we're looking to see if the difference is different from 0. You'll get more practice with this notation in the problem sets. Great job. You just saw one type of dependent samples t-test, where we test out and measure samples from two different populations. In the case of the last example, the populations were the number of errors made typing the 20-word text message within 30 seconds on each keyboard configuration. This is called the repeated measures design. And the null hypothesis, is that these two population means will be the same. Before we finish this lesson, I'll tell you about two other types of dependent samples t-tests. One is a longitudinal design, where we measure a variable at one point in time and then we measure the same variable for the same sample at a later point in time. And we want to know if there's a significant difference between the variable at time 1 and time 2. So the null hypothesis will be exactly the same symbolically as for repeated measures design. Only with this design we're putting a lot of time in between measuring the variables. And then, a third type of dependent sample's design is a pretest posttest, where we measure a variable, implement some kind of treatment, and then measure the same variable for the same sample after the treatment to see if there was a significant effect due to the treatment. Again, the null hypothesis is the same, that the variable won't significantly change before the treatment and after the treatment. You'll see examples of these two types of designs in problem set 10. Great job. T-tests are pretty complicated, so now dive deeper with Dr. Laraway. He's going to go into more detail on these concepts and then go through a full one sample t-test so you can get a lot of practice and really understand how to use these. One important aspect of any research study is something called the Effect Size. In experimental studies, or studies with a treatment variable, effect size refers to the size of a treatment effect. Makes intuitive sense. In non experimental studies. The term effect size may refer to the strength of the relationship between our variables. In the Z-test or in the t-test, the simplest measure effect size is the mean difference which is simply the difference between the two means. In the Z-test or the one sample t-test, the mean difference is x bar minus mu. Mean differences are great when we have variables with easy to understand meanings. In other words, those that we can understand without any specialized training. So, here's a little quiz. Which variable does not have an everyday meaning? The number of correct items on an exam, the number of minutes to fix a flat tire, score on the Laraway personality test, or the number of hours slept. In my opinion, score on the Laraway personality test does not have an everyday meaning. In other words, you don't necessarily know what the scores on this test mean without specialized training. By the way, I just made this test up. It doesn't really exist. Notice the rest of these, number of correct items on an exam, that has an everyday meaning. We all know what that means. Same with the number of minutes to fix a flat tire. And the number of hours slept. So in cases where we don't understand what the variable is telling us. A mean difference may not be the most informative measures. And for that reason, statisticians have developed other effect size measures, that we will now discuss. Now are there many types of effect size measures? I can think of at least six or seven right now. But most of them fall under two main groups. Two families, if you will. The first we call difference measures. The easiest difference measure is one we've already talked about, the mean difference. The next type are called standardized differences. And there are several different types of standardized differences measures. But we'll focus on one, and that measure's called Cohen's d. This is the standardized difference measure we're going to use throughout the rest of this course. Notice that Cohen's d as a standardized difference measure actually tells us about the mean difference. It does so in standard deviation units. The next family are called correlation measures. There are again, several different correlation measures, as with standardized mean differences. We're going to focus on one, which is called r squared. R squared tells us to proportion or percentage a variation in one variable. In other words, change in one variable, that is related to another variable, and sometimes, we say instead of related to, we say explained by. What we mean, explained by, is that we can explain a certain percentage or proportion of variation or change in one variable by knowing the value of the other variable. Now I'd like to talk about a topic that is sometimes confusing for students, and that is the topic of statistical significance. Statistical significance in the area of statistics has a very specialized meaning. It's kind of counterintuitive. It doesn't really fit what we normally think of in terms of significance. So, in everyday life, we use the word significant to talk about things that are important to us, things that are large or sizable, things that are meaningful. In statistics, the phrase statistical significance has none of these meanings. I'm going to write on the other side what significance does mean. Significance does mean that we've rejected the null hypothesis. Significance also means that our results are not likely due to chance or sampling error. Notice that this and this are just two different ways of saying the same thing. The important point is to know when you see the words statistical significance, it doesn't mean that our results were important. That they were large or sizable or that they were even meaningful. All it means is that the results are probably not due to chance. In other words, we've ruled random chance at or sampling error as an explanation of our results. That's an important point to keep in mind. So how do we know if the results of a research study were meaningful or important? One thing we have to know is what was measured? What were the variables? Did the variables have practical or social importance or even theoretical importance? Obviously, if our variables do not have any importance in terms of practical application, social issues, or theory, then clearly, it doesn't matter what the results were, the results are not important. Another thing to consider in assessing the meaningfulness of results is the effect size. How large were the results? That doesn't meant that small results are not important or that large results are necessarily important. But effect size is considered when assessing the meaningfulness of results. And it's possible for even very small effect sizes to be very important. A final thing to consider is can we rule out random chance for the results? In other words, can we rule out sampling error as an explanation? Now, this doesn't guarantee that our results are important, but it goes along way to helping us asses the rest of the import. Next, we need to know can we rule out alternative explanations for the results? These are those dreaded lurking variables we've been talking about before. So putting this all together, this gives you a rough guideline of assessing whether the results of a study are meaningful. What was measured, how large was the effect, could we rule out random chance as an explanation, and can we rule out lurking variables as an explanation for the results? We've already talked a little bit about Cohen's d named after the statistician Jacob Cohen. Cohen's d is an effect size measure, which gives us the standardized mean difference. Lets break this apart. We already know what it a mean difference is. Mean difference is simply the difference between two means. For example, between X bar and mu, as in the one-sample z-test and the one-sample t-test. The word standardized, you may recall from our discussion of z-scores. Remember, when we called z-scores standardized scores, what that meant was we were taking a difference between a raw score and the mean, and dividing by the standard deviation. Cohen's d does something very similar. It takes a difference in this case, in the one sample case between X bar and Mu and it divides by the standard deviation of the sample. So, it's called the standardized mean difference because we have a mean difference and then it's turned into standard deviation unit. So, the interpretation of Cohen's d is how far apart these means are in standard deviation units. Okay. We're going to talk a little bit more about correlation measures. We've already mentioned r squared. Remember, r squared gives us the strength of the relationship between two variables. Sometimes r squared is called the coefficient of determination. Because r squared is a proportion, it ranges from 0 to 1. And our r squared value of 0 means that the variables are not at all related. And our r squared of 1 means they are perfectly related. This rarely ever happens in everyday life. It's hard to imagine variables that are perfectly related. When conducting the t test, the formula for r squared can be derived from the information from our t test. The formula for our r squared is t squared over t squared plus our degrees of freedom. The t here is not the critical value, it's the value we get from the t test itself. In computing r squared, all we have to do is compute our t value from the formula, square it, put that in the numerator, square it again, add our degrees of freedom and then divide. Lets compute r squared. Here's some practice examples. So if t equals two and we have 20 degrees of freedom, go ahead and compute r squared. Okay, if we take our t value and square it and put that in the numerator, that is 4. If we take t and square it again, that is a 4 again. And we add our degrees of freedom we should get 4 over 4 plus 20. That gives us 4 over 24. And if we divide 24 into 4 and round to two decimal places, that gives us 0.17 or 17%. Let's try another couple of examples. 4 squared is 16. That goes in the numerator. Put 16 again into the denominator, and we're going to add 10 to that. That gives us 16 over 26. And if we divide 16 by 26 and round it two decimals, that gives us 0.62 or 62%. Third example, well, 1 squared is 1. That goes in the numerator. Put that again in the denominator. We're to add 9 to that. That gives us 1 over 10, which is .10, which is 10%. So, in each of these examples, we see how to get r squared. And I convert it from a proportion to a percentage and now let's now interpret that. For number one, we'd say that we can explain 17% of the variation in our deep ended variable by knowing our other variable. Here we can explain 62%. Here we can explain 10%. Obviously we're explaining more variation in this example than the other two. Whenever we conduct a research study and compute statistical results, we report those results in something called a Results Section. So, I'm going to outline a little bit what goes into result sections. The first thing we put in our result sections are descriptive statistics, for example the mean in standard deviation. The descriptive statistics represent the heart of our study, this tells us exactly what happened in out study and should always be reported. And we can report those in three ways. We can report them in text. In other words, we put them right in, in between our words in our results section. We can put them in graphs, such as a bar graph. Or we can put them in tables. Next, we'll want to report our inferential statistics. And this is either a hypothesis test, or a confidence interval, or sometimes both. So when reporting a hypothesis test, you want to give the reader the following pieces of information. You want to tell the reader, what kind of test did you conduct? For example, a one sample t test. Next, you want to give the actual value of the test statistic. For example, the value of t. Then, you provide the degrees of freedom of the test, not the formula, but the actual number that comes from the formula. You also want to give the reader the p-value, specifically the exact p-value, if possible. Next, if applicable, give the direction of the test. For example, is it a one tail test, or a two tail test? And always provide the alpha level. Always let the reader know what alpha level you used, to make your decision. For many disciplines, we use APA style to report the results of our hypothesis test. APA stands for American Psychological Association. The APA has a whole guide on writing research papers and one of the parts of that guide, tells us how to write inferential statistical results. I'm going to give you a brief introduction to APA style for statistics, as it relates to the T-test. So, here's a generic outline of how to present the results of a t test in APA style. First, we put the test statistic name, in this case it's t. Next to that, we put the degrees of freedom in parentheses. We put an equal sign, then we provide the t value, rounded to two decimal places with a comma, then we give the p value. Again, rounded to two decimal places. There are some exceptions in this rule, but for the most part, this is how you do it. And then, we put out the comma and we tell the direction of the test whether it's one tailed or two tailed. I'll give you a more concrete example in a little bit. Imagine the situation in which we have a t value with 24 degrees of freedom equals negative 2.50. If we're doing it by hand we only know whether the p value is less than or greater than 0.05, and we have one tailed test. This is how you'd write it out. If we used a computer to get these results, we'd end up with an exact p-value. An exact one tailed p-value for this situation rounded to two decimal places is 0.01. So, in that case, we can actually put p equals 0.01. In reporting confidence intervals, we need the following pieces of information. We need to know the confidence level. For example 95%. We need to know the lower limit. And we need to know the upper limit. And of course, make sure you identify what the confidence interval is for. In other words, what are we doing with the confidence interval. What is it telling us? Is it a confidence interval on a single mean? Or a confidence interval on the difference between two means, for example? Be clear. Now I'm going to show you a little bit of APA style for confidence intervals. Usually you put them in parentheses. But we often start by saying what kind of interval is this? So, it may some, say something like, here's the confidence interval on the mean difference and we present it this way. We'd state the level of confidence. Say that we're using a confidence interval which we can abbreviate CI, equals, and then in parentheses the lower limit. Comma upper limit. An alternative way of presenting this is as follows. We could put 4, with a dash 6. Another alternative way is actually writing the word to in there to. Finally, we wanted to present our effect size measures. And again, this could be Cohen's d, r squared or others or both. In terms of APA style, simply write d equals values rounded to 2 decimals or r squared equals, decimal place, and the values rounded to two decimals. One thing to notice, is in APA style, we don't use a leading zero for situations in which we have proportions. If the value cannot be greater than 1, we don't need a zero in this spot right here. So for r squared, because it's a proportion, we cannot have a value greater than one. So we'd write it just like this. So for example if r squared were 0.17, you would just write it 0.17 with no zero here. Cohen's d can be greater than one. So if we had a Cohen's d of let's say, 0.25, we would write d equals 0.25. Now we're going to do a full 1 sample t test from start to finish. And that includes the t test, the confidence interval, Cohen's d and r squared. We're going to do the whole thing, just like we would in a real research study. Before we do, it's important for you to gather some important information and some important tools. I'm going to list these tools and the information right here, be sure you get them. You'll need some paper and some pencils. You'll also need a calculator that has a square root and an exponent button. You're going to need the following formulas, so right them down on a separate sheet of paper and keep them handy throughout this example. You're going to need to do degrees of freedom, n minus 1. You'll also need the standard error of the mean, which is a sample standard deviation divided by the square root of the sample size. You're going to need the formula for the one sample t test and recall that is the sample mean minus the population mean divided by the standard error of the mean. The formula for the confidence interval is X bar plus or minus the margin of error. Remember the formula for the margin of error is t critical times the standard error of the mean. You will also need Cohen's d, which is the mean difference divided by the standard deviation for the sample. And finally, r squared, which is t squared over t squared plus degrees of freedom. Again this is not t critical, this is the t that comes from the t test. When working through this problem, here's some important points to note. One, write down all of your work as you do it. Two, write down and label all the values, every intermediate step. You're going to need the values throughout the example. And throughout, round every answer to two decimal places. Even when you're working through a problem, if we're doing these problems for real. In other words, these are actual data from an actual research study. We wouldn't round to two decimal places throughout, because that adds a rounding error. But just for the sake of you understanding how to work these problems and interpret them, two decimal places is fine. Here's the start of our problem. According to the Gallup organization, US families reported spending an average of $151 per week on food in 2012. Now, although, this data came from a sample, we're going to pretend that this perfectly represents the whole population. So we're just going to assume that this is mu. Make sure you write that down, mu equals 151. Now, imagine that there's a food cooperative. We'll call them Food Now, and they want to reduce their cost of food for their members. To do so, they implement some cost-saving programs. This could include things like, buying from local growers, and so on. So, here's a little quiz, to see if you are following. What is the dependant variable? What are we measuring? Is it the type of food purchased? Such as fast food verses vegetables? Is the amount of money spent per week on food? Is it the members of the Co-op themselves? Or is it the cost saving program? What do you think? The correct is the amount of money spent per week on food. That's what we're hoping to change. That is our dependent variable. What is the treatment from this same list you saw before? Is it the type of food purchased? We know it's not the amount of money spent per week on food, because we've already identified that as the dependent variable. So is it the members that co-op, or the cost saving program? Correct answer is the cost-saving program. That's what we're implementing to try to reduce the cost of food. So the two main aspects of interest in this study is the amount spent per week on food and the cost-saving program. Now I'd like you to choose the best null hypothesis from the following options. The program reduced the cost of food, the program did not change the cost of food, or the program increased the cost of food. What do you think? The correct answer is the program did not change the cost of food. Remember, the null hypothesis always states that there's no change, no difference, and so on. If we have treatments, the null hypothesis says, the treatment doesn't work. Okay, from that same list, I'd like you to chose the best alternative hypothesis. Which of these following hypothesis best reflects what we're hoping to discover in this study? Is it the program reduced the cost of food, the program did not change the cost of food, or the program increased the cost of food? The correct answer is, the program reduced the cost of food. That's what we hoped to demonstrate. Remember, alternative hypotheses pretty much always tell us what we're hoping to find in our research study. There's some minor exceptions, but for our purposes, that's generally true. Let's write the null and alternative hypothesis in statistical symbols. For the null, we use a capital H with a subscript 0 to indicate the null hypothesis. We put a colon to show that what comes after is the null hypothesis. We use mu subscript program to indicate what the average amount spent per week on food would be if we could implement this whole program throughout the entire co-op. What we're saying here is that if we implemented this new program, the average amount of money spent per week on food by the co-op members would be either equal to 151, the original population mean, or greater. In other words, there will be no improvement. Now, let's write the alternative. We have some H, the subscript A for alternative hypothesis. I'll write alt over here to remind us that that's what it is. We have a colon in the right place. We input the value of the population parameter. If we could go to all co-op members and give them this mu program, what would their average cost be per week? And we hope there will be a reduction, so we'll write less than 151. Be sure to write these down on your sheet of paper. Based on the alternative hypothesis, what type of test should we conduct? Choose the best option from the following list. Should we do a one-tailed test in the positive direction? A one-tailed test in the negative direction? A two-tailed test, or a three-tailed test? The correct answer is a one tail test in the negative direction. This is because, when we compute our T test, we expect a negative value, in other words, we expect x bar to be smaller than mu. So, here's where our critical region would be. Notice, we wouldn't do a two tail test because our alternative hypothesis says we're only looking for a negative difference. Which is also why we wouldn't do a one tail test in the positive direction and a 3 tail test, I just made that up. There's no such thing. So imagine if a co-op takes a random sample of its members of 25. Go ahead and compute the degrees of freedom if n equals 25. The answer is 24, because the degrees of freedom for one sample t test is n minus 1. So 25 minus 1 equals 24. Okay, now I'd like you to find tcritical. That is, the critical value of t if alpha equals 0.05. Don't forget the other two pieces of information you need to know, the direction of the test and degrees of freedom. Make sure you have three decimal places. The correct answer is negative 1.711. Negative 1.711 is the value that cuts off the lower 5% of the T distribution with 24 degrees of freedom. If the sample standard deviation is equal to $50, go ahead and compute the standard error of the mean. And again, round it two decimal places. Correct answer is $10. Recall that the formula for the standard error of the mean is the sample standard deviation divided by the square root of n. So here's how we calculate that. 50 divided by the square root of 25 is 50 over 5. 50 divided by 5 is 10. This $10 tells us that we expect sample means, to differ from the true population mean by $10, on average. So, remembering the sampling distribution of the mean, the mean of that distribution is equal to the population mean mu, which is 151. We expect most sample means to fall one state of deviation above and one state of deviation below. So we expect most sample means to differ from mu by $10 on average. So most sample means should fall between 141 and 161. Now, imagine that the co-op found, after their program, a mean of 126 for their sample. Given that, go and compute the mean difference. That is, the difference between the sample mean and the population mean. Correct answer is negative 25. So recall the mean difference, sample mean minus population mean, which is 126 minus 151 gives us negative 25. Okay, we have all the information now to compute the t statistic. Go ahead and do so and round at two decimals. The correct answer is negative 2.5. Call the formula for the t statistic x bar minus mu, divided by the standard error of the mean. So we have 126 minus 151 divided by 10, gives us negative 25 divided by 10, equals negative 2.5. Okay. For this next question, draw the t distribution. Do that first. Next, place t critical on the distribution. Put it in its proper location. Shade in the critical region. Finally, place t on the t distribution. Make sure you have all four of these completed before we move to the next slide. Does t fall in the critical region? Yes or no? The answer is yes. You should have had something that looked like this. Draw the t distribution. Place in the critical value. Shade in the critical region, and put the t value on this distribution. We know that negative 1.711 is right here. We know that negative 2.5 must be below it, which means it's in the critical region. Now I'd like you to identify the p value for this t. Is it p greater than 0.05, or is it p less than 0.05? What do you think? The answer is less than 0.05. Here's how we know that. Remember, we did a 5% test. Alpha equals 0.05. That means that this area beyond here. Is 5% of the distribution. Remember, the critical value cuts off the lower 5% of the distribution. The p value is this area, right here, cut off by our value of t. So remember, a p value is just a proportion of scores that fall beyond a score. So in this case, the one-tailed p value has to smaller than 0.5, because this small chunk here is smaller than this chunk right here. So, p has to be less than 0.5. Now again, when we're doing these by hand, we don't know the exact p value But we can use a calculator to look it up. I'll do that right now for you. The exact one tailed p value is 0.00985. You can clearly see that this is much less than 0.05. So p is less than 0.05. We would probably present this p value if we were writing this out in the result section. As p is equal to 0.01. That's because we round to two decimal places. So, are these results statistically significant? Yes, or no? The answer is yes. They are statistically significant. We know that because our p value was less than 0.05, which is the same thing as saying, we rejected the null hypothesis. Are the results meaningful, given the entire situation? Yes? No? Or maybe it depends? What do you think? The answer is it depends. For some people, saving $25 per week is trivial, for other people, saving $25 a week is very important. It all depends on your income. So, in the context of this situation, we are not sure, it all depends on how much money the people at the co-op make. Next I'd like you to compute Cohen's d, round to two decimals. The correct answer is 0.50. Remember, Cohen's d is given by this formula. So we take our sample mean, subtract the population mean, divide by the center deviation for the sample. So we get negative 25 over 50, which gives us negative 0.5. This tells us that these two means are half a standard deviation apart. Go and compute r squared, round to two decimal places. Correct answer is 0.21 or 21%. Remember that R squared is given by this formula. So for our purposes, remember that t was 2.5 negative. So we take our negative 2.5. We're going to square it here. We'll square it here, and then add the 24, and then divide. So we end up with 6.25 divided by 30.25, which gives us 0.21, again rounded to two decimal places. This tells us that the differences in food prices for the sample of 25 people, 21% of those differences are due to the cost saving program. Now please compute the margin of error for 95% confidence interval. Remember to use the two-tailed critical value with 24 degrees of freedom and alpha equaled to 0.05. The correct answer is 20.64. Remember the margin of error is given by t critcial times the standard error of the mean. tcritical in this case was 2.064, our standard error of the mean was 10. Multiply those together and we get 20.64. We have all the information we need to compute the 95% confidence interval for the mean. Please put the lower limit here and the upper limit here. Round at two decimal places. The lower limit is 105.36. The upper limit is 146.64. Remember, we got that by taking our sample mean, X bar, and adding and subtracting the margin of error. In this case, 126 plus, minus 20.64. What this means is that if the co-op implemented this cost saving program for all of it's members, they would likely pay on average between $105.36 and $146.64. If we want to round that to whole dollars. It's just to make things easier. We would say that they would spend on average between $105 and $147. That's our confidence interval. Welcome to Lesson 11. In Lesson 10, you learned about dependent samples or repeated measures. Just to refresh your memory, that could be where we give the same person two different conditions to see how they react to each one. Maybe a control and then a treatment, or maybe two types of treatments. Or, this could be longitudinal, where we measure some variable at some point in time, and then again at another and see if the variable changes. Or this could be a pre-test and post-test. What was the measurement of a variable before and after a treatment? These are just a few situations in which we would use dependent samples. This type of research design is really useful because it controls for indivdual differences. In other words, if we gave someone some kind of treatment, then those same individual differences will be present the next time we give that same treatment. That way we can see how two different treatments play out under the same conditions. Because there's controls for individual differeneces, we could then use fewer subjects. And this is more cost effective, less time consuming, and generally less expensive. However, there are also a few disadvantages. One of which, is carry-over effects. For example, let's say we have this new method of teaching math. You want to know if it's going to be effective. If you use the same group of students to test this new teaching method, inevitably, they're going to be better at math the second time around. So, if the first time we teach them one way, and then the second lesson we teach them a different way, they'll already be better at math from learning it the first time. Then we don't know if the results after the second treatment are due to the fact that it was effective or due to the fact that they've learned math before. That's just one example. The second measurement can be affected by the first treatment. And the order in which we give the treatment might influence the results. For example, say we want to test two types of pills. What if the first pill has some kind of interaction with the second pill? And so, by taking it in that order they affect the results. Therefore, in this lesson, you're going to learn about independent samples. Whereas, dependent samples deals with within subject designs, independent samples deals with between subject designs. In this case, the advantages of dependent samples are the disadvantages of independent samples. And the disadvantages of dependent samples are the advantages of independent samples. Does that make sense? With independent samples, we need more subjects because we need to randomize the two groups taking the two treatments. We need a larger end to control for individual differences as best as possible. That means it's more time consuming and generally more expensive. But then the advantages of independent samples are that we don't have carry over effects. Therefore, we can give one treatment to one group, another treatment to another group, and not worry about one treatment effecting the other, because each person or each subject only gets one treatment. With independent samples, we can do an experimental test where we give treatments to the subjects. Or observational, where we simply observe characteristics of two different populations, and then compare them. Everything is exactly the same. The Null and Alternative Hypotheses, the t-statistic, and the way we make our statistical decision. However, the standard error changes because it's based on two sample sizes and two standard deviations. What is this new standard error? In problem set 10 number 10, we saw that if you subtract normally distributed data from other normally distributed data, we get a new data set. So, here I'm symbolizing normally distributed data with a mean of mhu sub one. And a standard deviation of sigma sub one. And we're subtracting normally distributed data with mhu mu sub two, sigma sub two. We get a new data set that has mean of just the difference between these two means. But then the standard deviation, is the sum of the variances, and then the square root. For samples it's roughly the same. The standard deviation of this new data set, is even bigger than that for each individual data set. So, we know that this standard deviation of the combined data sets is greater than the standard deviation of just one of the samples. because if we square both sides, we get this. And we know that this is true, because these are the same. And so, therefore if we add the variants of the second sample on this side, then this side will be bigger than just this. Similarly, the standard deviation of the combined data sets is bigger than that for the second sample. You can think about it intuitively, because if we have two different distributions and then we subtract this data from the other. There's going to be even more error than what exists in each individual distribution. Like we might subtract this data value out here, from this one here. Or we could subtract a data value down here, from a data value here. And then when we combine them we have a much wider distribution, and this is the standard deviation. Now again with independent samples, for analyzing the difference between those means, remember how before with the one standard T test, our standard error was S divided by root N? Well now we're using this for S, because it's based on two samples instead of one. So, we can replace this S with our new one. Then we can re-write this, since both the numerator and denominator are under the radical sign. We can say it's the same as this, and finally we can re-write it like this. Now what happens if our tow sample sizes are different? Then we simply have sample size one for the first treatment, and the sample size two for the second sample. This is our new standard error for independence samples. Before, with paired data or dependent samples, we could simply calculate the difference between each value for each subject. And then we could calculate the standard deviation. Therefore, before we just had that one S, but now we have two different samples and they can each be different sizes. This also changes our degrees of freedom, but it's easy to understand before we just subtracted 1 from the sample size, well now we subtract one from each sample size and then add them. Subtract 1 from n1 subtract 1 from n2 and them, and that's the same as adding the sample sizes. And then subtracting n2 and here's the degrees of freedom we'll use for the rest of this lesson A more conservative approach when we're not using software is to use the smaller of n1 minus 1 and n2 minus 1. In other words, the smaller of the two degrees of freedom from the two samples. But for now we'll just use n1 plus n2 minus 2, because that's what you'll use later. Let's go through an example. You and your friends want to go out to eat. But you don't want to pay a lot. There are two areas of town you're thinking about, Gettysburg and Wilma. You look online and find the average meal prices at 18 restaurants in Gettsyburg and 14 restaurants in Wilma. The link to these average prices at all these restaurants is at the bottom. You want to know if statistically there's a significant difference between the meal prices in each of these two areas of town. Therefore your null hypothesis is that they're not statistically different. And your alternative is that they are. This is a non-directional test. To compare these two samples what do you need to know? Should we know the population mean for food prices at all restaurants in Gettysburg and Wilma. The average of each sample. The population standard deviations. The size of each sample. Or finally the sample standard deviations. The nice thing about t tests is that we don't need to know population parameters, but we do need to know the sample averages, the size of each sample and their standard deviations. Let's calculate those. What is the average meal price for each sample of restaurants from Gettysburg and Wilma. Round your answer to the nearest hundredths place or two decimal places. Using the spreadsheet, we should get 8.94 for Gettysburg. And I denoted this sample mean by X bar sub G for Gettysburg, and 11.14 for Wilma. Note that these are in dollars. Now what's the sample standard deviation for each sample of restaurants? In the spreadsheet, one thing you can do is write equals stdev, and that's the spreadsheet's function for finding the sample standard deviation. And then you'll put your cell numbers here. So in the case of Gettysburg you would have a2: a19. But it's also really good to practice doing it step by step, rather than just using one function to figure it out instantly. If you round to the 100th place again, we would get the sample standard deviation for Gettysburg is 2.65 and 2.18 for Wilma. Now we can calculate the standard error. Note that since S sub G is the standard deviation, if we square it, we have the variance. So we have the variance of the meal prices at Gettysburg divided by the number in our sample from Gettysburg, plus the variance of our meal prices at Wilma divided by the number of restaurants in our sample from Wilma. For the standard error, we don't need the means. We just use the sample size and the sample standard deviation. If we do this and round to two decimal places, we get 0.85. Here's how I'm symbolizing the standard error, since it's the expected difference between the meal prices in Gettysburg and Wilma. But really we can symbolize it any way we want. You can even write se or sem. However you'll remember it best. Now that we have the standard error, we need to calculate the t-statistic. Which of these is correct for calculating the t statistic, or are both correct? Both are correct. These two are simply negatives of each other. If we go with this direction, where by direction I mean the direction of this subtraction, we'll get a negative t statistic, because x bar for Wilma is greater than x bar for Gettysburg. A negative of a negative is a positive. If we go in the opposite direction, we'll get the same thing only positive. Since we're doing a two tailed test it doesn't matter if our t-statistic is positive or negative. As long as it's in the tail and in the critical region then we'll reject the null. Go ahead and calculate the t-statistic both ways, just to show that they're the same, only with opposite signs. Let's go in this direction first. We'll have 8.94 minus 11.14 divided by 0.85. Since we're using rounded values, we won't get answers that are very precise. You may have got to negative 2.59 if you round to the hundreths place. But if we use precise answers without any rounded values, we should get negative 2.58. And then, here, we're just doing the opposite in the numerator. 11.14 minus 8.94 divided by the same thing. Which will give us positive 2.58. Now we have our t statistic. Positive and negative 2.58 are both acceptable t-statistics, since we're doing a two-tailed test. If we were doing a one-tailed test, then we would only be concerned with one direction. Now we need to compare this to the t-critical value. Use the t-table to find the critical values for a two-tailed test at alpha equals 0.05. Remember that the degrees of freedom is the sum of the sample sizes minus 2. Let's bring up the t table. The degrees of freedom is 14 plus 18 minus 2 which is 30. Since this is a two tailed test and our alpha level is 0.05, we'll have 0.025 in each tail. Therefore, at 30 degrees of freedom and 0.025, our t-critical values will be positive and negative 2.042. Now we have our t-statistic, and we have our t-critical values. So what's our statistical decision? Accept the null or reject the null? If these are our critical regions with the probability in each region, .025 to make a total alpha level of .05. And remember this is the t distribution. And this t value here is negative 2.042. And this t value is positive 2.042. 2.58 falls above 2.042. And if we had gone the opposite direction, negative 2.58 falls below negative 2.042. Therefore, we'll reject the null. It appears that meal prices in Wilma and Gettysburg are statistically different at a significance level of .05. If we calculate the p value, remember our t statistic is 2.58 and our degrees of freedom is 30. GraphPad tells us that the two-tailed p value is .015, and that is indeed less than our alpha level of .05. Great job. We're done with this example. Let's go through another example. A dermatologist has developed a drug to get rid of acne. Let's call their drug drug A. He tested it out on six people. After four weeks, the following proportion of acne had disappeared from these subjects. Faces, 40%, 36%, 20%, 32%, 45% and 28%. A competing dermatologist also developed a drug to get rid of acne, which we'll call drug B. She tested it on five people. After four weeks, the following proportions of acne had disappeared from these subjects faces. 41%, 39%, 18%, 23%, and 35%. Here's all the data. This time, I'm going to provide the means and standard deviations. But go ahead and calculate these if you feel like you need additional practice. Again, we'll do a two-tailed test to see if the effects of these two drugs are significantly different. That means, that the null hypothesis, is that the population of those who use drug A will not be significantly different from the population that you use B. And the alternative hypothesis is that they will be significantly different. First, calculate the t-statistic. Let's just ignore the percent signs for now, as long as we remember that our units are in percents. We have the sample mean of A minus the sample mean of B, divided by the standard error. And if you calculate it, you should get about 0.4. Now calculate the t-critical values fora two-tilled test at an alpha level of 0.05. Since this is two-tilled, the t-critical values will be positive and negative what? Remember that our degrees of freedom will be the sum of the two sample sizes, 6 plus 5, and then minus 2, because we subtract for this sample and one for this sample. We get our degrees of freedom is 9. Now, when we look at the t table, we see that our t critical value should be positive or negative 2.262. Based on our t-statistic and our t-critical values, do we accept or reject the null? Which do we choose, the null or alternative? Since our t-statistic is in the center of the distribution, and it does not fall in the critical region, as defined by our t-critical values, we fail to reject the null. In other words, we accept the null. The two drugs do not have a significant difference in their effects on acne. Who has more shoes, men or women? I asked this question to everyone at Udacity. If you'd like, you can take the poll yourself and give us more data to work with. We're going to do a t-test to see if men or women have more shoes. Our null hypothesis is that there's no difference. The mean number of shoes owned by females, is equal to the mean number of shoes owned by males. Or that the difference is 0. Which gender do you think has more shoes? That will determine our alternative hypothesis. One alternative hypothesis could be that females own less shoes than males. Or that if we subtract the number of shoes owned by males by the number of shoes owned by females. We get a negative number. Or maybe we think that females own more pairs of shoes than males, in which case the difference will be positive, if we go in this direction for the subtraction. And if we don't have a guess, then our alternative hypothesis will be that they're different. The difference is not significantly equal to 0. Let's go with this alternative. I know a lot of guys who have a lot of shoes. And I also know a lot of girls that have a lot of shoes. For now we'll just work with the data provided by my co-workers at Udacity. And let's pretend that they're representative of the entire population of men and women. Open the link and calculate the mean and standard deviation for the number of shoes owned by males and females. Here let's calculate the average number of shoes owned by females: =average (A2:A8). We get 33.14. Here let's calculate the average pairs of shoes owned by males. Looks like on guy owns a lot of shoes. We get 18. Here we'll calculate squared deviations for females. And in column D, square deviations for males. In cell c2, we'll put equals a2 minus dollar sign a dollar sign 10. So we're subtracting the mean. And then square it. This time let's just drag it down. Now we have all our square deviations. We'll take the sum of squares and divide by n minus 1, which is 6. There we have our variance. And if we take the square root of the variance We get the sample standard deviation. Now I'll do the same for males, and then we get that the standard deviation is 34.27. If you're very comfortable calculating this step by step, you know exactly what to do and you don't have to remember anything or review how to do it. Then feel free to calculate the standard deviation using the function stdev. If we write equals stdev, and then put the cells a2 to a8, then we get the same thing. And same here. Equals stdev, We want the standard deviation of the pairs of shoes owned by men. B2 colon B12. That's a good way to check your answers. But if you don't know how to do this step by step, then I really recommend to continue practicing to the point that it becomes second nature. because that'll help you remember the process and understand what's going on in the calculation of the standard deviation. Now, we have almost all the information we need to do the t test. Now you have what you need to know to calculate the standard error. Remember that the number of females was seven in our sample, and the number of males was 11. So remember what you learned before. Here's the standard error with two independent samples If you plug in what you know, you get 31.36 squared divided by 7 plus 34.27 squared divided by 11. And that comes about to about 15.72. You can also use the more exact values in your spreadsheet to do this calculation. Now that we have the standard error, lets calculate the t-statistic. Calculate the positive value. So lets do the average number of shoes owned by females minus that for males, and then divide it by the standard error. What do you get? So we should do 33.14 minus 18 divided by 15.72. Then our t-statistic comes out to about 0.96. Now calculate the t critical value so that we can conduct a t test at alpha equals 0.05. Remember that this is a two tailed test due to what we chose for our alternative hypothesis. Based on this t critical value and the t statistic you calculated do we accept the null or reject the null? Our degrees of freedom are the sample size minus one plus the other sample size minus one. Or we can just add them and then subtract two. So we get the degrees of freedom is 16 in this case. Let's bring up our t table. When the degrees of freedom is 16, and we have 0.025 in each tail, since we're doing a two-tailed test at alpha equals 0.05. Then we get that the t critical value is 2.12. Since our t statistic is less than the t critical value, that means it falls in the middle of the distribution. 0.96 is maybe about here, whereas the t critical value is out here. Therefore we'll accept Ho. Based on our samples, there's no significant deviance that we would predict between the number of pairs of shoes owned by males and females. Next let's calculate a 95% confidence interval for the true difference between pairs of shoes owned by males and females. Remember, this assumes that this sample is representative of the entire population. In the last few lessons, you've learned that we calculate the bounds of the confidence intervals by x bar, our sample mean, plus or minus the t or z-critical value times the standard error. Well, now we have two independent samples so things change slightly. What do you think each of these values will be in this case? More than one answer may be correct. Remember, we're trying to find a confidence interval for the true difference between the pairs of shoes owned by males and females. Since we're concerned with the difference, we wouldn't use one of our two samples means for our point estimate. We would use the difference between them. And it doesn't matter which direction we go. We can either use the positive or negative t critical value as well. 0.96 is our t statistic. We don't want that. We want the cutoff for the middle 95%. And finally, for our standard error, we calculated that to be 15.72. And the standard error is never negative. Good. Now let's calculate the 95% confidence interval. Let's go in the positive direction. So the pairs of shoes owned by females minus the pairs of shoes owned by males. That means we won't be using this value for point estimate. For the lower bound, we have our point estimate for the differences, 15.14, minus the t critical value. Before using the positive value, we'll subtract it. That's the same as adding the negative t critical value, if we were to use that one, times the standard error, 15.72. And we get about negative 18.19. Since we got a negative value for our lower bound, then it's possible that males own more shoes than females. That if we subtract them, we'll get a negative value. For our upper bound, we'll do the same thing, 15.14 plus 2.12. And note that that's the same as subtracting negative 2.12, if we were to use the negative t critical value, times 15.72. This comes out to 48.47. Good job. We going to wrap this lesson up with one last question. What proportion of the difference in pairs of shoes owned can be attributed to gender? In other words, you found that the difference between the pairs of shoes owned on average by our sample of males and females, was about 15. How much of this difference can be explained by the fact that some are males and some are female? This is found by r squared, which is the ratio of our t-statistic squared, divided by t squared plus the degrees of freedom. To do this, we use the t-statistic that you calculated already, .96 squared divided by .96 squared plus the degrees of freedom, which is 6 plus 10. This comes out to 0.054 approximately. This means that only about 5%, or 5.4% of the difference in pairs of shoes owned is due to gender. The other 95% is can be explained by something else. We're not sure what it is, we would have to do additional statistical tests to figure that out. But, for example, it could be individual personalities. Some people would rather spend more of their money on shoes, or maybe people who are more outgoing need more shoes. Who knows. The standard error we've been using assumes that the samples are approximately the same size. We must correct for the sample size first by pooling the variances. So now, we're going to slightly change the standard error that we've been using. The pooled variance, which we'll denote S sub p squared, because remember that S squared was the variance and S was the standard deviation, is the average of the two sample variances that corrects for different sample sizes. Remember when we had one sample that the variance was the sum of squares divided by the degrees of freedom, which written out is the sum of this squared deviations divided by n minus 1. With pooled variances, we're doing almost the same thing. When we have two samples, we'll add the sum of squares for each, and then divide by the sum of the degrees of freedom. Let's go through an example. Let's say we have two samples, which we'll call x and y. We want to know if the populations that these samples are from are significantly different. First, find the sum of squares for each. Remember that to do this you'll have to find each deviation from the mean, and then, square them, and then add them up. You're welcome to write each deviation and squared deviation in these columns, but you don't have to. We just provide the boxes there, so that that can help you as you calculate the sum of squares. But you can always use technology, Excel or R/g, or whatever you want to use. Or you can just do it by hand rather than typing them in. Whatever is easiest, as long as you get these sum of squares. The first thing you had to do which you probably did, is find the average of x and y. 5 plus 6 plus 1 plus negative 4 divided by 4 is 2. 3 plus 7 plus 8 divided by 3 is 6. So we have our means. And here are our deviations. Here are our squared deviations. Then when we sum them up, we get 62 and 14. Now that we have each sum of squares what's the pooled variance? To find the pooled variance. We add the sum of squares, and then divide by the total degrees of freedom. Which is the degrees of freedom from x plus the degrees of freedom from y. 3 plus 2 which is 5. We get 15.2. Now we can use this pooled variance, the same way we used each individual variance before. But now instead of each individual variance we use the pooled variance. Now find this corrected standard error. Our full variance is 15.2. So the standard error is 15.2 divided by n1, 4, plus 15.2 divided by n2. This comes out to about 2.98. Now lets calculate the t statistic, which is the observed mean difference minus the expected mean difference divided by the standard error. Remember that our expected difference is zero, but it can be anything. What's the t statistic? Our t statistic is the observed difference. X bar minus y bar, minus the expected difference, which is zero, divided by 2.98, our standard error. This come out to about negative 1.34. Now we need to compare this t-statistic to the t-critical value. Remember that to find this we'll need the degrees of freedom, which is the sum of the sample sizes minus 2. Another way to look at this is the degrees of freedom of one sample plus the degrees of freedom of the other sample. After you find the t-critical value, what's our statistical decision? When you calculate the t-critcal values, remember that it can either be positive or negative. Just put the positive value here and find the t-critical value for an alpha level of 0.05. The degrees of freedom are 4 plus 3 minus 2, which is 5. Alternatively, we can just add the degrees of freedom of each. 3 plus 2, and then we find that our t-critical value is positive and negative 2.571. This t-statistic, -1.34, is not in the critical region defined by these critical values. Therefore we'll accept the null. In general, we want to use the pooled variance since it corrects for the different sample sizes. In the problem set, you'll get more practice using the pooled variance. But the way the t statistic is calculated is essentially the same. We divide the observed difference between the mean of X and the mean of Y by the standard error, which is calculated slightly differently. Where these guys here are this pooled variance. Note that this is the t-statistic when we expect that the true populations that these samples come from are the same. In other words, that the difference between the population parameters is zero. But, say that we expect that the difference is equal to 10, for example. Then here, we would subtract this expected difference from the observed difference and this observed difference is what we get from our sample. So, this is our full t-statistic for independent samples. But in general in this class, our null hypothesis has been that there's no significant difference between the two populations. In other words, we expect that the difference between the parameters is about zero. There are a few assumptions to keep in mind when using the pooled variance as there are with any method of measurement. First of all, X and Y should be random samples from two independent populations. Remember when we looked at dependent samples. Data coming from the same subject twice. In this case, the population should be independent. The second, is that the populations that X and Y come from should be approximately normal. This is less important when the sample sizes are very large, greater than 30. A third assumption is that the sample data can be used to estimate the population variances and finally. The population variances should be roughly equal, allowing us to use this pooled variance as an estimate of both. You're comparing three brands of clothing. Let's call them Snapzi, Irisa, and LolaMoon. A random selection from each of these have the following prices. Are any of these brands more or less expensive than the others. Use your intuition to guess if there are significant differences in price between any two of these brands. Or, is there not a significant difference between any of the brands? You might have just looked at the data and seen that the prices for Snapzi are less than Irisa and LolaMoon, or you might have calculated the means. The mean shirt price for this sample from Snapzi brand is $13. For Irisa, the mean is 48, snd for LolaMoon, 45. It looks like the price for Snapzi is different from both Irisa and LolaMoon. But, Irisa and LolaMoon appear to have similar prices. We could do three t-tests to statistically show if there's a significant difference or not, but in this lesson, you'll learn a simpler method. In lesson 11, you learned how to test for a significant difference in means between two independent samples. But in many research studies, we want to compare means from more than two independent samples. If we have three samples, we would need to do three T tests, A and B, B and C, and A and C. But what if we have four samples. How many t-tests would you need to compare all of them to each other? You can figure out how many t-tests we need by brute force. We would need one between A and B, A and C, A and D, B and C, et cetera. Then, you see that we need six t-tests. As you can see, the number of t-tests we would have to do does not equal the number of samples we want to compare. We have to do even more t-tests. You don't have to know this but the way to figure out how many t-tests we would have to do with n samples, is found be choosing 2 out of n. This is the symbolic notation for that, which is equal to n factorial divided by two factorial, since 2 is here, times n minus two factorial. So, for example if we want to compare 10 samples, the number of t test we would do is 10 factorial divided by 2 factorial, times 8 factorial. Remember that factorial means 10 times 9 times 8 times 7, et cetera, all the way to one, divided by 2 times 1, for 2 factorial, times 8 factorial. These parts cancel out and we're left with 10 times 9 divided by 2, which is 45. We would have to do 45 t-tests just to compare 10 samples. We want something a lot simpler. We definitely don't want to conduct that many t-tests but we can use the same ideas underlying t-tests to compare three or more samples. Remember with t-tests, the decision of whether or not two samples are significantly different is a function of the distance that they are apart from each other. And the variability of each sample, which we call the standard error. Remember that we found this with the pooled variance. When we compare three or more samples, we can do almost the same thing. We have some kind of distance or variability between means in the numerator, and some kind of error in the denominator. Let's focus on the numerator for a sec. How can we compare three or more samples? Read each of these options carefully, and as a hint, think back to how you learned to calculate the standard deviation in lesson four. And the rationale for using the standard deviation as the measure of variability for a sample. Also, in this case, total mean is the mean of all values in all the samples. Remember that if we use the maximum distance between any two sample means, that's pretty much finding the range of all the sample means. And this doesn't give us a good measure of the variability of sample means. That's because we could add a bunch of samples to this data set whose means are within this range. And the variabiltiy then wouldn't change if we use the range as that measure. We want to account for all samples. You also found before that the average deviation adds to zero. So this can't work either. If we find the distance each sample mean is from each of the other sample means, then we might as well do a bunch of t tests. We'll have to do the same number of these calculations as t-test anyway. And t-test are a better measure of whether or not two samples are statistically different. We've reached our answer. Find the average square deviation of each sample mean from the total mean. Since we're only concerned with the variability between means. For now we're not concerned with the variablity between each sample. We're only looking at the mean of each sample. And rememeber, this is how you calculated the standard deviation. If we had a data set, and we find the mean. We found each squared deviation from the mean. And then in the case of a sample standard deviation, we divided by n minus 1. That's exactly what we're going to do when we find the variabiltiy between means. The average square deviation of each value in each sample from the total mean also includes the error caused by individual differences between subjects in each sample. And we don't want to include this error in our measure of between subjects variability. This is the total sum of squares. And we'll reference back to this later. For now, we're only concerned with the square deviations of each sample mean from the total mean. The mean of all values from all samples. This total mean is called the grand mean in statistical terms. And we're going to denote this by x bar sub g. Let's get a little more familiar with the Grand mean, and what that really means. Let's say we have two or more samples, and we find the mean of each sample. Will the mean of sample means in the case of four samples, the sum of the sample means divided by 4, be the same as the mean of all values? In each sample. Always, sometimes, or never. The answer is sometimes. When sample sizes are equal, in other words, there could be five values in each sample, or n values in each sample. The grand mean is the same as the mean of sample means. Since there's an equal weight to each sample in calculating the grand mean. However, when sample sizes are unequal. We have to find the grand mean by adding all the values from each sample and dividing by the total number of values, the sum of the sample sizes. When sample sizes are equal. The grand mean is the same as the mean of sample means. Since there is an equal weight to each sample. And this is the same as the means of all values. Where capital N is the total number of values in all the samples. But when the sample sizes are unequal. We can't use the mean of sample means. We have to add all the values in all the samples and divide by the total number, the sum of the sample sizes. But in this lesson we'll only work with samples of the same size, so it's fine to use the mean of means. Here's a quick explanation in symbols, if you're interested. Let's say we have three samples. X, Y, and Z. The mean of X is X bar. The sum of each value, divided by the number in that sample. The mean of Y is the sum of all the values in Y, divided by the number in Y. And likewise for sample Z. We want to know if the mean of means. Equals the total mean, the sum of all the values in x, y, and z, divided by the total number in each sample. Well, we know that because the mean of x is the sum of the x values divided by the total number. That the sum of x is just x bar times the number. So we can replace that in each of these. The sum of the x i's equals the average times the number in x. The sum of the y values, is the average of y times the number, et cetera. Well these are not the same if the number in each sample is different. But if there is the same number in each sample, let's just call it n. Then we can rewrite this as n times the sum of the sample means divided by 3n. The ends cancel out and then you get the same thing. So, that's why if the sample sizes are the same, which they will be throughout this lesson, we can just use the mean of means. But, just remember, for later in lesson 13, we'll work with different sample sizes. So to calculate our grand mean we'll have to add all the values and then divide by the total number of values. Getting back to our analysis of the variability between samples by looking at their means and comparing it to the grand mean, what conclusions can we draw after we find these deviations? The greater the distance between sample means, the less likely population means will differ significantly. Or is it the opposite? The smaller the distance between sample means, the less likely population means will differ significantly. If we have three sample means and their means are very close, then they're less likely to differ significantly. Similarly if the means are far apart from each other, then they're more likely to different significantly. This is called between group variability. And that's what we're trying to measure. You already found that we're interested in the grand mean and the average squared deviation of each sample mean from the grand mean. And we're going to keep building on these ideas. Let's continue with this extension of the t test. Now that we have gone a little in depth in the numerator, now let's focus on the denominator. Just like we analyzed the variability of the sample or samples, which we used in creating the standard error, we need to consider the variability of each individual sample when we are comparing three or more. Let's ease our way into this idea, starting with reflecting back on how the variability of individual samples impacts the difference in means. In both of these situations, these two samples have the same means and therefore, they have the same distance between means. Nonetheless in only one of these situations are the means significantly different from each other. Which one? This one. The means are significantly different, because you can see that their variability within each sample is less. The more variability each sample has with itself, the less significant the sample mean differences are. The same goes if we have three or more samples. Remember how we used a sample mean to construct a 95% confidence interval for the true population mean that the sample came from? In this case, the true population mean would most likely be somewhere in here in this interval. And the the true population mean from the red sample would probably be somewhere in here. It's very, very unlikely that the true population means will be the same. But in this case, when there's more variability within the samples, the population mean could be anywhere from here to here for this blue sample, or here to here for the red sample. Thus, there's this overlap here, and it's likely that the populations that these samples came from have the same mean. Now what does this say about the process of comparing three or more samples? Check all that apply. This first one is true. The greater the variability of each individual sample, the less likely a population means will differ significantly. That's less likely than this case. And similarly, the smaller the variability of each individual sample, the more likely they will differ significantly. The variability of individual samples is called Within group variability. So you see that when we compare samples, we're simply extending the idea of the t test. We can compare samples to each other by seeing how far each sample mean is from the mean of means, or the grand mean, and this is between group variability. But we also want to look at the variability of each sample, and that's within group variability. Because this impacts whether or not the samples are significantly different. Since we're analyzing variabilities, this process is called analysis of variance, shortened to ANOVA. ANOVA can compare as many means as we want with just one test. We say one way ANOVA when we have one independent variable sometimes called a factor. In this course we'll only work with one way ANOVA. Just like with the z and t statistics, we want one statistic that describes these variances in relation to each other, and determines whether or not we accept or reject the null hypothesis. First of all, what do you think the null and alternate hypotheses will be when we're doing ANOVA? Let's just say we're comparing three samples. Do you think it's this group, this group, or this group? This first option might look tempting and in fact the null hypotheses is correct. But one problem with ANOVA is that we can only tell if at least one pair is significantly different. That's what makes our statistic big. We have one statistic that's either big or small. If we get a small statistic, we know that the within subject variability is large relative to the between subject variability. And none of the means are significantly different from each other. In that case, we would accept the null hypothesis. But if we get a large statistic, the opposite is true. The between subject variability is large. Relative to the within subject variability. In other words we might have some samples, and the means are different from each other, relative to each variability. But then we know that at least one pair of means is significantly different. So we accept the alternative hypothesis. But we don't know which means are causing this large between subject variability. This means that when we're doing ANOVA, if we get a large test statistic and reject the null. There's an extra step that involves seeing which means are different from each other. These follow-up tests are generally called multiple comparison tests. And you'll learn about these in the next lesson. We're going to get a little more familiar with the null and alternative hypotheses for ANOVA. If the variants of an individual sample becomes bigger, all else held constant. Does this lean more in favor of the null or alternative hypotheses? This one, or this one? Thats okay if you had to think about this. Let's say we haev two samples, here are the distributions. If one of them increases in variability, then its more likely that there sample means are not significantly different. This leans more in favor of the [UNKNOWN] hypothesis. And therefore our test statistic will be smaller, because there's a larger within group variability. Okay, let's do another one as the between group variability increases meaning the sample means get further apart from each other all else held constant. Does this lean in favor of the null or alternative hypothesis? This time we look at it like this. If we have two distributions, lets say, and if the mean of one gets further apart, this leans more in favor of the alternative hypothesis. Therefore, when we have this test statistic that measures whether or not there are significant differences between three or more samples. We have to construct this statistic so that as within-group variability increases the test statistic decreases. Becomes more in favor of the null, meaning that the sample means are less likely to be different from each other. And this statistic should also take into account that as between group variability increases, the test statistic increases. The sample means are more likely to be different from each other. We can do this by making a test statistic. Let's call it f. A ratio of the between group variability and within group variability. Which should be in the numerator, and which should be in the denominator? Between group variability should be the numerator, because as it increases. This will increase the whole test statistic, and this leans more in favor of the alternative hypothesis, that at least one pair of means is significantly diffferent. Within group variability should be the denominator, because as this increases the f ratio decreases, which you can see algebraically. If the denominator increases then the whole quotient will decrease. And this leans more in the favor of the null hypothesis that the means are not significantly different. Just like the t statistic the numerator indicates much group means differ. This is explained variation. Because it most likely results from differences due to a treatment. Or just differences in the populations. With the clothing example from the beginning of the lesson, the difference in the numerator can be explained by the fact that the clothes are from different brands. But the denominator is a measure of error. Within-group variability measures individual differences of subjects within each group. This is considered error variation because we don't know why subjects in the same group are different, whether they're people or shirts. They just are. Let's do a quick quiz, matching diagrams with the statistical outcome. Let's say we have three samples, and here are the statistical outcomes. Put the statistical outcome by each visual. Some of them might not be used, and some of them might repeat. So put A, ,B, C or D, one of them in each of these boxes. A should be here and here. If the variabilities of each sample were smaller, then it's more likely that the population means would be significantly different. But since the variabilities are large, then it looks like the null hypothesis would be true. And in this case, the between-group variability is small. Which results in a smaller f statistic, in which case we would accept the null. This one though, thought within group variability is small, and the between group variability is large relative to that. This is b. And here it looks like the population means from one and two would not be significantly different But 3 would be from both 1 and 2, that's d. We need to formalize how we will precisely measure each type of variability. We already decided that we're going to use the grand mean, and use the same idea as standard deviation to measure the spread of the sample means. We'll find each square deviation from the mean. And in this case we're going to multiply each square deviation by the sample size. Each square deviation is equivalent to the area of each of these squares. And then we'll mulitiply this area by their respective sample size. Here K represents the number of samples. That means we'll have K sample means. And then we'll add them all up. In this lesson though, we're assuming that all samples have the same size. So, we can get rid of the K. n is just a constant across all samples. That means that we can write the numerator between group variability like this. And then we divide by the degrees of freedom. When we say formal way of measuring something, that means we get one number. If we add up each squared deviation of each sample mean from the grand mean, multiply this by the sample size and then divide by the degrees of freedom. We get one number that describes between group variability. We need to do the same for within group variability. And we'll do something similar to the way we measured between group variability. We'll take the sum of squares for each individual sample from the mean of each sample, and then we'll divide by the degrees of freedom. What do you think the degrees of freedom is in this case? Let's say that we have three samples. More than one option may be correct. Each sample has n minus 1 degrees of freedom. If there are three samples, that means we would subtract 3. That's the same as subtracting the number of groups. And as you've seen before, we use capital N to represent the total number of values in all the samples. Capital N minus k is probably the easiest way to symbolize the degrees of freedom for within-group variability. So now we have our formal measurement for the f statistic. Which signals whether or not there is a significant difference between any two sample means, out of k samples. We can write this as the sum of squares for between groups. Divided by the degrees of freedom for between groups. Divided by the sum of squares for within groups. Divided by the degrees of freedom for within groups. This gives us what we call the mean square. And this ratio is our f statistic. What is the sum of the degrees of freedom for the between and within group variabilities? The degrees of freedom for between group variability is k minus 1. If we add this to the degrees of freedom for within group variability. And then we do a little bit of switching around. We get that the k's cancel each other out. And we're left with n minus 1. This is our total degrees of freedom. You learned before that the degrees of freedom is the sample size minus one. Well, this is the total number of values in all the samples. And we subtract 1 to get the total degrees of freedom. We have a total degrees of freedom and likewise we have a total variation. This is also the sum of the sum of squares for between and within group variability. This is the total sum of squares that we mentioned earlier. Each value, minus the grand mean squared and then sum them up. Basically what ANOVA does is partition the total variation into between group variation and within group variation. Difference is in the dependent variable or treatment are due to both between group differences and individual differences within each group. This means that only some of the variation can be explained by knowing which group is subject is in and the rest is unexplained variants. Now we have a formal f statistic. That is the ratio of between group variability to within group variability. If we could take all possible f statistics, what would this distribution look like. Well let's start with what we know. The f statistic is always negative, sometimes negative or never negative. The F-statistic is never negative. That's because we square each deviation, making it positive. And when you do arithmetic with positive numbers, you'll always get a positive number as the result. This then has implications for the f distribution. Since the F-statistic is always positive, the f distribution will only be in the first quadrant, and it looks something like this. Unlike the Z and T tests, the distribution of the F-statistics is not symmetrical. The F distribution is positively skewed, meaning it peaks on the left side and is stretched off to the right side. This distribution peaks at 1. This is because if there are no differences in the population means, in other words the between group variability is. Expected to be 0. Then the mean of each sample will still, likely differ by chance. Since the difference, then, is due to chance. The same way that each subject in each sample differs by chance, as measured by the within-group variability. Then the between group variability and within group variability will be the same. Therefore, when we divide them we get 1. And that's where this distribution peaks. Also note that when we're doing an F test, this will always be non-directional. Whereas when we were doing the Z and T test, our alternative hypothesis could have been that one population mean is less than another. With the F test, we only know if there's a significant difference. Our critical region will always be out here in the upper tail. But everything else is essentially the same. We choose an alpha level, usually 0.05. And then we find out if our F-statistic lies in the critical region or not. If it lies in the critical region, we know that at least two population means will be significantly different. And then just like we've been doing. We need the critical value. And we'll use a new table. What do you think this table is called? The A table, the M table, the F table, or the Y table? Its called the f table. What a surprise. We won't use the f table right now. Instead lets return to the clothing brand example from the beginning of this lesson and actually do ANOVA this time instead of using intution. Then you'll learn how to use the f table when we get to it. Here's the data from before. Calculate the mean of each and the grand mean. Here are the means of each. And then since we have the same sample size in each, the grand mean can just be the mean of the means. 13 plus 48 plus 45 divided by 3, which is about 35.33. But you can also calculate the grand mean by adding 15 plus 12 plus 14, et cetera, to 38, and then dividing by 12. That's how you would find the grand mean otherwise, if the sample sizes were different. Now find the sum of squares for between groups, using each sample mean and the grand mean. Remember that we also multiply by the sample size. The sample size is 4, and we multiply this by each squared deviation from the grand mean. And this comes out to 3,010.67. Now let's find the sum of squares for within subject variability. This one's a little harder. We take the sum of each value and subtract the respective mean. Then square it. Then we add them all together. This is a little trickier. You need to calculate each deviation of each value from the respected mean. Square them and then add them all up. Do the same for every sample. And then add up each sum of squares. You're welcome to enter your calculations here, but that part won't be graded. Only the complete sum of squares for within groups will be graded. First we're going to find every deviation from the respective sample mean, and then we'll square each deviation. Then, we can find each sum of squares for each sample and then, we add up each sum of squares. In this case, we get 862. Great job, this was the hard part. Now, what are the between group and within group degrees of freedom? If you forget, think about it logically. How many groups did we use when we calculated this sum of squares? And how many values did we use when we calculated this sum of squares? Then how much did we subtract from each one. We had three groups. Snapzi, Irisa, and LolaMoon. So there are 2 degrees of freedom for the between groups variability. For the within groups, we had four values in each group. And we subtract one for each group. That makes 9 degrees of freedom. Remember that this is the same as the total number, capital N which is 12 minus k, the number of groups, which is 3. 12 minus 3 is 9. Now, calculate the mean squares for each. This is where you simply divide the sum of squares by the degrees of freedom. These will give us our final measurements of between and within group variability. 3010.67 divided by 2, is about 1505.33. 862 divided by 9, is about 95.78. Finally now that we have our final measures of between group and within group variabilities we can find the ratio between them. Which is the F statistic. Calculate what that should be. Remember that the numerator of the f statistic is the between subjects variability. Because as that increases, the f statistic will also increase. And that means we're more likely to reject the null. And the denominator is the within subject variability. If we divide them, we get 15.72. Good job. Just like we've been doing with the z test and t test, we want to compare the F-statistic to the critical value that marks a low probability. Usually 5% of obtaining a particular F-statistic. We do this using the F table. There's a different F table for different alpha levels. If you already clicked the F table link, you'll see that we need two degrees of freedom. Those are the degrees of freedom for between group variability and within group variability. Try to use the F table now to find the critical value. The F Table's pretty self-explanatory. Here we have numerator degrees of freedom, which was 2. And here, denominator degrees of freedom, which was 9. And we see that the f-critical value, cutting off 0.05 in the right tail, is 4.2565. Finally, what will our decision be? Do we fail to reject the null, and therefore conclude that there is no significant difference between the prices at each store? Or do we reject the null in favor of the alternative, that at least two stores have significantly different prices? The F-statistic we calculated is greater than our critical value. Therefore, we reject the null in favor of the alternative. At least one group is significantly different. And I think we know which one it is. We easily realized this, simply using our intuiition at the beginning of the lesson. But usually the situations we use in ova four are much more complicated, and with much larger data sets. And it's impossible to look at lists of hundreds or thousands of values and just know if they're different. But also in those those cases, we wouldn't calculate the F-statistic by hand. We would use software. The purpose of this lesson was mostly for you to understand the reasoning behind the F-statistic, and learn how it's calculated. Great job. In this lesson, you'll become more familiar with ANOVA by going through a full length example that not only includes calculating the F statistic, but learning about and calculating a measure of effect size for the differences between three or more groups. This is similar to Cohen's d for the t tests. To improve the lives of dairy cows, a researcher examined 3 types of foods, they wanted to assess cows prefernece for one food over the other all were equally nutritous and afforadable. Together the farmer and the researcher gave the cows each type of food there were nine cows bessie frank and slim got food a Bob, Red, and Pippin got food B. Sam, Merry, and Chip got food C. They were randomly assigned to each type of food. Over eight hours this is the amount that these cows consumed in pounds. The data can be visualized with bar graphs. Before we get started with our analysis, match the independent variable dependent variable, null an alternative hypothesis, with the column on the right. Not all of these will be used. Put the appropriate letter next to each. This one was kind of tricky, especially with the independent versus dependent variable. The independent variable is the type of food. The type of food influences how much cows eat, which is the dependent variable. The null hypothesis is that cows will eat similar amounts of food. Whereas the alternative if you remember from the last lesson is at least two groups will significantly differ, in this case that means that cows will eat more or less of at least one food over another. So let's do ANOVA to see if cows prefer one type of food over another. We need to calculate the between group variability. So first, let's calculate the grand mean. Remember we're denoting this X bar sub G. If we add all these up we get 54, and if we divide by the total number, which is 9, we get 6. Now let's calculate the mean of each group. 2 plus 4 plus 3 is 9, divided by 3 is 3. Likewise the average of food consumed for B is 6, and 9 for Food C. Now we can calculate the sum of squares for between group variability. Remember, it's the squared deviation of each group mean from the grand mean. Then we sum all these deviations up, and multiply by n, the sample size in each group. The sample size in each group is 3. And we'll multiply this by the group mean in Food A, minus the grand means squared. Plus the group mean for Food B, minus the grand mean squared. Plus the group mean for Food C. Minus the grand mean squared. This gives us 3 times 9 plus 0 plus 9, which is 54. Now compute the sum of squares for the within group variabilities. That's the sum of the squared deviations of each value from its respective mean. Remember that's the squared deviation of 2 from 3, 4 from 3, 3 from 3, 6 from 6, 5 from 6, etc. And then you sum all these squared deviations to get some kind of measure for the variability for each sample. First we want the squared deviation of 2 from 3. Then we add the square deviation of 4 from 3. Then of 3 from 3. Then 6 from 6, then 5 from 6. 7 from 6, 8 from 9. 9 from 9 and finally 10 from 9. Turns out all the squared deviations are either zero or one because this is a simple example. Then we get that the the sum of squares for within group variability is six. Now, what are the degrees of freedom for between and within group variabilities? Since between group variability deals with the variability between groups, obviously, then we're concerned with the number of groups we have. There are three groups, and the degrees of freedom is then 2. For within group variability, we add the degrees of freedom from every group. 2 plus 2 plus 2, which is 6. Now we can find our mean's square. Remember this is just like the variance, where we took the sum of squares and divided by n minus 1. That's what we're doing for between and within group variabilities. 54 divided by 2 is 27 and 6 divided by 6 is 1. Now we can calculate the F-statistic. What is it? Hopefully that was pretty easy, 27 divided by 1 is 27. We use our F statistic to determine if we should accept or reject the null hypothesis. Where in this case the null hypothesis would be that the amount consumed by cows of food a, food b and c are all approximately the same which we're assuming means that cows do not prefer one type of food over the another. And the alternative is that they at least prefer one type of food over the other. So we have to compare our F statistic with the critical value. Use the F table to find this critical value and then decide, do we accept the null hypothesis or the alternative hypothesis. Here's the F table and we would need to use the degrees of freedom that you found earlier. Since there were three groups, remember that there were two degrees of freedom. And there were three cows per group who consumed the food. There were nine total values and the degrees of freedom for within group variability was six. They intersect here at 5.1433. Our F statistic 27 is much greater then our F-critical value. It's out here somewhere in the critical region. Probably way out there. Therefore, we would accept the alternative. At least one pair of significant differences exists between the amount of food that cows ate. We summarize our results in our ANOVA table. This is how the results are typically presented, even when we use software. Before we move on, I just want to show you something. Calculate the squared deviation of each value from the grand mean, and then add them up. What do you get? First of all, 2 minus 6 is negative 4. You square it and you get 16. 4 minus 6 squared is 4. 3 minus 6 is negative 3, and squared is 9, etcetera. If you add all these squared deviations up, we get 60. What is this showing this in our ANOVA table? What belongs in the blanks? There are check boxes up here, so check which two should go here and here. Sum of squares for between group variability, and within group variability, add to the total sum of squares, 54 plus 6 equals 60. You learned this is the last lesson, but I just wanted to remind you. And remember that the total sum of squares is the sum of each square deviation of each value and all the samples from the grand mean. Now that we have our F statistic, what can we conclude? We conclude that at least two types of food signifigantly differ from each other in terms of the amount eaten by the cows. By rejecting the null, we know that at least two of the foods differ in terms of mean amount consumed. But we don't know which ones differ. We can see that all three of the means differ from each other, but we don't know if these differences are due to sampling error. Therefore, we need to do additional testing to see which means are different. This additional testing is called multiple comparison test. So we're able to compare all of the means with each other. We wouldn't do a multiple comparison test until after we've done ANOVA. One of the most common multiple comparison tests is called Tukey's HSD, which stands for Honestly Significant Difference It's named after John Tukey, a very influential statistician. Tukey's HSD evaluates the significance of the difference between any two group means. In statistics, we say that this test will allow us to make pairwise comparisons. The way we calculate Tukey's HSD is just like the margin of error that you've learned before. Remember that with the z test, it was the z critical value, denoted z star times the standard error. And remember with the t test, it was the t critical value times the standard error. But now when we're comparing three or more samples, we have a new statistic called Q. And we multiply this by this square root of the mean square for within subject varibility divided by n. You might ask how is this similar to these. Well, the mean square for within subject variability is the pooled variance. It's just the average square deviation of each value from its respective group mean. Therefore, the square root is the pooled standard deviation. Q is the student highest range statistic, and we find this in yet another table. It's purpose is to adjust the whole HSD, so that it's less likely we commit a type one error. Remember a type one error is when we reject the null hypothesis when it's actually true since q increases when there are more groups that we're comparing, it makes it less likely to commit a type one error. Remember that if a sample mean was further away from the population mean, than the margin of error, it was considered unlikely. Well if two sample means are further apart than the Tukey's HSD then that's considered honestly, significantly different. So now, let's calculate Tukey's HSD and see if any of the sample means for the amount of food cows ate is honestly significantly different. Use the studentized range statistic table to find the critical value of q. You'll need to know the number of groups we're comparing, as well as the degrees of freedom for the within group variability. You'll notice that you can calculate Tukeys HSD for alpha equals 0.01 and 0.05. Calculate this for alpha equals 0.05. The q critical value according to the studentized range statistic table is 4.34. We multiply this by the square root of the mean square for within subject variability, divided by this sample size, which is 3. This comes out to about 2.51. This HSD value means that if any two samples, have an absolute difference greater than 2.51, the difference is considered honestly significant. Which are significantly different, A and B, B and C, or A an C? Or, are two of these differences significant, or are all significant? The distance between the means for food A and B is three, and this is greater than 2.51. For B and C it's also three, and for A and C the difference is six. Therefore, all of the differences are honestly signifigantly different. If we just looked at the means, we wouldn't know whether or not their differences are simply due to sampling error, but now we can conclude that cows prefer food C. Before we move on from Tukey's HSD, note that we can only compute this when the sample sizes are all the same, in this case they were all 3. We can also compute Cohen's d for multiple comparisions. And remember that this is a measure of effect size. You've learned before that Cohen's d is the difference between sample means divided by the pooled standard deviation in the case of independent samples tea tests. You just learned that the square root. of the means squared for within subject variability is essentially the pooled standard deviation. so thats teh formula for co hence d however now we haev three samples therefore well compute 3 cohens d one for each pair of samples compute cohens d for the difference in means going in the direction symbolized For the difference between A and B. We have 3 minus 6 divided by 1. Which is negative 3 for B minus C, we have 6 minus 9 divided by 1 which is also negative 3. And for A and C we have 3 minus 9 divided by 1 which is negative 6. Another effect size measure for one way ANOVA is called eta squared. This is just like r squared from our discussion of t tests. This has the same interpretation in ANOVA. This effect size tells us the proportion of total variation that is due to between group differences. This is also called explained variation. Knowing this, how should we calculate eta squared? Check all that apply. Two of these options are correct if a squared is the proportion of total variation due to between group diferences total variation should be in the denominator, that rules out these middle two as an option and were interested in the total variation due to between group differences. Therefore, between group differences must be in the numerator. Therefore, 8 of squared are these two. Now calculate 8 of squared. You already know the results from your ANOVA table, sum of squares for between group variability is 54, and we divide by 54 plus 6, which is the total sum of squares. 54 divided by 60 is 0.9. This is actually pretty huge. 90% of the differences in the amount eaten is due to the differences in food types. The other 10% remains unexplained. It could be because of the individual cows, how hungry they happen to be that day or something else. The statistician Jacob Cohen said that anything bigger than 0.14 is considered big. So 0.9 is pretty big. Before we move on, lets get to know eta squared a little bit more. Eta squared ranges from what to what. Write in the lower bound and the upper bound. I haven't taught you this, but intuitively, by understanding what it is, you should be able to figure this out. Eta squared ranges from zero to one. If eta squared is zero, this means that all group means are exactly the same. The distance between group means can never be negative. Therefore, it can never be lower than zero. But if eta squared is one, the between group variability equals the total variability. Meaning there is no within group variability. The sum of squares for between group variability can never be greater than the sum of squares for total variability. It can only be equal to it, when the sum of squares for within group variability is zero. That means that eta squared is simply the sum of squares for between group variability divided by itself, which equals one. We typically report the results of one-way ANOVA like this. Our F-statistic as a function of our two degrees of freedom, between and within, our p-value and eta squared. Note that when we're doing ANOVA by hand, we can't find the exact p-value. This is the same as with the T test. We only know that our p-value is less than 0.05, our alpha level, because our f-statistic is greater than the f-critical value. If we're using software, it'll be the same thing, except we can calculate the exact p-value. Notice that we've frequently used small samples in our examples and problem sets. However, we wouldn't normally use such small samples, especially in real research. But if we use large samples, we can't do ANOVA by hand. That would just take too long. We would use software instead. When we use software, we don't care as much about the intermediate steps. Really, we just want to compare the p-value with alpha. We know that if p is less than alpha, we'll reject the null. Since using software is so essential to doing statistical tests, we're going to provide you with some software output for you to see and analyze, so that you get familiar with it. This is real data that Doctor Laraway collected from his psychology students at San Jose State University. There are 188 students total, 90 of which were freshman, 44 sophomores, 23 juniors, and 31 seniors. And he asked them to self-report how many days every month they drink caffeine. You can already see that the number of days that seniors drink caffeine is higher than any of the others. But it also has a pretty large standard deviation, as does that for juniors. We want to know if at least one group of college students drinks more caffeine than the others. Doctor Larawayused the statistical program SPSS to answer this. Here's his ANOVA table output. According to this, which of the following would we do? You may have figured out that the Significance level is the p-value. You see that the p-value is less than 0.05 but greater than 0.01. Therefore, at the 0.05 level, we would reject H not. But at the 0.01 level, we wouldn't. You could have also used the T-table, and found the t-critical values for an alpha level of 0.05 and 0.01. And you would see that the F-statistic falls in between them. That's another way to decide what our conclusion will be. Let's stick with alpha equals .05, and decide that at least two of these college groups are different in terms of the amount of caffeine consumed. Then we use a multiple comparisons test to determine which two groups of college students consume significantly different amounts of caffeine. Here's the output, but I've removed some of the values in the mean difference column. Because I want you to try and figure out what the missing values should be. We see that the mean difference in caffeine consumed between freshmen and sophomores is 1.018. Therefore, since we're going the opposite way here, instead we're doing sophomores minus freshmen, this will be negative 1.018. It's kind of hard to see the asterisks, but they mean that the difference is significant. This does not have an asterisk. So the amount of caffeine consumed is not significantly different between freshmen and sophomores. For freshmen and juniors, you see that the mean difference is negative 2.909 when you're going in this direction. Therefore, it's positive 2.909 when you go in the opposite direction. This also is not statistically significant. For juniors and sophomores, it'll be the opposite of negative 3.927. This also is not significant. Here are the rest of the differences, and those that are significant are the difference between freshmen and seniors, and between sophomores and seniors. Now that you've practiced with ANOVA with the same sample sizes, and you've seen the F test output using software, we're going to do one more example with different sample sizes. In a study to reduce tumor sizes in breast cancer, researchers analyzed the effects of a placebo pill, and three types of drugs. After each treatment was administered, the researchers observed the reduction of tumor diameter in centimeters. For 20 people who had tumors, first calculate the mean of each group. The average reduction in tumor diameter for the placebo group was 1.5 centimeters, 1.6 centimeters for drug 1, 1.614 for drug two, and 2.875 for drug three. Now, calculate the grand mean. Remember, now that we have different sample sizes, we cannot take the mean of means. We have to add all the values up and then divide by the total number. The grand mean is 1.835. Now let's start filling out our ANOVA table. Calculate the sum of squares for between group variablity. Since the sample sizes are different, we have to multiply each sample size by the squared deviation of each group mean from the grand mean. After we mulitply the sample size by the squared deviation, then we sum them up. The sample size for the Placebo group is 5. And we'll multiply that by 1.5 minus 1.835, and then square it. Then we'll add the sample size for Drug 1 times the mean for Drug 1 minus the grand means squared. Drug 2 has 7 in the sample, which we'll multiply by the squared deviation. And drug 3 has 4. After we do all this, we get 5.45. Other than the different sample sizes, everything else is the same. So calculate the sum of squares for within-group variability. To do this, we would find the square deviation of each value from their respective mean. 1.5 minus 1.5 squared, plus 1.3 minus 1.5 squared, plus 1.8 minus 1.5 squared, etc., all the way to 2.7 minus 2.875 squared. This give us .836. Now find the degrees of freedom. There are four groups, so the degrees of freedom for between group variability is 3. There are 20 total values, and four groups so the degrees of freedom for within group variability is 16. Now calculate the mean square for each, and finally the f statistic. 5.45 divided by 3 is 1.82. And .836 divided by 16 is .052. The f statistic is the ratio between these two. And that gives us 35. This is so large that we already know we should reject the null. We know there's a signifigant difference in tumor reduction between at least two of these groups. Now what proportion of the difference between tumor reductions is due to the type of drug? To answer this you had to remember that 8 of squared is the proportion of explained differences and this is the sum of squares for between group variability divided by the total sum of squares. Which is just summing up these two. This gives us 5.45 divided by 5.45 plus .836. This is about .87. 87% of the differences between tumors reductions can be explained by the different treatments. Before we wrap up this lesson, we are going to talk about power. Remember when we talked about type two errors. This was failing to reject the null when we should have. We don't want to make type two errors. Which is basically when we fail to find treatment affects when they actually exist. Since research is so time consuming and expensive, we need to make it more likely that we find statistical significance when we should. We do this by increasing power, there are a lot of things we can do to increase power and I bet you can guess some of them, if we are testing the effects of different drugs like we did in the last example. Do we want to test more or less peopel do we want to give each drug to very similar or very different groups of people. And finally, do we want to test a small or large dose of the medication. This is assuming that we'll still be safe in the amount of the drug that we give the subjects. Ideally, we want to test more people. That will give us a better indicator of whether or not the drugs are effective. We also want to give each drug to very similar groups of people. This is why we randomize. And we also want to test a large dose of the medication, because we really want to see how that will effect the subject rather than other factors. Larger samples result in higher power. Remember the standard error of the mean? When n increases, the standard error decreases. Lower within group variability also leads to higher power. We can be more sure of significant differences when the distributions aren't overlapping. However, if the variability increases, Its harder to tell if there's a signficant difference. And finally, choosing treatments with strong effects sizes will increase power. To do this you can make treatment conditions last longer, make them occur more often, the treatments could just have strong effects. Or you can make the treatment conditions last longer, make them more intense make them occur more often etc. These are all ways to detect whether or not theirs a signifcant difference. Finally we need to make some assumptions in order to use one way Anova. The first in normality. All the populations from which the samples are from are normally distributed. Another is homogeneity of variance. The data come from populations that have equal amounts of variability and finally independence of observations. The results found from one sample won't affect the others. However, we can violate these assumptions under certain conditions. We can violate the normality assumption if the sample size is large. We can violate the homogeneity of variance assumption. If all the samples have nearly equal sample sizes, and the ratio of any two variances does not exceed 4. We have to maintain independence of observations but we can use randon assignment to conditions to help up meet this assumption. Let's do a quick summary of ANOVA to wrap up this lesson. I'm not actually going to rap this time though, we're just going to wrap up the lesson. If we have three or more samples and we want to know if any two of them are significantly different, we look at both the between group variability, and the within group variability. Between group variability is a measure of how spaced apart these sample means are from each other. And we do that by finding the grand mean and each squared deviation from the grand mean for each sample mean. We multiply each sample size by the squared deviation of each sample mean from the grand mean. Then we add them up. Then we have to look at the within group variability which is essentially the square deviation of each value in each sample from the respective sample mean. So we add up all sums of squares from their respective sample mean and then we have to find the average sum of squares for each by dividing by the degrees of freedom. In the case of the between groups, this is the number of samples minus 1, and for within groups this is the total number of values minus the number of groups. This is the same as adding the degrees of freedom for each group. There we have our F statistic. And if it falls out here in the critical region, past the F critical value, we'll reject the null. After making a statistical decision, we can use the multiple comparison test, one of which is Tukey's Honestly Significant Difference. Which is a value that if any two sample means have a difference greater than that value, they're considered honestly significantly different. You've also learned how to determine what proportion of the difference between mean is due the independent variable. You've also learned how to determine what proportion of the difference between means is due the independent variable. That's theta squared and that's a wrap. Welcome back. Up to this point, we've really only been working with one variable. Analyzing the mean and the standard deviation, and seeing where a sample mean falls on this distribution. And if that's typical, or not so typical. But now we're going to focus on two variables. With two variables, we have a different design. This means we need to change how we collect our data, how we visualize our data. However, our analysis is going to be pretty similar, and you'll see later. Now that we're starting to discuss two variables instead of just one, lets start by thinking about two variables that might be related to each other. For example height and weight. People who are taller usually weigh more. Or maybe time spent studying and your grade. May be the more time you spent studying the better your grade will be. Heres the weird one. The outdoor temperature and angle injuries. I read this article the other day, that said that the warmer weather causes the people to wear more flip flops and sandals that make your ankles more prone to injuries. Do you have any other ideas? Post about it in the forums, and maybe include some information that you found that shows this relationship. How about miles on a car and the value of that car? We would think that the more miles a car has traveled, the less the value. No matter what the variables we're measuring, we usually call one of them x, for simplicity and the other y. Usually we want to know how the y variable changes when we change the x variable. So x is often called the predictor variable or the explanatory variable or the independent variable. What do you think y is called then? When you answer this try and think of the opposite of what the x variable is called. You have your x variable, your predictor. And then you get the outcome, which is the y variable. X explains the response. Finally, x is independent and y depends on the value of x. These other two aren't used in statistics jargon. But if anything, they probably describe the x variable. Since X kind of determines what Y will be, what we can predict Y to be. In stand-alone, I don't know. I just made that up. If we have data on two variables like this, how might we show that they have a relationship? There's no correct answer here in fact there probably many ways you can think of to show that these have a relationship. Tell us some of your ideas. You may have come with a few creative ideas. In statistics, the best way to show the two numerical variables have relationship is with a scatter plot. Note that these variables have to be numeric. They can't be categorical. Remember you learned that in lesson one. So let's do it, lets make a scatter plot. To make it easier, on the X axis, we'll say that the miles are in 10,000's. So 60,000 will correspond to say, 6 on this axis and the value of the car we'll say is in 1000's. Click where on this graph these points belong. Here is a scatter plot, and now we can easily see a trend going like this. As x increases, y tends to decrease. The data don't fall exactly on a straight line but they definitely follow a clear linear trend. There is a line that best approximate this data and we are going to talk about that in the next lesson. Data doesn't just ahve to follow a linear path. It can also follow a curved linear path, like this. In this lesson we're just going to focus on linear trends. Intuitively we know that certain variables have a relationship. As I said before, height and weight or age and height tend to have a relationship. So now let's focus on which variables have a stronger relationship. Which pair of variables have a stronger relationship, and how do you know? In other words, how would you prove to someone that there's a stronger relationship in one of these without simply saying, look at the data, you can just see it? These variables have a stronger relationship, but there's no right answer for how we would prove this. You might have come up with your own creative way. For example, what if we drew the smallest ellipse possible around all our data, and then looked at the ratio of the major axis of this ellipse, to the minor axis were bigger ratio mean a stronger relationship. In lesson one, you saw a relationship between Height and Hand Length. And you used my Hand length, 6.75 inches, to guess my height. Based on this, you probably guessed that I'm not more than six feet, or 72 inches tall. But what if the data looked like this instead? Could we make a good guess about how tall I am? Not really. What makes this data have a stronger relationship than the other one? And what's one way to measure it? First of all, we can see if there's a direction to this relationship. This is probably true in personal relationships, too. In statistics, the direction of a relationship refers to how the y variable changes as x gets larger. In the case of Height and Hand Length, what happens as x, or Hand length, increases? As x increases, y or height also increases. This is a positive relationship. A negative relationship is when increases in x are associated with decreases in y. So it would look like this. Note that, these do not mean it's good or bad, these are just terms to describe which direction the trend is going in. And you may remember this from learning about graphing lines. Lines with a positive slope go upwards whereas lines with a negative slope go downwards, from left to right. We often refer to the relationship between two variables as the correlation. If two variables have a strong relationship, then they have a high correlation. We can also say strong correlation. But you're going to start getting used to the word correlation. Now, match each of these plots with the appropriate description. Which of these has a strong positive correlation? Or strong in negative which shows a weak and positive correlation and finally which is a weak but negative correlation. Put the appropriate letter in the boxes here on the left. A looks like it's weak, but positive. So we'll put A here. B looks a lot stronger, but negative. C looks to be weak, and negative. And D is strong, and positive. Good job! Now that you have a good grasp of the strength and direction of linear relationships, we need a way to quantify these characteristics. The number that quantifies the relationship is called the correlation coefficient, denoted by r. You have seen r before when we did t-tests. This is also known as Pearson's r, named after a statistician as usual. We will describe the formula so that you know what it is, and we'll focus on interpreting r. r is a fraction just like many of our other statistics. The numerator is the covariance of x and y. This is a function. This does not mean Cov or some number times x, y. This is the covariance of x and y which is in itself a statistic for how much x and y co vary, in other words how much they vary together. In the demoninator is the standard deviation of x times the standard deviation of y. And this describes how the two variables vary apart from each other rather than with each other. Another way of writing it is like this. Even though r is a ratio, it's not interpreted as a percentage. However, r squared is the percentage of the variation in y explained by the variation in x. You saw this before when we did t-tests, and you saw how we explained a certain percentage of the difference between sample means, or between a sample mean and the population mean, that's due to the different treatments. r squared here has the same interpretation. r squared is called the coefficient of determination. r measures the strength of a relationship, by looking at how closely the data falls along a straight line. If it falls perfectly along a straight line in the positive direction, then r is positive 1. And if the data fall perfectly along a straight line in the negative direction, r is negative 1. Data that are not correlated at all have a correlation coefficient of zero. Usually data will not fall along a straight line, but sometimes it comes pretty close. And then r in this case might have a correlation coefficient of 0.8, which is actually pretty high. For this next exercise try to match these scatter plots with the proper r, or correlation coefficient. Again write the appropriate letter in the boxes to the left of each value of r. The data in A fall along a straight line. Since that's in the positive direction, the correlation coefficient is positive one. The data in B follow along a straight downward sloping line. So the correlation coefficient is negative one. For C, we know it has a weaker correlation and it's negative, which has to be negative 0.73. For D, the data don't have a strong trend in one direction or another. They have a slightly positive trend, but not enough to convince us that the two variables are related. This low correlation is probably 0.14. Finally E look like it has a pretty strong positive correlation, which is 0.93. Good job. Now I have a few very tricky questions which doesn't require you to calculate the correlation coefficients but you really need to think about these. Let's say that the x variable is age in years and the y variable is age in months. For example, if we plotted all students' age in years on the x axis and their age in months on the y axis. What would the correlation coefficient be? There are always 12 months in every year. So if someone is one year old, then they're going to be 12 months old. If someone is two years old, they'll be 24 months old. If someone is four point five years old, they'll be 54 months old. This data falls along a straight line. And therefore, r is 1. Let's do another one. What if X is hours that you spent awake today, and Y is the hours you spent asleep today, in the 24 hour period. And we plotted that for everyone in this class. What would R be? Let's say you spent 12 hours awake today. That means out of 24 hours, you spent 12 hours asleep. And let's say you spent 24 awake. That means you spent 0 hours asleep. if we graph this for everyone, all of you will fall somewhere along this line. We're assuming that you can't be half-awake or half-asleep. And in that case, r is negative one. But normally, we deal with data that is not perfectly correlated. And we can't calculate it in our heads. Instead, we'll use software, and we can do it with Google Spreadsheets. Yay! To calculate Pearson's R in the Google spreadsheet, we write: Equals PEARSON parentheses, then here we'll have the start cell for variable x, colon, so all the way to, the end cell for variable x, comma, start cell for variable y, all the way to the end cell for variable y, close parentheses. About 60 of you took the poll that I distributed when I announced lesson 12. There were some interesting results. All the data is below, and there are two pairs of variables that I want to focus on: Age, and when you arrive at a party, and Age, and of number of pets. The first thing I want you to do is, visualize the data. Create a scatter plot for each. Does it look like there's a relationship, between age and when you arrive at a party? Or between age and number of pets? Here's the data. This party column is the time after or before eight o'clock, when the party starts, that you would arrive. If you arrive thirty minutes before the party starts, so at seven thirty, then the value you get is negative 30. If you arrive an hour after the party starts at nine o'clock then your value would be 60. When you make the chart, you're going to have to select ranges. Since age is in column c, we want c1 to c62 then add another range. And we want party I want i62. We also want to look at pets e1 to e62. So here we have our scatter part. It looks like party is really spread around and pets remains kind of consistent. It might be easier to visualize each of these separately. So, here we have our two scatter plots. It kind of looks like the older you are, the more on time you'll arrive at the party. Let's pretend that this is the line that best approximates the data. Then we would guess that if you're about 70 years old, you're more likely to arrive right on time. But if you're about 20 years old, you're more likely to arrive an hour late. With age and number of pets, it's really dfficult to see if there's a trend going on here. But it definitely looks like it's positive, maybe something like that. The older you are, the more pets you tend to have. Let's use the Pearson function for Google Spreadsheets to calculate R for each. What is R here and here? And in any of the blank space we write, equals, Pearson. And you know it's a function if it comes up.. We want to correlate C2 all the way to C62 with I2 to I62. We get negative 0.16 approximately. And when we correlate age with number of pets, we get about 0.376. Which one is stronger? The correlation between age and number of pets is a lot stronger, because it is further from 0. The closer to 1 or negative 1 the stronger the realtionship Even though two variables might appear to have a relationship if we look at the sample data, this could just be due to chance. And you've learned this before, when we take a sample and we get that the sample mean is really far from the population mean, this could have just been due to chance. Similarly, just because our sample data works like it has a relationship, the actual population of people might not or vice versa. Our sample data might not have a relationship but the population actually does. Things like this naturally happen with sampling error as you've seen before. And we are more sure as the sample size increases. The question is, how much variation due to sampling error is allowable before we decide that our results are not due to chance? Again, we do Hypothesis Testing. Now, r is the correlation we get from our sample data. Rho, which looks like a p, is the true correlation for the population. Given this what do you think the null and alternative hypothesis are Remember, we're hypothesizing about the population not our sample. Row is the correlation in between variables in the population but r is the correlation between samples. And we're hypothesizing that row is equal to 0 for the null/g. and not equal to 0 for the alternative. Just like you've seen before, this is the nondirectional alternative. This is the positive directional alternative, and this is the negative directional alternative. Hypothesis testing about the true population correlation is a type of t test. It uses the t distribution, and therefore, the t table that we discussed earlier. This is the formula, but you don't need to use it. And the degrees of freedom are N minus 2. You can understand this intuitively because we have n values, n for each variable, and we subtract 1 per group. We just wanted to show you this so that you see what it is, but in real life you'll use software to run this test. The important thing is that you're able to interpret it, and since you already know all about t tests, you know when to reject or fail to reject the null. For example, let's say we're measuring the relationship between two variables. And we have 25 participants. And let's say we do a non-directional test at alpha equals 0.05. Let's say we get a t statistic equal to 2.71. What do we conclude there is or there is not enough evidence to reject the null because there is or is not a significant relationship between x and y. You had to use the T table for this and the degrees of freedom is 23 and the t critical value is 2.069. This is because we're going a nondirectional test. In other words, our alternative hypothesis is this one. So we'll have 2.5% in each tale. With 23 degrees of freedom. Out t statistic is greater than this, therefore there is evidence to reject the null. Meaning that there is evidence of a significant relationship between x and y in the population. The observed relationship is not likely due to sampling error. No I'll show you something else that's really cool. Since we know that the sample statistics often aren't exactly what the population parameters are, we might want a confidence interval for the true correlation. In other words, a range of likely values for the population correlation coefficient rho. I used the statistical program R to calculate confidence intervals for Pearson's r, using this data. For the correlation between age and time you arrive at a party, the 95% confidence interval went from negative 0.3995, to positive 0.0914. And for age and number of pets, this ranged from 0.1369 to 0.5733. Based on these Confidence Intervals, what could you logically conclude? What would be your statistical decision for each? Well, you see that in this confidence interval, for age and time you arrive at a party, the true correlation for the population could be 0. The confidence interval crosses 0. In this case, there's not enough evidence to reject the null. However, in this case, with age and number of pets. We're pretty sure that the true correlation will not be 0. it'll be somewhere between point 0.14 and point 0.57. Therefore, we would reject the null. Well, let's see if we're right with these assumptions. I calculated the t-statistic for each one of these also using r. Use these to calculate the p value for each. You can do this with graphpad.com quick calcs. The link is below. After you get in to quick calcs, you need to choose statistical distributions and interpreting p value. Then choose calculate p value from z, t, f, r, or chi-square. You'll learn about chi-squared in the last lesson of this course. Click Continue, enter the value of r and the degrees of freedom, then click Compute p. Quick calcs will give the two tailed p value. So eneter the 2 tailed P relationship here and see if it's less than an alpha level of 0.5 For the p value for age and time you arrive at a party, you should have gotten 0.2066. This is a pretty high p value. Much greater than the alpha level. Therefore we will write, we would fail to reject the null. We have no evidence in support of the fact that our population correlation might be different from 0. For this p value, we get something small. 0.0028. And this is less than our alpha level. So, we were right, based on our confidence interval, we decided to reject the null. Good job. Feel free to play around with the other data from that pole, maybe you'll find a correlation between the number of eggs in your refrigerator and the number of icons on your desktop. Some of them have missing data, so you'll have to adjust for that. Out of these two that we analyzed in the lesson, we saw that only one of them was significant, age and number of pets. We can see then that as people age they tend to have more pets, that's kind of interesting right? But we can't say for certain that the older you are more on time you arrive for a party, our P value was too high. Let's focus on this one that's significant. Let's say we add an outlier to the data, like 20 and eight. A 20 year old who has eight pets. How do you think this will change the correlation coefficient? Go ahead and add this data point to the spreadsheet, and find the new correlation coefficient. The correlation coefficient now goes down to 0.231, which is a lot lower than this one. As you can see, r is sensitive to outliers just like the mean is. In other words, outliers can dramatically change the value of r. That's why it's important to look at data with a critical eye and think, what is causing this outlier to be there? Should we include it in the measurement of our statistics? Remember how before, when we created box plots, we looked at just the interquartile range, and outliers weren't used in the actual box-and-whisker plot. They were drawn as circles on the outside. Kind of like this. Sometimes it's good to take outliers into account. Other times, you want to see the trend of the data without them. We'll finish this lesson by talking about Correlation and Causation. We briefly discussed this in lesson one with the Golden Arches theory of conflict resolution which has since been disproven. But still, it's kind of funny to think that McDonalds might cause world peace. We found this really cute cartoon online the link is at the bottom it's a little joke that illustrates the difference between correlation and causation. But we're leaving out the last line because that's the punchline. First we're going to discuss correlation vs causation. Causation means that one variable caused another to happen. For example, the fact that it was raining caused a bunch of people to bring their umbrellas to work. But correlation just means that there's a relationship. For example, do happy people have more friends? Is it just because they're happy that they have more friends? Or, does them being happy cause them to act a certain way which then causes them to have more friends? We never know, there are always lurking variables. We might have two variables X and Y, and they have a relationship, but only because both of them are influenced by variable A. Or maybe, X influences Y through variable A. A is called a mediating variable. By just looking at scatter plots and the correlation, we don't know what the case is. We can just see some kind of relationship. In order to make causal statements, for one thing, the independent variable would have to occur before the dependant variable and we have to be able to rule out any lurking variables. With age and pets, this one's kind of easy because we know that having more pets doesn't cause you to age. But others, like happiness and number of friends are kind of ambiguous. Maybe having more friends causes you to be happy or maybe being happy causes you to have more friends. We don't know. Even if one comes before the other, this doesn't necessarily mean that the first variable caused the second. I'm sure you can think of all sorts of examples where that wouldn't be true. Let's go through a few examples. A teacher with a jazz appreciation class says that his students really like the jazz music that he's playing. And he concludes that the class is causing the students to enjoy jazz music. Another situation, researches note that drug use is correlated with delinquency. They conclude drug use causes delinquency and if we reduce drug use, we can also decrease delinquency. What's wrong with the logic in each of these situations. In the case of the jazz music, perhaps students really liked jazz before and that's why they took the Jazz Appreciation class. Rather than the jazz appreciation class causing them to like jazz. For the second situation, it's possible that people were delinquent and that's why they started using drugs. In both of these situations, there's ambiguous temporal precedence. We don't know which variable occurred first, if it was the class or the appreciation for jazz or if there was drug use and then delinquency. Those are just two examples where we can't infer causality. Here's another one and one that's near and dear to my heart as an educator. We found a correlation between SAT scores and students grades in college. Can we include them that increasing students SAT scores will also increase their grades in college? Or what about this one, some people who commit violent acts played violent video games before committing these acts. Therefore these video games caused violence. What are the flaws in the logic here, for these two situations? This situation is called a third variable problem. It's not necessarily true that high SAT scores resulted in high grades in college. It could be that motivation, effort, intelligence etcetera was a third variable that influenced both SAT scores And college grades. And this situation's called a post hoc fallacy. Just because a person played violent video games before committing a violent act this doesn't mean that the violent video games caused the violent acts. It could be that people who want to commit violent crimes also want to play violent video games. There is actually a lot of research on this that I'm unaware of. But certainly a simple correlation wouldn't be enough to determine this. Here is the punch line. Get it. Just remember, correlation is super-useful but it does not imply causation. In lesson 14, we examined ways we could analyze the relationship between two numerical variables using scatter plots and Pearson's r. Remember that r is a measure of the strength of their relationship, and looks at how x and y vary together as opposed to how they each vary independently. We also noted that we can describe linear data using a straight line drawn through the middle of the data. In this lesson, we will discuss this straight line in more detail, and explain why it's useful for describing linear relationships and making predictions. This straight line has several names. One is the regression line. Another more intuitive name is the line of best fit. We care about the line of best fit, because it helps us describe the data for one thing. In other words, we can describe the patterns shown by the data. This line makes it easier to see the relationship between x and y. In this lesson, we'll only focus on linear relationships. Meaning relationships that follow a linear trend in one direction. Not only does the line help us visualize the data, but it helps us make predictions. We can use the line to predict what the y value will be for a particular x value. By knowing the equation, we can plug in this x value, say x star, and find the corresponding y value, y star. Our predictions won't be perfectly accurate, but we can often get a fairly good estimate for the value of y. Let's do an example. I got this idea from Dan Meyer, a really great math teacher. The link to his blog is below, with more ideas around the upcoming example. Let's say you want to go on vacation, but you have a budget of $500. Where are some places you could go? How much will it cost to go to Reykjavic, where they have awesome volcanic beaches? Or maybe Dublin, Ireland? Or Kumasi, Ghana. You'll go anywhere. You just want to have an adventure. You decide to analyze some flights and plotted the number of miles they traveled and the costs of that plane ticket. Here's a scatter plot showing the nine flights you looked up. Here's the line of best fit. This shows the expected value of the dependent variable, y, for any value of the independent variable. Let's focus on this point, for example. For this x value, which looks like it's about 1,500 miles, we see that the flight costs almost $800. Let's say this is about 750 dollars. This is the observed value. But based on our line of best fit, we would expect that a flight that travels 1500 miles would cost more like 450 dollars. These expected values for any x value are symbolized by y hatt. Whereas the observed values are just are regular y values. You can see that we have an observed and expected value for all of these data values. Take this one, for example. We travel about 5,010 miles, let's say. And the actual or observed price was about $1500. But according to our line of best fit, we would have expected this to cost about $1200. The prices for both these flights were more than we would have expected based on all of our data. The difference between our observed and expected value is called the residual. There's a residual for every point in the data set. It might be 0, or it might be huge. The most important thing is that we're able to find this line of best fit and then use it to visualize the data and the relationship between x and y, and make predictions. To analyze this line, we need to know where it hits the y axis, and at what angle. This angle is generally referred to as the slope. Which is essentially the amount that y changes, when x changes by 1 unit. In other words, when x moves 1 unit from here to here, y changes from here. Note that this is a positive relationship, and we want to find, what's the change in y when x increases by 1 unit. That's the slope, but remember in the case of regression, this line represents how much y is expected to change, when x changes by 1 unit. Therefore, when we write the equation for this line, we denoted it by y hat. There are many ways to symbolize the y-intercept and slope. You may have learned the generic y equals mx plus b, which at least in the United States, is usually how it's taught. That's where m represents slope, and b represents the y-intercept. But really, we can symbolize them any way we want. The important thing is that once we decide on how to denote them, you have to remember that and continue to use that later. We can also write it like this, because addition can go in any direction. 2 + 3 is the same as 3 + 2. The slope times x plus the y-intercept is the same as the y-intercept plus the slope times x. Sometimes one of these, the slope or they y-intercept, might be negative. So it's just important to keep that in mind. As long as we keep those negatives and positives signs correct, the order doesn't matter. The slope and y-intercept are called regression coefficients. And sometimes when we use software to calculate their regression line, they'll present it in this first way. For this lesson, we're going to use b to symbolize the slope, and a to symbolize the y-intercept. Given this, what will be our generic equation for the regression line? Select all that apply. If our Y intercept is A, and our slop is B, then we get that Y hat equals A plus Bx and Bx plus A. This means that when X is zero, the Y value equals B times zero plus A. And since anything times zero is just zero, y hat equals a. The y intercept is always the y value when x is zero Now that you know what this line means, we have to find this line that best approximates the data. Let's just use intuition for now. These are all the same data set, but which line best approximates the data? This one? This one? This one? Or this one? Definitely this one. The trend is downward sloping. So it can't be any of these positive slopes and it definitely doesn't look like the data go through this value up here or here. And this one looks like it slopes downward too steeply. We can see which one is better, but how do we determine mathematically which is the best. For example, how do we know that this line, in blue, isn't slightly better? There are a lot of lines that could look like they best approximate the data. So we have to mathematically find the exact a and b, the y intercept and slope that will get us the line of best fit. How do we find these regression coefficients? Let's look at these examples again. Remember how we talked about observed and expected values? Let's say we're deciding whether this would be a good regression line. For this x value. The expected value is way down here. For this x value, the expected value is also pretty far. This x value looks to be pretty close to the expected value but these two are really far from their expected values. The same with this. The expected value for this value of x. Is way up here. So this residual actually isn't too bad. But the amount of the residual gets worse and worse as we continue. Same with this one. If we continued this line really far out, the residual for this data value would be huge. All of them would be pretty big. And these would be really big. But here, the residuals are pretty small, especially compared to the other possibilities. For these two points the residuals are about zero. One thing we can do to find the line of best fit, is to find the line that minimizes the sum of all the residuals. The only problem is that the residuals are either negative or positive. And so, if we add something like this, say. These negative and positive residuals. Would just balance each other out. We could have a line of best fit that doesn't go through the points at all, but still has a very small sum of the residuals. We're kind of in the same situation where we were figuring out how to measure spread. We looked at possibly the average absolute deviation then, from the mean. Similarly, we could look at the absolute residual. In other words, even if the residual is negative, just look at the positive value. This is one method. But the standard method is to minimize the sum of squared residuals. In other words, we'll square all the residuals to get rid of the negative sign. Add up all these squares. And then minimize this value. In symbols, this would be the observed y value minus the expected y value. Which, you remember, was y hat. And this is the y value that corresponds to that x value. On this regression line squared. When we have a function, we can use calculus to find the maximum value and the minimum value. When we say value, we mean the x value. Where does the max or min occur? In other words, at what x value? We can also use calculus to minimize this sum of squared residuals. And find out what our aggression coefficients should be. When we use calculus, we get that our slope is equal to this. This is a long tedious equation and you won't have to use it. It just so happens that this is equal to r. Remember Pearson's correlation coefficient? Times the ratio of the standard deviation of the y values to the standard deviation of the x values. Isn't that awesome? Let's use it to find the slope our regression line for the airplane example. Open the spreadsheet for the data for miles traveled versus cost of various flights. Try and make this scatter plot yourself within the Google doc. You don't have to, but it's probably good to know how to do, then calculate R. Remember that the function in Google doc is Pearson, and then you'll enter the name of the start cell for the x variables. Go all the way to the end cell for the x variables. Comma, start cell for the y variables, colon, end cell for the y variables. After you get this, enter the correlation coefficient here, round to two decimal places. Here's the data. Let's calculate Pearson's r anywhere in the white space. Equals Pearson, and we need to enter the x values. A 2 colon a 10. And then we need to enter the y values, b 2 colon b 10. We get that Pearson's r if we round to two decimal places, it's 0.91. This is a really high value, meaning that the relationship between miles and ticket price is very strong. R squared is about 0.83 which means that 83% of the differences in cost is due to the differences in miles traveled. Now we want to find the equation for our line of best fit. Remember that the generic form is y equals bx plus a or a plus bx, doesn't matter. Where b is our slope and a is our y intercept. You learned before that the slope is equal to Pearson's R times the ratio of the standard deviations. We already found R. Our next step is to find the standard deviation for the y values. And the standard deviation for the X values. Remember that since these are the sample standard deviations, we have to use the corrected standard deviation, dividing by n minus 1. The standard deviations are pretty huge. We get 508.19 for the standard deviation of the y values and 2315.34 for the standard deviation of the x values. Now we can find the slope. Now find the slope of the regression line using the fact that we know that the slope is equal to a Pearson's r or the correlation coefficient times the standard deviation of the y values divided by the standard deviation of the x values. For our slope, we get 0.91, which is r, times 508.19 divided by 2315.34; and this gives us about 0.2. Now that we know the slope, we can replace it in our generic equation. Instead of b, we'll have 0.2. We still need to know our y-intercept, though. Which of the following, if known, will enable use to find the y-intercept? There's one correct answer. A point on the regression line. The standard deviation of the X values or any one of the data values, meaning the x and y values for any one of these points. In order to find the y intercept. We need to know the slope of the line and a point that it goes through. Let's say that the line goes through this point right here. Then there's only one possible y intercept that this line could have. Such that it has this slope and goes through this point. The y-intercept would be here. So, therefore, all we need is a point on the regression line. We already found the standard deviation of x, and we also already have all of the data values. Well it turns out we already know a point that the regression line goes through. What do you think it is? This one's kind of tricky. As a hint, remember that the line of best fit best approximates the data. And the data consists of x and y. Just give it some thought. If you guessed that the regression line goes through the means, the mean of x and the mean of y, you're correct. The line of best fit will always go through the means of the two variables that we're interested in. Since the mean of x is the best estimate for the x values, and the mean of y is the best estimate for the y values, the best fit line must go through this point. So what is this point. What are x bar and y bar? We notate the point, much like we notate a confidence interval. So don't get confused between the two. The x coordinate is 2601.11 and the y coordinate is 680.35. Therefore, after we know the slope and a point on the line, we can find the y-intercept. After that we'll have an equation for the line. And once we know this equation, we'll be able to find any other point on the line. And therefore use our equation to make predictions. Here we know that since a point on the line is x bar comma y bar. We can plug that into our equation and now we can solve for a by subtracting 0.2 times x bar on both sides of the equation. These cancel out and we're left with a on this side equals y bar minus 0.2 times x bar. Now that we have our formula for a, the y-intercept, find what this y-intercept is for our regression line for flight costs. A equals y-bar minus 0.2 times x-bar. Y-bar is 680.35 and then we subtract 0.2 times x-bar, 2601.11, and we get 160.13. Now we can plug this value in to our general equation for the line. And now we can use this equation to make predictions and answer other questions. The first question is, how much would you predict it costs to travel 4000 miles? 4000 miles is on the x axis, and we're trying to predict how much it will cost in US dollars. Our predicted value, y hat, then equals 0.2 times the independent variable in miles, plus 160.13. If we solve that, we get 960.13. Since this is actually real data, now you actually know if you want to travel 4000 miles, expect to pay about $1000. Here's another question. How much additional cost is it per mile? Let's pretend you're traveling 100 miles, this will cost $180.13. If you want to travel 101 miles, this will cost you $180.33. The difference here is 20 cents and notice that's the slope. Remember when we said earlier that the slope is the change in y, after we change x by one unit. In this case the units are miles. So with every additional mile traveled, you would expect to pay 20 more cents. What's the expected price for a flight that travels zero miles? This is kind of a funny question because you'd think that if you don't travel anywhere the price would be zero. But it's not. We do the same thing we've been doing. If we travel 0 miles, we plug 0 in for x. And the we get 160.13, which is our y intercept. It's kind of hard to see with this graph. Basically, this means you're paying an additional $160.13 for every flight. No matter how far you're going, and then an additional 20 cents for every mile traveled. Time for your last quiz, which was our original question. If your budget's $500, what's the furthest distance you can travel? And remember this is just if you use what is expected, from the regression line you calculated. This ones the opposite. Now our Y value is $500, because ticket price is on the Y axis. We plug the Y value in for our equation and we need to solve for X. What distance traveled will cost $500. Since this is a positive relationship. Any distance less than this value after we solve for it will cost less than $500. We'll subtract $160.13 from both sides and then we'll divide by 0.2 on both sides. We get that x, or the number of miles, is about 1700. Therefore, if you travel less than that distance, you can expect that you'll pay less than $500. Now, we don't expect our predictions to be 100% accurate. There will almost always be some error in our predictions. And that's in terms of the residuals. How can we quantify how much error we have? Well, remember mean and standard deviation when we just had one variable? We had distributions that were fat and skinny. And our expected value was always the mean. But we could always pick a random value that's out here in the tails, and that's due to error. And how did we measure this error? We used standard deviation as a measure to tell us how far we expect most scores to fall from the mean. Now, let's take a step back. We have two data sets. We'll just call them red and blue for now. They have the same regression line, as shown by this black line. You're trying to estimate the y value when x equals 12. The expected or predicted value will be the same for both, since they have the same regression line. But we can bet that the actual or observed value won't be exactly equal to this. It might be somewhere up here or down here, we won't know. We can only guess. But, for which data set, red or blue, do you expect the actual measurement will be closer to the prediction. Well the red values seem to be scattered relatively far compared to the blue, around the regression line. The blue data set seems to be pretty consistent. Have a pretty strong relationship. So therefore, we're more confident that the actual value when x equals 12, will be closer to the predicted value. Now we need a way to quantify this and we do that much the same way that we calculate the standard deviation. We take each residual, in other words the observed minus expected value. We square them to get rid of negatives. That's the same as finding the area of each of these squares. That essentially we find the average square. But we correct for the fact that it's a sample, by dividing by N-2, where N is the number of points. In this case there are six. And then finally, we take the square root to put it back into terms of distance rather than squared values. This is called the standard error of the estimate. Although using the standard error of the estimate can help us assess the accuracy of our predictions. We can make even more accurate predictions by computing confidence intervals for our predicted values, our y-hats. In other words, when we get our regression line, we have our expected value which is this y-hat naught, for a specified value x-naught. However, the actual value might be anything from up here, to down here, or it might be exactly equal to our predicted value. Therefore, we might want a confidence interval around our expected value for where the actual value might be. Similarly, we might want a confidence interval for the true slope. We'll have a certain regression line and we'll have calculated the slope for that regression line based on our sample data. And this is just an example, the slope could be this, or flatter down to here. A confidence interval for a slope can tell us the range for which the true population's slope might be. You won't actually calculate the confidence interval in this lesson, because we're going to assume that you'll use a computer to get it. The important thing is that you know what it means. For example, let's say we have some sample data that looks like this. We calculate that the regression line is y equals bx plus a. Some slope and some intercept. But lets say then that we're able to look at all the population data. And it looks like this. Now it looks to be slightly downward sloping. Since we're assuming this is the true regression line, we'll use the common notation for the regression coefficients for the population. Oh, and these should have hats since they're the predicted values. If this were the case, where the sample regression line is positively sloping. But the true regression line for the population is negatively sloping. That would mean that the confidence interval for b has a negative lower bound and a positive upper bound. It includes zero within this range. Therefore, if we run a two-tailed hypothesis test for whether or not the slope is equal to zero. We would fail to reject the null, meaning there's no evidence that there's a linear relationship between x and y based on that sample. In fact, let's get more into hypothesis testing for slope. This tells us pretty much the same thing that we get when we do a hypothesis test for r. If the test for r is significant, then the test for slope will also be significant. This is because both tests ask the same general question. Are the two variables linearly related? So, let's just explore this a little more. And that'll also give you a little review of the hypothesis test for r. I've briefly mentioned that beta 1 is how we typically symbolize the population slope. And beta not is how we typically symbolize the population, y intercept. In other words, when we do linear aggression with all the population data, we get that the best fit line intercepts the Y axis at beta not. Remember that lower case B is the slop of the best fit line when we do linear aggression for this sample. And we're using a for the y intercept for this sample. Now what do you thing the null and alternative hypothesis are, when we do hypothesis testing for slope? Do you think it's this group, this group, this group, or this group? To answer this, you have to remember which symbols we used for the population slope, and that's beta one. Since we're doing hypothesis testing for the slope, beta one will be the symbols we use when we represent our null and alternative hypothesis. Just like before with all our other hypothesis tests, our null is that, whatever we're doing the hypothesis test for, in this case the slope, is equal to zero. And again, we could have a negative directional alternative or a positive directional alternative, or a non-directional alternative. Again, this is a type of t test. It uses the t distribution and the t table. We won't tell you how to calculate the t statistic, we'll just assume that you'll use a computer to conduct it. And by now you should be able to know the interpretation of whatever t statistic comes out for the output. Then you'll know when to reject or fail to reject the null. The degrees of freedom are important, though. And again that's just like when we did hypothesis testing with R. The degrees of freedom are N minus 2. Where N is the number of data points. So we have N values of X, and N values of Y. For example, let's say you're measureing the relationship between two variables, X and Y. And you have nine data points. So nine values of X, the independent variable and nine values of Y, the dependent variable. And let's say we do a non-directional test at alpha equals point zero five. And then let's say that we calculate our T statistic and we get five point seven seven. What can we conclude, their is or their is not. Enough evidence to reject the null, meaning there does or there does not appear to be a significant relationship between X and Y. For this you needed the t table. And you would've found that the t critical value, when there are seven degrees of freedom, because remember the degrees of freedom is n minus 2. And with 0.025 in either tail since it's a non-directional test, the t critical value is 2.365. The t statistic that we calculated, or that the computer calculated Is greater therefore there is evidence to reject the null, and there does appear to be a significant relationship between x and y in the population, based on our sample data. In fact, the t-statistic I gave you is the one that we get when we do a hypothesis test for the plane flight distance versus cost that we talked about earlier. When we do linear regression in R, the statistical program that I've briefly mentioned before, this is the output. Usually, you'll do software to do any type of hypothesis test. Whether it's on a sample mean or two sample means or the correlation coefficient or the slope, or whatever. The important thing is that you understand the meaning of the software output. Here you see the t value for this slope is pretty big, 5.77, the same value I gave you earlier, and the p-value is pretty small. There are three stars by it, indicating that the p-value is small enough to reject the null. Here are the significance codes, which list the alpha levels. We see that three stars means we can reject the null at the significance level of point 0.001. Since this slop is significant, we know there's a true relationship between the distance traveled and the cost of the ticket. Note that we can also conduct a hypothesis test on the Y intercept, and in this case we see that's its not significant. The p value is pretty high at 0.2 and there are no starts by it. This means that we're not totally sure that the true intercept is at 161.38775. However, in many cases, the estimate for the intercept is not of interest to researchers. One reason is that this value could have no meaning in real life. Because x equals 0 may not be realistic. For example, the price of iPods at the year zero AD. iPods weren't even invented back then. Let's talk about factors that affect simple linear regression. you already saw before that outliers affect the value of r, the correlation coefficient. Well, outliers also affect the linear regression line. This could be our regression line with this sample data. But an outlier, let's say it's out here, can seriously pull this regression line down. Outliers are always important to take note of when doing any kind of statistical analysis. We're pretty much done with the lesson now. So, we're just going to recap some key concepts that you learned in this lesson. Simple linear regression is when we look at the relationship between an independent variable. Which we often symbolize by x, and a dependent variable, which we symbolize by y. We have data values for x and y which we can plot in a scatter plot. For each point, these x and y values are our observed values. When we calculate the regression line, we get our expected y values. For this x value here, the expected y value is here, and we denote this by y-hat. The distance between the observed value and the expected value is called the residual. And this is an error term. We find the equation for our line of best fit, by minimizing the sum of squared residuals. And we get that our regression line has a slope equal to r, Pearson's correlation coefficient. Times the ratio of the standard deviations of the y to the x values, plus our y-intercept. Since we know that our line of best fit goes through the mean of x, and the mean of y. Then we know the y intercept is equal to the mean of y minus the slope times the mean of x. We can then replace that here. Our full equation for the regression line might look scary, but now you know how to calculate all of these. You know how to calculate Pearson's r and the standard deviation of the y values and the standard deviation of the x values. And you know how to calculate the means. You'll never have to do this by hand anymore, just through Google spreadsheet. After we have the equation for the line of best fit, we can input values of x to get the predicted value of y. It's super useful and super awesome. Great job. This next material is bonus. It's optional but it's really cool and it will only take a few minutes to go through. So far we've only used one predictor variable. In the case before, the predictor was the distance traveled, and we were using that to predict the flight cost. But often times there are many factors that can predict the dependent variable. When we use multiple predictors, we're doing multiple regression. The purpose is to explain more of the variation in y. Basically we regress y on predictor 1, predictor 2, all the way to however many predictors we want. Why don't we do a simple linear regression for each one? The reason we just have one equation is not only because it's easier but also because when we include several predictors, we can calculate the relationship that each predictor has with y independently of the other predictors. For example, in my master's thesis, I wanted to look at things that impact student effort in math class. Remember that effort is the construct. So I measured this by things like whether or not they did their homework and time spent studying. A lot of things can influence effort. Specifically, I was interested in how much students value mathematics. Did they believe that math will help them in their careers? And also how much they enjoy math class. And their relationship with their teachers. It's possible that having a good relationship with their teachers can make them enjoy school more. And if they enjoy school more, they'll put forth more effort. Or maybe students place value on subjects that they enjoy. When we include all of these as predictors we no longer have a simple slope. Instead we get regression coefficients for each predictor variable. These will be numbers. Just like how before when we had one variable we found the slope and that was our regression coefficient. In this case we'll have three. These coefficients tell us the rate of change in y, given a one unit change in each respective variable holding the others constant. In other words we see the mathematical influence of one variable while statistically controlling for the influence of the other variables. We could go into more details but we won't. Multiple regression is not usually covered in elementary statistics courses. But it's still valuable to know about, and be able to interpret. Apart from the regression coefficients, most programs that conduct multiple regression will also provide a multiple correlation coefficient called r. This is similar to interpretation to Pierson's r. But it involves one outcome or response variable and more than one predictor variable. This tells us the strength of the relationship between y and the combined set of predictors. Usually we're more interested in r squared, which tells us the proportion of variability in y explained by our set of predictors. Here's an example of a study that Dr Laraway did. He looked at the relationship between religiosity and self-esteem, as the two predictor variables, with the number of days in the last month that college students reported drinking alcohol. Just for some definitions, religiosity refers to a person's overall level of religious feelings, practices and attitudes. Self esteem refers to what extent a person feels that he or she if valuable, good or worthy. Higher scores for these two predictor variables indicate higher levels of these characteristics. His sample included 203 participants. When doctor Laraway did the regression, he got that the y intercept was 2.763. This tells us the predicted number of days that the participants used alcohol in the last month, when their self esteem and religiosity were both zero. Here are the regression coefficients that he got. Based on these, fill in the blanks. Each of these blanks are numbers multiplied by the variable. The regression coefficients show how the dependent variable changes with the respect to each of these independent variables. And therefore the regression coefficients are multiplied by these values. Whatever they are. Let's go in depth a little bit more. Let's say someone has a religiosity score of 5 and a self-esteem score of 7. What is the predicted number of days in the last month that this person drank alcohol? We just plug these into our regression formula. Our predicted number of days then will be 2.763 minus 0.149, times 5 plus 0.093 times 7, the self-esteem score. We get about 2.7. So, we predict that a person with these scores would have drunk alcohol almost three days in the last month. Moving on, Dr Lararway found that the regression coefficient for religiosity was statistically significant, with P less than 0.001. And he found that their regression coefficient for self-esteem was not significant, with probability equals point 0.6. This means their appears to be a relationship between which of the following. None, one or both may be correct. The only regression coefficient that's significant is for religiosity. Therefore, that's the only one that we can conclude has a relationship with days of alcohol use. In this case, we would reject the null, but since the regression coefficient for self-esteem is not significant, we would fail to reject the null. And there is no evidence that there is a relationship between self esteem and alcohol use. Note also that since the regression coefficient is negative, that means that increases in religiosity are associated with decreases in the number of days students drank alcohol, but does this mean that religiosity causes college students to drink less alcohol? Not necessarily. Remember, these data are purely correlational. We can't make causal statements using these data, for the reasons described at the end of lesson 14. Remember, correlation versus causation? Anyway, there are many variations of multiple regression. Including ways of adding or dropping predictor variables. Deciding what variables should be included in the final equation, types of criterion variables, etcetera. It's often difficult to account for all the possible factors that could influence the dependent variable. Maybe if you go on to more advanced statistics classes, you'll get to learn those methods, but now you have a basic understanding of a very complex topic. Great job! Linear regression is pretty complicated in general. There are a lot of things to learn. So finally, we've included some links to a lot of great applets for understanding correlation and regression. You;ll see them in the instructor notes section. So play around with them if your interested. And keep up the great work. Here's a list of the major things you've done so far. The z-test, t-test, ANOVA, or f-test, correlation and regression. And all of this has used numerical data. We've been able to take the average and standard deviation. Calculate our test statistics and do hypothesis testing. To start off this lesson, we're going to go a little deeper into numerical data. There are three different kinds that we want you to pay attention to. So I'm going to ask you a few questions just to get you thinking. Here are three examples of data: percentage correct on a test, finishing order in a race and temperature in degree Celsius. So, just imagine for example that we had 100 values for each. We have 100 students, and we know what percent each of them got on a particular test. Or we had 100 runners, and we notes who finished first, second, third, etcetera, all the way to 100. And then we have, let's say, 100 different cities around the world, and we note their average daily temperature and degree Celsius. For each of these data examples, indicate which characteristic fits. Are the data a simple rank. Or are they ranks with equal intervals? Meaning that the distance between a value of five and six, let's say, is the same as the difference between eight and nine. Or, are they ranks with equal intervals and an absolute zero? Meaning that there can never be a value less than zero. This is kind of a tricky question, and hopefully I explained it adequately without giving away the answers. Give it some thought and make your best guess. Let's start with this one, finishing order in a race. The first person crosses the finish line so he or she is number one. The second person might cross 30 seconds later, let's say. That person is number two. The third person might cross immediately after the second person. But it doesn't matter that person is still number three. It doesn't matter the spacing in-between when they finish the race. They'll still be assigned that rank. Therefore there are not equal intervals between the rankings and their is no absolute zero because their can't be a zeroth ranking. Let's go to percentage correct on a test. If one student gets a 100%, another gets 90%, another student gets 87%. Then their definitely be some kind of rank. The person that got a 100% scored first place in the class. But there are also equal intervals. A person who scored 100%, scored one percentage point more than a person who scored 99%, and so on. Their score on the test makes sense in relation to each other. We know how much better one students did than another. Unlike finishing order in a race, we just know the ranking but we don't know how much faster one runner finished the race than another. And there's also an absolute zero. A student can score 0% and get nothing right. But here she can't score any lower than that. However temperature and degree Celsius can be zero and it can be lower. Zero is kind of an arbitrary value that we've assigned to a particular temperature. But just like percentage correct on a test, there are equal intervals between the values. The value by itself makes sense in relation another value in this data set. We call this type of data with just ranks ordinal data. There's a clear order to the data but the distance between the first and the second is not necessarily the same as between the second and the third. Or the third and the fourth, etcetera. Temperature and degrees Celsius is an example of interval data. The spacing between the values are equal but there is no real zero. The kind of data that has ranks with equal intervals and an absolute zero is called ratio data. Now that you know about ordinal, interval, and ratio data lets take a quiz where you decide which type of data these examples are. Lets put each of these types of data into context. Lets say you're trying to create a dot plot showing any trends in revolutions throughout history so you look up when revolutions occurred. And you put dots there. Or let's say you want to figure out if republicans and democrats are more likely to major in the sciences, humanities, or business. Or maybe you want to find out if rats that have learned one maze will learn a second maze more quickly than rats that never did a maze. In this case you'd look at the time it takes for each type of rat to finish the maze. For the final example, maybe you want to determine if your year in college influences your reaction to stressful stimuli. So you take a sample of freshman, sophmores, juniors and seniors And you look at each group's overall reaction to stressful situations. One of more of these maybe not be either ordinal, interval, or ratio. Keep that in mind when you do this quiz. Years is interval data. The time spent between the year 1848 and 1849 is the same as that between 2000 and 2001, but there's no absolute zero. We had the year zero, and we had the year 200 B.C. Etcetera. College majors of Republicans and Democrats are not ordinal, interval or ratio. This is nominal data, or categorical. And we're going to focus on this type of data throughout this lesson, after we finish talking about the different kinds of numerical data. The time it takes for rats to finish a maze is ratio data. They're are equal intervals between values, but length of time in seconds or minutes, or whatever has an absolute zero. Finally, level and college is ordinal data. Freshmen preceed Sophmore which preceds Junior which precedes Senior All the tests we've done so far are called parametric tests. I just felt like writing that in cursive. They test hypotheses and make assumptions about the parameters of the population, mu and sigma. But what happens if we ask a question or take a measurement using a scale that's not interval or ratio? For example, what if I asked 100 people, Yes or No? Or what if I asked 100 men and 100 women if they prefer the beach or the mountains. We can't say something like the average favorite vacation spot is the beach. Instead, we use frequencies and proportions to describe these data. In this lesson, you're going to learn hypothesis testing techniques that do not require parametric information, like mean and standard deviation. These are called nonparametric tests. I don't know why it's so fun to write parametric in cursive. I think I'll just keep doing that. The nonparametric test you'll learn today is the chi-squared test, where chi is another Greek symbol that looks like an x. Let's learn about the chi-square test with an example. This is Mount Shasta, and this photo was taken by Professor Rogers. Mount Shasta is the solitary giant of Northern California. And it rises 14,179 feet or 4322 meters. It's the fifth highest peak in California. Doctor Rogers would like to successfully climb Mount Shasta this summer, but he needs statistic to help make some choices. According to summitpost.org, only about 33% of the 15,000 summit attempts each year are successful. Being an amateur mountaineer, Doctor Rogers plans to make his summit attempt with the aid of a professional guide company. He's considering a guide company called Summit Up. I made that up, but let's go with it. And they state that of the 100 trips they had last year, 41 successfully reached the summit. Does this represent a significantly better success rate than would be expected by chance? In other words, should he hire this company? What do you notice about these data compared to the types of data sets we've used before? Select all that apply. All of these are true. This is all the data that we have. There's no way to calculate a mean or standard deviation. And the data are based on frequencies and proportions. The proportion of successful attempts out of unsuccessful attempts for Summit Up. And for the whole population of everyone that has attempted to climb Mount Shasta. The data is nominal. Successful versus unsuccessful. And finally the data are not based on normal distributions. They're just frequencies. If we go into this point a little bit more, what would our raw data look like? If we could document each attempt to climb Mount Shasta. The first, second, third, all the way to n. the result would either be success or failure. Rather than writing out n success or failures, we'll just count the number of successes and failures and either write them as a frequency or proportion where the proportion of successes plus failures has to add to 1. So, what do we need to know to make our decision about this guide company, Summit Up? Well let's start with what we would expect, based on what Summitpost.org told us about the number of successful attempts. Let's pretend that we don't know how many successful climbs Summit Up has had. Based on summitpost.org's statistics, how many attempts would we expect to be successful for Summit Up out of 100 attempts? And how many attempts would we expect to be unsuccessful? Since we see that for the whole population, 33% of attempts are successful. Then out of 100 attempts we would would expect 33 to be successful, and 67 unsuccessful. This not only goes for Summit Up, but basically if we were to pick any guide company. We would expect to reach the summit 33 times out of 100. But we know that Summit Up has had 41 successful climbs out of 100 attempts. So what's our observed frequency? Enter both the successful and unsuccessful frequency out of 100 attempts. Observe frequency. In other words, what we observe of this particular guide company, Summit Up, is 41 successful attempts and 59 unsuccessful. Now we want to find out if this is significant. Do we really have a better chance of climbing Mount Shasta with Summit Up guide company? Just like with all tests, we have a null hypothesis. If there's nothing special about this guide company, Summit Up, meaning they have no effect on our likelihood of a successful summit attempt. Then we would predict our chances of success to be the same as that for the population. That means that our null hypothesis is basically our expected frequency. We could turn this into percents as well. If our null hypothesis were true and the guide company provides no added value to our likelihood of success. Then we would expect a successful outcome about 33% of the time and an unsuccessful outcome 67% of the time. Basically the null hypothesis states that there is no difference from our known population. Before we continue with our example, let's explore the null hypothesis a little more because it changes depending on the situation. Let's say we're asking if people prefer Coke or Pepsi. What do you think our null hypothesis would be? Type in the percent for each. If we don't know if people prefer Coke or Pepsi, then our null hypothesis would be that there's no preference. 50% prefer Coke, and 50% prefer Pepsi. Let's do another one. There are 2,000 people at the rodeo. We want to answer the question, if there are more males or females. So, what do you think our null hypothesis would be? Similar to the last question, if we're not sure, then we'd assume that there are equal numbers of males and females. Since there are 2000 people and we're guessing that there will be the same proportion of males and females, then our null hypothesis will be that there are 1000 males and 1000 females. Let's do one more. Let's say we asks 200 people what genre of music they liked best, rap, pop, house, or country. Our null hypothesis again assumes no preference. Out of 200, how many people would you expect to choose each one as their genre of choice? We would assume for our known hypothesis that equal numbers of people prefer each genre. 50. 50. 50. And 50. The null hypothesis provides our expected proportion. And when we know the total number of subjects, in this case 200, the null hypothesis provides the expected frequency. These are the values we would expect to find, if there was nothing different about each one. If one genre of music were not more popular than another. Note that when we calculated these frequencies, we found them by taking the expected proportion times the total number. Our expected proportion in this case would be 1 fourth, or 0.25, since there are four categories. Let's get back to our Mount Shasta example. In this case, remember our null hypothesis would be based on the population. So knowing which null hypothesis to use, it can be tricky. You'll continue to keep building up an intuitive sense of what the null hypothesis should be, depending on the situation and the question you're asking. In our example, we now know what kind of success to expect based on the null hypothesis. And we also know what kind of success the guide company observed last year. And now we need to test how well these obtained sample frequencies or proportions fit the population proportions or our expected frequencies. This is called the kai squared goodness of fit test. You might be wondering about the alternative hypothesis. The alternative hypothesis is basically that the null hypothesis is not true. In this type of test we really only need to use numbers for the null hypothesis. As usual with the tests you've learned about, there's a test statistic. And it's symbolized by kai squared. Before we get into the details of this test statistic, think about it yourself. How would you statistically show that the observed frequency is greater or less than the expected frequency? Would you subtract one from the other? Would you add things? Multiply or divide things? I'm being very vague. But before you move on to the next video, maybe pause and think about how you would construct a statistic such that if the observed frequencies are very different from the expected frequencies. We'd get one number that's bigger than if the observed and expected frequencies were similar. When we calculate the chi squared statistic, we take the observed frequency, f for frequency, sub o for observed, minus the expected frequency. Square it, divide by the expected frequency, and then sum them up. Because we have more than one observed and expected frequency. In this way, we're looking at the difference between each observed and expected value. We square it to get rid of the negatives. And then we make it a ratio of the expected frequency. Now, is chi squared larger or smaller when expected values are closer to the observed values? If expected values are closer to the observed values, then this difference will be less. Therefore, chi squared will be smaller. In the case of our Mount Shasta example, if our observed frequency is closer to the expected frequency, we'd be more inclined to think that Summit Up is not that much better than any other guide company. If all observed values are equal to our expected values, then what would our chi squared statistic equal? In this case, this difference would be 0. And 0 divided by anything is still 0. If you add up a bunch of 0's, you still get 0. Let's keep exploring this formula to shed insights on how the distribution of chi squared values would look like. Chi squared values are always, sometimes, or never negative. And therefore what would that make a chi squared test? Non-directional or one-directional? If we subtract expected values from observed values, we could get positive or negative. But then we square it, so the numerator will always be positive. Frequencies are also always positive. We can't get negative ten of something. Negative ten successful climbs. That doesn't really make sense. So, all of these values will always be positive. And then you'll just sum up positive values. Therefore, chi-squared statistics are never negative. And just like the F-test, this means that it's one-directional, in the positive direction. This is what the chi-squared distribution looks like. And again, we have a chi squared critical value here denoted by chi squared star. That cuts off a certain probability. Remember that the total area under this probability distribution is 1 or, 100%. And just like we've been doing with all of our other tests, we want to find the critical value that cuts off a certain probability as defined by our alpha level, 0.05 or 0.01. In this class we've been using 0.05. Also just like the F-test, in addition to being one-directional, the chi-squared distribution changes with different degrees of freedom. Let's explore why. You learned that the chi-squared statistic adds squared values, so you can never have a negative chi-squared value. When the null hypothesis is true, you expect that the observed values are close to the expected values. Which means in general the chi squared value will be smaller than if the observed frequencies are really different from the expected frequencies. But, as we add more categories, the more or less likely we'll get a larger chi squared statistic. Which one? if we have more categories, then we have more of these to add up. We have this ratio for the first observed and expected value. We add the ratio for the second observed and expected value, all the way to n. On average, we'll get a larger sum for ten categories compared to two. So in general, the more categories, the more likely we'll get a larger chi squared statistic. So then what does this have to do with the shape of the distribution? Well, here's kind of a tough question. Which chi square distribution, this one or this one, has more degrees of freedom? And why do you think this is? The more categories we have, the more degrees of freedom we have. And the larger our chi-squared statistic will be. We don't want to reject the null, in other words decide that the observed values are significantly different than what we would have expected. Simply because there are a lot of categories. Therefore, the more categories we have, we need a higher critical value in order to reject the null hypothesis. For this distribution, the critical value for 5% in the tail is further to the right. So this distribution has more categories and therefore more degrees of freedom. All chi-squared distributions are positively skewed, with the skewness decreasing as the number of categories and therefore degrees of freedom increases. So you see that this red one, it has nine degrees of freedom and the skewness is a lot less than for the chi-squared distribution with one degree of freedom, this yellow one. As the degrees of freedom increase, the chi-squared distribution better approximates normal distributions but they never become perfectly normal. Let's get back to our example for Mount Shasta. We have our observed and expected frequencies, so try calculating the chi squared statistic. It doesn't matter which order we go in. Our observed frequency of 41 minus our expected frequency of 33 squared divided by 33, plus our observed frequency number two minus the expected frequency divided by the expected frequency, is about 2.89. Good job. Now that we have our chi squared statistic, we need to compare that to the chi squared critical value, just like we've done with all the other tests. Just like with the t test, the f test and correlation, and regression, to do hypothesis testing we need to find our degrees of freedom. What do you think the degrees of freedom will be in this case? You've seen something like this before in lesson 10. As a hint, let's pretend that we don't know any of the observed direct expected frequencies and we just know the marginal totals. How many of these four boxes could we choose any way we want, such that we can still have these marginal totals? In instructor notes, I put a link to this quiz in lesson ten, if you want to review that before answering this quiz. We could put anything here, lets state 80. But then these three values are forced. Therefore, there's only 1 degree of freedom. I said before that we can't have negative frequencies, and we can't. I just use this to illustrate the idea that we only have one degree of freedom. In the case of goodness of fit, this is the same as the number of categories minus 1. In this case, we only have two categories, successful and unsuccessful. And therefore we have 1 degree of freedom. Now use GraphPad QuickCalcs to calculate the two tailed p value. The link is in instructor notes. You need to calculate p given the chi squared statistic. You'll also need the degrees of freedom. After you calculate p, tell me if this is significant at alpha equals 0.01. Significant at alpha equals 0.05, or not significant at either level. Let's open GraphPad QuickCalcs. We want to calculate p from the chi square distribution. Again, we'll push continue, and we'll go to the bottom where we input the our chi squared value and the degrees of freedom. If we compute p, we get that our p value is 0.0891. This is not significant at 0.01 or at 0.05. Since our industry standard is an alpha level of 0.05 and this does not meet that threshold, then we're going to guess that there is no reason hire Summit Up over another guide company. While this would be significant at an alpha level of 0.01 let's say, some might consider this marginally significant. But this is a %9 chance of committing a type one error. Meaning that if we don't conclude that Summit Up is better than average at getting climbers to the top, there's about a 9% chance that we're wrong and that these differences are just due to chance. We're just going to make our decision based on if we meet this threshold of 0.05. Chi-square test for goodness-of-fit look at how well our observed values match our expected values for a certain variable. For example, this variable could be your response to the question do you like ice cream? Response one would be yes. Response two would be no. But of course there might be a whole number of responses for a particular variable like which season do you prefer? In that case there would be four responses, spring, summer, winter, fall. But chi-square tests can also help us determine whether or not two variables are independent. In a chi-square test for independence, instead of just having the observed and expected values in the two rows, we'll be looking at the number of participants or subjects who answered response one for variable one. And response one for variable two or the number who answered response two for variable one and response one for variable two, etcetera. We're going to illustrate the chi-square test for independence with a real research example. To get us started, imagine you're just about to cross the street at an intersection when 2 cars get in an accident right in front of you. A week or so later, you get a call about the accident from one of the drivers' insurance companies. On a scale of 1 to 10, 10 being the best, how well do you think you could remember all the details of this accident? It turns out people are pretty bad at remembering facts. The Lincoln instructor notes will take you to a video that explains just how bad people are at remember facts. Which is especially important when they are witnesses of a crime or accident. It turns out that the way questions are worded influences our memories. This video shows some research from Elizabeth Loftus. Who has made a career out of demonstrating the fallability, and malealbility, of eyewitness testimony. It will provide some background info for our next example, so pause this video now, and click the link. After watching this clip, you should know the background and reasoning for the study. For the purposes of this next example, we'll simplify it a little bit. We just want to know if the wording of a question influences how well people remember details. So let's get started. In an experiment to see how wording affects people's memory, 150 students from the University of Washington were shown a one minute film clip of a car accident. After the students watched the clip, they were separated into three groups. Group one was asked, how fast were the cars going when they hit each other? Group two was asked, how the fast were the cars going when they smashed into each other? And group three was asked nothing about the speed of the cars. After one week all the students were asked, did you see any broken glass? There was no broken glass in the film, but seven out of 50 students reported seeing broken glass in group one. 16 out of 50 in group two. And six out of 50 in group three. Let's summarize our results in a table. We know that 50 students were assigned to each group making 150 total. But fill in the frequencies for the number of students who said no, they did not see broken glass. And the marginal totals for the yes and no responses. Since each column has to add to 50, these are the frequencies for the students that said no in each group, and these are the marginal totals. Our goal now, is to see if there's independence between these groups: hit, smashed and control. By independent, we mean there's no consistent predictable relationship between them. Our null hypothesis is that students' response for whether or not they saw broken glass, is independent of the wording used in the question. Whether or not hit, smashed, or no question about the speed of the cars was used. Now, what we have to do to show independence is to compare our observed values, which are these, to the expected values. Calculating our expected frequencies is actually pretty tricky. They're not only based on the number in each group, but also the marginal totals for their response. Try and figure out the expected frequencies yourself. But first, here's a hint, 29 out of 160 students said that they saw broken glass. So in the hit group, how many would we expect said yes out of 50? Similarly, 121 out of 150 students said no. So just in the hit group, how many would we expect would say no out of 50? Give it your best shot. The proportion of students who said they saw broken glass is 29 out of 150, which is about 19%. There was 50 students in the hit group so we would expect that this proportion of 50 would say yes within that group. This comes out to 9.67 even though it's a decimal and we wouldn't expect 2 3rds of the students to say yes, this is still our expected frequency. We do the same thing here. If we expect 29 out of 150 to say yes, then we would expect how many out of just 50? This is just another way to look at the same thing we did here in the hit group. This is the same as 29 divided by 150 times 50 which is the same thing. 9.67. This will also be 9.67. Note that we could have put different numbers of students in each of these groups. We didn't have to divide them evenly. In the problem set you'll get more practice with unequal groups. Now for no, we expect 121 out of 150 to say no. And so out of a group of 50, we would expect 121 divided by 150 times 50 to say no. This is 40.33 and this will be the same in all of them since each group has 50 students. To simplify things, we can calculate the expected values by simply doing the column total times the row total divided by the grand total where the grand total is the total number of people involved in this study. However its really important that you understand intuitively how to calculate the expected frequencies. Don't just use a formula because I give it to you. Its better to understand it than to plug and chug. By plug and chug, I mean you plug values into the formula and you get something out, but without really understanding what you're doing. Hopefully I explained it enough to where you understand it intuitively. But if not, feel free to write in the forums. You can also email me because I email you every week. So you should have my email address. Now that we have our observed and expected values, let's calculate the chi-squared statistic. Remember that this is the sum of each observed frequency minus each expected frequency squared divided by the expected frequency. Since we're squaring this, it doesn't really matter which order we do the subtraction. We could subtract the observed frequency from the expected frequency instead. Anyway, go ahead and calculate this. You have 6 observed frequencies, and 6 corresponding expected frequencies. I won't write all of them out, but I'll just get them started. For this first group, we have observed minus expected squared divided by expected. We don't have to go in any particular order since addition is commutative, but it does help to go in some kind of sequence so that you remember which one you have used in the equation and which one you haven't. If we do this one next, we get 16 minus 9.67 squared divided by 9.67. And we'll go all the way to 44 minus 40.33 squared divided by 40.33. This comes out to about 7.78. Now, let's calculate the degrees of freedom. Again, let's pretend that we don't have any of these values, but we do have the marginal totals. How many values here of these six can we choose, so that we'll always have the same marginal totals? The answer is two. This one and this one. Notice that this is the same as the number of rows minus 1, times the number of columns minus 1. There are two rows and three columns. So we have one times two, which is two. Now, you can either use the chi squared table or GraphPad QuickCalcs to decide if we should reject or fail to reject the null. There's a link to the chi squared table in the instructor notes. I'm not going to tell you how to use it, but I think you can figure it out. Or you can just use GraphPad QuickCalcs to calculate the p value. Use the alpha level of 0.05 in making your decision. Alright, let's bring up the chi squared table. It says at the top, probability of a larger value of chi squared. We want the probability of getting a larger value than the critical value to be 0.05. With two degrees of freedom, our critical value is 5.99. Therefore any chi squared statistic greater than 5.99 will have a probability less than 0.05. If we think of it on the chi squared distribution, this means the probability of getting a larger value than 5.99 is 0.05. Therefore, we would decide to reject the null, since chi squared, 7.78 is greater than that critical value. But let's also use QuickCalcs. 7.78 with two degrees of freedom, we get that the two tilt P value is 0.02, which is, indeed, less than 0.05. Great job. We can conclude that the verb, smashed led to more yes responses for the question, did you see broken glass. These results are consistent with the view that the questions asked subsequent to an event can cause a reconstruction in one's memory of that event. As we said at the beginning when we first introduced the chi-square test for independence, we evaluate the relationship between two variables. In this case, the type of verb used smashed, hit or control. And the response, yes or no. In this case, we have a two by three contingency table. Two rows and three columns. You've seen on our other lessons that testing for significance is only part of the story. We should also measure the strength of the relationship between these two variables. This is our measure of effect size. When we have a contingency table that's greater than just a two by two matrix, we use Cramer's V. This is also symbolized like this, and might be called Cramers phi coefficient. This is equal to the square root of our chi square value divided by the total number of subjects in our study, times k minus 1. K is the smaller of the number of rows or columns. What would k be in this case? We have two rows and three columns. So k would be 2, the smaller of the two numbers. If we had the same number of rows and columns, then k would just equal that number. So k equals 2, and if you remember, n was 150, because we had 50 students assigned to each group. So calculate Cramer's V. Also, just so you don't have to go back and look through the videos, the chi squared value was about 7.78. Now, calculate Cramer's V. Cramer's V then would be 7.78 divided by 150 times k which is 2 minus 1, which is just 1. This comes out to about 0.23. What does this mean? Well, remember Jacob Cohen, you calculated Cohen's D before. He's a very renowned statistician. He established some guidelines for Cramer's V, which help us decide if this is a large, small or medium effect size. This table provides his standards for interpreting Cramer's V as k increases. However, Cohen is quite cautious in his suggested use of these values. He treats them more as labels to aid future power analysis rather than hard and fast descriptors. But these can kind of give us the idea that the value we got for Cramer's V is a small effect. Before we wrap up, let's go over some assumptions and restrictions for chi square tests. Just like with all tests, we have to make sure that we're adhering to certain guidelines in order to do this test. First of all, we have to avoid dependent observations. In the case of the Loftus and Palmer study, the car crash one. Independence would be violated if the participants contributed to data in more than just one of these cells. For example, if they were asked both how fast were the cars going when they smashed into each other? And how fast were the cars going when they hit each other? In other words these 16 participants are totally different from all of these other participants. Whose data is in the other cells. Another thing we should do is avoid small expected frequencies. And in general have a large number of participants. Remember that our chi-squared statistic is just based off the sample. We have a sample of observations and corresponding expected values. If we were to use this chi squared statistic to make inferences about the population that this sample is from then the total number of participants should be at least 20. And a conservative rule of thumb specifies that each expected cell frequency should be at least 5. Recall that in our examples, the observed and expected values were the observed and expected frequencies. Let's summarize this lesson. Chi-Square tests can check for how well observed values fit the expected values for categorical data. In this case we have some variable that's categorical, and there are different responses for that variable. In our first example when we were helping Professor Rogers decide whether or not he should hire that guide mountaineering company. The variables was success. Our responses were successful versus unsuccessful. In that case there were just two responses. But there can be as many as we need for a particular categorical variable. And in the case of goodness of fit, our expected values are what we guess for our null hypothesis. However in the case of independents our expected values are based on the results of our marginal totals. Lets just assign some random numbers here. We have an m by n contingency table and lets say the total number of subjects in the sample was 100. We would expect that 15 out of 100 of our subjects would have response 1, of variable 2. Therefore, we would expect that same proportion to have response 1 of variable 1. If there were 30 total in response 1 of variable 1, then we would expect 15 out of 100 times 30, for this cell. And here is what we would expect for the other cells. If it's easier for you to remember, the expected values are the column totals times the row totals, divided by the total total. And then for both goodness of fit and independence chi squared tests. We calculate the chi squared statistic by taking the sum of each squared difference between the observed and expected values divided by the expected value. Then we can use the chi squared table to determine if our results are statistically significant. That's if we're doing it by hand, but usually we'll use software, and the important part, is you know how to interpret our results. Hey, congrats! You've finished the course! I'm so glad that you stuck through with all these lessons and we hope that you've really learned something in this class. I'd love to hear how you've been able to apply these skills to your daily lives. Whether it be doing your own research project or analyzing some data that you saw in the news. Keep learning and stay udacious. Ta-ta! Congratulations on completing inferential statistics. We really hope you've enjoyed this course. You now have the skills you need to analyze your very own dataset and make accurate predictions. Now you're ready to move on to your final project, where you'll perform your own statistical analysis on a dataset. With your selected data set, you'll investigate a hypothesis of your choice, and present your findings in a short report. This report will include visualizations, descriptive statistics, test results, and an interpretation of what you've found. If you're looking for what course to go to next, maybe try out our introduction to data science class or introduction to computer science if you want to expand your programming knowledge. Both of these classes are linked in the instructor's notes box. Either way, we really look forward to seeing you again soon. Welcome back. Let's do a quick summary of what you learned in Lesson 7. So we started with a population, in this case Klout scores. The number in this population was 1,048. The mean Klout score was 37.72. And the population standard deviation was about 16.04. Then, we know from the central limit theorem, that if we took all possible samples, all of the same size, and then found the mean of each sample and then graph the distribution of those sample means, we would get a normal curve with a mean equal to the population mean and a standard deviation equal to the population standard deviation divided by the square root of the sample size. Remember that this is called the sampling distribution. In Lesson 7, we specifically looked at the sampling distribution created from all samples of size 35. So, if we take all samples of size 35 from this population, we find the mean of each sample, and then we plot the distribution of sample means. The mean of the means will be the same as the population mean, and the standard deviation of this distribution is the population standard deviation, divided by the square root of 35, which came out to about 2.71. Remember also, that the standard deviation of the sampling distribution is called the standard error. Now, if we plot this sampling distribution, it'll look something like this. One standard deviation away from the mean would be 37.72 plus 2.71 and minus 2.71. So, we know what the distribution should look like of all sampled means of size 35. So then we said, what if we get this sample of size 35 and the mean is 40, and everyone in this sample uses the app the Bieber Tweeter? The first part of this lesson is all about estimating what the mean Klout score would be if everyone started to use the Bieber Tweeter. So if everyone in the Klout population started using the Bieber Tweeter, what would be our best guess for what the mean Klout score would be? Would you say the mean Klout score would be 30, or would it be the same, 37.72, or 40, or maybe 60? Well, we can't make any conclusive statements, but we only have the statistics for this one sample, and so our best guess would be 40 since we have a sample of people who just used the Bieber Tweeter, and the mean clout score of them is 40. This value, 40, is called a point estimate, and that's because it's only one number. So this is our best guess for the population mean if everyone started using the Bieber Tweeter. So now here's a follow up question, if everyone started using the Bieber tweeter, and we actually calculated the mean klout score, do you think this mean would be exactly 40? No, just like you learn in lesson one where if you have a population and you take a sample, the sample statistic won't be exactly equal to the population parameter, most likely. There's a range of values for the mean that any samples could have. So same with this, if we had some kind of intervention on the population, so in this case, the Bieber tweeter, and everyone started using that. Then our best guess is that the mean would be about 40, but it could be anything around 40. So in other words, there's a range around 40. That the true population mean would be, if everyone were to use the Bieber Tweeter. Now how big is this interval? We're going to decide actual numbers that should be bounding this interval. In order to do that let's think back about this sampling distribution. Think back to what you've learned before in lesson six. Approximately what percent of sample means fall within what distance of the population mean? Now this is kind of a tricky question. Remember we're not talking about the population, we're talking about the sampling distribution. So what percent of sample means fall within how far of the population mean? Now you learned before that in a normal distribution, about 68% are within one standard deviation of the mean and 95% are within two standard deviations of the mean. But this is for regular, normally distributed population. Here we're talking about the sampling distribution. So each score in the data set is a mean, and the standard deviation is the population standard deviation divided by the square root of the sample size N. So 95% of sample means will fall within two times the standard error of the population mean. So, let's redraw this sampling distribution. So, if this is the mean of the population which is the same as the mean of the distribution of sample means. And let's say this is two standard deviations away from the mean which is the same as the mean plus 2 times the standard error. And this value here is the mean minus 2 times the standard error. Now, 95% are between these two values. So, getting back to this example, 95% of means are within two of these standard deviations on either side of the population mean. This distance, 2 times the population standard deviation divided by root n, is called the margin of error. So now, what does that tell us about this value of 40? If everyone started using the Bieber Tweeter, we're guessing that there'd be a new population distribution of Klout scores, but we don't know what it would be. But we do know that one of the samples of size 35 has a mean of 40. Now, if 95% of sample means are within 2 times the standard error, then this mean value has a 95% chance of being within 2 times the standard error of this new population mean, if everyone used the Bieber Tweeter. Let's zoom in here a little bit. We know that 95% of sample means will be within two standard deviations from the mean. Most likely, this sample mean here is one of the 95% that will fall within two standard deviations of the new sampling distribution, if everyone used the Bieber Tweeter. So, in other words we have this new predicted population mean. We'll call it mu sub BT for Bieber Tweeter. Our best guess for this is 40, but it'll be within an interval around 40, most likely. And we know that 95% of sample means, just of people who use the Beaver Tweeter, will be within this interval. Well, most likely, this is one of those 95%. Let's assume that this is in that interval. If this sample mean is indeed within this interval, then what could the population mean be? Well, if 40 is in this interval, we know 40 has to be greater than this but smaller than this. Do you remember how to solve inequalities like this? So, we're trying to solve for the population mean. So, let's subtract it from all three sides of this inequality. If we do that, then these cancel and these cancel. So, we're left with negative 2 times the standard error is less than 40 minus the population mean is less than positive 2 times the standard error. Now, I'll subtract 40 from all three sides of the equation. So, here, we get negative 40. And now, the last thing we need to do is to make this positive. So, we'll multiply by negative one on all 3 sides. When we multiply or divide by a negative number, these inequalities go the opposite way. When we multiply this by negative one, we get positive 40 plus 2 times sigma over root n is greater than positive population mean is greater than positive 40 minus 2 times the standard error. And now, we have this gorgeous interval. So, let's recap really quick. We know that 95% of sample means will fall within two standard errors of the population mean. We're guessing that the sample mean we have is one of those 95% that falls within this interval. And if that's the case, we're pretty sure that the population mean, then is between this sample mean plus or minus 2 times the standard error. So now, I did the hard part for you. Now, it's time for you to tell me what these values should be. That most likely contain the true population mean if all the people in our Klout population were to start using Bieber Tweeter. What would be our interval estimate that most likely contains what this population mean would be if everyone in our Klout population started using the Bieber Tweeter? So, figure out what this is and this. And just a reminder, here are our population parameters, the mean was 37.72 and the standard deviation was 16.04. And then, for our sampling distribution, the mean was 37.72, like you learned in the last lesson, and the standard deviation is the standard error, which is sigma divided by root n Which we found to be about 2.71 So you could have used either the population standard deviation or just the 2.71 for standard error. So our x bar is 40. We have 40 minus 2 times 2.71 as our lower bound, and then 40 plus... 2 times 2.71 for our upper bound, so that comes out to 34.58 and 45.42. So we're pretty sure, that if everyone started using the Beaber Tweeter, then the mean of the population, and the mean of the corresponding sampling distribution of size 35, Would be between these two values. So our distribution could be anything from here, centered at 34.58, where this value 40 is still within that 95% range. Or to here centered at 45.02, where our mean is still within that 95% range. This interval from 34.58 to 45.42 is called the 95% confidence interval for the mean. So now, remember that we said approximately 95% of sample means will fall between this interval. 2 times sigma divided by the square root of n is approximately our margin of error. But let's now use exact values. So, in any normal distribution where the mean is mu, we know that approximately 95% of the data is within two standard deviations of the mean. But what's the exact number of standard deviations that bounds 95% of the data? So let's bring up the Z table. So here's our Z table, area under the normal curve. And we want this probability here. Well first of all, if 95% is here and the total area under the curve is 100%, then that means 2.5% is in this tail and 2.5% is within this tail. So we want to find out what this Z value is when there's 2.5% less than that Z value. So let's bring up the Z table. We want this gray region to be 0.025, or 2.5%, so if we try to find 0.25, we see that it's here, at a Z value of negative 1.96. So this here is negative 1.96, and this total area up until this Z value is 95% plus 2.5% which is 97.5%. So let's look for 0.975, and we see that it's here, at positive 1.96. If we found that the Z score here is negative 1.96, then since this is symmetric and mu is 0 standard deviations from itself, then we know that this would be positive 1.96, but I just wanted to show you, using the z table, how they're symmetric. So now, let's say that this is a sampling distribution. Because of this, we know that 95% of sample means fall within 1.96 standard deviations, or in this case standard errors, from the population mean. This is exactly the same concept you learned before, except before we were estimating, about 95% or within two standard deviations of the mean. But now, we are just going to use exact values and we're going to do that from now on. So, exactly 95% of sample means fall within 1.96 standard errors from the population mean in a sampling distribution. So now, this time using these exact values, let's calculate the confidence interval for the mean if the entire population of Klout users used the Bieber Tweeter. And just to refresh your memory, we took a sample of Bieber Tweeter users of size 35 and found that their mean Klout score was 40. If we assume that this mean score is one of the 95% that lies within 1.96 standard deviations from the mean, then what's the range of possible values that the population mean could be? And remember, this is the would be population mean if everyone used the Bieber Tweeter. So, what's going to be your lower bound and what's going to be your upper bound? Give it your best shot. Now, this is an incredibly tough question. Even though we've gone over this before, and I've taught you how we compute this, it is pretty difficult. And it takes some time to really grasp what's going on here. So, we're going to go through this again. Bear with me, and you'll get it eventually I promise, if you don't already. We know that 95% of sample means will be within 1.96 standard deviations of the population mean, which is also the mean of sample means. Now, let's just zoom in on this part over here. This sample mean doesn't change. It's there. But if it's going to be one of the 95% of sample means that falls within 1.96 standard deviations from the mean, then the population mean can range from here to here. So, this here is the range in which the population mean of everyone who uses the Bieber Tweeter is most likely to fall. So, you see that this range is 40 plus 1.96 standard deviations and 40 minus 1.96 deviations. And we know that since this is the same shape as our sampling distribution, that our standard deviation is the population standard deviation divided by the square root of n. I know I just said a lot of confusing things, so let's write it out. So her,e we should have 40 minus 1.96 standard deviations, and then 40 plus 1.96 standard deviations. And we know that the standard deviation is our standard error, okay? So, we already know that our standard error is 2.71. So, you have 40 minus 1.96 times 2.71, and 40 plus 1.96 times 2.71. So this is about 34.69 to 45.31. So, our confidence interval for the population mean is something like this. So, if everyone were to use the Bieber Tweeter, our point estimate is 40, and our interval estimate is our 95% confidence interval. So now let's put this in general terms. Let's say we get some sample mean, X bar, after an intervention some sort. So in the case of our example we've been using the intervention is the Bieber Tweeter and we want to find the population mean after the intervention. And it can be anything ranging from here to here. So long as this sample mean is one of the 95% within 1.96 standard deviations of the mean, and remember that one standard deviation is the standard error. The population standard deviation divided by root n. So first of all what's our point estimate? Is it mew sub i, mew, 1.96, or z bar sub i.? Well if we take a sample of size n of just those who have been applied this intervention. And then we take the mean. We get our point estimate, and that is x bar sub i. Okay, what's our interval estimate for the mean of the population after the intervention? Mu plus 1.96, mu minus 1.96 to mu plus 1.96, this, or this, think about it carefully. Well, this value here is just a point. It's mu, which is what we already know, the mean clock score, 37.72 in our last example, plus 1.96. It doesn't really make sense to add a z score, so this value is not an interval and it doesn't even make sense. Here, we're getting a little closer because now we have an interval. But as I said before, it doesn't make sense to just subtract a z score because this is the number of standard deviations away from the mean, so we want to find the value along the x-axis for this interval. This one looks more promising. We have our sample mean. And we subtract 1.96 standard deviations, and since the standard deviation is sigma divided by root n that would be this value here, and then if we add 1.96 standard deviations that would give this value here. And that is our confidence interval. So, we found the answer. But let's explore this one anyway. This one we can't even figure out what it is because we don't know this population mean after the intervention. So, we're not able to compute this. If you selected this one, then you were pretty close. So, good job. So let's get back to our Klout score example. Based on the fact that our sample mean was 40 of everyone who used the Bieber Tweeter our population mean could range from 34.58 to 45.42. And we found that the original population mean 37.72, is well within this interval. Basically this means that a sample of size 35 with a mean of 40 could have been selected by random chance. It's very possible. But what about in our second example from lesson 7 where we got a mean Klout score of 40 again but with a sample size of 250. Now our sampling distribution becomes a lot skinnier because we have a much bigger sample size. So this is about 1.01. So now if we a 95% confidence interval for what the mean Klout score would be if everyone used the Bieber Tweeter, will the range be bigger or smaller than the one you calculated when the sample size was 35. it'll be smaller. With 40 as our point estimate we want this to lie within two standard deviation of the true population mean, whatever it is. And now, a standard deviation is less then half the size that it was before, before it was like 2.71. So now, try to construct this confidence interval for the mean Klout score if everyone used the Bieber Tweeter. Let's call this mean Mu sub BT like we have before. So, let's again look at how we solved for the last confidence interval. We still know that the true population mean, Mu sub BT, is going to be within 1.96 standard deviations of our sample mean. So, our sample mean is 40. And if we subtract 1.96 standard deviations, we subtract 1.96 times 1.01. So this is 38.02, approximately. And then, to find the upper bound of this confidence interval, we just add 1.96 standard deviations from the mean. So, this is about 41.98. It's pretty simple when we lay it out like this, just generically x bar plus or minus the z score, in this case 1.96 since we want a 95% confidence interval, times the standard deviation. But, really understanding the concept of the confidence interval is really complicated. So, if you need to, keep replaying these videos. Keep thinking about it. Don't worry. It took me a really long time. I'm still completely understanding this stuff. It's complicated and it requires some thought and patience. So now, we have this confidence interval and that ranges from about 38 to 42. So, it's a lot smaller than the one we had calculated when we were basing it off of a sample of size 35. Bigger samples give us more precise estimates by giving us smaller intervals because they shrink the standard deviation of this sampling distribution. So, they shrink the interval in which a sample mean might feasibly lie. Now, notice that this confidence interval doesn't even contain the original population mean 37.72. Therefore, it's very unlikely that a random sample of size 250 with a mean of 40 could have been selected by chance. This is evidence for treatment effect. So, with a bigger sample size, we have a smaller interval for which we're pretty sure the true population lies. So, with a 95% confidence interval with the sample sizes being 35, we had a pretty big interval. But with a 95% confidence interval with sample sizes being 250, we had a much smaller confidence interval. The greater our sample size, the smaller our confidence interval for the true population perameter after the intervention. And the better we are able to estimate our population parameters. We're usually content with having a 95% confidence interval, but what if we wanted to be more sure that the population mean was within this interval? So, what if we wanted a 98% confidence interval? So, let's do the same thing we did last time. We wanted to first find the z squared values that encompass 98% of the data. So, I took out all the 1.96 values here, here, and here. So, how bout you write this z score value here? And remember you're trying to fill in the blank right here. So, the first thing you probably did was note that there's 1% here, and 1% here, so that the total area under the normal curve adds to 100%. So, now let's bring up the Z table. So we're looking for a proportion of 0.01 that's under the curve to the left of the Z score. Our closest bet is right here, .0099. So, that's the closest value in the Z table to .01 or 1%. And that corresponds to z square of negative 2.33. So, this z squared value is going to be negative 2.33 and you should have written here positive 2.33. So, 98% of sample means fall within positive 2.33 standard errors from the population mean. Okay? So if we know that 98% of sample means will lie between negative 2.33 standard deviations away. And positive 2.33 standard deviations from the mean. Then what's going to be our interval estimate, or our confidence interval, for the population mean with the treatment? In this case again the Beiber Tweeter app. So this is going to be from what value to what value? So now, we're assuming that this sample mean is one of the 98% that falls within 2.33 standard deviations of the population mean, in this case Mu sub BT. And if that's the case, then Mu sub BT must be, in turn, within 2.33 standard deviations of this sample mean. So, the sample mean minus 2.33 standard deviations, which is 1.01, will be our lower bound for this confidence interval. So, this comes out to about 37.65, and then our upper bound for the 98% confidence interval be 40 plus 2.33 times 1.01. So, this is 42.35 approximately. So basically, we got the sample mean 40, and we decided that it's possible that it's either here or here on the distribution, such that 1% of the data is either above it or below it. Before, with the 95% confidence interval, we said most likely it's going to be a little bit closer to the mean, so that 2.5% of the data is above it and 2.5% is below. But now, we're being a little more lenient. We're allowing this sample mean to be a little bit further from the population mean. And, so now, we have a slightly bigger interval. But now, we're more sure that the true population mean will be in this interval. Recall that before the 95% confidence interval was from 38.01 to 41.99, so it was a little smaller than this. Good job. Positive and negative 2.33 is called the critical value of Z for 98% confidence. And if you remember, positive and negative 1.96 are the critical values of Z for 95% confidence. If you worked your way through lesson 6 and really understood it, then finding the critical values of Z should be pretty easy for you. But it's still a pretty difficult conceptual idea. So don't get too discouraged if you're finding this pretty confusing. I would suggest watching these videos and pausing them all the time to really think about what I'm saying and writing. There are also a lot of really important definitions. So you have to remember what a Z score is, and what the standard deviation is, the sampling distribution. And you have to remember that the standard deviation of the sampling distribution is called the standard error. And then you need to be able to express values of sample means in terms of standard errors away from the population mean. If you can do all those things and you understand these concepts, then you'll be fine in this lesson. If not, do go back and review those videos. And do the problems that's in quizzes. Because that will really help you. Let's do one more example of where we would use a confidence interval before we move on. Well, hopefully you're enjoying the class so far. How would we measure this? Maybe one way is to look at the number of minutes that each person has watched of the videos. Divided by the total minutes that are available we could call this something like the engagement ratio because this provides some indication of how engaged you are. If you're watching more video then you are probably more engaged. As of March 11th, when I'm recording this, there are 8,702 students enrolled in the course. I made up this hypothetical data set the link is below, unfortunately I couldn't get the actual data for every student, but I think the shape is pretty close to what it really is. And that's just the nature of the mook. A lot of people sign up because it's free and easy, but they don't progress through the course or they just sign up with the intention of doing it later. However if you're taking the course for credit, which I think about a hundred of you are, you're probably up here on the distribution, making your way through the course to get credit for it. So first of all, calculate the mean and standard deviation for this data. This is a pretty big data set, so try to make use of the keyboard shortcuts that you've learned before. Treat this as a population and round to the nearest thousandths or three decimal places. OK, so here we have our data. And from cell A1, if we push Cmd down, or on a PC I believe it's Ctrl down, then we see that there are 8,702 students. So first let's calculate the average. So again, equals. Average, cell a1 colon a8,702, enter. So we get that the engagement ratio is about 0.077, if we round to the nearest thousandth. Okay, now that we have the mean, let's figure out the standard deviation. And we don't use a correction when we're treating this a population. So first let's find the square deviations from the mean. Equals a1 minus $d$1. To keep this mean constant. Because otherwise if we just put D1 and then we copied this cell for all of column B then it would subtract D2 from A2, and then D3 from A3, and there's nothing there. We want to always subtract D1 and then we'll square this quantity. So there's are first square deviation. We'll copy this, Cmd+C or Ctrl+C. We'll go to cell A8702, go to B8702, Cmd shift up arrow key. All of column B is highlighted. Cmd+v. Now we have all our squared deviations. Now let's take the average square deviation, which is the variance. Equals average B1 colon B8702. So here's what we get for variance. And now for standard deviation we'll take the square root. Equals sqrt D2. So if we round the standard deviation to the nearest thousandth, we get 0.107. So here are our population parameters. Let's say that I want to bring up this ratio. So I want more people to watch more of the lessons, so I'm going to spice things up. Well in lesson nine you're going to learn how to formally determine whether or not something had an effect. This is called hypothesis testing. And I'm thinking about writing a song on how hypothesis testing works and then singing it for you in the next lesson. Like, for example, do you remember learning the quadratic formula? X equals negative b plus or minus the square root of b squared minus 4ac. All over 2a and those are the roots of a quadratic function. Some people learned it in middle school as a song and maybe that made learning it a lot more fun. So I'm thinking about writing a song on hypothesis testing. And by the way, this is no joke, I'm seriously considering this. However, I don't want to run the risk of a song hurting rather than helping your engagement with the course material. So, maybe I'll create the next lesson with the song, but only make it available to a random sample of 20 students. And I won't force them to watch it, I'll just do the same thing I always do, just make it available every week, just like I make the other lessons available. Then I'll look at this ratio. Let's pretend that for this sample we get that the mean ratio is 0.13. So then based on this sample, if we make this lesson available to all 8,702 students, what is our point estimate for the engagement ratio? Well, since the sample had average engagement ratio of 0.13 we would guess that the population will have an engagement ratio of 0.13. This means that on average out of every hour of content available students would watch almost 8 minutes if we introduced this new lesson with a song. Remember that since the distribution is skewed the number of minutes students would watch is heavily pulled towards the left, towards zero, because there are probably a lot of students who are enrolled but aren't watching any of the lessons. Of the students that are taking the course and progressing through it this number would be a lot higher. But on average it's about eight minutes. So you just calculated the point estimate. What about our interval estimate? Because we know that the population mean will probably be somewhere around 0.13. And remember this is the population mean if we gave everyone an access to this musical lesson. So what's the standard error of the mean that we would use to compare this sample mean with the means of other samples of the same size from this population? Again round to the nearest thousandths. So if this is our population. Where the y-axis is frequency. And the x-axis is the engagement ratio. We know that if we picked every possible sample of size 20 from this distribution and then we calculated the mean of each sample and then plotted the distribution of means. It will be normal. The mean of this distribution should be the population mean, which you calculated to be 77 1000ths. And the standard deviation of this distribution is the population standard deviation divided by the sample size, which is 0.107 divided by the square root of 20. And if we round to the nearest thousandth, we get 24 1000ths. So this is a standard error. When we found that, that the mean engagement ratio of our sample of size 20 was 0.13, we can compare it on this distribution, in terms of standard error. We want to know how many standard errors 0.13 is from the mean. So, we found that the standard error is 0.024, but we still need our interval estimate. Let's say that we want a 95% confidence interval, abbreviated CI, for short. I'm going to put our standard error of the mean over here. And I really should make this capital N, since it's the population, and n is the sample size 20. Based on the results from our sample mean, which was 0.13, then the true population mean, if everyone has access to this musical lesson will be somewhere around 0.13. And within 95% confidence interval, we're assuming that this sample mean is one of the 95% of sample means that will be around the true population mean. We're going to call the true population Mu sub I. I for intervention. So, this means that if this sample mean is within this 95% of Mu sub I, then Mu sub I, then Mu sub I is can be here or here. See how that sample mean is on the lower bound of that 95%. Whereas here, the sample mean is on the upper bound of that 95%. So if Mu sub I can either be here or here, in this interval surrounding 0.13, then this is our confidence interval. What are these values? So, this value here should be what? And this value here should be what? I know this is pretty complicated and that's why this entire lesson is pretty much on confidence intervals. Let's just analyze a good old normal distribution. So here we have 95%, well if 95% is in the middle that means 2.5% is here and 2.5% is here. So what's the Z score here? If you look at the Z table, and look for 0.025, or 2.5%, you see that it's at negative 1.96. So Z is negative 1.96 here and positive 1.96 here. So 0.13 can either be 1.96 standard deviations above the mean, like it is here if this were the population distribution. Z 0.13 is 1.96 standard deviations above mu sub i. Or 0.13 can be 1.96 standard deviations below the true population mean. So if this sample mean is 1.96 standard deviations below the true population mean, then this true population mean would be the sample mean plus 1.96 standard deviations away. And you found that 1 standard deviation is 0.024. However, if the sample mean is actually greater than the true population mean, then the true population mean is 0.13 minus 1.96 standard deviations. And then if we calculate what these values are we get .083, approximately, and .177. So then our interval estimate for the 95% confidence interval has a lower bound of 0.083. And an upper bound of 0.177. Now really quick, always remember what these numbers mean. Most numbers have units of some sort. This is the ratio of minutes watched of all the lessons to total minutes available. So we're predicting that, if I incorporated this musical lesson then the entire population of 8,702 students will watch between 83 thousandths and 177 thousandths of all content available. So if we set it up like this and we want to know how many minutes students will watch out of every 60 minutes available, 60 times 0.083 is about 5 minutes. And then on the upper bound, they could watch up to 0.177 minutes for every minute available. And out of 60 minutes, that's about 10.62. Since we found that the engagement ratio right now Is 0.077. That's about 4.62 minutes watched out every hour of content available. Then according to our calculations, we're pretty sure that this musical lesson will increase the amount video watched to at least 5 minutes and possibly up to 10.62 minutes for every hour of content available. So if all of this actually happened, then we could say that my song increased the engagement. Now before we move on from confidence intervals, let's generalize it. Which of the following formulas represents what you did to find the 95% confidence interval for the population mean used to buy? So, you have four options. Check all that apply. And up here, just to refresh your memory, we have the definitions of each of the symbols. And z is this z score value here where 95% is in between these two z scores. All right, so let's look at the first one. We have our sample mean, x bar minus the z score times the standard deviation meaning we have this number of standard deviations less than the mean. And that's what we did with z here being 1.96, since this is 95%. And then over here on the other side, we have our sample mean plus z amounts of standard deviations or standard errors. So this is what we did to find the confidence interval. However we did not subtract Z standard deviations from the population mean. Remember the original population mean was point zero seven, seven, If we did that then we would get a confidence interval for the original population mean. And that's not what we're interested in. This third one, we have our sample mean, minus and instead of Z we have the actual Z value, 1.96 times the standard error. So we have 1.96 standard errors less than this sample mean. So that is what we get. And same with over here we have 1.96 standard error plus the sample mean. And this one doesn't really make sense because we can't just subtract the number of standard deviations, we need to multiply this number of standard deviations by what the standard deviation actually is. To find the distance away from the sample mean we need to go. Hopefully that makes sense. Great job. Now, we have our general form of the confidence interval. So, for some y percent confidence interval, this could be 95%, 99%, anything really. Usually, it's somewhere in the 90s. Then, the lower bound of the confidence interval is the sample mean minus the number of standard deviations away from the mean. Where this z value is the number of standard deviations and sigma divided by the square root of n is the standard deviation of the sampling distributions. And then, on the upper bound, we just add the number of standard deviations away that we want. If we have a 95% confidence interval, we'll have less standard deviations away than we will for a 98% confidence interval. Remember that we get this z-value from the z-table. So, if this is a 98% confidence interval let's say, then we'll have 1% in the tails. And then, we'd look for the z-score for which 1% is less in a standard normal distribution. The distance that we go away from x-bar, on either side, is the margin of error. And as you can see, that's equal to z times sigma divided by root end. And that's half the width of the confidence interval. While this engagement ratio certainly tells us something about your engagement, there are a few flaws as with any operational definition and method of measuring a construct, in this case the construct of engagement. For example, what if you logged on and were watching lessons, but then your friend called. And so, you spent an hour talking to her while the videos continued to play. Or sure, you can come up with a few flaws in using this as the sole measure in engagement. So, here's another option. You could self report how engaged you are on a scale from one to ten. Since engagement doesn't necessarily equal learning, I'll also ask you how much you feel like you're learning on a scale from one to ten. So, go ahead and tell me how you'd rank your engagement and learning by clicking the link below. When you're done, just click Yes. The purpose of this quiz is just to make sure the video is paused. Giving you enough time to click the link and rate your engagement and learning. Thanks for doing that. So now remember we're trying to decide if me singing a song that I wrote about hypothesis testing is going to increase your engagement in learning. In the last example, confidence intervals were really great when we were trying to estimate the actually population parameters. For example the engagement from our example before. In this case, the dependant variable, so the engagements ratio, makes sense by itself, we understand what that means. The number of minutes students spend watching the course to the total amount of minutes that are available. But in this case, your self-reported measures of engagment and learning aren't as well defined. And there subject to your individual perceptions of what good, engagement and learning are. Obviously higher numbers, like nines and tens are better, but what does it really mean if you report your engagement as seven. In this case we don't care as much what the actually between one and ten will be. We just want to see if the intervention or the song possibly has an effect. So, we're not going to use confidence intervals for this. Now, we don't know the results yet because I'm just giving you this poll. But let's pretend that we get an average of 7.5 for how engaged you are, and an average of 8.2 for how much you think you're learning. So, these are hypothetical population parameters. Mu sub E for engagement, and Mu sub L for learning. We would say the associated standard deviations are 0.64 and 0.73. Now, let's say our distributions look like this, for each measure, so they're negatively skewed. And let's say I want to get these means up, I'm going to try to do this by writing a song about the concepts in the next lesson, and then singing it. Maybe it'll help you stay engaged, because hearing a song can be entertaining. Maybe it'll help you learn, because hearing a song can you remember the concept and what it is. However, I'm not totally sure at this point that it might actually help rather than hurt your learning. So, instead of giving everybody this lesson without a clue as to whether or not it's going to help, I'm just going to give this lesson to a random sample of 20 students. Then, I have them anonymously take the same poll you just did. Let's say we get the following sample means. Now ultimately we want to know if hearing this song, on hypothesis testing in lesson nine, will result in higher engagement and learning. So what statistic should we calculate to determine this? Should we simply note whether or not the sample means are less than or greater than the population mean? Or should we calculate the actual difference between the population mean and the sample mean for each measure, or should we found out where each sample mean falls on the distribution of sample means from each population, or should we find how many population standard deviations each sample mean is from the population mean? Certainly, looking at whether the sample means are higher or lower than the population means provides some indication of whether or not the song may have helped or hurt student engagement and learning. But this barely tells the whole story. We could look at the difference between the population mean and sample mean, but the population is based on thousands of scores, whereas the sample mean is based off of just 20 scores. So this isn't an accurate comparison. We also can't find the z score of a sample mean using the population standard deviation sigma, because sigma is based on the variability of each individual score, not the variability of a mean of scores. Therefore, the third option is correct. Using the central limit theorem, we need to look at the distribution of sample means for all samples of size 20, and then find where our two sample means fall on these distributions. So if we have a distribution of sample means for all samples of size twenty from each of these populations, what will be the mean and standard deviation of each sampling distribution? So remember, M is the mean of sample means and SE is the standard error, so the standard deviation of the sampling distribution. Well, the means should be exactly the same. So here it would be 7.5, and here 8.2. But the standard deviations are the population standard deviation divided by the square root of the sample size. This is 0.64 divided by the square root of 20, which is about 0.14. And here, it's 0.73 divided by the square root of 20, which is about 0.16. So, they have almost the same standard deviation. So, this is about what the sampling distributions will look like, maybe a little bit skinnier. I can't draw much skinnier than this, though. So remember, we tested out this lesson with the song on 20 students. And we got these means. Where do these fall on the sampling distributions in green? In other words, what's the z score of each sample? So we're trying to find where 8.94 falls on this distribution. So it looks likes it's about here, so this z score is going to be really high. For this one the z score falls about here, which looks like it's still kind of in the center of the sampling distribution, so now if we calculate the actual z score... We'll do 8.94 minus the population mean, which is also the mean of sample means. And then divide by this standard deviation of the sampling distribution. Now to keep things more precise, instead of using the rounded standard error, I'm going to put this in my calculator. It's okay if you used the rounded error. You should have still gotten it right. So if we do it like this, we get about 10.06. Which is pretty big. And here if we calculate z-score. We'll say 8.35 minus the mean of the sampling distribution. Divided by the standard deviation of the sampling distribution. So we get about 0.92. We're going to call this zees of en, zees of l. So now what's next? Well, what's the probability of randomly selecting a sample of size 20 and getting a mean of at least 8.94 for engagement, and 8.35 for a learning? We have to use the z table and the z scores that we just calculated. With the z value this high, the probability of getting the z score is off the charts. It's so low that it's near impossible. However, the probability of getting a z score of 0.92 is 1 minus the probability that we get less then a z score of 0.92. So, let's look at the z table for that. So here's 0.9, and this would be the probability of getting a z score less than 0.92, 0.8212. And so, the probability of getting a z square greater than this, which is the same as the probability of getting a mean greater than 8.35, is about 0.18. This probability is a lot greater than this one. So what does this mean, what can we conclude? There's only one that's true here, and that's that the song seems to have had an effect on engagement, but not learning. Think of it this way. Let's assume that the song had no effect. Let's say we picked a random sample of size 20 from the population of learning scores. It's quite possible. You calculated about an 18% chance to randomly select a sample with a mean learning score of 8.35. This is a pretty high probability. However, again, let's assume that the song had no effect and we picked a random sample of size 20 from the population of engagement scores. We get that the mean engagement score is way out here, with a z score of over 10. That's not likely to happen if we pick a random sample. Therefore, there's evidence to suggest that the song had an effect on engagement but not learning. However, we still can't be totally sure that the song caused higher engagement without setting up a proper experiment and implementing proper control conditions. Great job. In this lesson, you learned about estimating population parameters with confidence intervals and then analyzing whether or not a search and treatment may have had an effect. So the examples we've used so far are the Bieber tweeter, we wanted to see it's influence on klout scores. And then the other treatment was the song about hypothesis testing for lesson nine, which I seriously might do. After all this talk about it, I'm really curious about whether or not this will work. I guess we'll just have to wait and see the results of the poll where you rated your engagement and learning. Speaking of which, that's the dependent variable. In the next lesson you'll learn how to formalize the procedure of deciding whether or not the sample receiving treatment had likely or unlikely statistics. What if we got different mean engagement and learning scores for our sample. And calculated that the probability of getting these means is 5% for engagement and 10% for learning? Here I just put something. We could actually calculate what these mean values would be? Given the probability of getting those means, but that's not really important. The question here is, how about these probability values? Are these likely or unlikely? And remember, these are your ideas. What would you consider likely or unlikely? Well you may have found that it's kind of difficult to decide upon a threshold for what constitutes likely or unlikely, that's why statisticians have decided what this threshold should be and we've stuck to it ever since. Statisticians have determined three conventional levels of likelihood, or should I say unlikelihood. If the probability of getting a sample mean is less than 0.05, or 5%, 0.01 or 1%, or 0.001, which is 0.1% then it's usually considered unlikely. Getting a probability less than 0.1% is just very unlikely. These are called the alpha levels. Now let's just take a quick quiz to make sure you understand what alpha levels are. Let's focus on an alpha level of 0.05. Which of the following are true? If the probability of getting a particular sample mean is less than alpha, it is unlikely to occur. For this next option, let's say that the z-score here, cutting off the top 5%, is z star. If a sample mean has a z-score greater than z star, it is unlikely to occur. Your third option is if the probability of getting a particular sample mean is unlikely. The sample mean is in the orange region. And finally the alpha level corresponds to the orange region. True or not? Check all that apply. Our alpha level is our criterion for deciding whether or not something is likely or unlikely. If the probability is less than alpha then it is considered unlikely. If a sample mean has a z score greater than z star this means the probability of selecting this sample mean is even smaller. Therefore, this second is also true. If we consider sample means, with z-scores greater than z-star to be unlikely, then if it falls in the orange region it would be considered likely, in this case. Therefore this third one is not true. And finally, the alpha level is .05, which is 5% which corresponds to the green region, not the orange. We decide that if the probability of obtaining a particular sample mean is less than the alpha level. Then it will fall in this tail which is called the critical region. This z score value, which cuts off the critical region from the rest of the distribution, is called the z critical value. If the z score of a sample mean is greater than the z critical value. We have evidence that these sample statistics are different from the regular or untreated population. In our example, this was the population that had not watched the musical lesson. Let's say that the probabililty of this red critical region, is 0.05, or 5%. In other words, the alpha level is .05. Then what is this c critical value here? For this, we'll have to use the z table. So if 5% is above the C score, then that means 95% has to be below the z score. So lets look in the table and find .95. So in the body of the table again, we have the probability. Less than a certain z score. This is the same as the proportion of values in the population, less than a certain z score. Let's zoom in a little. Here we have 0.9495, and 0.9505. Hopefully you can see that. 0.95 is in between those. So if you said either 1.64 or 1.65 then you got it right. Let's just say its about 1.65. Okay, now let's say we're interested in a critical region of only 1%. Because we decided that a sample mean with probability less than 0.01, will be considered unlikely. So what z score marks this cutoff? Well, if 1% is here in the tail, then that means 99% is now here. So let's bring up the z table again, and then we'll look for 0.99. So here's 0.9901, which looks about the closest. So it's here at 2.33. Okay, and finally, let's find the Z score that marks the cutoff for the smallest alpha level. Again, if 0.1% is out here, then this must mean that this yellow or orange region is 99.9%. So that means we're going to look for 0.999 in our z table. Here's 0.999 and that's at 3.10. You'll notice that there are a few 0.999's. We want the first one, because that's the one that's closed to 0.999. And then the others will just get bigger. So this one's 0.99905, I don't know, something like that, we can't really tell. But we know that this first one is going to be closest to just plain old 0.999. And so that is at 3.08. So let's blow up our sampling distribution a little bit. So we have three critical regions. The smallest one has a probability greater than this z score of 0.01, and this z score here is 3.08. And remember that the middle of the distribution, the mean, has a z score of zero. Then our second smallest critical region is point zero one. So that's the probability of being greater then this value, which you found was about two point three two. And then our biggest critical region is point zero five. So that's the probability greater than a z score of 1.65. The z scores corresponding to these alpha levels are called the z critical values. So we'll find our sample mean. Calculate the z score, and the distribution of sample means, or the sampling distribution. And then look to see where this mean falls. Let's say it falls in the critical region, any of them, that means it's unlikely. If it falls out here in the green region or the smallest critical region, we'll report the smallest alpha level, because that means our sample statistics are really different from the population parameters. And therefore there's strong evidence of an effect from whatever treatment was imposed. For example, say we take some sample mean from some sample size n and then we calculate the z score. So remember we subtract the mu and then divide by the standard deviation of the sampling distribution. And let's say that we get the z score is 1.82. We would say that this is significant at p less than 0.05. And that's because if we look at where this z-score falls on the distribution, it falls in between 1.65 and 2.32. So this z-score, 1.82, is somewhere in this red region. Meaning that the probability of having obtained this sample mean is less than 0.05. But it's not less than 0.01. So it's only significant at probability, or p, less than 0.05. And remember 0.05 is the alpha level. Try a few yourself. Let's say that we find some sample means from a population and these sample means have the following Z-scores. What alpha levels could we put here for these particular Z-scores? So fill in the alpha levels as propotions. So 3.14, is greater than this z critical value for an alpha level of 0.001. So, a z score of 3.14 occurs out here somewhere in the green region. So this is significant at probability less than 0.001. And 2.07 is between 1.65 and 2.32. So that z score, occurs somewhere in this red region. So it's significant at p less than .05. Hopefully this is making sense. Let's do that last two. 2.57 will occur somewhere in this blue region, because that z score is in between 2.32 and 3.08. So the probability of getting this z score is 0.01. And finally, this is a really huge z score so, this sample mean occurs somewhere way out here, far from the mean. But we don't conventionally have an alpha level that is that small, so we'll just say that the probability of getting that sample mean, is less than 0.001, good job. So just to recap, if our sample mean falls in the critical region, for an alpha level of 0.05, It's unlikely and therefore probably didn't occur by chance. Something's going on, and there are different levels that help us decide how sure we are. Think of it like playing darts. The standard height from the floor to the bullseye is 5 feet 8 inches, and then you have to stand 7 feet 9 and 1 fourth inches back from the dartboard. Then dartboard makers had to decide how big the dartboard should be. Bigger targets or even the wall are easier to hit. But it's also more likely we hit them by chance. Smaller targets are harder to hit, but when we hit them, we can be more confident we didn't hit them by chance. So the bulls eye should be big enough that it's possible to hit it, but small enough so that if we hit it, it's due to skill. There has to be a balance. These targets on a dart board are analogous to the critical regions, with the boundary of each region on the dartboard analogous to the different alpha levels. We can easily get a mean within this area, in the middle of the distribution, but if a mean lands beyond a z score of 1.64, the probability of getting this mean is less than 0.05, the alpha level, and it's not likely to have occurred by chance. It's even more unlikely to occur by chance if the mean lands beyond z equals 2.32. The probability there is .01, and it's super unlikely to have occurred by chance. If the mean lands beyond z equals 3.08. This is like striking the bullseye. So let's get back to this example, where I would sing about this concept, and we want to know if it's going to help students learn and increase their engagement. But let's pretend that after I sing, the 20 students exposed to this lesson report a mean engagement score of 7.13. So remember, they're rating their engagement from 1 to 10, and then we're going to take the average rating of those 20 students, and we get 7.13. For now, let's not focus on student learning. So first of all, what should the z score be for this sample mean? Just to remind you we're pretending that the population mean was 7.5, and the population standard deviation was 0.64. So, remember, when we find the z score of a sample mean, we look at it compared to the whole sampling distribution So we have to find the mean and standard deviation of the sampling distribution. Sorry I'm making you calculate this again even though you did it in the previous lesson. I just want to make sure that you continue to get enough practice doing this. So for the sampling distribution, where we take all samples of size 20, because in this case our sample consisted of 20 students. We have that the mean is the same as the population mean, and the standard error, which is the standard deviation of the sampling distribution, is the population standard deviation, divided by the square root of the sample size. So this is about 0.14. So then this means that this sample mean has a z score equal to 7.13 minus the mean 7.5 divided by 0.14. If you put the more exact values into your calculator, then you'll get a more precise z score rather than using the rounded standard error. So we get about negative 2.59. If we look at the distribution of sample means and here is 7.5 the population mean and the mean of sample means negative 2.59 is way over here, so what does this mean. If our critical region is only up here, above the mean. Then, any sample mean in this yellow, or orange, area will be considered likely. However, with a z score of negative 2.59, would we necessarily consider that likely? So instead, let's split the critical region in 2. Again, let's focus on just the alpha level of 0.05 for now. If we split 0.05 in 2, then that means 2.5% is on this side. I drew this 2.5% disproportionately larger just so you can see. But let's just pretend that this area in blue is 2.5%. And that also means that 2.5% is on this side. So in total they make 5%. So this is for an alpha level of 0.05. So this means we have different z critical values. What is this z critical value that should go here? Just write it as a positive value. There are two ways you can figure out what this z-critical value is. One is to look at the z table and find the value for which 2.5% is less than that. So, if we look for 0.025, or 2.5%, we see that it's here at negative 1.96. So, that means that this z-critical value is negative1.96 because 2.5% is less than that. And since this distribution is symmetrical, that means that this z-critical value would be positive 1.96. Another thing you could have done is noted that if 2.5% is up here in the blue region, then 97.5% is below the z-critical value. Then, you could have looked for 0.975 in the z-table and then you would get positive 1.96. This is called the two tailed test. When we split the alpha level in half, we see that its two tailed. And then our z-critical values become both the positive and negative of the same number. Now, our critical regions are both here and here, not just in one tail. If we use this alpha level and do a two tailed test to see if getting this sample mean is likely or unlikely, what can we say? Can we say it is unlikely to have gotten a mean engagement score of 7.13? A mean engagement of 7.13 does not fall in critical region. There is evidence that my singing made students less engaged or a mean engagement score of 7.13 is significant at probability less that 0.05. Well, you can see that negative 2.59 is less than the z critical negative 1.96 at the alpha level of 0.05. Since its in this critical region, we can say that it is unlikely to have gotten a mean engagement score of 7.13. However, this mean engagement score does fall in the critical region. However, there is evidence that my singing made students less engaged. Perhaps I should just stick to talking when I teach a lesson. And we can also say that a mean engagement score of 7.13 is significant at probably less then 0.05. The probability of getting an egagement score this far from the mean in either direction is less than 0.05. That's basically what that means when we do a 2 tailed test. So let's say we got a sample mean that is about here. When we determine whether or not this sample mean is significant at 0.05. We want to know if a probability of obtaining a sample mean this far from the mean of means is less than 0.05. And it doesn't matter what direction. So if we have this z score, the probability of getting a z score that far below or above the mean is the area here and here. So now let's calculate the z critical values for two-tailed tests at the other two levels. So first let's do the significance level of 0.01. If both critical regions add to 1% then what percent should be here and here? Well together, these should add to 1%. Which means that in each region, there is a half of a percent, which is 0.5. Lets convert these to proportions, 0.005. So now, what are the z critical values here and here? This time, it'll be plus or minus something. So if we bring up our handy-dandy z-table, we're looking for 0.005, because 0.005 times 2 is 0.01. So here's 0.0051 and 0.0049, so z-critical value is negative 2.57, or negative 2.58. It doesn't really matter too much, we'll just say it's about negative 2.57. So if a proportion of 0.005 is less than negative 2.57, a proportion of positive 0.005 is greater than positive 2.57. So our z critical values are negative 2.57 and positive 2.57 and we can just shorten this to plus or minus 2.57. Okay, now we're going to do the same thing for our smallest alpha level. This time, I won't ask you to calculate the percentage in each tail. If we split 0.001 in half, then we get 0.0005. So that's the probability in each tail. So if we look at our z table and look for 0.0005, we get about negative 3.32. So again that means that this z critical value is negative 3.32. And this z critical value is positive 3.32. Great job calculating those z critical values for both one and two tail. What does this mean? Depending on our criteria for what constitutes likely or unlikely if we get a sample mean in the critical region here for a one-tailed test or here or here for a two-tailed test. Then we decide that most likely we did not get this sample mean by chance. Note that in the case of a two-tailed test. The z-critical values are the same used to calculate confidence intervals, Here, you're just applying the same ideals to different situations. When we do statistical test, we'll set our own criteria for making a decision, in other words, we'll choose an alpha level. And then we'll decide that if the probability of obtaining a particular sample mean is less than that alpha level, Then there's evidence of an effect. Usually we're going to stick with an alpha level of point zero five. At an alpha level of point zero five we have two possible outcomes in either situation, one tailed or two tailed. Either the sample mean will lie outside of the critical or inside the critical region. We notate these two outcomes as the null hypothesis. Which we call h not or h sub zero and the alternative hypothesis h sub a. Other ways of notating this could be h sub one but I'll just stick with h sub a for alternative. The null hypothesis assumes that there is not significant difference between the current population parameters. And what will be the new population parameters after some sort of intervention. So we'll notate it like current population parameter mu is equal to the population parameters after the intervention, where when we say equal, we don't mean exactly equal, we just mean they're not significantly different. The alternative hypothesis guesses that there will be a significant difference. Either the current population mean will be less than that after the intervention, or the current population mean will be greater. Or that it'll just be different, and we don't predict a direction of the treatment. For the null hypothesis, when we're guessing that there's no significant difference between these two parameters Our sample mean will lie somewhere outside the critical region, in this white space. Note, also, that for a 1-tailed test the critical region could be on the left tail, not just the right. And the alternative hypothesis guesses that there is a significant difference, and that means that our sample mean will lie somewhere in the critical region. We can't prove that the null hypothesis is true. We can only obtain evidence to reject the null hypothesis. Let's take a simple example. Let's say, our null hypothesis is that most dogs have four legs. And we define most as more than 50% of dogs. This 50% is analogous to our alpha level where we set our criterion for making a decision. Our alternative hypothesis is that most dogs have less than four legs. Note that this is like a one tailed test. Where we're not guessing that most dogs have a different amount than four legs. We're guessing most dogs have less, so there's a direction. Now let's say we sample 10 dogs and find that all of them have four legs. Did we prove that the null hypothesis is true, that most dogs have four legs? Well no. We have evidence to suggest that most dogs have four legs. Because in our sample all of them had four legs. But we didn't prove that most dogs have four legs. In this case we didn't prove the alternative hypothesis either. But based on our sample, we'll simple fail to reject the null. Now let's say we sample 10 dogs, and 6 of them have 3 legs. Is this evidence to reject the null hypothesis that most dogs have 4 legs? Yes, purely based on our sample, we found that most dogs have less than four legs. If this is the outcome of our sample, then we'll reject to the null hypothesis in favor of the alternative. Let's extend these ideas to a more complicated situation. You might have received a survey that I sent out, asking you to rank your engagement and learning, from 1 to 10. Thank you to everyone who took this poll. I collected the first 235 scores, as reported by Saturday, March 16th And I put them in a link at the bottom so you can click on that and analyze the data. I removed any blank responses but that's it. One thing to note before beginning to analyze this data is that I didn't state a very clear operation definition of engagement and learning. FOr example if someone has only had time to view one lesson, but then life got in the way. Their engagement might be one. That doesn't necessarily mean that they didn't enjoy what they were watching. So, there are many arbitrary reasons why someone may choose the number they did. This is a good example, where confidence intervals are not necessarily the best way to analyze this data after some kind of intervention. Because there's not a real meaning to the scores people give, but in general, we know that higher scores are better, and this is where hypothesis testing comes in. First let's get to know this data. Here are histograms showing students' responses. For this next example, let's just focus on engagement. What are the mean and standard deviation engagement scores? Even though the responses are a sample, just treat it as a population for the sake of this example. You're welcome to also calculate the mean and standard deviation for learning if you're interested. But we really only need the parameters for engagement. Let's calculate the average and standard deviation in this spreadsheet. After you Copy and Paste the engagement data into your own spreadsheet. Then in any blank cell, we'll type equals average, and in this case, B2 column B236. And if we go to the bottom, we see that there are 236 rows, but the first row has a header. So there are 235 values. Now we have the mean in cell E1. Now in cell D2, we can type equals parentheses B2 minus $E$1, and parentheses. [INAUDIBLE] 2, for squared. So that's the first squared deviation from the mean. We'll Copy that cell. I'll go to cell C2, hit Cmd, Down Arrow. Go to cell D236, Cmd ,Shift, Up arrow key. Highlighting column D, Cmd V. Now we have all our squared deviations. The average squared deviation is the variance, equals average D2 column D2, 36. There's our variance, and now the standard devation is the square root of cell E2. There you have it. Let's round to the 100th place. We get the mean is 7.47 and the standard deviation is 2.41, if we round two decimal places. Let's say we want to know how a song about the content are used to teach the content might impact engagement. We're going to set up for a hypothesis test, which will compare our current population with what we predict will be our new population after this treatment or intervention. Remember, in a hypothesis test, we have the null hypothesis and the alternative hypothesis. And the null is that there's no significant difference between the population after the intervention is given and the current population parameter. And the alternative could be one of three things, that the current population is less than what it will be after the intervention, that it's greater, or that it's just different. In this example, what could the null hypothesis be? Check all that apply. Since the null hypothesis guesses that there will be no significant difference, that's equivalent to saying that this intervention will not make learners more engaged. Or, that the intervention will result in the same level of engagement. What about the alternative hypothesis. Again check all that apply. These are three ways of symbolizing the alternative hypothesis. And in real terms, this is like saying, the intervention will make learners more engaged. In other words, the population parameter after the intervention will be greater than the current population parameter. Or it'll change how much learners are engaged. Which is symbolized by this way of writing the alternative hypothesis, or will make learners less engaged. Our next step is to decide which of these alternative hypotheses we want. These two alternative hypotheses are one-tailed tests. If our sample mean lies out here in the critical region, significantly higher than the current population mean, then we're guessing that the population parameter after the intervention will be greater than the current. Similarly, if our sample mean lies in the critical region below the current population parameter, the population parameter with the intervention will have a lower score than it does currently. And finally, if our sample mean lies in the critical region either higher or lower than the current population parameter, then we conclude that the new population will be significantly different. We choose a one-tailed, or directional hypothesis test when we predict a direction of the treatment effect. For example, when we predict that a song illustrating the concepts in this lesson, will increase or decrease engagement. Alternatively, we would choose a two-tailed, or non-directional hypothesis test, when we do not predict a direction of the treatment effect. In general we use two-tailed tests because they are more conservative. We are less likely to reject the null when it's true. We'll get more into that later. But also because, as you saw before, we might be wrong about the direction. We might predict that the treatment's going to increase engagement, when in fact it decreases engagement, in which case we should have a two-tailed test. If we just use the one-tailed test in the positive direction, we may miss the fact that the treatment is doing the opposite of what we would expect. One exception to this general rule is when we're comparing a new treatment with an established treatment. In such cases, we often only care if the new treatment is better than the old one. We don't care if the new one is much worse. So that's when we would use a one-tailed directional test. Let's say we don't know how the song might impact engagement. It could make students less engaged, or make them more engaged. Therefor, we're going to put here, muse sub song. because the song is the intervention. We're going to try to test if this parameter will be significantly different from what you've already found. Our next step is to set a criteria for making a decision. Where our decision will be to either reject or fail to reject the noll. We have to choose our alpha level, the alpha level we'll generally use is .05, which means that for a two tailed test, we have 2.5% or .025 and a porportion of .025 on the left tail. Then we'll find the z-critical values. And finally, we'll find the z-score of the sample mean, and see if it falls in the critical region. Based on that, we'll decide whether to reject or fail to reject the null hypothesis. That's an overview of everything you were going to do in this lesson. Before we get cracking, let's make sure you understand what it means to reject the null. Select the appropriate word in each sentence, so that they all describe what it means to reject the null. One point of clarification, when figuring out this second one, just ignore negatives. Let's say the z-critical value is positive, then how big should be the z-score of the sample mean to reject the null? If we reject the null, that means we're accepting the alternative hypothesis, which is that there is a significant difference. That means the sample statistic we get is very different than what we would expect for all the other samples we could have gotten from that population. That means our sample mean will fall within the critical region. If we just look at the positive z-critical values, then our z-score should be greater than the z-critical value. This means that the sample mean will lie in the critical region. And finally, the probability of obtaining the sample mean will be less than the alpha level. We set this alpha level as a guideline for the probability of getting that sample statistic. And if the probability of getting that sample statistic is low, that means there's something going on. And we have evidence to reject the null. Good. So, let's zoom in on this little diagram. For an alpha level of 0.05, meaning that 0.025 is in each tail, what are the z-critical values for here and here? The total area in this region is 0.975. Or, you could just look at the area in this region 0.025. And then, find those proportions in the z table. If we look for the proportion 0.975, we find that the z score is 1.96. And if we look for the proportion 0.025, we find that the z score is negative 1.96. Now let's say we take a sample of 30 students and we make the lesson with the song available to them. Afterwards, the mean engagement score they report is 8.3. Where does this value fall on the distribution of sample means for all samples of size 30? To answer this remember that this normal distribution represent the distribution of sample mean. Remember that the mean of sample means should be the same as the population mean. And the standard deviation should be the population standard deviation divided by the square root of n. We want to find where on this distribution the sample mean will fall. In other words, what's the z score? Recall that the z-score is the sample mean minus the population mean divided by the standard deviation of this distribution. But the standard deviation is the standard error. When we calculate this, we get about 1.89. Now that we've found a z score of this sample mean, then at an alpha level of 0.05, do we reject the null or fail to reject the null for a two-tail test? Our z score is 1.89, which is less than a z score, 1.96. Therefore, our sample mean will fall somewhere in this white region. One of the 95% of sample means surrounding the population mean. Therefore, we fail to reject the Ho or the null hypothesis. There's not enough evidence that the new population parameter, will be significantly different from what it is now. After we implement this musical lesson. In other words, we would guess that engagement will stay the same based on our sample. However, what if we had gotten the same sample mean, 8.3, but with a random sample of size 50? Then, where would the sample fall on this distribution of sample means? Again, find the z-score. The z score is the sample mean, minus the mean of means, which should be the same as the population mean, divided by the standard deviation of this sampling distribution, which is the standard error. Sigma divided by the square root of n, that's 8.3 minus 7.47. Divided by 2.41 divided by the square root of 50. And this turns out to be about 2.44. Now let's ask the same question at an alpha level of 0.05. Do we reject the null or fail to reject the null. This time the z score is 2.44 greater than our z critical value, that means that our sample mean falls somewhere up here. The probability of obtaining this sample mean from a sample of size 50 is really small less than 2.5% and that's what we need to know for a two tailed test therefore, we'll reject the null. We have evidence to believe that the song had an effect on engagement. We formalize this by writing p for probability, is less than 0.05, our alpha level. Because the probability of obtaining this sample mean, from a sample of size 50, is less than our alpha level. The reason we reject the null, is because this probability is so low. Let's pretend that the null hypothesis is true, and there's actually not going to be a difference between our new population and our current population parameters. If the null hypothesis is true, what's the probability of getting this sample mean? Rated in terms of a proportion. This is just what you did in lesson seven. You calculated where the sample mean falls on this distribution of sample means, and then you used the Z table to find the probability greater than that value. This is exactly what you did in Lesson 7, when you found where a sample mean falls on the distribution of sample means. And then you used the z table to find the probability that you'll select a sample mean greater than that value. If you look in the z table for this z-score value, 2.44, you'll see that a proportion of 0.9927 is less than that z-score. That means the proportion greater than 2.44 is 1 minus 0.9927, which is 0.0073. If our sample mean falls within the critical region, we then conclude that this mean comes from an entirely different distribution. In other words, the treated population. Another way to put it is that we believe that our treatment had an effect. Of course, it is possible that we made a decision error. In other words, it's possible that we should have kept the null hypotheses, that there is no significant difference, and that the only reason we got this sample mean was due to chance, because there's a 0.0073% chance that we get at least this sample mean. However, the chances are small, less than our alpha level. Therefore, we reject the null hypothesis. But again, we don't need to find the probability of obtaining at least the sample mean. All we need to know is that the z-score of the of the sample mean is greater than the z-critical value. 2.44 is greater than 1.96. That alone tells us that our sample mean landed in the critical region and then we can reject the null. This type of statistical analysis is prone to misinterpretation. As we discussed in the last solution video, it's possible that those who were able to view the musical lesson happened to be more engaged students already and we wrongly attributed the high mean engagement score to the song. Of course it's unlikely we found the probability of getting that sample mean engagement score was about 0.007, but it's still possible to have randomly obtained a sample with that mean. But this means that one of four things can happen, here's a diagram that's no completely filled out, but it shows the four possible things that can happen. The columns represent the decision we made, we either rejected the null or retained the null, based on, the results of our sample. The state of the world is, what is actually happening. And we don't know what it is. We're just making a guess. Either the null is true, that there is not any significant effect from the treatment, or the null is false. There is a significant effect. Each of these four boxes represents a different situation. The truth, and then the decision we made. In two of these four cases, we made the correct statistical decision, but in the other two, we made the wrong decision. Check the two cases in which we've made the wrong statistical decision. I'm going to use red to represent wrong and green to represent correct. If it turns out the null is true, and there's no significant difference. But we reject the null, that's making the wrong statistical decision. However, if the null is true, and then we decide based on the result of our sample, that we're going to retain the null. Then we made the correct decision. Similarly if the null is false, and then we reject it, we've also made the correct decision. But if the null is false and the treatment actually did have an affect. But we assumed otherwise, then we're making the wrong decision. It's always possible to make the wrong decision. But we minimize our chances of doing so, when we have a large enough sample size. When we randomize our sample, and when we implement proper experimental controls. If we reject the null when the null is actually true, and we conclude that there was a treatment effect. We commit a type I error. I'm writing in Roman numerals. And if we fail to reject the null even though it's false, this is called a type II error. Let's go through a simple example of type one and type two errors. Have you ever sipped a beverage that was too hot, and you burned your tongue? Let's say that the null hypothesis is that the beverage is fine to drink. It's not too hot, it's perfect. The alternative hypothesis then, is that the beverage is too hot to drink. So, now, four things can happen. One thing, which often happens to me, because I'm hasty, is you decide the beverage is fine to drink but then it's too hot and you burn your toungue. Another thing that could happen is you decide that the beverage is fine to drink now and indeed it is, then you enjoy drinking it. Another thing that can happen is you think the beverage is too hot now so you wait until it cools but it's actually fine to drink, and then by the time you drink it it's cold. And finally, you might think the beverage is too hot, and indeed it is, so you wait to drink it, and then by the time you do, it's perfect. Now put A, B, C, or D, in the respective boxes it belongs here, and remember what H not, and H a are. If we decide that the beverage is fine to drink now, that means we're accepting the null hypothesis. However the beverage is actually too hot and you burn your tongue. So the state of the world, or reality, is that the beverage is too hot. Therefore we've committed a type II error. If we decide that the beverage is fine to drink now and indeed it is. Then we accepted the null hypotheses and the null is actually true. So we made the correct decision. If you think the beverage is too hot. That means you're agreeing with the alternative hypotheses. But if it's actually fine now, in other words the null is true. Then we're committing a type I error. And finally, you think the beverage is too hot, so you accept the alternative. And indeed, the alternative is true. Or, H0 is false. So we made the correct decision. Let's do one more. Have you ever left your house with your umbrella thinking it's going to rain but then it doesn't? And then you just end up carrying your umbrella around all day? Let's say that the null hypothesis is that it's not going to rain and the alternative hypothesis is that it will rain. Now we're going to do kind of the opposite from what we did the last quiz. This time, decide which of these correspond to rejecting the null or retaining the null. Or, if the null true or the null is false.. Write A, B, C, or D in each of these boxes. Just to clarify a little more, if the null is true, which of these, A, B, C, or D, corresponds to that. If we decided to retain each H naught, in other words we decided that it's not going to rain, which one of these, A, B, C, or D, should go in this box? Let's say we reject the null, that means we think it's going to rain. That means we're going to bring our umbrella. Alternatively, if we retain the null, we decide it's not going to rain, then we won't bring our umbrella. Remember that, these two rejecting the null or retaining the null correspond to our decision. If the null is true, meaning that it's not going to rain, then we should put A here. And if the null is false, that means it rains. Hopefully, this helped illustrate the concept of type one and type two errors. Hopefully these last two examples illustrated the concept of type I and type II errors. But now to wrap up this lesson, let's apply type I and type II errors to the question of whether or not a musical lesson will increase engagement. Remember that our population parameter was 7.47 and this is from the actual responses from the survey that I sent out. And the actual population standard deviation was 2.41. Our new hypothesis, then, will be that the new population mean after the intervention will not have a significant change from this population parameter. In other words, there won't be a treatment effect. And the alternative hypothesis is that there is a treatment effect. So the two parameters will be significantly different. Now let's say that we have a sample of size 30, and the mean engagement is 8.3, however the true engagement of the population if we were to implement this musical lesson, will be 7.8. Now in real life we don't know what muse sub song will be but let's pretend we do. If this happens, then which of these four quadrants represents the results of our hypothesis test? This is a pretty tough question and it requires you to remember what we did before and then really understand Type I and Type II errors. Give it your best shot. Let's say our alpha level is 0.05 and this is for a two-tailed test. Let's draw a diagrams that we can better visualize what's going on. Here's mu which is 7.47 and we want to know if the new population parameter will be significantly different. We decided significantly different if it falls in the critical region. Remember that the z score here for an alpha level of .05 is 1.96. and negative 1.96. Where does the real population mean fall in the distribution of sample means? Remember, that the standard deviation, of this distribution, is the population standard deviation divided by the square root of n. So the z-score of 7.8 is 7.8 minus 7.47. Divided by 2.41, divided by the square root of 30. This is about 0.75. Since 0.75 is less than this z critical value, but greater than this z critical value, the true population mean does not fall within the critical region. But we don't know this, we base our decision off of our sample. If we find the z score of our sample, We do the same thing that we did here, but instead eight point three minus seven point four seven divided by two point four one over root 30. And this is about one point eight night. Therefore, our sample statistic also falls outside the critical region Based on the results of this sample, we would fail to reject the null, and in this case we made the right decision. Because this population parameter also falls outside the critical region, close enough to the current population mean. Since we retained the null when the null was true, then this quadrant represents what happened. Now, what if the true population mean is still 7.8, and we still got our sample mean 8.3? But we got that from a sample of size 50. Now, which quadrant represents what happened? Again, we have to find the z scores for each of these. The z score slightly changes because our sample size is now 50 instead of 30. Now our z score here becomes about 0.97, which is still outside the critical region. But the z score for our sample mean is as high as 2.44, approximately. And that's in our critical region. In this case, based on the results of our sample, we would reject the null. However, we're pretending that the new population parameter will be outside the critical region, close to the old population parameter. In this case, we would reject the null, when the null is actually true. Then we're committing a Type I error. In this case, it just so happened that we randomly selected a sample of students who were already really engaged. After all, that is possible. There's a 2.5% chance that we get a mean with a z score greater that 1.96. So you see that statistics is always prone to misinterpretations. Data itself only gets us so far, the important things is how we collect that data. What's our sample size? Is our sample random? There are many things to take into account when making statistical decisions. To finish off this lesson, let's once again return to the question if music will increase engagement. This goes out to all my students around the world. Hm[MUSIC] So you got some sample mean x bar is 5 and the size of 17. Is this typical? Well, what you gotta know? The sampling distribution is the way to go. Way to go. You've got the sampling. Distribution 09 to 0. So you got some population mu1, sigma 2, you need a derivation for the standard deviation. Need motivation it's as simple as pi. Sigma divided by root n. Why? 'Cuz it's the central limit theorem. Now find your z, subtract the mean and divide by se, what's the probability? It's not a lot. Less than alpha level, you reject H0. So what do you think? Will this increase engagement?