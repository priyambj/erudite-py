Welcome to the Udacity and Twitter course real time analytics with Apache Storm. I'm Lewis, and I build courses here at Udacity. And I'm Karthik, I'm Engineering Manager and Tech Lead for Storm at Twitter. Storm is a real time infrastructure for easily expressing your real time analytic techniques. It is heavily used by Twitter for all real time analytics. Storm is fairly simple, can be used with any programming language, and as our hackathon participants discovered, it's really fun. In this course you'll be exploring Storm topologies, implementing data spouts and processing data bolts by connecting to the real live Twitter garden hose. And in your final project you'll be calculating worldwide top trending hashtags and displaying tweets that contain these hashtags, using the interactive data visualization library, D3. Thanks for joining this class. Let's get started with Apache Storm. The Udacity and Twitter Course Real-Time Analytics with Apache Storm is considered a T-shaped course because we touch on a number of different topics and go in depth in Storm. We use the virtualization technology Vagrant Cloud and Virtual Box. And using git and Vagrant commands we launch a virtual machine, the open-source Linux distribution Ubuntu. We then use the Java build tool Maven to explore real-time analytics with Apache Storm. We also use the programming languages Python and Java. Along with packages Beautiful Soup and lettuce, and I'm not making that up, to connect to the Python microserver Flask. And the in-memory key-value store Redis to finally drive real-time visualizations using D3 written in JavaScript. In Storm, we'll explore concepts like a spout, a bolt, and a connection called a topology. We explore these components written in both Java and Python. And finally connect to the Twitter sample stream using the open API Twitter4J. We then explore more complex topologies using streaming joins. And finally, you'll work on your own final project ideas as you watch our students work through a hackathon here at Udacity. Also, since we can't cover everything, some of the topics we don't cover include the programming language Clojure which most of Storm is written in, an actual cluster administration to submit your Storm topology to. . And finally, ack which allows at least once processing. Finally, after you're done with this course or even as you're working through this course, you'll be happy to know that Udacity has courses that cover Ubuntu, git, Python, Java, D3, and JavaScript. Let's get started in Real-Time Analytics with Apache Storm. So before we go to Storm oh, in terms of what Real Time analytic's and what are the batch analytic's? I want to just give a perspective of what analytic's means. I mean why do you have to do it? So according to Wikipedia there are two aspects analytic's and the first is discovery. I mean in the sense like, you have lots of data that is being accumulated or repeated after time, and somehow you are to find some kind of patterns in the data, that is what we call discovery. The second aspect of the analytic's is communicating. So if your analytic's is not able to be communicated very well to the. Either the upper level management or the appropriate group that is going to make the decision is not going to be very useful. So, there are two aspects of analytic's that are going to affect the company in general. One is to increase the sales which is contributing to the bottomline. The second aspect is to make the company more efficient in terms of cost savings. Or in terms of people savings. So those are the kind of things that analytic's makes you help you make those decisions. So, and then there are several different type of analytic's. The first one is what we call is a cube analytic's. I'm not I'm not sure how many of your heard about the cube analytic's, but. So it's, it's mostly what we call is a variant of business intelligent, or probably business intelligent analytic's. And where most of your analytic's are either you take a larger amount of data, you do some kind of filtering, and do some aggregations either based on. Some kind of dimensions. So typically your data will have a notion of dimensions. For example, let us take the retail data that you have purchased this product at this store at this time of the day. So, this data has three dimensions. One is the product itself, and the second one is the time, and the third one is the store. So, so when you have these multidimensional data modeling all you can there are a lot of analytic's that he can do with this data. One is, so what he can do is like slice and dice these dimensions. And the data might not only have three dimensions. It can out in a number of dimensions. And you can slice and dice this data across this dimension. So that you can know how much product were from this, sold in this area during this quarter. So those kind of analytics. That is what typically characterizes a business in intelligent analytic's. The second type of analytic's is Predictive Analytic's. So this essentially means like we take a mode of data mining and statistics, and apply to a. Large amount of data. And this was pioneered after the business intelligence came into being. And then a lot of Internet companies, like Google, Yahoo, Facebook, and all piggybacked on this because it gives you. Lot of information about the user behavior, and how to model user behavior, those kind of stat analytics. And this analytic's inten, engine tend to apply a lot of statistics. And broadly they're into what is called linear regression. And some kind of data clustering to identify how the similarity of data is there or not, and there's some identifying outliers in the data, then some kind of associations too. For example in the retail same example, you can figure out whether two products are being bought together. For example. People tend to buy beer and chips together, because when you are having beer you want to have some kind of snacks, right? So what is that type of snacks? So if you wanted to find those kind of association, this is what the predictive analytic's can do. And, moving on, there is, different dimensions of analytic's, so there are broadly two different dimensions, and one is real time, so where you are able to analyze the data instantly, I'll go into more detail about the real time analytic's, what it means, on the following slides. The first is the Real Time, and followed by batch. And batch typically takes a few hours and days to even get required. And there is the two broad sort of analytic's. So you have any questions and answers. Cool. So, so real time analytic's. Again real time analytic's can be broadly divided into two different segments. And the first one is what you call as real time streaming. And in streaming what you do is you do analytic's immediately after data is produced. So the moment where data is produced, it goes through some kind of real-time pipeline and it's constantly spitting out analytic's. And one of the characteristics of streaming is the analytics is running continuously on and on for days and months, keeps on running. The second broad real-time analytic's is what we call as interactive. And so in this case, what happens is like the analytics is the data, whatever data is stored in some kind of persistent storage, later it might be in the disks, or flash, or whatever might be the case, and whenever you like to look at some kind of analytic's on top of this data. You kind of come up with a query or I do a job, whatever it is. Then moment you find the job, and it should return it in a few seconds or even in milliseconds. So the whole idea is like you are exploring the data, then you discover something, then immediately you wanted to drill down farther into that. So that it's kind of funny. It gives you a more often interactive session. And in order to be interactive, you don't want to be like, I type a query, go and have a coffee, and chat with my friends, and come back, and the analytic's is ready. Because that cycle is too long. So in a manner of seconds and milliseconds, if it comes back, then you need it more productive. So the focus of the, the whole Udacity code says with respect to streaming. Because the storm is essentially a tool, to get your streaming analytics down as quickly as possible. That is a border. So now what is real time? I mean in different domains and different markets and different enterprises, real time means different things for different people. So loosely, like, we define, real time in certain ways. But before we get to what exactly it is, let us see the spectrum of, what are the various type of, what do you call? The time duration. So the first is, what we call is a [INAUDIBLE] systems so [INAUDIBLE] systems tend to have, kind of defined millisecond budget. And, that is probably the fastest, kind of analytics that you can think of. And it is very latency-sensitive. In the sense, like, whenever a, let's say you go to an ATM and say I'm withdrawing some money, and immediately the account has to be updated. Similarly, like whenever a tweet is what do you call, posted by you, like then you might have a lot of followers. And those followers have to get the tweet right away. Within a budget of 500 milliseconds. So think about doing that kind of, what do you call, fall-out. That's what we call the term. The fall-out for 30 million users. For example, in the case of Obama having 30, or 33, or 35 million users. How do you replicate that tweet, whatever he does for all his followers in within 500 milliseconds. So on the other end of the spectrum is like the batch which takes a lot of hours and days and that is a characteristic by high throughput, because the reason why the batch takes long amount of time is because of in the sense like you're munging through so much data. And you've there's a lot of data than the disk class is limitation of 100 megabytes per second. You can more, go more than that so, so in order to what would combat limitation? We'll employ multiple disks, so that you can get some, there some simultaneously across each disk. Seen parallel. So, when you employ a lot of such disk and the data is much higher than that it takes hours and days and hence is very important for this kind of anal, batch analytic's. And mostly the batch analytic's is used for doing idol queries. And in between the two. It's what we call loosely a real time. I mean, in the sense like, it's like, a few seconds. And one of the characteristics of the real time is what you call this approximation. So the real time may not be very exact. I mean, there could be requirements where it could be exact, but from a strong definition point of view or from intuitive notion point of view. Real time is more an approximation. So, so that it gives the visibility into your business right away, or, or that you are monitoring for real time analytics right away. And still, it gives you some kind of a approximate AD. And so, one of the I mean. Downsides of the approximation is how close are you to exact? So there should be some kind of, bounds to, the actual real value, right? So as long as the, the approximation is like 5 to 10 percent off from the, actual values, then you're okay. So, so then what we also do at Twitter is, like, we do run a batch analytics. But that is, comes a little bit like four or five hours later. On fixes this approximations of real time as real time keeps forging ahead. So, so that is where we kind of what do you call this a more of real time [INAUDIBLE]. [INAUDIBLE] is a few seconds. Now why we do Real Time, we should be care that we, things should be in real time. And world is going, going real time. And of course tutor is synonymous with real time and that there's a bunch of examples. First there's real time trends. Of course, Twitter is known for break, break out news stories. Break, break our incidents. And that spreads like fire in real time. So, so to be able to be on top of. Real time clients. Second aspect is real time conversations. As you're watching a sports game, and surf some either somebody scored or something that they did and people, you want to instantly start a conversation, Twitter is one the places where people start a conversation and millions of conversations keep going. At any given time. And third aspect is real time recommendations. As these real time trends and conversations are emerging. To the ones who take advantage of those, by injecting the appropriate product recommendation during the time. So that people can do some kind of impulsive buying. Or even add, so that you can click on those ads, which translates into bottom line for us. So then finally, like, Real Time search, where, as, new events and Real Time incidents and trends are happening and you wanted to search it because, in your, in the context of [INAUDIBLE] you will be getting those incidents with respect to whomever you are following. But the search that I put in allows you to search for all the tweet. So in other words as these tweets are coming in, we have to continuously mix them so that when you go to it is Real Time. In the sense like all the tweets that happen in the last second or last few second is immediately available to you. As part of the search. So we have a budget of less than 200 milliseconds in order to do that. So some kind of discussion. Louie you want to? Louis you want to [INAUDIBLE] some discussion about. Sure. The big data applications. Sure so this is just a question for the group. You know, what? What are some the examples of Real Time systems that you've seen? Real Time alo,. Applications, and maybe ideas that you want to see. So, my background is in particle physics- Mm-hm. Which are characterized by like, huge streams of data that we have to be like, sort a grocking as they come in. And one of the realities of that situation is that basically, we're producing much more data that we can actually write to disk. Mm-hm. So, in real time, we have to decide whether to write an event, or to like throw it away and wait for the next one to come in, because we can take like, a fraction of a percent of our data. So, we have to have these like dedicated setup like hardware software systems, that can do like super like, low latency,- Wow. Decision making- Mm-hm. To try and decide, whether we're going to write it down or not. We call it a trigger, but that's one of the reasons I'm so excited about this, I was like, oh, it's a trigger for like, real stuff. Oh, yeah. [LAUGH] Not just particle physics. Yeah, so we use this, we use this all the time. This is like our bread and butter. How much data did you produce? I mean in terabytes or in petabytes- Oh, yeah, yeah, yeah. So, we do, we write, we produce 40 million a second. Mm-hm. Right, a 100 a second and I think each event is tens of megabytes. Oh wow. That's a lot data. So, yeah. It's huge amounts of data. Mm-hm, mm-hm. Yeah, yeah. So, we have, we're cheating a little bit because we have dedicated hardware and sensor systems. Mm-hm. Set-up to be super super fast. Mm-hm. But that is a [INAUDIBLE] much more flexible. Yeah but but with those sensors, the sensors as tools. Fixed the data somehow. Because they're just tapping into all those instruments and all the various things. Once you tap the data. Mm-hm. Then, you run through some kind of data time pipeline. Something like a storm or probably our dedicated systems. Yeah, I mean the other this is, is that it's a big complicated thing and things break. Mm-hm. So, you also need to be running analytics on the machine because there's a sensor that might go down and you need to detect that right away. Mm-hm. because it can shut down the whole machine. So, that's another sort of side project [INAUDIBLE]. Like monitoring those sensors itself. Exactly. Exactly. Yeah, mm-hm. So, that's pretty cool. Yeah. So, anybody else encounter any new real-time systems or. Well, [INAUDIBLE] we do front detection. Fraud detection. Fraud detection. Wow, okay. So, we do see malicious behavior online, in real time. And, as soon as somebody's doing something suspicious or no, anomalous, we should stop that from happening. Mm-hm. So, that's kind of like in the seconds, milliseconds domain. Okay. Yeah. Yeah, that, that's super-cool. I mean, I do [INAUDIBLE] kind of a location information also, location history also, of where people buy things. Right, so, if there a pattern is beyond that, whatever the history that you have seen, then immediately you would figure something, whether to, so do, go to the latency of, whether you in the, loving the courage card to go through, or it's just only for the red [INAUDIBLE]. So, it's not just necessarily credit cards. Mm-hm. But also wired money out to a different person. Mm-hm. That could be stopped. Mm-hm. Day or two to clear the money. Mm-hm. And there's plenty [INAUDIBLE]. But, you can. We start to this wire or you know, to, to, to [INAUDIBLE]. Okay. Yeah, that's pretty interesting, definitely. Yeah. So now let us see, well, how we can do some simple, real-time analytics. I thought I'd walk through some simple examples, how we can do real-time analytics. So there you go. Our Louie is there. [LAUGH] And of course, there's there's boy's boss. And his boss says, like I need a real-time dashboard of the tweets mentioning these media sites and nytimes.comm cnn.com and usatoday.com. So, so how do you go, guys go about doing these things? It's a really simple problem and it has to be done in real time. So what is the simplest approach you can go about doing this? It's from a bunch of Tweets. Assuming that you have access to the Twitter data. So you are looking for all the Tweets that contain these three URLs, right? So, so. [CROSSTALK]. Take a tweet and check whether they have the URL or not. Yes, right. So let us see what it comes up with. So, here you go. So a very simple JAVA/Python program which kind of serves as a backend. And is going to consume the tweets and parse the tweets for the URL and just count them. Right? So then in order to present the nice interface to his boss, so that he can use he can use it, so write a, you'll write a small web app that in turn contacts the program every few seconds and to display the count. So that when Lewis' boss goes to the website, the realtime dashboard is always refreshing. Right? So to give that in perspective,. So you have this job on Python backend, and that is going to keep accessing that to the stream. And the backend also does the same filters and calculates the count. And there is a web app and there is a real time dashboard that boss is going to use. Whenever the AirTime request comes or when the, whenever the board uses the Real Time dashboard, you go to the web app that in turn gets the data from the Java/Python backend. And you can get the visibility in real time right away. Right? Now, the questions. What are the issues with this solution? So the first one is a process can fail. What do you do with it? Because you're about to make crash for certain reason, that you're not aware of. Or even machines can fail. So which means, there's no notion of availability. Because real time means, we are continuously making the data available. All the network is available. So these are two important stuff, right? And because of that computations is lost and your real time liveliness is lost. Right? So these are the first set of problems. So now how do you survey process failures? So one simple way to do it is run multiple instances and load balance across them. So that even when you have got this down, you can go all the way back. And they are doing essentially the same thing. And similarly, you can do that. Java/Python Backend SL2. And they are just doing a redundant computation continuously. But any given, assuming that they are going on the consuming data at the same rate. You should be able to [INAUDIBLE] takes from one of them at least. Right? And the second option is you can duplicate the web server, because web servers are stateless. On the other hand, you don't have to replicate the Java/Python program to do the same work across multiple nations. Instead, you can by do a MySQL on a Postgres database, that's a key-value store kind of equal length. And whenever the web server requests the data, then you can pick it up from there, MySQL Postgres. So those are very simple things that he can do to make it more make our solution more sturdy. And again, part of the issue is with this solution. I think you guys, one of you already told that scalability is one of the biggest problems. Yes, as you can see you can't process data at scale. So the whole idea is like you get a large amount of data. So we get around totally like around 600 to 700 million tweets every day and how you can keep up with it. Because you have to look all the tweets in order to figure whether those URLs are data or not, right? Let say for example, your input data rate is 10,000 tweets per second. Your Java back, Python backend is capable of doing only like say, 1,000 tweets per second, that is a processing rate. Now, so like it takes ten seconds to even process one second worth of tweets. So which means like you will be, by at the end of 10 seconds, 90,000 tweets are going to be backing up already. So which means its kind of, because you were processing rate much lower than the input data rate and they going to keep back up always. So you will never catch up with the input data rate. Right? That is an issue. And if you look at this simple example. So there are two cases you are to consider. One is your input data rate is less than processing rate. [SOUND] The next one is when the input data rate is greater than the processing rate. And so when the input data rate is less than the processing rate. Yes, the Java/Python back end solution will work previously, because I'm able to control it. And when it goes beyond that, that's where the problems occur. And this is a simple exercise, but I think this is pretty straightforward for you guys to do it. And so if your input rate is 500,000 tweets per second and your processing rate is 10,000 tweets per second, how many seconds does it take to process? One second of input. So how many tweet, tweets will be waiting in the queue at the end of processing? Any ideas? Now let us see how we can handle the case where your input data rate is greater than the processing rate. That assuming we are getting 10,000 tweets per second, we can develop them into small four packets. Which is, each one is doing 2,500 tweets per second. And literal Java/Python backend do only 2,500 seconds. So which means, what we have essentially done is you're duplicated whatever you are doing with one data stream. By dividing the data stream into four streams. And running the same your Java/Python Backend program. And this is simply know what everybody calls a sharding rate. So essentially you're kind of divided input into multiple streams. And you, each of the same instances of the program is working on the different streams. So if anybody gets the URL they will know that these are the three URLs everybody has to look for and each one will have a localized count. Somehow. Right? Now the web app that you wrote initially, now it has to go to all the guys in order to collect our global count. Because each one will lower count on nytimes.com. Because the tweet that is containing the nytimes.com might come here, or might come there, might come there too. It can be in any of those streams, right? So similarly the nytimes.com might occur there as well too. So now, if you wanted to make it such a way that the New York Times start count comes into only one guy. Rather than the web app going to several guys, think about the fact that you need to do like 10,000 of these. So then web app has to branch out 10,000 guys and get it, right? And you don't know whether they're going to be alive or not too, right? So you get into all these other complex issues. So inside a sim dad, like, if you want to get the New York Times account at one single place, what do you do? So which means like, we need to do another level of some kind of, Java Python backend. And we have to route the data that this, this guide is producing, on account he's producing. Especially with respect to some news articles, like newyorktimes.com, to one guy. So that, if everybody is routing thenewyorktimes.com URL to one guy, that one guy can accommodate the count appropriately. Right? So that is what what we call as some kind of moving data based on some groupings, or some kind of positioning strategies. And so you'll go into those groupings and packaging strategies more detail. And this is kind of a, what do you call, a basis for most of our big data processing. Sharding and partitioning So if you understand these two concepts most of the beginner later the infer or the infrastructures are all based on these two concepts. So, if, so in this case if if all the accounts that we just seen. And periodically for descending deleted on the NewYorkTimes.com to one of these guys. Then this guy will have all the cones for NewYorkTimes.com, right? So in reality, what happens is like they have this notion of queues and that's how it works. Each of those process also have this notion of queues. And once this data stream is processed across each one of them they dump into the queue of the other process which is responsible for that. Newyorktimes.com or USAtoday.com or. So that variant of using queues is the fact that this guy can go at it's one speed, as long as there is enough buffering inside. So that as long as you're keeping up with the rate of the processing and not letting the buffer go beyond certain limit. As long as you can do that both of the processes can go at a different speed. Now, what are the problems that it represents? First of all, scaling is painful. So, think about the fact that I ought to start these end process on n different missions. And if we have a multi stage like the way at least two stages in this case. And there are other jobs that will require eight stages, nine stages in all. Think about managing that processes on these different stages across different, hundreds and thousands of machines. It gets really painful, because what happens if one guys dies? And I don't know what the guy died, or whom died, and how to relocate that to another machine. So these are all, all kind of operational problems, which is nightmare to handle any of, in any way. So on poor fault tolerance, like worker failures, or even process failures, and mission failures and relocating them. And third one is coding is very tedious. I mean think about writing your own Java program, and for two different stages, on hooking them up together in certain fashion. Then and making sure the data is moving to the right processes and all, right? So it makes it tedious to write all these things again and again. Especially for every job that you have to do, right? Because there could be a lot of analytic job that you'll keep running. And finally like there's no guarantees in terms of whether what do you call, whether the message that I sent out from one stage to another. Is it lost? Or has got, again, replayed. Or it's post-processed. So you don't have any idea but there are, reasonable you get results in a clean, cleaner way. So what I meant is, there are two different kind of guarantees. Engine build realtime system tend to provide. But this hard oscillation will not provide that. So one of the first one is at most once. And another one was call at least once. And finally, there's call exactly once. So at most once, what it does is, your data, the realtime data is guaranteed to either processed or not. So at most once is up to, only once it can be processed and at least once, your data is always guaranteed to be processed at least once. But there are instances where it could be processed multiple times. And exactly once is to ensure that your data is processed only once. And exactly once will give you very exact result. This end's like the other two will have approximation. The second one, at least to once will have a word, probably word counting or averaging or whatever might be the case. But at most once will give, kind of a low, an approximation from the point of if you do an undercount. If your count is aggregation that you are doing. Okay, so that is the broader discussion, definitely. I can go into those details, I mean, major details, but I think that I'll take it to offline. So if you want we can talk [INAUDIBLE] detail. Sure. But the cost is exactly once is probably a higher cost, at most once is the least cost, at least once is. That's is the, some So, that is probably the broadest spectrum. So what we need in if you really want to do some kind of real time analytics. Some kind of a guarantees. Like the one of the guarantees. Okay, like am I getting at least one. So that most ones are exactly ones. I need some kind of guarantees. So that when you analytics you know what you can do with it, right? Second one is, scalability of course. And scalability without having to deal with mission failures and process failure. So that you can scale. Just dump a new set of missions, and automatically the job is, take those mission if it needs, right? So, automatically scalable and usually scalable. And robust fault tolerance. Whenever any failures occur, automatically it should the system should relocate some of the jobs automatically. So that you do not do any [INAUDIBLE]. You won't want to get woken up at 3 a.m in the morning because a pager went off, right? And finally you want to do, do a concise code. You just out write analytics. You should not be writing about how to route the data from one stage to another stage. And going from one mission to another mission and all these meanings and aspects. Which is not really related with analytics. It's more of a, intra or a more of a plumbing that you learn how to do for every program that you write Storm provides exactly that. So Storm is a platform for analyzing streams of data and as they arrive or the moment it is produced. So that you can react quickly based on that analytics. Storm has a small set of primitives that you can use to express your real-time computations without having to worry about how it's being done. All you need is to like express what you want. It knows how to do those things at the bar. Now one of the biggest things people ask for is what is Storm versus Hadoop. So they are actually complementary. In a sense like Hadoop does a big batch processing, where your regular map produce job and your final map produce job and it runs on several nodes on the probably for a few hours or days analytics and that is more of according to our definition is batch. And as Tom predicted, fast reactive and real-time processing. Because the model trains think the data is produced, storm takes over and starts spitting all those analytics in a continuous fashion. So those are the two and and I mean, a lot of people ask why I need to use to Tom when there is Hadoop. I mean, on thing that people think about saying hey, by the way, like where can I use the Hadoop job. Which is same as whatever this Tom job does. But except the fact that I keep running it every few seconds. Or every few minutes or whatever might be the case. But if the data rate at which you are producing is even higher than the amount data that can, that a Hadoop job can produce within the three minute or every cycle of the job still you will not be able to produce the, using Hadoop. So you are to still count on Storm. And again Hadoop has this what do you call this two stage. It's map phase, as well as reduce phase. That's only two stages. On the other hand, Storm can express multiple stages. You can have any number of stages as much as you want. And the data is kind of flowing within these stages continuously. So that, those are the kind of underlying huge differences between Storm and Hadoop. In addition to these big big batch versus real-time processing. So now, like, let's look at what is a Storm data model. Hm, the Storm has a bunch of primitives so one is Spouts, which is actually the, actually the source of that stream. So it could, it could be anything. So it could be coming from either Postgres or MySQL or we have the notion of a distributed queues, where the data gets dumped the moment it is produced and those data are available in distributed queues like Kafka and Kestrel. And Bolts are the units of computations where the data is actually transforming in certain fashion. Either it is aggregated or filtered or it could even be what you call joined with some other streams as well. So you can do, what you call if there are multiple streams coming in. For example, that tweet stream and there is a stock quote stream. So I wanted to know in the window of the last ten minutes what is the sentiment of a particular, what do you cal uml, stock? For example, I wanted to know the sentiment of stock, named for Google in the last ten minutes, then you need to join within the tweet stream, as well as the stock stream. So that you can see whether the price is reflective of the sentiment that is expressed by people on Google for the last ten minutes or so. So which means, actually it's a, a stream join. So then there is the third one called Tuple. A Tuple is nothing but a unit of data that is entering the stream. And that is nothing, but an ordered list of several elements and Topology. And the Topology is a kind of DAG that stitches the spouts and bolts in a certain fashion, so that you have the desired computation that you want. So bolt expresses smaller unit computation. But the combination of the spout and bolt in a DAG fashion, which expresses the data flow, as well as the computation will express your job structure or the design computation that you want ultimately. So for example if you look at simple Storm Topology, this is the way it would look like. Here are two spouts and these two spouts are the ones where the data is getting produced or probably accessing the data that is being produced. And that injects into three different bolts. And finally, there are two set of bolts that in turn. So it's only the data produced here goes through this data transformations or data aggregations. And which in turn goes into next stage where further aggregations or even writing off aggregation writing in Tor. Persistence Tor where you can query them back. So that is what happened. This is a gives again an idea of Topology. And to give an example of the work on Topology, which is one of the very simple topologies. So we will have a something called a Tweet Spout that does talking to what do you call it. A Twitter current shells and getting the Twitter stream in a continuous fashion. And after that, there is a Parse Tweet. So you get the tweet and after that the Parse Tweet Bolt essentially, kind of picks at tweet and breaks it up into small words and figures out what other distinct words on those Tweet. Then finally, like it sends to the word count bolt, which in turn picks up the word from different parse tweet bolts and counts those distinct words. And finally, it might be ready into process install like bolt or Hadoop. And so, for example, we'll have the like World Cup used to get one million tweets. And out of that Soccer for two, 400K times. Right? So, and that is what I call is a logical plan. I already express you spout and bolt, you're just here describing your deck under. But however, when it goes into execution in the actual missions, physically in those host processes and, and missions. This is what you have. So there will be several instance of this Tweet Spout. That are, you will be executing across different missions. And similarly, you have bolt. And similarly, your word count. This is what we call lesser Parallelism. So for every spout and bolt you're to express, what is called as a Parallelism. So I want like tendencies of this spout and I want 20 instances of this bolt and I want 15 instances of the, like word count bolt. How do you decide this Parallelism? Well, because like, so that also leads into grouping. So now, we have seen the data flow from spout and bolt. Now how do you say, where to go? For if, for example, the spout task one and it has to go to the bolt task two. I mean, how do you say that? I mean like, because there is some semantic associated with it, right? So when a parse street bolt and it's a two pull, which word count bolt it should go. And Storm provide as, what we call the notion of groupings. So, so first is Shuffle Grouping, which means it can go anywhere. So I slot it randomly can go to any task in the next stage. So that is what Shuffle Grouping means. And Fields grouping is essentially, hash based. So you tell them that that these are the fields that you should care about and based on the values of these hash fields this should go to the appropriate bolt. So that is what is called a Fields Grouping. And all grouping, in the sense like that every Tuple that I produce goes to all the bolts of instances in the next stage. And Global Grouping is you get everything to one single instance. So with this and most of the time you end up using these two. And with these two, you can pretty much get all your processing done. I have not seen any examples so far, where this was not enough. So pretty much, you can get everything done. We've seen Karthik introduce the theory and motivation of the Storm data model. Amazingly, his presentation is all that is given to real hackathons held at Twitter, Stanford and Berkeley. In this course, however, we will help you connect Storm concepts to Storm code syntax in the remainder of lesson one and lessons two and three. Finally, in lesson four, you will complete a simple final project to complete the course with the option to hack away further on more complex projects along with our hackathon participants. Let's get started linking Storm concepts to Storm syntax. The Storm Data Model contains two simple computational units, spouts and bolts. In this course, we will identify spouts with a red triangle to represent sources of data for the topology. In other words, coming from Postgres SQL, MySQL, Kafka Spout, or Kestrel Spout, or the Twitter Sample Stream. Next, we will identify Bolts with a blue square to represent units of computation on data. In other words, filtering, aggregation, join or transformations. I'm using a red triangle and a blue square, simply because I can't really [LAUGH] draw the spout and bolt like Karthik. The next concept of the Storm Data Model is that spouts emit tuples or an immutable ordered list of elements. Bolts receive a tuple, do internal calculation and emit a tuple. Know that tuples are immutables. So, a new tuple is emitted rather than a modified tuple. Finally, spouts and bolts are linked together in a Storm topology to create a directed acyclic graph, meaning simply that there are no cycles. Although cycles are permitted, we do not typically use this feature. Vertices are computational elements, either spouts or bolts, and edges are streams of data. Finally, edges are defined by four typical groupings. There are more, but these are the typical and most often used. Shuffle grouping randomly distributes tuples to the next stage of bolt instances. This shuffle grouping in Storm allows you to automatically rebalance tuples. The fields grouping groups tuples by single column value or multiple column values. The all grouping replicates all tuples to the next stage of bolt instances. This is a rare grouping because there typically is fan out with all grouping, basically. You need to be careful that all of your data is not going to be replicated to such an extent that your topology fails. We do use this in lesson three. The final grouping is the global grouping which sends all tuples of a single stage to a single bolt. You will have the opportunity to work with all of these grouping types in the remainder of the course. Let's get started with code. On a final note, linking Storm concepts to Storm syntax, navigate to storm.apache.org at anytime throughout the course, and click on Documentation. As an open source project, Storm has excellent documentation. And as our hackathoners discovered, sometimes, it was easier to simply look at the Java docs for any questions they had. As of this recording, use the Javadocs on the left hand pane and not the documentation Javadocs in the middle of the page. Which again, as of this recording, goes to Nathan Marz's old Javadoc's site as a version 0.8.1. Navigating back to the updated Javadocs you see that the Storm core documentation is at least up to 0.9.1. Remember, our Storm version is 0.9.2 incubating. For example, searching for tuple and clicking on tuple, we see in the method summary the getString by position method along with getValue at position method. Along with a number of different methods that may be useful for you. As you can see by all packages and all classes, this course is touching the surface of all of the different things that you can do with Storm. So with that quick reminder, let's continue on with the course. Now that you have Vagrant and Virtual Blocks set up on your local machine, either Windows, Mac OS X, or Ubuntu, or another version or distro of Linux, we now are going to clone the gate hub repository for the course. So in step one on your local machine at the command prompt, navigate to the directory you want to create the course. I've created a directory called Real-Time$. Next type in the command git clone with address https://github.com/Udacity/ud381. You can also simply copy and paste. So I'll copy this, go back to my directory, type this in and hit Enter. Once you see your repository cloned, you can do an ls which stands for list. You see ud381. Do cd into ud381 and now we do an ls again and we see that we are in the ud381 directory. Going back to the directions in Step 2, I'll scroll down a bit. Depending your download speed, the following command will take about 20 to 30 minutes. You can continue on with the course once you hit Enter and the process can take place in the background. So, go back, make sure you're in ud381, and type the command vagrant up. So type in, make sure you're ud381, vagrant up. So that's two words, vagrant up, hit Enter. Just wait a few seconds and make sure you see the download process begin. It shows that the box can't be found and what's happening is your computer is connecting to the Vagrant cloud to download the box udacity/ud381. So notice the progress here may take 20 or 30 minutes. And we can just continue on with the course. Great. And hit Next to continue. Continuing on with our basic installation, you can use your favorite text editor for this course on your local machine. If you do not have a favorite text editor, I recommend using the open source tool, Atom, for it's simplicity. This requires OS X 10.8, or Windows 7 and 8. So please check that your computer's compatible. For Windows users, be sure to unzip the file after downloading. So you can simply go to atom.io, you can copy and paste, or simply type in atom.io. And depending on you operating system, I'm using OS X, it should give you the correct download button here. So, simply click on the button, and begin the download process. Another option includes Sublime Text Editor 2. This is a great option to test out, and we do encourage you to purchase it after your review period, if you decide to go down the Sublime Text option. You can also complete the course using complete virtual box commands in Ubuntu like Vim or Emacs, but we will not cover this in this course. Once you're done with your option, click Next. Step one for this lesson is to check that your downloads are complete. You should see a command prompt waiting instead of a download percentage. If you see a download percentage, that means your box is still downloading and you can feel free to continue on with the course and then just come back to this reading node, when you're ready to move forward. So, once your download's complete, in step two in the reading node, you're able to log in to your virtual machine. You'll be using the SSH or Secure Shell command to remotely log in to your guest machine. So on your local machine at the command prompt, type in vagrant shh. Hit Enter and what's happening now is you are logging in in a secure shell into your Ubuntu machine. And when successful, you'll see that your, your command prompt has changed from whatever file you are in to vagrant@ubunu-tu1404 for i386, meaning that we're using a 32 bit machine. in step three, once you're in your Ubuntu machine, you can check your Storm version simply by typing storm version, and you should see as of the time of this recording, 0.9.2-incubating. Incubating means that's an Apache incubating project, but Storm recently graduated to a top level project so this number will be changing soon. But it's good to know that the API and the entire course should remain the same. If you don't see a Storm version here that may mean that you haven't logged into your Ubuntu machine. So in your local machine type vagrant ssh again. This is a common mistake. In storm 4 we navigate through the vagrant machine by typing cd. . And that goes up one level Ls for list files. Do cd. one more time and you'll see that we're in a top level directory. Next type cd /vagrant. The forward slash is an absolute directory that will take us to the vagrant folder. Once we're in the vagrant folder, type ls, and you'll see the same files that you saw when downloading the file ud381. These shared files between your host machine and the guest Ubuntu machine is called guest additions. So any file that you put in your local machine into ud381 will show up in your forward slash vagrant file here. Optionally in part 7 I'll show how quickly to log out. You simply type log out, Enter, and you'll see the connection is lost and we're back in our ud381 folder in your local machine. Either Windows, Mac OSX or your own Linux machine. Next you can re-login with vagrant ssh and cd into /vagrant. Ls will show us our files again. Optionally in step eight, on your local machine you can copy a picture of yourself or avatar in a .jpg or .png format into your local ud381 folder. This will be put in to the visualization at a later time so that you can personalize your visualizations for your screenshots. So, download a picture of yourself. I've downloaded mine. And I'm in a Mac OSX so this is my finder window. And here's my picture, louis@udacity. And I've opened a second window here with UD 381. And so I'll just copy. And drag this over to, to my file. Next in our Ubuntu machine, I'll just type LS, and I'll see LewisATUdacity. So again, this is the virtual machine. And I copied the picture into my local machine and this shows the shared files. In the next optional step nine, simply move the picture into the folder vis/static. And you do this using the command, mv, or move. Lewis, and I can type in Tab to complete the entire JPEG file name. Next type the destination viz/static, s-t-a and Tab. Hitting Enter will actually move that file. Ls will show that it's gone. And we can move into the viz directory static. Well let's go into viz, ls. And you see that there's a static folder, cd static. And finally, in optional Step 10, simply come into this folder and type ls. And you should see a Twitter logo, the Udacity logo, and yourname.jpg in the viz/static file. Great job, and you can continue on to the next lesson. In this course we will be using Maven, a build command tool, in order to compile and package our storm program. This is another top level open source Apache software foundation project and more information on Maven can be found at maven.apache.org. So, in step 1, navigate to your top level/vagrant/ file. So, again, if you're following along with optional exercises, we are in our viz/static, and we can do cd/vagrant/ and ls. That will show the lesson 1 through lesson 4. And again, this is the file that shared with your local machine. In step 2 of the routine instructions, navigate into the folder lesson 1, by typing cd lesson 1. cd lesson 1, again, you can use tab completion, and you that there's one, two, three, four different lessons, so we need to specify lesson 1. And we'll type ls to see stage 1 through stage 3. Finally, in step 3, let's go in to stage 1. Again with cd stage 1. cd stage1 ls. So in step four we're going to run our first maven command by typing the command letters mvn followed by a space and clean. Mvn stands for Maven. So the full command is mvn clean and hit Enter and you should see, after downloading files, a build a success. If you don't see build success immediately don't worry about it, what's happening is Maven is downloading dependencies for our project and depending on your download speeds you may not get all the dependencies in the timeout session. So if you don't see build success simply run mvn clean again until you see build success. Next in step five we're going to compile and package our storm program by typing mvn package. So again remember that you want to be in lesson 1, stage 1 type in mvn package, and hit Enter. The first time we hit maven package we will see a number of dependencies downloaded that are needed to support the storm program. This process may take five to ten minutes, so please be patient. Finally once you see build success you can type in a command tree, T-R-E-E, and you'll see that your new class files ExclamationTopology, ReporterExclamationTopology including jar files two jar files, one with dependencies and one without, are created. So congratulations on building your first project. Please continue on to learn how to submit your project to Storm. In this section we'll move on to simple storm topology submission. Well, you'll submit the program that you just built in Maven. The topology you just built takes in a TestWordSpout linked to an ExclamationBolt using a shuffleGrouping linked to the same ExclamationBolt using another shuffleGrouping. We'll be exploring the design of topologies and the syntax, including groupings, throughout this course. So for now, let's simply submit the storm topology that we just built. Step one is to view your files by typing ls. And again, all of these directions are in the written directions, and you see the pom.xml file. We'll speak about the POM more in later lessons, but it stands for the project object model. Here's our source files and we see a new class file, target. Again with tree, T-R-E-E, and we see the compiled class including the jar and jar with dependencies. In Storm we will be using the jar with dependencies. Finally, to submit our first topology, we use this class structure to run our first program. Remember to use the Tab shortcut to help fill in the following command. Also note the space between storm and jar, and jar and target, and finally target and our ExclamationTopology. And also remember there is no .java after our ExclamationTopology since we're submitting the compiled class. First type storm jar second target, and again target with a tab we'll find this target directory. Next, we want this jar file. So we can type in udacity tab, and remember that we have two jar files, we actually want. We actually need the jar-with-dependencies, so hit the hyphen, and the hyphen tab will get us the jar-with-dependencies. Finally, we want udacity.storm, and we're going to submit our ExclamationTopology. Once again, that's udacity.storm.ExclamationTopology. And remember there's no .class when we submit. Hit enter and you'll be running your first storm program. Storm runs for a while and then shuts down. And we can scroll up to see some of the output. We see that there's a word and exclaim1 and exclaim2. And we see that there are names followed by exclamation points. We'll explore this further. But for now, great job on submitting your first Storm program. Sorting through command line output like this, will get boring really quickly. So I've created a visualization that we will use throughout the course. We'll launch this with the following commands, but what I recommend is to have one tab open, where we will compile and package our Storm program in Maven, and run using the Storm submission process I just showed you. I recommend you create on your local machine, and depending on your operating system this can either be a new window, where you launch a new batch shell, or simply a new tab on your terminal. I've created three new tabs here. But here we see the tab in real time, this is in my local machine and if I type ls I can see that here is ud381, cd into ud381. And now instead of vagrant up, we simply log into the machine using vagrant ssh. So now you see I have one tab open with our mvn storm commands, and one tab open that I'll be using for our visualization. In step one of the written direction, simply cd into /vagrant/viz and ls and you'll see the Python program, app.py, along with the static and template folders that, if you follow the optional exercise, you put your picture into. In step two, launch your Flask microserver by typing python app.py. Python app.py and hit Enter. Here we see that our flat server is running on an IP address of 0.0.0.0:5000, which simply means that it's running on our local machine on port 5000. In step three of the written instructions, open a new tab in your web browser, either Chrome, Firefox, or Internet Explorer, type in the following address, or simply cut and paste. It's http://127.0.0.1:5000. I'm just going to copy and paste this. Copy, paste. And once I hit Enter, I'm now communication with our virtual machine through port 5000. And you should see the visualization, Udacity and Twitter bring you real time analytics with Storm. Along with the Udacity and Twitter logo. Next, if I look back to our flash server, we see that we have a GET request that just came in for channel, WordCountTopology. One note here is that if you're having problems making that connection, that's okay. And we do have a troubleshooting section at the end of this lesson, that can review a few of the common troubleshooting solutions. Finally for an optional personalization exercise this requires your photo or icon to be located in the folder /vagrant/viz/static. And remember this was the optional exercise to a previous lesson. So in step one of the optional exercise open your text editor, and again I'm using Atom in your text editor, and if you're using Atom click Open and click. On the top level file, ud381, not any of the individual files, but on the top level and click Open. And you should see the file hierarchy appear on the left-hand side like so. Once you have the top folder open in your text editor, navigate to. Your vis templates section. So we go to vis templates and open the file cloud.html. So cloud html. This is our basic html file for our visualization. We see that the Udacity logo, and Twitter logo, are defined right here in the body h1 tag. To add our image, we simply copy and paste the Twitter logo, and change the name to the picture that you defined. Mine was LouisAtUdacity.jpg. Let's personalize the page further, by coming to the Word Cloud exclamation points, and just naming it with your name. So I'll say Louis's Word Cloud. Next, navigate to your visualization, and simply hit the refresh button. And you should see your picture, and your name with your word cloud appear. So congratulations, on personalizing your visualization. And as we go through the course, you can take screenshots of your visualization and post them to the forum, and we'll know it's you. Great job. Now that your visualization is running we need to connect Storm to Flask and D3. We've chosen Redis to make this connection, and this is your first exercise in modifying Storm, and remember throughout this course, at the end of each exercise, your code will match the following stage. So at the end of this exercise your code will match stage two. If you run in to any problems along the way, you can find the solution in stage two, and come back and either modify your code or just continue on. In step one, if you're following around with the videos you already have a second tab open with your python flask server, but if not, you can follow the commands to open another shell in your tab, Vagrant SSH in to your machine, navigate to the correct lesson and restart your visualization. Therefore, in Step two of the written instructions in your second tab, you should already be at Lesson one, Stage one, if not, simply cd into /vagrant/lesson1/stage1. Ls should show your pom, source and target files. In your text editor, navigate to lesson one, stage one, source, jvm, udacity, storm and open the file ReporterExclaimationTopology.java. Maven uses a ProjectObjectModel.xml file, or the POM.xml, to manage product dependencies. Therefore, to make our visualization run, we're using the lettuce module to connect our Java program to redis. So in the first to do part one of four, is to import the lettuce module into the POM.xml file, the XML dependency must be located in the section. If you are unfamiliar with XML we will cover this in detail in the solution video so, copy and paste the dependencies into your palm file. And your palm file is located in again lesson one, stage one and palm.xml, and you can simply scroll down and find where I want you to copy and paste the lettuce redis. Once that's completed, go back to your command line and type in mvn package simply to confirm the build success. Once you have build success following along in step six of the written instructions, you are going to go back into your ReporterExclamationTopology and fill in the import statements for your lambdaworks.redis.RedisClient and lambdaworks.redis.RedisConnection. Once your import statements are entered in part one of four, scroll down to step two of four. In part two of four, we're in the class exclamation bolt and we're going to create an instance of our redis connection redis. After that's complete, scroll down to, to do three of four and instantiate the redis connection and the actual connection, and the syntax is provided in written instructions. Finally, in step four of four, simply uncomment the long count and the reddish.publish to the stream word count apology, which will take in the exclamated word.toString, separated by the bar, and our long to stream of account. This is the connection that will connect up with D3 through our flat server to run our visualization. Finally in step seven, go back to your command line, and package your program, and submit again, using storm, jar, target, udacity. Remember the hyphen with dependencies, and this is udacity.storm and this reporter exclamationTopology and hit enter. And, once you hit enter to submit your storm topology, you can bring back your visualization and youll see that names along with three exclamation points or six exclamation points will be shown, and if you hover over any of the names, you'll see the long count 30 appear. So, good luck on connecting your visualization and once you see your visualization running, congratulations on completing stage one. Remember that your code will now match stage two, so any problems along the way, simply look at stage two code in order to complete stage one I walk through my solution of visualizing Storm with Redis and d3, here. In step 1, I opened a new tab on my local computer, and typed vagrant ssh with login success. In step 2, I navigated to lesson 1, stage 1, and you can simply copy, and paste. In step three we're using the lettuce module to link Java to Redis. So in your local text editor, navigate to lesson1 > stage1 > source > jvm > udacity > storm and ReporterExclamationTopology. Next, we copy the dependency to dependency area and open the palm.xml file. Next, we scroll down, find the COPY AND PASTE, paste our dependencies, and finally uncomment. Notice that you need to copy and paste your dependency outside the comment statements. Briefly, if you're unfamiliar with XML, XML is simply a tree structure. Where there's an opening and a closing tag for every element in a nested fashion. Therefore dependency is closed by dependency, group ID is closed by group ID. And a larger section, build for example, is always followed by a closing tag of build. Finally remember to save your palm.xml file. And step 5, on the command line run MVN package. Once you have a build success, we move on to step 6. We first import the RedisClient and RedisConnection. You can do this simply by copying, navigating back to your report exclamation topology, and paste your import statement. Next in step 2 of 4 at the class level, we declare our RedisConnection variable. Notice that the class level we declare but in the prepare method, we instantiate. We can simply copy, scroll down to the class level, where we see another variable OutputCollector_collector declared but not instantiated, and finally paste to declare our RedisVariable. So in the prepare method we are going to copy and paste to instantiate the RedisConnection. And we scroll down to the prepare method where we see another variable _collector. Instantiated with collector and paste to instantiate the redis connection. Next we copy the actual connection, and again in the prepare method we instantiate the actual connection. Finally, in step 4 of 4, in the execute method, we define the logic of what happens when a tuple comes into our. We scroll down to the execute method that receives a tuple and complete some logic. Finally, in part 4 of 4, we simply un-comment the long count = 30. And our redis.publish. Throughout this course we'll have a number of opportunities to both update and use all of these different methods. So for now, let's just run our updated topology. In step 7, we build again, by typing maven package. And once we have another build success, in step 8, we submit the new topology ReporterExclamationTopology. You can copy and paste the ReporterExclamationToplogy submission here, or you can simply enter at the command line using storm jar target with a tab udacity hyphen tab because we want the jar-with-dependencies throughout this course. Udacity.storm and the ReporterExclamationTopology. That one submitted, hopefully you will see your visualization running. Your word cloud, with a number of test names, with three or six exclamation points following each name. One final point, you may need to do a Ctrl+C at the command line to terminate your Storm topology. That simply your Ctrl+C and you'll see your command prompt up here again. Congratulations on modifying your first Storm topology. Congratulations on completing Stage1. Now navigate to your Stage2 in Lesson1. So, Stage2 > Source> jvm > udacity > storm and open your reporter exclamation topology. If we scroll down to the topology in the main section, we see that we're using a spout called new TestWordSpout. If we scroll up, we see that the TestWordSpout comes from storm Core in backtype.storm.testing.TestWordSpout. These were the four names that we saw up here. Using TestWordSpout is a nice start, but let's now add a RandomSentenceSpout to visualize sentences. Your assignment is in four parts in the reading node. You may need to do a Ctrl+C in order to stop your storm program, but once that's done and you get back to your command line, you're at lesson1, stage1. And navigate to lesson1, stage2 and do cd/vagrant/lesson1/stage2, and LS should show your .pom files and source. Notice if you're using Atom then you can see the file structure that I've added in a new spout folder, then you can open and we see the RandomSentenceSpout.java file. In part three of the written instructions, you must modify the reporter exclamation topology, to import the random sentence spout. And please enter the import statement here in begin stage2 exercise part 1-of-2. And don't forget the ending semicolon required in Java. As a hint, you can look at the package structure of udacity.storm and form your import statement. Once your import statement is completed, you can go back to your command line. Remember that you need to be in lesson1, stage1. And write mvn package. Once you have build success with your import statements, in part four of the written instructions scroll down to your main method where you see the TopologyBuilder builder with the spout word and the bolt exclamation one and exclamation two. In part 4a, I'd like you to ignore the TestWordSpout, add in the random sentence spout with an id or component id of rand hyphen sentence, using a parallelism of ten. And finally, connect exclaim one to the RandomSentenceSpout with a shuffle grouping. And again as a hint, this idcomponent should match your RandomSentenceSpout. Finally, package your program and submit. Once your storm project is running you can check all your visualization. And finally, once your Storm Topology is running, you should be able to go to your visualization, and see now random sentences being visualized. And once you see your visualization, you know you're done with stage2. Congratulations. Here's my approach of adding a spout to the topology. In part one of the command line, we navigate to lesson 1 stage 2. I can do that by cd dot dot, or cd space doe dot, ls, and cd stage2. In part 2, in stage 2, we navigate to the new ReporterExclamationTopology.java file. Therefore in lesson one, stage two we go to source JVM udacity, storm and the reporter exclamation topology. Notice again, that this is the entire solution to stage 1, if you ever run into problems. In part 3 we see the added spout under the spout folder. Random sentence spout, which is here in storm spout and random sentence spout. We then enter the import statement, with the package structure using package udacity dot storm as a hint. Scrolling down to the begin stage two exercise part one of two we import our random sentence spout. And that's simply, udacity.storm.spout.RandomSentenceSpout, which corresponds to the structure udacity storm spout RandomSentenceSpout. You'll find throughout this course that I like to test often, so, run mvn package, just to get in the habit. Back at your command line, type mvn package in lesson one, stage two, and we see a, we have a build success, which means we imported our random sentence outcome correctly. After our build success in Part Four, we update the topology builder to first, ignore the TestWordSpout. Back in our ReporterExclamationTopology, we scroll down to our TopologyBuilder, and we want to ignore the TestWordSpout, which we can do by simply commenting. In part B, we add the RandomSentenceSpout with an id rand-sentence, using a parallelism of ten. I'll use the test word spout as an example. And I can copy and paste. We want an ID of ran sentence. And instead of a new test word spout, we want a new random sentence spout and finally we leave the parallelism at ten. In part C, we connect the exclaim one bolt to the random sentence spout using the ID rand-sentence. Back in our topology builder, you could either update the shuffle grouping connection from word to rand-sentence, or you can simply common tab that section, copy and paste and use the syntax of word, to link rand-sentence. In this way we've connected our spout of rand-sentence to the bolt exclaim1 using a shuffle grouping connection. Next we see the bolt exclaim2, is connected to exclaim1 using another shuffle grouping. and in this way we can build up a topology. Next, we package our program, using MVN package. Next you can use the tree method, t-r-e-e, to see your file structure and next we submit our program using storm jar target. Remember you can use tab, Udacity, space jar with dependencies, and the structure Udacity.storm ReporterExclamationTopology. And hopefully with a built success and submitting your Storm program, you are able to enjoy your visualization with random sentences and exclamations appended at the end. Preach on modifying your first topology. Great job connecting the random sentence spout to your topology. Notice that we're emitting the sentences to redis-send our visualization twice. The first time emits three exclamations appended to our sentence, here in the exclamation bolt. And the second adds three more, here in the second exclamation for a total of six exclamations appended to the sentence. In our visualization this is our random sentence spout here is exclaim one and here is our exclaim two. Finally they are connected with a shuffle grouping and the second is connected, again, with a shuffle grouping with an exclaim one to an exclaim two. And again in exclaim one we see three exclamation points, and exclaim to three more appended for a total of six. In part one of this exercise, I'd like you to modify your topology to omit only sentences with three exclamation points. Do not modify the exclamation bolt themselves, but instead modify the topology. There are actually three ways to link your random sentence spout to accomplish this. I'd like you to implement all three. So in order to append only three exclamation points, one solution is to connect your random sentence spout, directly to exclaim one bolt. And here we'll remain consistent and only use a shuffle grouping. Once you've accomplished this and run your visualization. The second apology is again to use the same random sentence spout. Connect only to exclaim two and again using a shuffle grouping. Finally the next way to emit only sentences with three exclamations is to connect your spout to exclaim one and exclaim two using a shuffle grouping. I'd like you to build and run each topology. And remember that the sentences should only come out with three exclamation points appended to the random sentences, not six. Finally, choosing one of the topologies, in part two I'd like you to navigate to stage two. Into the spout and the random sentence spout. Scroll down and remove the original sentences and write new sentences that let us know how you're finding the class. Or share your favorite song lyrics. Take a screencast of your visualization and post this to the forum. Remember that the visualization should have your personalized sentences, appended with only three exclamation points. Comment on two other posts, and optional if you already have a Twitter account, why don't you tweet a screenshot to your followers? And as an example, just remember that your sentence should have only three exclamations at the end of it. And these are the original sentences, but we'd like these sentences to be personalized to let us know what you think about the course or perhaps your favorite song lyrics. Good luck with stage two and the end of lesson one. Hopefully you found this exercise interesting as a way to modify topologies. So this is the topology we're starting out with, and we have our random sentence spout linked to the exclaim1 bolt using a shuffleGrouping that appends three exclamation points to the end of our sentence. Next we have the exclaim2 bolt, linked with a shuffleGrouping to exclaim1. That appends three more exclamations for a total of six, to each of our random sentence. To tackle the first topology, we simply want the random sentence spout connected to exclaim1 only, using the shuffle grouping. This one is simple. We simply break the connection between exclaim1 and exclaim2. In code, we can simply comment out exclaim2. Once exclaim2 is commented out, we see that the random sentence connects to exclaim1. And we should have, not six, but three. Let's run through the maven package and submitting our program to Storm. Remember that your lesson should say lesson1, stage2, but I'm working on lesson1, stage3 in this solution. So, in lesson1, stage2 type mvn package. Once we have BUILD SUCCESS, we submit our program, storm jar target/udacity remember with dependencies, udacity.storm, and this is ReporterExclamationTopology. Once the program is submitted, we can go back to our visualization, and sure enough our sentences have three exclamation points. So I think our topology number 1 is a success. Going on to topology 2, notice that our random sentence spout is connected only to exclaim2, bypassing exclaim1 using a shuffle grouping. To do this, notice that we'll need to break the connection of our random sentence spout to exclaim1. And instead connect our random sentence bout directly to exclaim2. In code this is also very straightforward. In our topology we first uncomment exclaim2 and comment exclaim1. But notice that exclaim2 connects to exclaim1, which is no longer in our topology. If we were to build and run this topology, we would get a build success, but a runtime failure. Let's try it. We mvn package, we get a BUILD SUCCESS. But when we submit our topology, we get a failure and our visualization does not run. To fix this, we need to link exclaim2 to rand-sentence, and that simply is defining the shuffleGrouping to rand-sentence. So we delete our our exclaim1 and we enter in rand-sentence. That forms this connection from rand-sentence to exclaim2. And we should get three exclamations. Let's try this. We'll mvn package again. And once we get build success, I'm using the up arrow and submit our job again. Finally we come back to our visualization. We see the Storm program running in the background. And here are our sentences with three exclamations. This is just off the side of our SVG container. And there we go. So that completes topology number 2. Finally, for our last topology, notice how exclaim1 and exclaim2 are both connected to the random sentence spout. In code, we already have one connection to the random sentence spout for exclaim two we simply want this topology. Both bolts will add on three exclamation points to a random sentence, but the total output coming from both bolts will only be three exclamations. To do this in code is also very simple. We simply uncomment exclaim1 which is already connected to the random sentence. And our topology is complete. We run mvn package again. Remember to save your file. And once we get a build success, I won't use the up arrow this but I'll write in store jar target, using the tab key udacity-jar-with-dependencies udacity.storm.ReporterExclamationTopology. Once we submit our jar we go back to our visualization. You can hit refresh just to make sure that your visualization starts out blank, and finally our topology three is correct. We see three exclamations coming on our random sentences. So finally our, we can say that our final topology is correct. And for the final point, remember that you can go to stage2 > spout > RandomSentenceSpout. Scroll down to the sentences and uncomment the sentences and add in your own to personalize your visualization. You can take a screenshot or video and post this to the forum. And why don't you comment on two of your other fellow learners visualizations. Great job completing stage2 and lesson1. Great job completing Lesson one, Covering the Store Fundamentals. In Lesson two, you'll be implementing word and sentence counts, connecting to live Twitter guarded hose. And using open source bolts to calculate rolling word counts to drive your Word Cloud visualization in T3. Good luck and we can't wait to see what other visualizations that you can come up with In lesson one, I implemented the entire exclamation topology for you. In lesson two, we will explore more of the code and syntax to implement spouts, bolts, and connections in topologies. Let's walk through the basic Storm components, along with the terminology and syntax link. Throughout the course, I'll try to use a red triangle to represent Spouts. Here in code, we see the class WordSpout here in our topology as new WordSpout(). We don't give it a name and pass it as an anonymous instance into our builder.setSpout function. We see that the component id, word-spout, is the first argument here. And we also have a parallelism of five. Our next component is the CountBolt. We see that the CountBolt is also passed into our builder, through the setBolt method. It again is anonymous with a parallelism of 15. We see the component id, count-bolt is the first argument count-bolt. And here we come to the connection of the Bolt and the Spout. We see that the Spout does not know that the Bolt is connecting to it. The Spout simple emits tuples. The CountBolt subscribes to the Spout tuple stream by this method, fieldsGrouping. We see that the connection is made by identifying the connection word-spout through the component id. And again, an anonymous object is instantiated with new fields word. Word is defined in the declare output field in the definition of the class WordSpout. Therefore the connection fieldGrouping connects the WordSpout to the CountBolt with the fieldsGrouping connection. Now let's get started with a quiz. In lesson two, stage one, you'll implement the CountBolt and connect this in your topology to a ReportBolt. I've implemented the connection to readus in the visualization for you in the Report Bolt. You job now is to decide what connection to use between the ReportBolt and the CountBolt. What is the best connection between the CountBolt and the ReportBolt. Choose the best answer based on our use case to send all CountBolt streams to one ReportBolt. Is it the shuffle grouping, a fields grouping or a global grouping? Please check the answer that you feel is the best in our use case to send a all CountBolt streams to one ReportBolt. I would argue that the global grouping is needed here because we want the entire stream to be sent to a single bolt, the ReportBolt. Because we have a parallelism of one for the ReportBolt, technically a shuffle grouping would also work. All streams are shuffled to one ReportBolt, but this adds computational overhead for one bolt. Finally the fields grouping means we hash or bin on a subset of the tuple attributes or fields. For our use case then this is also not the best solution due to computational overhead for one bolt. Hopefully you chose the global grouping to connect the report bolt to the count bolt. Great job. For the, for the collector, when you admit, so you admit object, but then when you get them you have to cast them to the appropriate objects that you want. [CROSSTALK] If you use, if you use- Yes, that's right. Use the value method. Mm-hm, mm-hm. So then, and so- And I think like. Does that method actually just returns a based Java object? And what's the- There is some method, there are some methods that for example if you know it is a that some is something Yeah sure. Those things are there, but you, you are doing your own objects then you should know what you should cast it out as. But if you just use get value it just like. Mm-hm. Then you would just cast it. Okay. Because and that could be anything. Okay. Makes it more than a life form so he can pass any objects. Mm-hm. So if anybody do that. In lesson 2, stage 1, you'll implement the Count Bolt methods execute and declare output fields to complete the word count topology. We've also implemented a reporter bolt to connect your Redis database, and D3 visualization. You must link the reporter bolt viewer topology to visualize your results. In part 1, on your local machine, in your favorite text editor, and again, I'm using Atom. Navigate to lesson two >stage one > source >jvm >udacity > storm and open up the word count topology. Switch over to your virtual machine and make sure you are in lesson two stage one. You can check that to make sure that you're in your virtual machine. Folder /vagrant/lesson2/stage1. In part 2, implement the CountBolt method execute, scroll down to the class CountBolt, and scroll down further to the method execute, execute takes in a tuple. Begin by filling in the missing code labelled part 1 of 3. This code takes in a tuple, extracts the word using the syntax provided, which you will have to uncomment, that takes in a tuple, uses the getString method at position 0 to extract the word. Next, write code to check if the word is present in the map declared in the prepare method. CountMap that is a HashMap with key string and value integer. Right code to check if the word is present in the map. If the word already exists in the countMap increment the count by one. If not add the word to the HashMap with a value of one. Finally after your countMap is updated, admit the word and count out the collector by simply un-commenting the provided code. Notice that the collector method.admit is being called with the word and the countMap.get word which will return the value count. In part 3, since we are emmitting the word and count, we need to declare output fields, word and count. In part two of three, un-comment the line that calls the declare method on the output fields declarer that declares the field word and count. This completes the count bolt implementation. Remember to package your program in maven, submit the program to storm to view on your command line. In part 4 of this lesson, once your program is running correctly to the command line, the report bolt must be connected to the topology. Scroll down to your TopologyBuilder. In the mean method you'll see the TopologyBuilder declaration builder. The syntax we used in our quiz is here. Here the Spout word-spout is connected to the count bolt using the fields grouping on field word. And finally, in part 3 of 3 use the set bolt method to add the provided report bolt with an id report bolt and global grouping with parallelism of one. After implementing the count bolt and implementing the report bolt to your topology, you are ready to run your Storm word count with visualization. In part five of the written instructions, scroll up to the report bolt. Just to review the syntax we've used to publish the word in count. Here we have a redis instance that we call the method publish. Here's our word, here's our count. And here we use a pipe character to separate the two. The visualization takes in the string published by redis, splits according to the pipe and loads the word in count. Finally, in part six of the written instructions, at the command line, run maven package. Remember the syntax is mvn package. And once you see BUILD SUCCESS, run your topology by submitting to storm. Storm jar target/udacity, remember to use tab,- tab to get the package jar-with-dependencies.jar udacity.storm.wordCountTopology. Remember that for your visualization to run, your Flask server needs to be running. If it's not running already, go into the viz directory and type python app.py. As your program runs, refresh your page, and you can see Storm running in the background, and your visualization should appear. Notice that the size of the words is linked to the count, so as the topology runs longer, our names increase in size. Good luck with the exercise. Hopefully the lesson one, stage one wasn't too difficult, but let's work through the solution here. First, in defining your execute method, scroll down to the execute method, uncomment the line that takes in a tuple, gets the string at position zero, and names it word. We check if the word is present in the map. If it's not present add the word with the count of one. Countmap.put(word,1) else the word is already there, hence get the count. Integer val equals countMap.get(word) and increment the count and save it to the map. With countMap.put, word, and increment the value by one. After countMap is updated, emit the word and count to the output collector by uncommenting the line, emit collector. That takes in the values, word and countMap.get, word, which is our count. Once that's completed in part two of three, we need to scroll down to our declareOutputFields method and simply uncomment the outputFieldsDeclare, that calls method declare with new fields word and count that we just emitted. Finally, for part three of three, scroll down to our topology builder section and here we have our builder.setBolt, with an ID report bolt, which is a new report bolt with a parallelism of one. Finally, remember from our quiz that we want a global grouping, so we have .globalGrouping. Which takes in a component ID, CountBolt. To visualize this, this is our Spout, here is our CountBolt and our ReportBolt. Using the same color scheme as our quiz. Note we have our field grouping connecting the Spout to our Bolt, CountBolt and finally a global grouping. Which takes all streams from the count bolt to the report bolt, and finally our report bolt connects to our visualization. With your Flask microserver running, go to your command line, lesson2 stage1. Do a mvn package. Once you see build success, I'm going to clear this. Submit your topology using the command storm jar target/udacity hyphen. Remember that we, and hit tab. Remember we want the dependencies in our jar. Udacity.storm.WordCountTopology with camel case. And we see as our storm topology runs, we switch to our visualization, and we see that the name and size are linked to the count. So as our topology runs longer, the count increases and the names get bigger. Great. Congratulations on finishing stage 1. Your coach now match stage 2. So I've got, I've got a question about the collector. Mm-hm. And so I just kind of wanted to ask in more detail how it works. Mm-hm. And, and what, for example, would be the difference for it to just, instead of having a collector just to return a tuple for that function and. If you, I mean, output collector is the only way Storm knows which to put to. Put it into the, pass it on to the next stage. So, so how does it pass it on, exactly? So, like the data, the way the, when you do an image- Mm-hm. So once you have the image, then internally, it captures into the queues. I mean, [INAUDIBLE] remember, I was talking about the destructive queues, and all the various other things. When you do an code you're doing all those queues or whatever it did. And again then the tuples on those queues are doubly collected by the center thread and assembled into a global send thread. And which in turn goes to the appropriate destination. And this is all within a Java virtual machine. With it all running in the Storm worker JVM process. Okay. Yeah. So that's, that is the connect begin, the collector and how it goes underneath, right. Mm-hm. So, under collect results. Also, in addition to that, I believe that you can do even something about stats collections, too. So collec, output collectors already collecting the output tuples. But also it can collect some stats also. And so can you emit anything? Like, what's, what's kind of the restrictions on what can you emit? Collectors, I think they do a bunch of methods, right? I mean, I mean I known all the image methods, and they make a particular tuple. They make a ten set of columns, or whatever it is. So there are, these all order emits that you can do, to emit different ways and different forms. Well, I mean, within the tuple, you emit values, right? Mm-hm. And then the value takes a bunch of arguments. Yes. Can those arguments be any object? Yeah, it can be any objects. Yes, it can be any objects. And in the schema where the declare outputs field or whatever, right? There's another layer third or low, right? You can say column one is named word, column two is sentiments col, like something like that, right? So then whenever you do Emmet you deal with only the columns, the number of columns. You don't deal with key value kind of thing where word is this one and this one, right? Mm-hm. So, so what the thing is like it's kind of like an [INAUDIBLE] to put any kind of object that's powerful enough. Because ultimately, we do serialization, right. So either Java serialization or the serialization or whatever. So that objects are all serialized, then again deserialized on the don't stream. What's the best programming method for when you emit, to make sure that the order, because you access by offset, right? Zero, one and two? Of the tuple? But you don't know- The offset is nothing but column number. I see. Columnal, it's a column like we don't say that going So for example, let us say you are emitting a tuple which has four columns. Right. Like Word 1, Word 2, Word 3, Word 4, an offset of zero means you are one. Offsets of one means you are two. But in the emitter, you emit, you emit by position, so it's position dependent. It's just a programming question, if I switch the order in the emitter then, I have to tell everybody downstream that's looking that they have to. They aren't looking, yeah. You always stick to the scheme of whatever you are providing, because that is the only contract that you have, right? Because if you don't stick to the schema, whatever Right. In the wrong Yeah. So. Just over time I noticed when I added something, you know. Mm-hm. You always had to add it at the end or I'd break everything. Yes. Which was, which was a little troublesome. Great job finishing lesson two stage one. Remember that in your terminal you may need to control C in order to get your command prompt back. I like to clear my terminal to have a clean working space. Now, similar to lesson one, you will connect the random sentence spout to your topology. However, now we are going to count sentences. Here's a thought question. When updating your topology what string grouping is needed to insure sentence copies are sent to the same count bolt? You'll be implementing this soon. In part one, navigate in your favorite text editor, again I'm using Atom, to lesson2 > stage2. Source > jvm > udacity > storm. First, you will copy and duplicate your word count topology to create a sentence count topology. Following the Java convention you'll need to update the required class and constructor names to match the file name. In part two, in your new file SentenceCountTopology.java. Write an import statement for the spout random sentence spout that I've included here. Notice that this is under udacity, storm, spout, random sentence spout. In part thee in your topology builder, scroll down and remove the word spout and connect the random sentence spout with an ID. Sentence-spout. In Part 4 in your Topology Builder, connect the sentence spout to the CountBolt. Here again, what connection is needed to count individual sentences. Similar to the word count topology, what field does this require? Remember that you must check the random sentence spout declaration for the correct field. Finally, in Part 5, using Maven, package your Storm program. And submit your topology. Notice that the visualization will run because the report bolt is already connected to the count bolt. Once you have a build success, you can submit your storm as usual. Remember to use the dependencies jar. And making sure that your flask server is running, you'll see that the sentences now are being displayed. And as the count increases, the sentences should increase in size. And there it goes. Remember after your visualization is running, why don't you go into the random sentence spout and change the sentences so that you either let us know what you're thinking about the class. Or letting us know how you're feeling. Or even just entering some of your favorite song lyrics. After you get your visualization running, take a screen shot and post it to the forum. And comment on two other of your fellow learners' postings. Lastly, if you already have a Twitter account, why don't you tweet a screen shot to your followers. Your code should now match lesson two, stage three. Hopefully you're able to complete lesson two, stage two. I'll walk through my solution. First after creating the sentence count topology, scrolling down to part one of two, I asked you to import the random sentence spout, which is located in the Spout folder of Storm Udacity. Therefore, it's simply import udacity.storm.spout.RandomSentenceSpout. Once this is imported, I ask you to make sure that your class name and your constructor matches your file name, SentenceCountTopology. Therefore public class SentenceCountTopology and private SentenceCountTopology constructor. Scrolling down to the main method and your topology builder, I asked you to remove the word spout. I simply commented it out. But here's our word spout. And attached the new sentence spout to the topology. Simply that's .setSpout, sentence spout, and our anonymous declaration instance of a new RandomSentenceSpout with a parallelism of one. Next I asked you to attach the count bolt to the sentence spout using a field grouping. A field grouping was the third question. Remember that a field grouping creates a hash or a subset of all of the spout or connection into the bolt. What this means is that each individual sentence, each unique sentence, is going to be sent to the same bolt and therefore we increment the count. I'll just scroll a bit to the right so we can see the rest of the syntax. Notice that the fieldsGrouping connects the sentence-spout using the new field, sentence. This may have been a bit tricky but I asked you to look at the declaration in random sentence style. And scrolling down, we see that the declare output field names the new field sentence. Therefore, to connect the count-bolt to the RandomSentenceSpout, we need to connect on the field sentence. One final point is that the count-bolt is already connected to the report-bolt. Using our color scheme once again we see that here is our RandomSentenceSpout. Here is our count-bolt, and finally our report-bolt. We're making the connection here using a fieldsGrouping. And we're making this connection using the globalGrouping. So hopefully that's clear on how our syntax translates to our visualization. Finally, to run and build our topology, we go to our command line. Submit our topology. Storm space jar target tab udacity tab hyphen tab. Again, with dependencies udacity.storm. And this is our SentenceCountTopology. Navigate to your visualization, and as you see Storm running with no errors. We see the words from the random word spout growing with count. You can also hover over one of the sentences to see the current count. There we see 32. And if we let it run a bit further, we see we have 49. Finally, I asked you to go to the RandomSentenceSpout and change the random sentences to your own personalized version of the RandomSentenceSpout. Hopefully you were able to do that, create your visualization, take a screenshot and post it to the forum. And hopefully comment on two others. Remember, that at the end of stage two, your code will match stage three. Great job. Nothing like a year free of [INAUDIBLE] and all the things you might have done this [INAUDIBLE] like the combination of technologies. Like I mean in the [INAUDIBLE] you maybe wanted to look at [INAUDIBLE] and real time and sudden fashion, right? If you think you are going to focus on more on the batch [INAUDIBLE]. I will not produce then hive, then probably scalding and cascading all these various tools, that they need to understand and how those tools work, in general all right. And if you run the real time aspect of the things, then you might want to understand more on the as well as the technologies, like storm and And some of the abstractions, like something brought the maps into the storm to Paulise, right? These technologies are very important to know how you can extract data, and how can do starts, doing some simple analytics on top of it, right? And your peer, your pull and analytic person then not only. Understanding these components help you better understand how to write analytic and also you will know how to tune it to get the results that faster, as well. And the analytic of the extra burden of knowing some of the statistics, and the data-mining algorithms, and all the various, like being in a Christian clustering and all the various other outcomes, but they need to know about it. Mm-hm. Yeah. Good job completing lesson two stage two. Now, we're going to modify the distributed sentence count topology that you just created, by creating a new sentence word count topology. What we're going to do is count the words in the sentences, that you're now inventing. In part one, navigate to lesson two stage three. And duplicate the sentence count topology, and call it the sentence word count topology. Following the Java convention, you'll need to update the required class and constructor names again to match the file name. In part two, implement a new bolt SplitSentenceBolt that extends the base ritual. Your SplitSentenceBolt takes in a tupal, with a sentence at position zero, extracts the sentence. And splits the sentence into words, and emits the words in tuple. My suggestion is to copy and paste a similar bolt such as the count bolt, update the class name, and implement the prepare, execute, and declare output field method. To clear the new field, sentence dash word. I also list a few helpful Syntax hints in the written instructions. In part three, scroll down again to your topology builder, and update the topology to make a random sentence. Split the sentence using your new Split Sentence Bolt and send the output to with the Count Bolt. One implementation question to consider. What stream method should you use, to connect a sentence to the split sentence bolt? Do sentences need to be sorted by field in order to be split? Finally, in part four build, and run your new topology. Once that's running correctly notice that the report bolt is already connect to the count. Again personalize your random sentences, take a screenshot, or screencast in video, and upload it to the forum. And comment onto other posts. Finally, your sentence, word count topology should now match less than two stage four. When you're on your final storm solution, you'll see that the random sentences are going to be split, and the words are going to be counted. So now, the words instead of the sentences are going to show up in our visualization. As the topology is running, you see that word seven grows, because my sentences have two sentences with the word seven, four score and seven years ago and Snow White and the Seven Dwarfs. And good luck with the assignment! I walk through the solutions for lesson 2, stage 3 implementing the SentenceWordCountTopology. In part one, I asked you to navigate to stage 3, and duplicate the SentenceCountTopology, and rename it the SentenceWordCountTopology. Also in part one, I asked you to follow the Java convention and make sure the class name matches the Java file name and the constructor. You didn't need to do this, but I also think it's a good idea to build our package as we develop so that we make sure we're not making any syntax errors. We do that by mvn package remembering that on your virtual machine you're in lesson 2, stage 3. And we find we have a build success, which is great. I'll do a mvn clean and clear the screen. In part two you're implementing a new bolt, split sentence bolt, and I suggested that you extend the base rich bolt by simply copying and pasting the count bolt. Let's do that now. I'll select and copy the count bolt. I'll create a new area that says split. For a split sentence bolt obviously the class name will need to change. SplitSentenceBolt. That extends the base rich bolt. And now I'll just clear out the prepare, execute, and declare output fields methods. Here in prepare we no longer need our count map. In our execute method all of this will change. So we can delete that now. And for our declare output fields we'll simply clear out some of these comments. Remember that this split sentence bolt is going to be taking in a tuple of a sentence. And we'll need to split that sentence into individual words or tokens. I gave you useful syntax hints with string delims and string tokens, so I'll just copy that in now. The first step in our execute method is going to need to, again, take in a tuple and extract the sentence from position zero. That's simply a string sentence, sentence equals our tuple.getSTring at position 0. Finally notice that our sentence.split method is returning tokens. Let's just iterate through the tokens and emit each value. Remember that the syntax here is collector.emit new Values. Finally in our declareOutputFields we're emitting value token. I'll call that a sentence word. Therefore we tell the schema that we're emitting a tuple consisting of one word sentence word. And in our OutputFieldsDeclarer.declare method we have new fields and not word but sentence-word. Now that we've implemented our split sentence bolt, let's make sure this compiles. Mvn package. Great, we have a build success. Next in part three, I asked you to update the topology to make a random sentence, split the sentence using your new split sentence bolt that we just implemented, and send the output to the count bolt. The implementation question I asked was, what stream method should you use to connect a sentence to the split sentence bolt? In other words, should we use a field grouping, a shuffle grouping, a global grouping or an all grouping? Do sentences really need to be sorted by field in order to be split? Let's work on that now. Scroll down to your topology building section. And let's just look at the topology as it is now. We have a sentence spout, a count bolt, and a report bolt. These are connected by the fieldsGrouping and a globalGrouping. Continuing on, notice I've labeled our sentence spout, count bolt, and report bolt and removed the code for now so we can look at the topology. In this exercise, we want to add in our split sentence bolt between the sentence spout and count bolt. In Storm this is very easy. We first want to add in our new split sentence bolt, connect out sentence spout to our sentence bolt. In answer to the implementation question this should be a shuffle grouping. And finally, we want to reconnect the split sentence bolt to the count bolt using the fieldsGrouping. Remember that the fieldsGrouping will hash or bin on each word and send each word to the right count bolt. Let's do this in code. The first step is to copy and paste the syntax of a bolt. Let's change the component id to split-sentence-bolt. And that of course takes our new object, SplitSentenceBolt. Let's leave the parallelism of 15. And here the connection of the sentence-spout to the sentence-spout should be a shuffleGrouping. And lastly, remember that the shuffleGrouping does not need to hash on fields. And therefore we can remove the fields grouping. Graphically we can see that the sentence-spout is now connected to our split-sentence-bolt by the shuffleGrouping. Finally to connect the count bolt using the fieldsGrouping to our split-sentence-bolt, remember that we have to change our fields component. And it's no longer word and we actually named it sentence-word. To see this we scroll up. We scroll up to our declareOutputFields. And remember that our field declaration sentence-word needs to map in our topology. Finally, we see that the sentence spout is connected to our split-sentence-bolt using the shuffleGrouping. The split-sentence-bolt is connected to our count-bolt using a fieldsGrouping and fields sentence word. And our CountBolt and our ReportBolt remains unchanged. Let's compile this and run our package by submitting it in to storm. The command here is mvn package. Remember that you should be in lesson 2, stage 3. Let's look at our hierarchy using tree. We see that SentenceWordCountTopology has been created. Let's scroll up a bit. Our class SentenceWordCountTopology has been created. Finally we see that our jar files, including dependencies, are there. So submit our program using Storm space jar target/udacity. Again hit using tab hyphen tab udacity.storm.SentenceWordCountTopology. Once we've submitted our program, we can go back to our visualization. We can see our program running in the background, and here we see that seven is one of our largest words due to the count. Here we have 14. There we have 39. As the program runs the random sentence spout is spitting out new sentences. And our program is splitting the sentences and counting. Seven is the largest word because we see four score and seven years go and Snow White and the seven dwarfs. Finally in part four I asked you to personalize your random sentences. So you can go to the random sentence spout, change the sentences, take a screenshot or screencast or video and upload it to the forum. And how about commenting on two other posts? Remember, that your sentence word count topology should now match Lesson 2, Stage 4. Great job. What are the challenges that you find with Storm? The challenges that we're finding in Storm is the operation aspect of it. Mm-hm. I mean, like keeping. Like, we're running hundreds of jobs, right? And keeping them alive. And there are some business-critical processes and jobs that keep running all the time. The downtime of the jobs cost us business dollars. Mm-hm. So we have to keep them from running all these. So one of the first challenges we faced was when initially Storm was deployed so it was using two people as I was mentioning. So, now as we were scaling because our data was growing. So we were hitting the zookeeper very hard. And that became an issue and zookeeper was not able to keep up. And zookeeper is not a, a, a what we calls a scalable system. It's a scale up system. Mm-hm. Because the reason why is like when you write something and the zookeeper is replicated to all the nodes before it [INAUDIBLE]. So, so you can't do that with zookeeper. So, so then we figured out what is the issue in storm that really causes that. So we, we figured out there are two issues that caused it. One is, whenever you have a, these workers, and a large number of workers happen. When the, there's a, the only way knows that a particular worker is alive right now is by worker sending a heartbeat to the zookeeper and zookeeper storing that. So when the large number of, big cluster of few 100 missions on probably around ten workers, 20 workers, that's lot of workers. And all these workers are writing their heartbeats into the zookeeper. And that made the zookeeper price go very high and brought, and [INAUDIBLE] zookeeper also. Which is not a good thing, so what we do here is like we look into it here in order to facilitate such scalability. The mood lit functionality of [INAUDIBLE] to separate things. So, for instance, a process is [INAUDIBLE] going to zookeeper. Nice. And the column is [INAUDIBLE] and the zookeep, the members which are done on the lasers [INAUDIBLE] looking at the zookeeper. So that's all [INAUDIBLE] problem. Then there were additional topic for zookeeper as well too. We chose called the Kafka offsets. So when I'll be reading some data from the Kafka, distributed queue, so that Kafka offsets also [INAUDIBLE] zookeeper. Okay, but, for like every two minutes every one minute, and last time when I read it, I wasn't exhausted, because remember when another process fail and machine failure occurs, the small fails which means I would at least target the affected machine so it can be restarted. When I restart it doesn't know where to start from, because it was processed til up to end. I don't know where to start from now, right? I can go back to one. Mm-hm. Which just means it's, there'd be a lot of errors in the right? So it's, every two seconds, it saves where it is. So, mm, so then it when it comes back out, then it picks up where it left off. By looking at the offset, in the zookeeper, oh, I was there before here. Then he starts from there. So that traffic also was causing the zookeeper. And now, what we have done is, we have moved that out and we arrive at [INAUDIBLE] too. So those two are the one of the biggest issues will stop. And so, generally keeping up with the scalability of the stock itself, it's [INAUDIBLE] It's now time to connect your topology, to the real time Twitter sample stream. To do this, we need to obtain actual Twitter credentials. So, in step one navigate to twitter.com, and either create an account or log in, here. So, you can either sign in, or sign up for Twitter. I'll simply sign in with my username, which is Lewis@udacity. Now once you're signed in you can go to a new window, and following the reading instructions in step two navigate to apps.twitter.com. You can see here I already have one app created, but for you, you would create go to the upper-righthand corner and click on Create New App. In step four, enter the name of your app this could be your name-udacity-twitter-storm for the description you can put in udacity and twitter course real time analytics with apache storm. Here in the website, put in the fully qualified URL address, and you can use http://www.udacity.com/ud381. Next, scroll down to the developer rules of the road and read through these very carefully. And finally click on Yes, I agree, and create your Twitter application. So, here in Details, Settings, Keys and Access Tokens, click on the tab Keys and Access Tokens. Once your consumer key and consumer secret keys are shown, copy and paste these into either a text editor, or directly into a topology in the order they are provided. Once your consumer key and consumer secret are copied and stored safely, scroll down and click on create my access token at the bottom of the page. Finally, once your status is updated scroll all the way down and copy your access token and access token secrets. Remember to copy them in order, so you have your consumer key consumer secret access token and access token secret. Finally remember that the key going to be entered in the topology builder phase and one caveat if your uploading and sharing your project publicly on Github or Bitbucket, remember to always delete your secret keys. These keys should remain secret only to you. Congratulations on obtaining your credentials, please continue on with the course. After obtaining your Twitter OAuth credentials you're now ready to connect to the Twitter sample stream to begin analyzing real time tweets. In step one, navigate in your command line to the text editor to lesson two, stage five. Also in your text editor, go to lesson two > stage five > source > jvm > Udacity > Storm and we're in our TweetTopology. In step two, scroll down to the topology section, and you see the TweetSpout credentials, here. Remember to enter your OAuth Credentials in order that they appear, your customer key, secret key, access token, access secret. I've created a few fake credentials here that approximately match the same size. And that's it. Next, scroll down to your Topology,. In step three, review the tweet topology. What do you expect the output to be? Where will it be displayed? How about you post your predictions to the forum and comment on two other posts. Next, run your topology using Maven package and submit your Storm topology. Once you have built success, remember to submit your topology by looking at the tree structure. I notice that we have the tweet topology class. Storm jar target udacity with dependencies, udacity.storm.TweetTopology. Finally, following the written directions in step four, I'm asking you to complete the topology by attaching a ParseTweetBolt with a parallelism of 10. Followed by a CountBolt with a parallelism of 15, and finally a ReportBolt with a parallelism of one. What are the needed groupings to connect each topology? Package your topology using Maven and rerun. Remember to refer to lesson three, stage one if the tweet topology for the solution if this is needed. Finally, in part five, take a screenshot or video and upload this to the forum. Explain the apology you created and why you chose each grouping to connect your spout and bolts. Comment on two other posts, and optional now that you have a Twitter account, you can tweet your screen shot with your thoughts on the class. Good luck. Great job completing the Twitter word count topology. Hopefully you were able to access live tweets using your own oath, oath credentials using the Twitter 4AJPI and run your visualization. I'll walk you through my solution here. First after entering my credentials I asked you to simply run topology using mvn package. And once you had a build success, you could look at your structure, here's your TweetTopology and your jar-with-dependencies. And submit using storm jar target/udacity,again using tab. And we have udacity .storm.TweetTopology. Finally I asked you to update the topology. First in part one we attached the parsed tweet bolt with a parallelism of 10. I also asked what grouping is needed. Remember our builder syntax is builder.setBolt. We want this to be the parse-tweet-bolt. And that's a new ParseTweetBolt. And a parallelism of 10. Finally, the grouping that's needed in the parse tweet bolt is simply a shuffle grouping. And we connect that to the tweet spout. Using the component ID tweet-spout. And remember to end with a semicolon. Next, we want to attach the count bolt with a parallelism of 15, and again, what grouping is needed? Syntax here is, builder.setBolt. This is the component ID, count-bolt, a new CountBolt, and a parallelism of 15. Next, the grouping nets needed is a field grouping, because we want to parse each word coming out of the parse-tweet Bolt. And hash it on the unique word that's coming in order to be counted. So we have fieldsGrouping. Connect to the parse-tweet-bolt. And for our fieldsGrouping, we need to declare the new field with the name of the field. The name of the field is something that we need to check. Let's go to the ParseTweetBolt now. I'm doing that by searching ParseTweetBolt. And we can scroll down to the declare output field where I declare the output field, tweet word. That's the field we need. Let's jump back to our topology and enter in the new field, tweet word. And finally end with a semicolon. In part three we want to attach the report bolt with a parallelism of one, and again I ask what grouping is needed. Here we have builder dot set bolt. Report bolt is our component I.D. It's a new report bolt. Parallelism of one. Again the question is what grouping do we need? We want all of the streams coming from the count bolt to go one report bolt in order to connect with our visualization. Therefore we need a global grouping and the syntax here is simply to connect to the count bolt. With the component ID component ending with the semicolon. Let's check our solution by maven package. So notice I get a build failure and it turns out that I have a syntax error. So I'll leave this in just because this is very common. In maven it's a good check for your syntax and simply put. I have an extra parentheses, right there, in the ParseTweetBolt. So, I want to declare the ParseTweetBolt, but have the parallelism of 10, and that's how I can tell that my topology was incorrect. So, let's clear this, and repackage. And I leave this one in too because we see another build failure, and it's new FieldsGrouping, with s. let's repackage again, and finally we have a build success. Now let's submit our program using storm jar target again using our tab, and this is udacity dot storm tweet topology. Finally as our program's running, we can bring our visualization. And we see that as the tweets are coming in through our live tweet spout, the tweets are parsed, counted and displayed using our visualization. The longer the topology runs, we see that the words grow in size. We can hover over a word, and we see that there is 158 instances of. This http word. We'll clean this up in further exercises using different filters. But, for now, it's pretty cool to see that our tweet topology is counting words correctly in real time. Great job. And, hopefully, you're able to take a screenshot or video and post this to the forum. What are the best practices for your customers? For the people you take I mean, the best practices so far, whatever we have today is that low key has more value. And, the topography, the CEO output is doing well online. Another way to do that is you can actually construct a topology. Yeah. So, you have this spout. And then this [INAUDIBLE] is out and think about that it, that apology has only one of it's [INAUDIBLE] Mm-hm. And, see how, you know? Output looks like [INAUDIBLE] Yeah. Then, if it is fine and not doing well. The you have the bolt. Mm-hm. Then, you'll look at what is output of them both is suppose to be, right? Yeah. So, so you, [INAUDIBLE] stage by stage and ,. You're breaking up the topologies. RIght? Mm-hm. That's one way to do it. And, then of course the open model cluster is it allows you to do that kind of integrating testing except without a distributed cluster in a local cluster. Right? Mm-hm. So, that's one. I mean, I do really wish that we can do some kind of simple unit testing on IV pole. Than the spout level where you will say this is my input that is my output for every spout or every volt that you have right? Is there a way to say like what is your throughput per you know, like say like, how much data you gain from your spout and then like how long it takes for your bolt to run. Mm-hm. because you know, because you gave that example of say you have. 200,000, you know- Mm-hm. [INAUDIBLE] per second, but your [INAUDIBLE] can only do, like 50,000. Yeah, yeah, yeah, yeah. So, so but the way we do that is, I mean, that is different from testing. Yeah. But, it's more of a what we call as a capacity planning to determine how much [INAUDIBLE]. Yeah. And, typically, your testing occurs on a smaller amount of data. Like what we used here on the sample data, right? So, like a 10% sample, 20% sample, or 50% sample, right? Yeah. When you go into full blown data that is all you, when you know what your decide about the right? So, what we do is like we take the same samples that you use to debug your program on. Mm-hm. And, when the sample and look at your Yeah. So, what we do is at every bold and every spot level in the customer intacts that we have we look at the CP usage. Yep. So, we have a climbing dumps of how much CPU it say. Because even if for a few days before it goes into production. Yep. So, that then so that you can capture the traffic radiations or what a. Or, during a particular time of the day and for a particular time of the week. Or, or all the things, right? So, once you have know the CPU usage. Then, you can work. By the way, you want topology that can tasks seem to take 90% of the time. Yeah. 90% CPU usage, which is not a good thing, because you needed some head room for, if any, or any surge in traffic. Yeah. Right? So, between, so you should not be doing a 90% identity census. Instead we should be going down to like the, say keep it at 60%. Like 60%, 50% to 60%'s the normal. So, that you are determining the number of [INAUDIBLE]. Where the CPU is saying it's 50% approximately, right? So then, and you keep that running, then you have a, another HTML file that the, the person to go up and down. Mm. That is saying, you really shouldn't need it that good, right? Mm-hm. So, that's a way then. For the 50% CPU utilization, based on our current number of instances, current number of sample set that you have. And, what is your least sound currency using. Then, you can determine okay this many number of things inferences we give the [INAUDIBLE] for direction. Okay. So, that's the way you decide that. And, we have the CPU map for every bulge and every [INAUDIBLE] So then you can determine at every stage how much [INAUDIBLE] Great job on completing lesson two stage five. In lesson two stage six, we've split the tweet topology from stage five into individual files for spouts, bolts, and the tweet topology. This structure is more typical of larger programs. And this exercise is intended to give you practice searching for required fields in your correct spout, bolt, and topology class files to complete a real topology. In part one, complete the tweet topology to attach the tweet spout with the parallelism of one. Attach the parse tweet bolt with a parallelism of 10 and the count bolt with a parallelism of 15 and the report bolt with a parallelism of 1. Remember to enter your Twitter OAuth credentials. And finally, build and run your final topology. Refer to your solution in lesson two stage five if necessary, but try to solve the exercise without referring to your previous work. Finally, refer to lesson three stage one if you need the solution. Lastly, when you've completed your tweet topology and it's running correctly why don't you post a screenshot or video to the forum and comment or help out some of your fellow learners. Good luck. Great. So, hopefully, you found stage six not too difficult and pretty straightforward. So, the first step would be to go to your TweetTopology and enter in your OAuth credentials. And, here, you'll need to comment out, comment out the earlier credentials. I would suggest at this stage, package, and run. And once you get a build success, submit your topology and make sure it runs. And if you submit your job, you'll actually see nothing, as we don't have a topology yet. So let's go back to our topology. In part zero now, we need to attach the tweet spout to the topology with a parallelism of 1. So, here we have builder.setSpout. Let's call this a tweet spout with a parallelism of 1. Notice that our syntax tweetSpout is simply calling the tweetSpout object that we named with our credentials. At this stage, we package again. And once we get a build success, we can use the up arrow, submit our job again. And now, we see the twitter4j TwitterStream. Let's go on to part one. Similarly to stage five, this is simply builder.setBolt parse-tweet-bolt new ParseTweetBolt with a parallelism of 10. And again, we want a shuffleGrouping that connects our tweet-spout component ID to the tweet-spout. In part two, we want to attach the count bolt with a parallelism of 15, and the grouping here is the fields grouping. The one difference is that our parse tweet bolt is now its own file. And to figure out the fields grouping needed, let's go to the parse tweet bolt. And now we scroll down to our declareOutputFields and we see that we have the tweet-word. We copy the tweet-word, go back to our topology, and enter this in the new Fields tweet-word. And finally, in part three, we attach the report bolt with the syntax builder.setBolt with a component ID report-bolt, new ReportBolt parallelism of 1. And again, we want a globalGrouping to send all streams from the count-bolt down to one bolt in the globalGrouping report-bolt. Finally, the component ID to make that connection is count-bolt, listed there. Remember, to save the program, we'll mvn package. And finally, once we get the build success, we can look at a tree structure. Now we have individual classes, and our TweetTopology. Remember again, that we want our jar with dependencies. Here, the structure is udacity.storm.TweetTopology. storm jar target udacity.storm.TweetTopology. Finally, as storm runs, we come back to our visualization. And we see as the Tweets are being parsed, we saw our visualization count grow. And again, we can hover over a word and see that 150 HTTP splits were occurring. And again, we'll clean this up using filters in later exercises. And, congratulations on finishing on the Tweet word count. The complete topology. So had like an architecture question. I struggled a bit. I still don't think I understand the right, the best practice for if I want to look at a trend over time. Mm-hm. Do I store that in the bolts? Do I store that in Redis and, and then how do you? You know where does it? Where do I persist say, you know, the last 10 minutes of data as I'm [CROSSTALK] a, a, a histogram of, of Tweets, say. Okay, so if you are doing Window-based computations, typically the idea is to keep that in a Bolt or whatever it is, right? So bo, and the bolt is computing the or moving into aggregate online. What do you call it? The time [INAUDIBLE] bits of [INAUDIBLE]. So all bolts check the clock and then they decide now I'm going to basically report in my aggregated data. Yes. And push it out. Push it out. So now the venue, before that. So, the time window that remains [INAUDIBLE] right? So now once you designed the time window then at the end of the time window you can emit this or this time window. This is my percent of analytics. Okay. Alright so that gets told into the particle persistence store. I see. That should be stored in the partition so, so that you can go and look back much longer. Right? Okay. Because then if you are doing a refinement at interval. Then if you wanted to go back to five days ago, or ten days ago, or twenty days ago, yes, then you can go back to the para, para, persistent tour and see those values. Right? Okay. Whatever it is, right? Another like if it's a moving aggregate, then for a eh, every change in that time. For example the last 10 or five minute of [INAUDIBLE] then every one minute elapses then everyone [INAUDIBLE] moving out the gate. [INAUDIBLE] right? I see. But, so on the other hand if it is a time window then every five minutes [INAUDIBLE] persistently and a moving window of one minutes. Moving window over five minutes. And every minute you write that. Yes. Persistently. Then you can get the moving aggregate or be [INAUDIBLE] time. Okay. And then, and then, aggregating into a database versus Redis was, was, I guess Redis is more just instantaneous for collecting the output of many bolts. Yes. No, I mean that's all, I mean this is just something [CROSSTALK] that's. It's just an aggregator, basically? Yes. Okay. I mean it is, I mean as long as if you don't worry about the output of that outtakes. It doesn't have to be persistent, I just need it for some look up on whatever it is. And after a certain time then it's, cannot, I don't need it. Means, then you can do what it is. But on the other hand if you need that an outtakes early, then you can store it in a e-Value store Congratulations on completing Stage six. What we're going to do now is actually use some of the existing code in the official Apache Storm, Storm Starter to implement a RollingCountBolt. This exercise builds on your completed Stage six code, so we'll start here. Again, if you did not complete Stage six. You can always copy the Tweet Topology file from lesson three stage one to begin. Remember as well, to enter you Twitter OAuth credentials. In this new exercise, we're going to use an existing rolling count bolt from the storm-starter code to visualize rolling tweet counts. In step one, find and review the rolling count bolt from the standard storm-starter code. >From the link provided. And simply copy and pasting the address or searching for Apache.storm storm-starter, we'll be able to find the RollingCountBolt. This is an exercise of using. Bolts that we didn't create. But we can use their functionality. So in step two, copy the RollingCountBolt.java, so you can simply copy and paste this. Add a file, Roling CountBolt.java, and there we go. Next in your text editor, add a new folder under the Storm folder, and label this Tools. So we now have the structure udacity storm tools. Next following along in the written instructions. In Step 4 determine which files from the Storm Starter code is required to run the RollingCountBolt. Copy these dependencies into our tools folder. Where are these files located? Hint. Take a look at the import statements. In step five. Add the import statement to your copy of the RollingCountBolt And in step six. Without connecting the RollingCountBolt. Run maven package to check your import process. Next in step seven. Update your topology to ignore the CountBolt and use instead the RollingCountBolt. Connect this to your report bolt. Finally, in step eight of the written instructions, build and run your topology. And finally, when you run your topology you'll see a rolling count of real tweets. And once you see your visualization running you'll see that your topology is taking in an outside bolt, can a useful part of your development process. Good luck. So hopefully you were able to connect the rolling count bolt. And I'll just show you the solution, and probably more importantly the process that I took to do it. So in the first hint after creating your rolling count bolt.java file, and again, we are taking this code from the Apache Storm, storm starter rolling count bolt file. After Copying and Pasting this into our project, the first thing we need to do is change the package name from storm.starter.bolt to our package. As an example we can go to our tweet topology, and see that our package is udacity.storm. You can Copy and Paste that into our storm.starter.bolt. I'll leave this as a comment. Next, we see the import statement used in the original Storm starter, is storm.starter.tools, and there are three of them in the import statement. We created our tools folder under Storm, and so therefore, our import statements are going to be udocity.storm.tools. I'll just Copy and Paste these, I'll comment out the originals, and I'll simply change these to udocity.storm, and there we go. Well now what we need to do is copy it nthlastmodifiedtimetracker slidingwindowcounter and tuplehelpers into our tools folder, and we can find those with the path udocity.storm.tools. Therefore we come to storm/examples/storm-starter/scr/storm/starter and tools. Remember this is the structure of the storm.starter source code. And we see we have N, NthLastModifiedTimeTracker, SlidingWindowCounter, and we see in storm.tools, we have NthLastModifiedTimeTracker here. And we have SlidingWindowCounter here. Let's go ahead and Copy those two. And we create them in the appropriate place in Tools. Add a File. NthLastModifiedTimeTracker.java. And we'll Paste that in. Let's make sure to change our package name to udacity.storm.tools and we navigate back, come to our sliding window counter, Copy. Add a file in tools. Remember the name is called sliding window counter, SlidingWindowCounter.java, and paste. And just to make sure see our m.last modified and our sliding window counter in our tools folder. Next we come back to rolling count bull. And we see that storm.starter.util is our tuple helper. And we'll just put those in the tools as well. So we want starter, util, and TupleHelpers. We can Copy and Paste and complete the same exercise. We're going to tools. Add file. TupleHelpers.java. And we'll Copy and Paste that. And again, we remember to change this too. Udacity.storm tools. Make sure we do that with our sliding window counter too. Now taking this approach and we have three files. Let's run a Maven package and we see that we have an error, and the error is the slot based counter. And this means that we have at least one additional file to copy. Back in storm.starter.tools we find the SlotBasedCounter, and again we see that here where the symbol class slot based counter is missing. So, we open SlotBasedCounter. Complete the process, Copy. Let's add it, again, to tools. Add file, SlotBasedCounter.java. Copy, Paste. Change the package name, udacity.storm.tools. And let's Rerun. And here we need to be careful on another build failure, and I'm just leaving this in to show you the process that I went through. And here we see the variable TupleHelpers is missing, and It turns out that I have the same structure as udacity.storm.utill, but we don't have a utill folder. I put TupleHelpers, in tools. So, this needs to change. Well, Maven package again, and we have a build success. Finally, in step seven of the written instruction, we want to ignore the count-bolt and use the rolling count-bolt. Remember that we'll have to check the rolling-count-bolt for the correct syntax. To do this, let's Copy and Paste an existing bolt. Let's ignore the count-bolt, set the rolling-count-bolt. Component ID with the new rolling-count-bolt. Note that the default constructor for RollingCountBolt with no arguments takes in the DEFAULT_SLIDING_WINDOW_IN_SECONDS and the DEFAULT_EMIT_FREQUENCY_IN_SECONDS. The DEFAULT_SLIDING_WINDOW_IN_SECONDS takes in the number of window chunks, which is 5, times 60. We don't want to wait this long, so we see that the rolling count bolt that takes in arguments windowLengthInSeconds and emit frequency in seconds. Let's use this constructor in our topology. Here for the rolling count bolt in seconds, and let's set this as 30 seconds with an emit frequency of ten seconds. Finally we want to connect this to our report bolt. And again we can Copy. Comment out our old report bolt. Finally, once we connect our report bolt to the rolling count bolt, let's try to run this in Maven. Maven package. And we have a built success. Let's submit the job, storm jar target. Udacity.storm.TweetTopology. And I'll leave it on the terminal, simply because we see that we have a runtime error. Basically, the halting process that a worker died at the report bolt. And this is one of the difficulties of Storm that I found after working through it. In that Storm has runtime errors that are usually type based. So let's explore our rolling count bolt a bit more in detail to figure out what's happening. If we go to the report bolt which is the worker that's dying, we see that the report bolt expects a tuple with a string word that's labeled word and an integer count of count. Looking deeper at the rolling count bolt, we scroll down to the emit function and the declare output fields, first we see that declare field is declaring an object and a count. Again the report poll is expecting word and count. To make it more robust we could either change the report bolt to take in a string at location zero and an integer at location one, or we can change the field name to match the report pulled. I'm going to call object word, in order to match the report poll word and count. The next problem we have in the report poll is that the count, looking up to the emit function, is a long. Therefore, we need to change the long to an integer to match our report bolt. To do this we can use integer intCount equals count of not null, and returned count.intValue or null. And simply this is an if then statement and instead of returning count, we will return our in count. So therefore we have word and count. Word is as expected. We will get an object that's a string of the words that we're counting, and an integer of in count. Finally we just recheck our topology, and see that the rolling count bolt is connected. And we'll go back to maven package. Finally, we have a build success. And we'll submit storm jar target. And submit our TweetTopology. And we see now that the program continues to run we'll open up our visualization, and we see that the rolling count bolt is now driving the visualization. And we know the rolling counts of each word is being displayed. And finally we know that we can take the rolling comp bolt or other bolts that are open sourced or spouts word for that matter, and integrate them into our storm typology to be able to use other's codes and develop more efficiently. Great job. Congratulations on completing lesson two. You are now able to connect to, to the sample stream, and able to visualize all the tweets and the corresponding running coms in real time. In lesson three, we'll go beyond Storm basics to explore Storm's multi-language capabilities by parsing URLs, using Python. We'll also explore more complex topologies, like calculating rolling top hashtags. And finally, we'll process multiple tuples simultaneously using streaming joins. These are the concepts that we use at Twitter for all of our production topologies. Let's get started. In this section, we explore Storm's multi-language capabilities by readying a bolt in Python. Due to implementation details Python is rarely used in production Storm topologies due to Python's inefficiencies. However, Python is useful as a prototype or low usage process. And many of our Udacity students are more comfortable with Python compared to Java. Let's get started with basic usage. In step 0, navigate to Lesson 3, Stage 1 and remember to add your Twitter credentials into the TweetTopology.java file. Also you'll need to install the Beautiful Soup python module in Ubuntu. In your virtual machine run the following command sudo space apt-get install python-bs4 this stands for super user do. And install command for Ubuntu for the python module, beautiful soup. You can simply copy and paste the command and after downloading, you'll see your command prompt once again. Next, in Step 1, maven requires a folder resources to contain the storm.py and all python files. We need to register in the folder location in the palm.xml file. Scrolling up in the TweetTopology file, you'll see the section, ADDING MAVEN RESOURCES FOR PYTHON SHELL. Open your pom.xml file here and add in your Maven resources here. Once you've added in the Maven resource file, in Step 2 in your text editor or command line, add a Resources folder to /storm. This new resources folder allows Maven to package your python files in the resources directory within your final jar file. You're now ready to write python bolts. In Step 3 in your browser, find the Storm multilang resources by following the link below. In Step 4, create new files, copy and paste Storm.py and split sentence py into your resources directory created in Step 2. In step 5 you can review the usage of python bolts linked to Java in WordCountTopology here. And in Step 6 we see that the inner class SplitSentence extends Shellbolt implements IRichBolt is needed. In Step 7, create a new file in your Storm folder. Not the resources folder. And name the file SplitSentence.java. Copy and paste the interclass SplitSentence into your new java file along with the required import statements. Remember as well that your package structure needs to change to Udacity.storm. And to complete your split sentence class definition in step eight, remove the static statement. Since you are now defining a class instead of a static nested class. In Step 9, update your topology to use the SplitSentence bolt that internally calls the python program splitsentence.py. You answer the implementation question, what grouping is needed for this topology link. Finally, in Step 10, package and run your topology. When one's completed, you'll see the visualization will be the split sentence word count using a python bolt. Remember that your code should now match Lesson 2, Stage 2 for reference, including file structure and Palm to XML file. Good luck with the assignment I hope you found the basics of Storm's multi-language capabilities interesting and potentially useful to your topology design process. I'll walk through my approach here, but as always, feel free to skip ahead to the next section if you didn't encounter any problems. In step zero, after adding your credentials to the tweet topology file, end of step one-creating a resources directory and making sure it's in your Storm folder. We've registered this folder location in the palm.xml file by copying and pasting the Maven module. So we copy, navigate to the palm.xml file, paste and finally uncomment. See that the structure directory that we created is source/jvm/udacity/storm. Which corresponds to the source jvm Udacity Storm file structure we have. Make sure you don't list the resources directory here. This ensures that the resources folder will be a folder in the jar file as required by the storm.python module. In step three, you could follow the link below, or navigate from storm.apace.org. Click on any programming language. And click on adapters in Python. Not example topology, which takes you to Nathan Marse's older code. Once in the adapters in Python page, click on Storm > Storm starter, and finally, multilangresources. Finally, in step four, we copy this storm.py and splitsentence.py files. Remember that these files go in the resources directory and create storm.py and splitsentence.py. Finally, copy and paste the storm.py and splitsentence.py files. In Step 5, after reviewing the usage of Python bolts, and my hint in Step 6 that shows the inner class SplitSentence extends ShellBolt implements IRichBolt is needed. In Step 7 we create a new file in the storm folder, making sure it's not the resources folder, named splitsentence.java. After copying and pasting the interclass splitsentance.java into your new java file and the required import statements, remember that your package structure needs to be updated. Remember, package.udacity.storm is udacity.storm. We imported all of the import statements from the prior file, but for sure we know we don't need the random sentence spout, we can comment this out. Finally, in step eight, we remove the static statement since we're now defining a class. And in step 9, we update the Topology to use the SplitSentence bolt. We navigate to the tweetToplogy file. And scrolling down to the Topology section, we comment out the old ParseTweetBolt and entering our new SplitSentenceBolt. I've given this a component ID of python-split-sentence,. And finally the implementation question, we need a shuffle grouping. And finally we connect this to the tweetSpout. Once we've made the connection between the tweetSpout to the python-split-sentence bolt, we need to complete the connection to the count bolt by changing our field grouping. First we change the component ID parse-tweet-bolt to python-split-sentence. And next we update the fields from tweet-word to the field declared in SplitSentence. To find this we navigate to SplitSentence and scroll down to the declareOutputFields. And see our new field declared is word. Notice that the splitsentence.java bolt is declaring the output field word and not our python splitsentence.py file. Now that we know the field word, we navigate back to our tweet topology and update the new fields tweet-word to word. And now we're done with our topology connecting the tweet-spout to our python-split-sentence bolt, from our python-split-sentence bolt to the count-bolt. And using the existing connection from the count-bolt to the report-bolt. Also in step zero, remember to install the Python Beautiful Soup package, making sure you have a built success. Finally in step 10, package and run your topology. Again, it's Maven package. And once you have a build success, we submit our project Storm jar target Udacity with dependencies, Udacity.storm.tweet Topology. And as we see our tweet stream in the background we see that the visualization begins to update. This is using the word splitter bolt in Python. And finally, remember that your code will now match Lesson 3, Stage 2. Now that we've covered the basics of using Python in Storm, lets move on to something a bit more interesting. And, what, what about what happens when you call super and you go outside like, to process a Python script? So in terms of working set, is that, is that, do you, are you consuming memory from within the virtual machine or are you running outside? The reason I ask is if you wanted to have a, a large fan out like, 50 instances of a volt and they're all going to run a Python program. Mm-hm. Unix will usually keep the program memory one copy, and then and then the data is, is separate from the- So, Python runs extremely friendly, but then stop? Yeah. So, I mean a Debian is ee, a Debian process on the [INAUDIBLE] occur in Java, all goes through the model of the stoneworkers that it will be mentioned to you, right? On another hand, the Python Topologies worked differently. They acquired and also, like, the Python processes what do you call it, created separately hanging off from the JDM, some work. So then, what, what the, some worker gets the data, and the data is to the Python has a couple be, couple bases, and goes into a Python process, and it comes back to GVM before it opts to DVM. So, that is a very different [CROSSTALK]. So, the child-child process, so there's, if there's 50 instances, there's 50 childs. Yeah. So, it's, yes. Processes, basically. After completing stage one to connect a simple split sentence bolt using Java and Python, you are now ready to use python for something a bit more interesting. Python packages can be used to speed up development especially for computations that might be more difficult in Java. In this section we will use the beautiful suit package to parse tweeted URLs and emit text. In step zero, navigate to lesson three stage two, and remember to add your twitter credentials into the tweet topology file. In part one, we installed Beautiful Soup in Ubuntu package in Python by using the apt-get command. You can copy and paste sudo apt-get install python-bs4, into your terminal. This stands for super user do, apt-get is the install command for Ubuntu, and Python-bs4 is the Beautiful Soup package in Python. Running this the first time will give you a build success but if it's already installed you won't see any build. Also after our hackathon, a number of students asked us about how to install different Python commands using the vagrant file. So in lesson four if you're interested, we walked through how to add this command to your vagrant file, that prevision through virtual machine. In part two, modify your tweet spell to omit only the first URL contained in a tweet, remember to check if the URL is empty before admitting. And we used the Tweeter 4JEPI which can be found here, remember that Tweets are called status. On the Twitter4J.org site, if you search for your URL, you'll see the method getURLEntitites. And getURLEntities has a method getURL that returns a string. And I've given you a few syntax hints that states just that, status.getURLEntities, remember we're getting only the first URL at position zero.getURL that returns a string. Next its time to create your own python file in part three, create a new python file in the resources folder called urltext.py. In part four use the urllib2 and BeautifulSoup libraries, to define a new class, URLBolt. An important hint is to wrap your procedure in a try/accept statement to avoid connection errors, connection errors will actually crash Storm, due to the implementation of Python in Storm. Here's a documentation for your reference, and if needed, a few syntax hints in Python. In part 5, create a Java bolt URLBolt.java, and remember that this is located in the storm folder and not the resources folder. And finally in part 6, update your Topology to visualize URL words using a Python bolt. And finally, in part 6, update your Topology to visualize URL words using a Python bolt. Notice that your word cloud is now a parsed word cloud of URL words from webpages that were processed using beautiful soup in Python. Remember as well that your code should match less in three stage three, so if syntax errors or problems occur, please look ahead for the solution or review the solution video. Good luck. So getting started in part 1, install the beautiful soup in ubuntu by using the apt-get command, and again you can do this by copying and pasting in the lesson 3, stage 2. In part 2 modify your TweetSpout to emit only the first URL contained in a Tweet. We do that in lesson 2, stage 2, navigate to the TweetSpout. We see that in the TweetSpout, the current logic states that the status.get text is how we're getting our Tweet messages. First comment out the queue.offer and using the syntax hint we create a string URL, we check that the status.getURLEntities.length is not equal to 0. If length does not equal zero, we assign the URL using the getURL function in the hint, and finally using the same syntax, we add the URL in the queue. Scrolling down in the TweetSpout we see the next tuple method. We'll pull from the queue, check that the queue is not null, and if not it admits the string using the collector.emit function. Finally the declare output fields method, you can leave the field as tweet. Next it's time to create your python file in part 3, create a new python file in the resources folder urltext.py to an adamant ad file urltext.py. And again, remember that it's a python file that goes in the resources directory. In part 4, we define the main functionality to use urllib2 and BeautifulSoup from bs4, to define a new class URLBolt to open and receive URL's parse intimate text, I use the splitsentence.pi example from stage one. Using SplitSentenceBolt as an example, we create the URLBolt class. First we get the url coming from the tupBolt.values at zero. Next in the URL lib2 library we get the HTML file using urllib2.urlopen(url).read. Finally from the syntax hints using beautiful soup making the soup. We have soup equals beautiful soup that takes in the html we defined. Next, using the syntax hints again, we use the soup.findAll method that searches for title and paragraph tags. And finally, we check if the urlText is not null. In Python this falsies. So if urlText is link 0 this will be false. But if it's true, we form a list comprehension, this is why I kept the storm.emit word syntax here but we'll, we can now get rid of this. And using the storm.emit syntax we emit t dot string for t in url text. Finally again in the hints, we want to wrap this in a tri except past statement, because any 404 error will crash Storm, once we wrapped the try except pass, we update our URLBolt class. And now we have our URLBolt class .run statement. And, finally, we include our import statements, urllib2, and from bs4 import BeautifulSoup. Finally in part 5, create java bolt URLBolt.java and remember this is located in the storm folder and not the resources folder. Once created, again, keeping the import statements the same, we can copy the splitsentence.java file into our URLBolt.java file. Next, we update the class to URLBolt, update the constructor from SplitSentence to URLBolt, update splitsentence.py to urltext.py and finally, I declared my new field text instead of word. Note again that the Python bold URL text.pie does not declare an output field, and this must be declared in the URL bolt.java. The URL bolt will be connected to our Python parse bolt using a shuffle grouping. Therefore, the field we choose will not be using a. Field grouping, but we will call this text. Finally, in part six, we update the topology by navigating to the Tweet topology, we add the URL bolt with a component ID python-URL-bolt. And here we have a shuffleGrouping connecting the tweet-spout. And finally, to complete the topology, we see that the tweet-spout connection to the python-split-sentence bolt needs to be updated to the python-URL-bolt. And one final check on our topology, we have the tweet spout, the python-URL-bolt. The python-split-sentence bolt, the count-bolt and the report-bolt. This is connected with a shuffleGrouping of the URL to the Tweet spout. Another shuffle grouping of the Python URL bolt to the split sentence bolt, the split sentence bolt to the count bolt and finally the count bolt to the report bolt. Just labeling our components a little nicer, we have the tweet spout, the Python URL bolt, the Python split sentence, the count-bolt, and finally the report-bolt that connects to Redis and our live visualization. We see that the python-URL-bolt actually calls the urltext.py file, and finally the python-split-sentence bolt is calling the splitsentence.py file. So notice that if you don't follow the hint in the split-sentence-bolt to check for empty values, you'll get a runtime error of NoneType object has no attribute split. What's happening is, the URL being passed is going to be empty but our split sentence bolt is trying to parse it and so we get a runtime failure here, and to fix that we'll simply go and add a check. For empty values. And so here following the hint, we simply check for empty values. Remember that Python is so if tuple.values at zero is empty, this will evaluate to false. If it exists, that is true. We set our words, and if words, again checking to make sure that that's not empty. We simply go through and use storm.emit for that word. Finally, in part six, update your topology to visualize url words parsed using a python bolt. Again running maven package and submit as usuall. And finally we see our word cloud updating. So great job on Lesson 3, Stage 2. Remember that your code will now match Lesson 3, Stage 3, if you ran into any issues. Great job. And another question I had, I guess. One of my friends who can't be here now wanted to know what are the use cases of, say, Storm versus Hadoop? Mm-hm. You know, like I know, you know, Storm and Hadoop can be used, like, at the same time. I feel like Hadoop is more of like, you do batch processing, so maybe store the Storm data in this, like, Hadoop cluster and then, do some analysis there, but. Yeah. If you could just list some stuff. Yeah, I think the the Hadoop is generally used in param called store m query. Uh-huh. Store m query in the sense like once the data comes in, you store the data forever in that cluster. Yeah. And whenever you'd like to run analytics, you can run against it. Mm-hm. And so because the shared size of the data, the amount of time it takes could be longer. Or shored up depending upon the size of the data that you are looking at, right? Mm-hm. And Hadoop storage system, and also an analytic system. Yeah. Okay. On the other hand there the Storm is essentially a streaming analytic system where you will. As the data is being produced immediately you want to do the analysis. Mm-hm. Because if you don't do the other Storm does not have a way, but which Yeah. The whole idea is to as the data is flowing immediately do that out takes and eye for the trans patterns whatever's happening on the data right away. Unlike Hadoop there will just store the data, and when you decide oh, I wanted know about these things. Yeah. Then you can do it. On the other hand in storm you ignore [INAUDIBLE] what you are looking for. And you program it to look for what you want it to look for. Are there some use cases where, say, Storm wouldn't be appropriate. You know, for example based, because you have to do things in parallel. Mm-hm. Or if you want to, like, do things and keep track of, like, per user. Mm-hm. And, like, show this graphically. So make prod example the, take the example of top and tax. Uh-huh. And that is when I use Storm application. Yeah. Because as the tweets are coming in a, what do you call it? A real time fashion. Then you're looking at continuously computing the top ten hashtags because it might be changing. Because suddenly something happens suddenly things will change, right? Yeah. But if we look at the lab that [INAUDIBLE] top ten trending hashtags, right? Yeah. And when you look at those hashtags, and that's kind of changing depending upon what time of the day you open that, right? So that is ideal Storm application. Mm-hm. And it because you got the, it's continuously changing. It's looking at the data and all the. Now, on the other hand, like if you wanted to look at something like, by the way, I was interested in I mean, I have all this data, but just couldn't for several years. I wanted to know, like, what were the hashtags that were trending for, like, two years? Yeah. The right? So that is more a part of. Mm-hm. Because it's looking at a lot of data, plus also looking at trending not of the last ten minutes or five minutes, it's looking at over like a year, or months full of data, right? Yeah. So that's not what our storm is made for. Okay. Okay. So that is that, that thing is like how real-time you want to be versus. How lazy you want to be in that cardboard, right? Mm-hm. That is the distinction between the two. Congratulations on completing Stage 2, and implementing bolts in Python. You're exploring some of the more advanced concepts of Storm apologies, and now we're going to add more cool functionality. In Lesson 3, Stage 4, rather than displaying a word cloud, what we're going to do is calculate and display the top N words. In this stage we're going to switch out the rolling CountBolt for the CountBolt, and we're going to use open source bolts, the total rankings bolt, and the intermediate rankings bolt. This is an extremely useful function, and one of the applications is display the top-N hashtags, that continuously update in real time. This is a complex assignment that requires some patience to import and connect successfully. But this is also an authentic real world assignment that you may want to use as the basis of a starter project. Our hackathon groups use this stage as a launching point for most of their ideas, let's get started. In part 0, add your credentials to your Tweet Topology, and rename your Tweet Topology to the TopNTweetTopology. Next, in part 1 remove the RollingCountBolt, and use the CountBolt. And as a hint, remember this requires modifications to connect. Next, import and connect the total rankings bolt, and required dependencies. At this stage I give multiple hints below in the reading node, to help you along, but I would really urge you to try working this out on your own first. This is a realistic experience working with open source tools, and it requires some trial and error. Delving into code and finding examples. Use the hints or simply check out my approach in Stage 5, if you want further help. If you're still with me and you want further hints, one is to use MavinPackage often and learn from build failures. Mainly to bring in import statements not listed as dependencies. And then two, after you've duplicated the total rankings bolt, read very carefully that the total rankings bolt, requires the intermediate rankings bolt, and that this bolt is configured using a global grouping. In hint three remember that the tuple schema must match between the count bolt and the intermediate rankings bolt. I also suggest connecting the count bolt to the report bolt, as a type of print statement to test your implementation. But remember that this connection can fail when the tuple schema is changed. In hint 4, I give some helpful syntax to connect the ReportBolt to the TotalTankingBolt if needed. But remember that since we're using open source bolts with no documentation, I actually had to look though all the dependencies in order to get this syntax. Once completed with part one, in part two, visualize your topology and change the TopN variable you created, to change the number of words to keep. Displaying TopN words is an extremely useful function, and one of the applications is, display the TopN hashtags. We can go from our word cloud that looks like this, to displaying, say the top 100 words that looks like this, and finally to this, where we are displaying the top 20 hashtags. Next, in part three, add a filter using skip words list, to the parse tweet bolt to skip undesirable words. What I did was actually use my top N visualization, to choose the words to skip based on words that come up often in Twitter. Next, in part four, update the tweet spout to emit hashtags instead of words, and remember that, hashtags simply begin with the # character. This is a fairly trivial update, and finally in part 5, visualize the top end hashtags. Remember to hit refresh periodically, and you can also use the Visualization http://127.0.0.1 :5000/basic. This visualization is simpler, and can be used as the starting point for some of your project ideas. But as you can see as the counter updated the size and location, of our top hashtags move along the screen. Once you're completed with the top and hashtags, take a screenshot and post it to the forums, comment on two other posts and, why not Tweet your image to your followers? Good luck. This was a complex but, in my opinion, authentic exercise using open source bolts to implement some pretty cool functionality. I hope the process was useful, and I walk through my solution. I broke my approach up into 3 major stages, imports or Part 1 of the written instructions. Linking to Topology, which is also Part 1 of the routine instructions, and incorporates a lot of the syntax hints. And finally, filtering and emitting. Which is Part 2 through 5. You can skip ahead to solution sections that you had difficulties with. And let's begin with imports for my approach. In part zero of the written instructions we simply add our credentials to the TweeTopology, and rename to TopNTweeTopology. So we navigate to lesson three stage four. Rename TweetTopology to TopNTweetTopology and remember to change your class name to TopNTweetTopology. In Part 1, we remove the rolling count bolt and use the count bolt. And in code we can simply comment out our rolling count, un-comment our count-bolt and remember to update the connection to count-bolt instead of rolling count-bolt. And once your visualization's working, you can move on to importing and connecting the total ranking's bolt to the required dependencies. Once we're at this stage, we simply navigate to the total rankings bolt using the link. And this will begin the process of a fairly tedious cycle to. copy>create>update import statements, build using Maven, which leads to more copying, creating, updating and building. So, let's get started. We first copy the TotalRankingsBolt. We navigate to the storm folder, not the tools and create our file name TotalRankingsBolt.java. We then look at the package structure and update. And our package structure again is udacity.storm, because udacity and storm. We see that there's another dependency from storm, starter, tools rankings, and we update the import statement and plan to. Copy Rankings into Tools. We navigate to storm starter tools, and find Rankings. We copy and past the Rankings.java file, create them in our Tools folder, paste and remember to update again. Now at this point without looking too deeply at the code, we might be ready to build. I'll just walk through this to show the types of errors that come up, and how you can use these errors to then repeat this entire cycle. We first Maven package. And we see a build failure. And build failure is that Rankable is not in our dependencies. And if we search for Rankable in our Rankings file, we see that we're creating a list of Rankables. Therefore Rankable should be in our dependencies. And we'll go through the process again of copy > create > update, using these build failures. We find rankable, copy > create > update, and finally try to build again. And now when we see the BUILD FAILURE, we see that there's an abstract ranker bolt that we're also missing. Navigating to the total rankings bolt. We see that TotalRankings extends an AbstractRankerBolt. We could have discovered this dependency if we walk through the code more in detail, which you could have done, but this approach is simply using the Maven life cycle, and build-cycle, in order to discover our dependencies. We navigate to storm > starter > bolt. And we find the AbstractRankerBolt. Again we copy > create in storm folder not tools. And paste the AbstractRankerBolt, in the AbstractRankerBolt file. We then update our package, and again check our dependencies and see that we have Rankings and TupletteHelpers. Which we already have as ranking and TupletteHelpers. But remember to update this import statement as well. And notice that I put TupletteHelpers not in util but in tools. An after our Maven package, we finally get our BUILD SUCCESS. So, great job importing. Next, let's check our connections, and notice that we haven't yet connected any of our components to our topology. Since our aim is to connect the TotalRankingsBolt to our topology, let's navigate there now. And if you've seen in hint two, or simply read the documentation, or the comments really. We see the intermediate rankings is necessary, and we configure this bolt with a GlobalGrouping. Therefor, we'll need to import intermediate rankings. We navigate to Storm Starter Bolt, and see intermediate rankings. We copy, create again in Storm, and not Storm tools, create IntermediateRankingsBolt.java. Update our package, and again look at our input statements. We see that we have Rankable. But we don't have RankableObjectWithFields. So let's first update our import statement, and once updated, let's go and get RankableObjectWithFields. This is in Stormstarter tools. And Rankableobjects with fields. We complete our cycle again by copy, creating, and updating with RankableObjects with field.java. Paste, and finally update the package structure. And notice that we don't have any further dependencies. Finally let's build, and hope this works. Great, and now that we have a BUILD SUCCESS. We should be done with imports, and we can move on to Linking to Topology. We know move on to our linking topology stage of our solution. If you have followed along with the solution so far, you notice that the count bolt is now connected to the report bolt. But we would really like the connect bolt connected to the intermediate rankings, total rankings and then report bolt, such as this. If this is our goal for the topology, we should start here looking at the total rankings bolt for the centex needed. Navigating to the total rankings bolt. And scrolling down to the constructors, we see that a TotalRankingsBolt calls a constructor with topN and calls super. Let's then look at the AbstractRanker class, and once we navigate to the AbstractRankerBolt, we see that the constructor for super takes in topN, and emits a default emit frequency in seconds, of two. Doing a search on topN, we see that the count equals topN. Therefore we see that we need to define a new integer topN. Navigating again to our topN tweet topology, and scrolling down to the topology builder section. We set the total rankings bolt with a variable topN, I named this component id total-ranker, and scrolling again to the total rankings bolt, we see again that this bolt requires an intermediate rankings with global grouping. Therefore we create the intermediate ranker and globalGrouping link. We then set our intermediate-ranker, intermediate-ranker, with a new IntermediateRankingsBolt. Again, with an integer, TOP_N. Which requires a fieldsGrouping of count-bolt. And finally, a new Fields, word. And we set the TOP_N count. And I'm setting it to an integer of 1,000. We've now updated our topology, to connect the total rankings bolt to the intermediate rankings bolt. And the count bolt is still connected to the report bolt. Therefore, we need to make the connection between the report bolt and the total rankings bolt. And again, according to hint number three, we want to check the connection between the count bolt and the intermediate rankings bolt that we've created. We make the connection between the report bolt and the total ranker bolt. We do this by updating the component id total ranker. And remember that we'll then need to check the connection between the total rankings bolt and the report bolt. Focusing the count bolt to intermediate rankings bolt connection first, we navigate to the intermediate rankings bolt, and we see that an incoming tuple is handled by a rankable object with fields. We then navigate to rankable object with fields. And we see that the schema that rankable object with fields expects is an object and a long count. Therefore we navigate to the count bolt, and we see that our count bolt is actually emitting integers. The way I approach this was to change our integer count to a long, first by changing the private map. Changing the count map to string long. Initializing the map from integer to a string long, and changing integer val to long val, finally I actually missed this the first time, but during compile time realized that we need to change our initialization to a long as well. Next, now that we've updated tupple schema for count bolt and intermediate rankings, let's test out the topology. But notice that the total rankings bolt and report bolt is still not checked. So to run the topology, well have to remove that connection. Let's do this now so that we can run our topology. We can do this simply by navigating to the topN tweet topology, and simply comment out the report bolt. We can test our topology by doing a Maven package. And if you recall from lesson one, you can send the output of the storm using the pipe character and grep total-ranker. And here we see the output of total-ranker, and we see that total-ranker is receiving words. Our next step then, is to connect the total rankings bolt to the report bolt. And I gave you a number of syntax hints in the reading node. Here however, I'll give you some of the intuition of how I came up with those syntax hints. First, since we're making the connection of total ranker to our report bolt, we first navigate to the total rankings bolt, and see that the total rankings bolt extends to abstract ranker bolt. Navigated to the abstract ranker bolt. We searched for the execute method and we see the function emitRankings has been called, and emitRankings is emitting values of rankings.copy and finally we see abstract ranker bolt is emitting rankings. Therefore to connect the total rankings bolt to the report bolt we need to modify the report bolt to handle and post rankings. At this point we're close. Let's go to the Rankings for java file. And we find a method, getRankings, that returns a list of Rankable. Finally, navigating to the Rankable class, we have what we're looking for, namely, getObject and getCount, which is a long. Finally, we navigate to the ReportBolt. And again, remember, we're making this connection between Total Rankings Bolt and the Report Bolt. And we scroll down to our execute method. We can first comment out the existing logic and build up our connection. Given a tuple from the total rankings bolt, we know after our code exploration that we can use getValue at position zero, which will return rankings that I'll call a rankable list, of class rankings. Given our rankings, we call the function getRankings. We iterate through the list of rankable and call the function getObject. And next toString to extract our string word. And finally getCount which is long. We publish to redis with word separated by the bar and a long.toString(count). We update our import statements. And here ii'm simply importing everything in our tools including Rankings and the list that we need that are in import statements in our other files. Finally, we navigate back to topN tweet topology, reconnect our report-bolt to the total-ranker. And now that we've made this connection, let's check. Package and Maven and run, and hopefully, your visualization works with a new world cloud that's using the topN, and here r I'll define topN equaling a thousand, to show. And if so, our connection's correct, and more importantly, we're finished with the second major section of Linking to the Topology. Great Job. We are now on to the last major part of the solution which is Filtering and Emitting. In part two of the written instructions, I hope you changed the number of words to keep by changing the integer value TopN. And now we're on to part three, adding a filter using a skip words list to the ParseTweetBolt to skip undesirable words. We first navigate to the ParseTweetBolt. And at the class level, I created a private string array called skipwords. Don't forget your new import statement to bring in arrays. Next, if you followed my hint, you can use your top end visualization to choose the words to skip that will be Twitter specific. After doing this multiple times, notice that I've skipped words like http, https, retweet, follow. And we do this by scrolling down to the execute method and simply add in the logic that we emit if the ray does not contain the skip words in the token. And lastly, we only emit tokens if they have a length greater than three. And finally, we have our word cloud, and notice this is one of the first times we can show word cloud without blurring out a lot of nasty words. In part 4, we complete the trivial step to emit hashtags instead of words; remembering that hashtags begin with the pound character. And we can do this simply with the logic. If token starts with a pound sign, then we emit. We can make this more efficient by keeping the code flexible for this example, led to this simple solution. Once we see that your word cloud is emitting only hash tags, we can move on to part five and visualize the TopN equals 10 hashtags. Remember, that's in your topN TweetTopology file. Change your count to ten. Once your visualization is running, remember to hit refresh periodically. What's happening is your D3 array is collecting a number of topN hashtags. And finally, if you like, you can do forward slash basic and navigate to that page, and you'll see a simpler visualization that I created that you can also as a jumping off point for your final project. Congratulations on tackling and completing one of the more complex assignments of this course. And take a screen shot, post to the forum, comment on two others. And if you like, tweet your image to your followers. Remember that you compare you solution to lesson three, stage five. If you continue to have any questions. Great job. We're onto streaming joints, that last major topic in the course. Let's first look at basic exclamation topology we started with to see just how far we've come. In lesson one we started with a simple TestWordSpout, and while simple these spouts are very useful as you develop storm topologies to test. In the final lesson one program and exercise, we implemented simple exclamation topologies to connect multiple bolts to the same spout. We're able to simply connect multiple bolts to the same spout, so if we have multiple spouts, linear topologies are simple. We haven't, however, covered this configuration. How do we connect multiple streams to a single bolt, and process two bolts received from different streams differently, depending on which component produced it? This is the final topic we'll cover in this course, a process called streaming joints. It would be great in our topology if we could simply use what we know about stream groupings and simply connect them to multiple bolts. Here we have builder.set bolt, bolt 1, then new exclamation bolt with a parallelism of 1 using a shuffle grouping to spout 1, which is the topology I've drawn on the left. It would be great if we can simply change our topology from this to this and add a shuffle grouping to spout 2 to make the connection from spout 2 to bolt 1, and storm allows us to do just that. Adding input streams is as simple as method chaining multiple stream groupings, and we will explore this in the next assignment. We will see that if the downstream bolt handles incoming two poles in the exact same way, we're done. However, what if two poles arriving from different sources are arriving in different forms? Here for example a tuple of a key, and a second tuple coming from the second spout of a key and a value. How does the downstream bolt dynamically handle tuples from different sources? Perhaps with different fields or even different object types in the same locations. The answer is to use the tuple method, getSourceComponent that returns a string with a componentId defined in the topology as the componentId for example spout 1 or spout 2. And finally, once we have the componentId, we can define in our ExclamationBolt file how to dynamically handle different tuple schema using simple code with the executed methods. Such as accessing tuples and processing them differently depending on which spout they came from. In this code, in the exclamation bolt for example, I'm basically check if the component ID here equals spout 1, notice that spout-1 is the component ID I defined in the topology. And if so access spout 1 tuple, here this could be the key, and do some cool stuff. Else if the component equals spout 2 here, access about two components, perhaps the key and the value, and do some other cool stuff. Now, once we have this comp set completed, we can design simple or complex topologies, such as this, or this, or this, or anything else you can imagine. Simply chain together stream groupings, including all groupings and field groupings, on one or more fields, and you can construct real-time processes to fit your needs, let's see this in action. In this last programming assignment before the final project, I've added two new spouts. The MyLikeSpout and MyNameSpout. You will update the exclamation topology and ExclamationBolt to process these two spouts. Your new ExclamationBolt will create a dictionary of key value pairs that are received from the MyLikeSpout. And emit a sentence appended with three exclamation points when a name or key is received from the MyNamesSpout. In Part 0, navigate to Lesson 3, Stage 6. And in part 1 initially both spouts emit similar tuple schemas of string. Import both spouts and add them topology. In part 2, connect the ReportBolt to the exclaim1 bolt and remember that you must modify the ReportBolt to make this connection. I've used an integer count of 30 and remember that this count changes the text size of the visualization. In part 3, without modifying the ExclamationBolt yet, use two shuffle groupings to join and connect to exclaim one, build and submit your visualization. Once completed with part 3, your visualization will have names and name favorite pairs, but they're not connected. So, in part four we modify the like spout to emit name and favorites instead of pair. Also remember that this requires the declaration output fields to be updated. In part 5, modify the ExclamationBolt to dynamically create a favoritesMap when a name, favorites tuple is received from your MyLikesSpout. I also give you a few syntax hints if you need them in the written instructions. Next, if a tuple from the MyNamesSpout is received, check your favoritesMap that you've created and see if it exists, and if so, emit the sentence, NAME's favorite is FOURITE. Append three exclamation marks to the end of your sentence and build and run your topology. Once your basic visualization is running with name and favorites dynamically joined using a streaming join and three exclamation marks appended, it's time to move on to stage six. Connect your ReportBolt to exclaim2. Modify the ExclamationBolt again to append three more exclamation points for a total of six. Finally, build and submit your visualization. After your visualization is running and six exclamation marks are appended at the end of your sentences, all using the single ExclamationBolt, in step seven, simply change the my like spout and my name spout to omit names and favorites of your own. Or change the ExclamationBolt as well to omit sentences of your choice. And finally if you like, take a final screen shot or video and post it to the forum. Comment on two other posts and how about posting to your Twitter followers. Good luck with this final programming assignment. In Part 1 navigate to Lesson 3, Stage 6 and simply import both spouts and add them to your topology. So we navigate to our exclamation topology and add import statements for udacity.storm.spout, MyNames, and MyLikesSpout. Next we scroll down to our topology builder section, comment out our word, and simply add the spouts. I added a parallelism of 10, but you could have chosen what you liked. And here's the MyLikeSpout, MyNameSpout, and I have chosen component IDs my-likes and my-names. So in the remainder of the solution, I left the component ID up to you just to demonstrate that whatever component ID you choose in your topology, you'll then need to use that in your definition of each bolt. So my-likes and my-names are mine. And in Part 2 we connect the report bolt to exclaim1, not exclaim2. And we update the report bolt using integer count of 30. And a way I approach this is, is I first commented out our word tuple. And I instead use getValue at position zero and cast it to a string and use that as our word. I commented the count. And simply uncommented in the integer count. Remember that the count drives the visualization, but here we're not longer counting using a count bolt. We're simply displaying it and so we need to synthesize. And finally, I connect the report bolt to exclaim1 using the globalGrouping. In part 3 without modifying the ExclamationBolt, I use two shuffle groupings to join and connect exclaim1. You can refer to the streaming join syntax video, but this is simply shuffle grouping my-likes and shuffle grouping my-names. And don't forget your semicolon. And remember that this is the streaming join syntax, when tuples have the same characteristics. So I'll be accessing the first position in each of these. We simply use making package and submit. And we see that our visualization has names, but also names and favorites pairs. And finally the exclamation bold already appends three exclamations. In step 4 we modify the MyLikeSpout to emit name and favorite instead of a pair. And remember that this requires the declare output field to be updated. We navigate to the MyLikeSpout. I'll comment out the collector emit statement, and instead we emit name and favorite. And instead of outputting the fields pair, we output the fields name and favorite. In part 5, we modify the exclaim bolt to create a favorite map, when a name and favorite tuple is received from the MyLikeSpout. We first navigate to the exclamation topology and at the class level we declare a favorites map, or whatever you called it, as a map that takes in a string and a string. Remember that this will the name and the favorites. Next in the prepare method we initialize our map. I called it favoritesMap. That's a new hash map that takes in a string in a string. And the execute method would we define what happens when the ExclamationBolt receives a tuple. You can use the component hint and simply declare a string. I called it componentId, that takes in the tuple received and used the getSourceComponent method. Next we can remove the previous logic. And use the componentId to modify the behavior, depending on where the tuple came from. First, if the componentId equals my-likes, and remember, if we scroll down to our topology that my-likes is the componentId that we chose for the MyLikesSpout. We know that the tuple is of the form name favorite. And therefore at position zero we have name and at position one we have favorite in both our strings. Therefore we're able to query the tuple and get back the string name and string favorite. Next we simply check if the name exists in our favorites map, and if it does not then we simply put the name and the favorite as our key and value. And finally, notice that if we receive a tuple from my-likes we do not emit. In the next step, if the tuple is received from the MyNameSpout, we instead check our favoriteMaps to see if the name exists. And if so, we emit the sentence of names favorite is favorite. And finally we append three exclamation marks to the end of the sentence. In code we simply add in an else if statement that checks if the componentId that we obtained from the tuple equals my-names. And remember, again, that my-names is the component id that I selected when I defined my MyNamesSpout in our builder. And finally, we build in the logic or what we do if we receive a tuple for my-names. We simply extract the name, and we see if the name is in our favoritesMap, and if it exist we simply query the map to get our favorite. Remember that this map was constructed from tuples received for my-likes. And then we build our sentence. We simply create our exclamated favorites, we append the name, name's favorite, favorite with three exclamation points. And to complete, remember that we need to emit in this stage our new values, which are exclamated favorites. And for completion I am still using the exclamated word as our declare output fields. And finally we package and run our initialization and we see that our name and favorites are constructed to create kind of fun sentences with three exclamation points appended. In part 6, we connect our report poll to exclaim2 now. Modify the ExclamationBolt again to append three more exclamation points for a total of six. We scroll down to our topology builder section and simply change exclaim1 to exclaim2, and in our ExclamationBolt definition we simply add in our new else if clause if the componentId equals exclaim1. Finally if the tuple comes from exclaim1 we extract the sentence from the tuple at position zero, create our exclaim, exclamative word, and append three additional exclamation marks to it and emit. Finally, after building and submitting, your final visualization should have our words with names and favorites constructed into a sentence. And six exclamation marks appended to the end. Optionally, in stage seven you can update your MyLikeSpouts and MyNameSpouts to personalize your sentences. And take a screenshot or a screencast and put it up on the forum. Congratulations on finishing lesson three. Congratulations on completing lesson three. In lesson four, you'll be working on your final project, after watching an Udacity intro to Hackathon. Hackathoners form two teams to work on topologies of their own design, after completing the same lessons as you and connecting to the same Twitter garden hose. Your project can be much simpler if you follow the basic guidelines, or you can reach much higher by following the Udacity Hackathon as inspiration to aim much higher. Good luck, and we can't wait to see what you create. Congratulations, it's final project time. For your final project, our boss needs a real-time, dynamic visualization of tweets that contain worldwide top hashtags. To implement this final project, begin with a Storm Topology that calculates the TopN Hashtags. Write a new Bolt that takes in TopN Hashtags with a streaming join of real-time tweets. Next, visualize only real-time Tweets that contain the TopN Hashtags. If dynamically calculating and joining TopN Hashtags is difficult, you may pre-calculate the TopN hashtags, halt your Topology, and enter these statically into a new Spout. However, you must use at least one new Bolt and a streaming join to complete the basic implementation of your final project. After this basic implementation is complete, update your screencast or screenshot to the forum at the end of this lesson. You may then add on as many extensions as you like, including changing the entire project, using any number of data sources and any visualization you either find or create. You do not need to use the Tweet stream after you've implemented your basic project. We hope you continue on through this lesson as you work on your final project to work alongside our hackathon participants. See the types of questions come up, particularly around designing topologies. Finally, at the end of this lesson when you see this screen, post your final project basic implementation to the forum and comment on two additional posts. If you like, Tweet out your image to your followers using Twitter. Next, again, at the end of this lesson, if you've worked on extensions throughout the hackathon along with our students, post them at the final project extension forum and comment on two other posts. Tweet your projects to your followers. I hope you have fun exploring your final project, and good luck. We can't wait to see your basic implementation and any extension ideas that you come up with. Okay. So where are you guys now, with your respect to your project? So we wanted to do, to try to sort tweets on Ebola into certain buckets. The ones we came up with were based on certain words, whether we think they sound like misinformation, that's was Mm-hm. An idea you sort of prompted us with. Another one that we came up with was sort of panic mongering words. Uh-huh. Coz that there's a lot of that. Mm-hm. And it maybe is an offshoot of that coz before we came in here we were starting to think of if there's correlation of especially these panic mongering words with political words. Mm-hm. Coz it's becoming more and more of a political issue. Mm-hm. So that's the rough, sort of, structure, I think, of what we're trying to do is. So what are the buckets that you're applying to do that one is misinformation, and political tones, and. And maybe one was just critical information. Okay. Mm-hm. Mm-hm. And the basis for that was there are some Twitter handles that are being trusted more, with ,. Been coming from trusted sources. So, we could use those as references. Oh, okay. So, for example, cities might have a Twitter account. Right. And that might be verified by Twitter so, that is an authentic account. Exactly. Yeah. Okay. And in terms of the technology. So, basically, we're going to follow. We connect the spots as, to the [CROSS-TALK]. Mm-hm. Hm. originally, we were planning to do some, lo, location, v, visualization, but since we know only a small part of the, the Twitter we have visual location data Mm-hm. And, we don't want to, e, e, e, exclude that all those other things, so. Well we, we probably got an inquiry audit where it's just for those of us- Okay. That's and the part. Later we can figure out if we want to add location visualization or not. Okay. I think like even this playing the bucket and coming up with some kind of score-able Percentage of misinformation versus. Right. Political tones, or even panic mongering and all, right? That itself is a good fate of how much, you can measure it. As long as you can measure it and give me the count, or give me the percentage, how much is going on, right? Yeah. That itself is a, a good thing to have. It's a very practical application. So, we're on Team Awesome. Just came up with that. Mm-hm. So, our project is based on doing sentiment analysis and displaying that geographically. So, we're doing it on various components. One based on emoticons and emojis. But another one based on the words itself and kind of using a natural language processing to get, you know, is it positive or negative. And then, we are also getting URLs associated with, you know, is it happy or sad. And, at the end we'll kind of display visually, the top URLs that were happy or sad. and, you know, you can click on it and go to it eventually. And also, we're also having another component of geographical stuff. So, we have a map of the US and the various cities, and so when you say, hover over it, you'll see, like, the top tweet that's going there, or- Mm-hm. The top words, and, you know, we're color-coding it and various things, like your happiness ranking. And then, another one, which we're going to be doing as well, is based on saying top, you know, things that you want to look at. For example, Apple, or Google, and things like that. Mm-hm. Counting them and the sentiment associated with that. So, we'll show how that progresses over real time. Okay. So, there seems to be like, multiple projects. [CROSSTALK]. There are multiple projects, but. To follow people in multiple jobs, different jobs in today. well, I know it's like you one spout, right now. we, I think one spot, right? And then, we might have various filters, right? Mm-hm. That are all happy happening and trained for it. And so, we have a bunch of it already comprised of just, you know, finding the score, output sentiment, you know. And- I think that probably your best way to do is like spilt it up into multiple even though you might be thinking about sharing the spout. Uh-huh. Because. I use from an, what is your focus point of your having one type of analytics versus a different type of analytics, right? Mm-hm. So, think about, like, just a sentiment on the day you are based on using emojicons and the happier you are in the words that are in the tweet which is whether it's happiness or not, right? Yeah. So, that itself is a big, to pull you on it's own. Mm-hm. But then, the second one that you mentioned about what is second, the second [CROSSTALK]. Brands. Yeah, brand. The sediments of brands over time. Yep. That is a completely different one, because the other correlate, some specific brands, right? Yeah. so, brands themselves are already input, because Topology doesn't know about brands, rightt. Mm-hm. So then, there are hundreds and thousands of brands. Yeah. [INAUDIBLE] to do that, right? So, when you want to do the brands, like the brands themselves and the corresponding words. And, you try to correlate a particular brand world with happiness and all the various things along with it. Right? Yeah. So, maybe it's. This is a good wall [INAUDIBLE] right? Uh-huh. This is a kind of brand where I sent him and which where he's [INAUDIBLE] all the time. Yup. So, that way like what happens is like it keeps the cold isolated. Uh-huh. Rather than mixing everything. Your endurance time does not prevent you from mixing these two in some fashion because, ultimately it's a [INAUDIBLE] program. Yup. But, in terms of complexity and direction of complexity. And, during part of it, it makes it easier to have [INAUDIBLE] in Java. It says that, that you would say. But, we can build the code. I mean, like, the spout takes up can be reused and some other components of computations that whatever, you can stack them as a library, you can link it into each both separately. Sure. Yeah. Lets now explore the Twitter4J API if your interested in modifying your data spout using the link in the instructors node we open the de facto official Twitter API Twitter4J as you've already explored in lesson 3 tweets are called status. We can search for the get text function, to explore what we have been using. And we see that getText returns the text of the status or tweet. Navigating to the TweetSpout, say, in lesson two, stage six, you see again that we've implemented this for you. Scrolling through the Twitter4J status options, you see we have information regarding a number of interesting data points. That you may want to explore in your final project. One could be getFavorite count, which indicates approximately how many times, the tweet has been favorited by Twitter users. Or maybe geoLocation, which returns a geoLocation object. Clicking on the geoLocation object, and scrolling down, we see in the method summary. The getLatitude, and getLongitude methods. These functions were used by one of our Hackathon's teams to drive their US map visualization. Navigating back to the status page, another interesting status method is getLang. In your final project one simple extension you could add is to emit tweet temples in only your favorite language, or emit language data in your emit schema. Another approach is to modify your Twitter4J listener. We navigate to the Twitter4J package. And navigate down to Twitter stream. Scrolling down a bit further, notice the function firehouse, which starts listening on all public statuses. This function listens to all public tweets, how cool is that? Here's an MIT Technology Review article from November 12th 2014, talking about the Twitter Firehose, and the ebb and flow of geolocated tweets as a social microscope about New York. It shows that the same functions we're exploring, are being used in popular research, culture, and analyses. Navigating back to the twitter4j. Twitter stream. Above the firehose, we see the filter method. We click on the FilterQuery, and scroll down to the Method summary, and we see that we can filter queries based on count, follows, language, location and keyword using track. If we click on Locations. We see the function our Hackathon team uses, to filter queries from Twitter based on a double 2D array. In the package overview, remember then after your basic implementation is complete, you can explore all types of data, filter queries and data types coming from Twitter, or choose data completely on your own from another source. I also hope you're following along with some of the discussions in our Hackathon teams. And use this as inspiration for some of your own awesome projects. So one thing. Mm-hm. That I was thinking about as I was making this list is, so these are all the terms that we. Mm-hm. Think are associated with certain buckets, but we're kind of making educated guesses right now. Mm-hm. In practical terms how easy is it to iterate on this a couple times where we might, you know, we try this, we realize that oh there's, there's ten more words we want to add to this. Mm-hm. Bucket that sort of thing. Yeah, I think like, one way to do simply do that is like, on this, this data, whatever you have. This is kind of more model-building data in, machine learning terms, right? You can continue enhancing the model by adding more and more words as you, you come to learn about it, right? So, a simple way to do that is either, ,. You can, from a programming point of view, if you wanted to read those words and the correlation word, you can have some kind of file format that you can suck it up as a resource before launching the topology, right? So, when you, so if you think more or less improve quite a bit, than you. Build a login you want. I see. So that way it can, it can keep going. But if you wanted to [CROSS-TALK]. FLAC files That is a simple way to do it. I mean, on the other hand, if you want to do, do a sophisticated model, then you have to identify twigs that you think there is some mod, you hold along what other things that is. The thing is applicable, but it's part in the moral. Then you are to do a feedback. Right. Loop kind of thing, right? Mm-hm. So on the feedback loop, you can do it in two ways, right? One is you can have feedbacks itself because you can then [INAUDIBLE] into a stream also. It's not that when this word, words are pulled out as a stream. It's kind of a slow stream. The data is not incoming in realtime but once in a while it comes, right? So then whoever is computing the feedback loop, they should know, oh these are the words I have, but this word is still, I think it's occurred enough number of times, along with some panic-mongering words. Because some of the words. What do you call it, more of a correlation. Yeah. For example, in parent modding if multiple words are given if you incorporate a new word, if there is a enough support for that word, because it is a current parent mongering word. Right. Sort of like, there's a likely. Trends that also occurs with it, right? Mm-hm. That also is a right? Right, right. So that's a Then you have a discovery phase, right? That's also computed as a stream separately, and once you have enough confidence on that word. Yeah, now we include that word also as a right? So that is more sophisticated, more Automatically learns itself, as well as Unexpectedly, a number of questions at the hackathon revolved around our virtual machine setup, and in particular Vagrant, Vagrant Cloud, and Virtual Box, which are controlled by a Vagrant file written in Ruby. Weaves a web server micro-framework flask written in Python. And the in memory key value store message publisher Redis, that drives the data driven document, or D3 framework written in JavaScript. We briefly cover these topics in the next few optional sections, along with links to other Udacity courses and resources where you can learn more. So it, it sounds like we want to, since we have two, two maybe big buckets. Mm-hm. That we want to sort things. Maybe three. Mm-hm. misinformation, panic mongering. Mm-hm. Reliable sources. Mm-hm. Maybe political stuff. Mm-hm. So each of those would be a separate sort of output bolt, is that? Yes, because I, I think like because each set of words that is characteristic, this computational bucket is different. Mm-hm, right, yeah. So, you might need a different kind of bolts for that. So probably like you might you might have one single spout. A spout which takes all the Twitter data, right. Mm-hm. Then,. So then you might have one bolt that might be doing your panic stuff, right? Mm-hm. Then a little bolt which is doing that on this information. And bolt with the current resources. Right? Mm-hm. So now in the credible sources you might want to say, like, now which data, so, this bolt is going to send the data [INAUDIBLE], right? So now let us state the correct credible in bolt. In this case, you must have [INAUDIBLE] credible accounts, right? Right. Mm-hm. And this credible accounts are the ones that's going to. I mean, if a lot of credible accounts are there, are probably, if the accounts are too small, then all the bolts can carry those accounts. Okay. All right, so then you can use that survey by which, okay, I'm getting all these tweets and the status from this account, okay, this is a credible source, right? Then on the credible source, whatever you have to do, you have to, if it requires further stage processing, yes, you need another bolt. Right. You can stage it and finally you can say something of a credible score or whatever it is, right. That is the output going to be right. Yeah. And this information you already have whatever, you can break up those words and like, like the credible accounts, you have already input some kind of a, panic words, right? Right. And then the you have to include the logic of how many times this for a particular tweet you want to attach a score. So the whole panic that tweet this. Uh-huh. Yeah. So, so like if more panic words are out, again, then the degree of panic is much more higher than this one, right? Right. Come up with a score. And, each tweet is assigned a certain code like that, right? Then, at some point, then you would make another bolt, which is aggregating those scores of the tweet, and slowly aggregating and coming up with one score. So that you can have a, what is the panic score, or whatever that is, right? Uh-huh, yeah. That is another one, right? So similarly here. Then I think the misinformation. How much is the degree of misinformation. Again, for each tweet, what are the words that carry the misinformation. Because let's say that you accumulate some characteristics of what Ebola is not as compared to, and people are saying those things. Compare it with Ebola, then find out how much is the degree of misinformation. Then give the misinformation code. Except having this, to follow the same pattern. Yeah. Except the, the way you compute it will be different. Mm-hm. Okay. So because this is encapsulating computation logic, right? So it will be very different in terms of this one. So once you compute them, then all of the scores you can make available to that is, then you over the dashboard and the dashboard will pick that from where it is or whatever, and then display those scores. And update the scores continuously. You've already downloaded Vagrant in VirtualBox. Two technologies used to provision, and run Ubuntu 14.04 LTS virtual machine used throughout this course. Follow along with the links provided in instructor notes and you'll see that VirtualBox is virtualization software developed and maintained by Oracle. Virtualization refers to the creation of a virtual machine that acts like a real computer with an operating system. For example, your computer could be running Microsoft Windows. And for this course we've given you a virtual machine that has the functionality of a computer with Ubuntu Linux. This is possible using the virtualization software VirtualBox. Next, we use Vagrant, a free and open source software, for creating and configuring virtual development environments. This can be seen as a wrapper around virtualization software such as VirtualBox and around configuration management software such as Puppet or Chef. In our setup we pull a base image or virtual machine disc image from the Vagrant cloud and if you recall from lesson 1 this step required about 15 to 20 minutes during your first run. If you recall from lesson 1 I promised to demonstrate modifying the Vagrant file and the privison.sh file to provision the machine for the Python beautiful suit package. Let's walk through this simple modification now. >From lesson 3, stage 3, recall that we needed to enter a command, pseudo app get install Python BS4. You may have noticed in lesson 3, stage 2 that if you failed to do the install step and you submit your topology, your process dies, where no module named bs4 exists on your machine. To modify your box at the provisioning stage, step 1 is to destroy your current box. First close any connected web pages. Next, stop your Python flat server, if you followed my directions for setup you'll have another tab where your flat server exists. Use Ctrl+Z, so Ctrl+Z to send it to the background, and next kill %1, which will kill the program in the background. Next, log out of your Ubuntu box in both tabs. Log out and in your second tab, log out. Now that you've logged out of your Ubuntu virtual machine, in your local machine, make sure that you're still the original ud381 folder. Type Vagrant destroy in your local machine. Followed by yes. And notice that the virtual machine is now destroyed. Once your VirtualBox is destroyed, you can navigate to vagrantup.com and follow along by clicking on the documentation. Once in documentation, click on the Vagrantfile link. In the Vagrantfile link, notice that the syntax of Vagrantfile is in Ruby. Next, click on the Provisioning link. And scroll down to the shell link. We use a simple shell provisioner, and scrolling down, and scrolling down you'll find the syntax for external script. We use the Ruby syntax config.vm.provision with option shell, and specify the path to our provision.sh instead of script.sh. So, navigating to our text editor. And navigating to the Vagrant file. We simply add in the config.vm.provision with the shell option and our path is provision.sh. We then navigate to the provision.sh file and we see a number of Ubuntu commands to install when the virtual machine is being provisioned. We add in the python-bs4 package. And remember to save your provision.sh file. Once saved, we come back to our Vagrant up command in our terminal. And notice now the new package of Python-ps4 is being updated. We also see that our provisioning script fails because some of the files already exist. Notice that the provision machine only takes about 20 to 30 seconds to start up. And this is much, much faster than the 15 or 20 minutes that it took during our initial run. This is the because the base image has already been downloaded from Vagrant cloud and now with our Vagrant and provision files we're simply provisioning or installing our required software and starting services needed for the course. After provisioning our machine we Vagrant SSH. And now we can check if our Python bs4 package has been installed correctly. We can do that dpkg-l and let's search for python-* to look for all Python packages with a dash. And scrolling up we see that the python-bs4 package, the beautiful suit package, has been installed correctly. Finally, if you're interested in building more virtual machines, or playing around with provisioning machines. I encourage you to explore the Vagrant documentation further, and have some fun doing it. So all the tweets come in from a lot of places. Mm-hm. And then they form some master, big pipe. And then Storm processes that. What do you use to aggregate all the tweets from [CROSSTALK] is it storm in reverse? [LAUGH]. Actually it's not there's a sophisticated pipeline and scaling system that runs those things. Okay. So like, we have what you call Ascribe running on pretty much all the, the servers, and so whenever you create and do engagement with the computer application and everything. Yeah. Those are all captured as log units. I see. Those log units are captured on the [INAUDIBLE]. So, I the logs and collected like on the door. And we can more of the that's the main part. And once you collect them, then it goes to something called a Skype application system. Where it's coming from several hundreds of thousands of nodes [INAUDIBLE] aggregated nodes. And those nodes in turn pump the data into HTFS for complete persistence. Okay. And also push towards Kafka. And that is to be a queueing system, that I said. And up all these out of it. Out of the Kafka. Okay, thank you. No, that's very interesting. Yes. [CROSSTALK] So that, that pipeline/ is a pipeline unless One thing I'm trying to understand is like at what point do we decide that we need, say, another layer of bolts. For instance there could be a single bolt that could calculate the values along information and credibility dimensions, a single bolt. Mm-hm. Then we wouldn't need this. Yeah, definitely yes. Because the thing is like the advantage of this one is this is more relaxed. Great. To make the logical, clear, what you call panic versus misinformation credibility is more [INAUDIBLE]. And it kind of all on their own. Okay. On the other hand, if you can put all of them into three. You can do that too. There's nothing, like, stopping you from doing that you can do all of them, except the fact that your competition is going to up. That's right. Competition is going to go up but number of instance will be probably some of the number of instances of each one of the model, Mm-hm. Involved with here. Hm. Right. Right. Yes, yes there's nothing that prevents you from doing that except you are your competition would change a little bit, and this avoids that, this thing. When you avoid this stage, then you are avoiding the network traffic that's going in from the one stage to another. And the cost of putting the data into a network, which is called serialization. Right. And the cost of reading back from the network, reserialization. Yeah, makes sense. And the moving the network data and the network, that also costs. So you are avoiding that so which means you could be less latency. Okay. then, Oh, okay, so that one, in this case, like this is more of a, what do you call? Fun grouping? Now that we've covered provisioning Vagrant files and the provision.sh file to add in our Python package, we next talk about Flask. Flask is called a microframework because it keeps the core simple but extendable. Flash has no database extraction layer, form validation, or any other component where preexisting third-party libraries provide common functions. As you can see from the home page, Flask is fun and easy to setup. In this quick video, I'll demonstrate how easy it is to modify Flask. Make sure your visualization is running or launch your server again by navigating to the viz in your terminal directory and entering python app.py. And in your local browser, navigate to http://127.0.0.1:5000. And next in Atom, navigate to your viz folder and open app.py. I used the tutorial to create a Flask application named app and a decorator function syntax to create an app.route with the forward slash location. To change this location, we can simply copy and paste, and I've renamed the new route /hack. And after defining our route, we define the function show_hack. We then navigate back to our browser and enter in our new location, /hack. And we see that our code updates immediately and the same Twitter storm word cloud template is accessed using our new location. Now that we've seen how easy it is to modify paths in Flask, our next job is to explore the key-value paired database Redis. Mm-hm. I know Lab Door Architecture is getting pretty popular as a buzz word nowadays. Mm-hm. But sort of looking beyond that into having exactly one system where you can potentially re-process from your, your message buffer, whatever else. Do you guys have any plans to extend Storm Architecture to do that? I mean [INAUDIBLE] exactly from this point of view I see advantage, for doing exactly once because it can deduce the constraint on the backside plane. Yeah. Because any going. Mm-hm. And that in turn if you make that exactly ones without having too much resources added into the existing pipeline. Then you can save a lot of cost on the batch pipeline because easily can go to the point where you cannot once a day or. Mm-hm. Probably can only on demand if the real biplane is steady enough. [INAUDIBLE]. Now we've given, having said that now the batch, I'll do good if you're doing exactly once. So now, that might be hard because the reason why it is especially when you're doing real-time pipeline, there are some failures that's possible. Even though you might be exactly want, there could be failures, not on the arctic pipeline, but on more of the data capture pipeline. Right. Right, for example, operationally as it could be mission going down but the data in that might not have been collected until the mission was brought back up, right? True. But as on the batch pipeline the data can lazily get there, eventually the batch will pick up, right? As long as there's enough time to transferring the two on the real time pipeline there is a bunch right? Mm-hm. So which means so you can not live with exactly one semantics, i'm not saying that, no I don't want any batch anymore, right? Right. To explore the Redis connection, navigate to redis.io, and you see that Redis is in open source, advanced key value, cash and store. You can navigate to Commands, and in our Storm setup, we focus on the Redis Publish, and Subscribe functionality rather than Storage. Clicking into the command Publish, we see that Publish consist of a channel and a message, implementing this in JavaScript is straight forward. Remember that we used the lettuce package to connect from Redis in Java. You can search in Google for Java Redis Lettuce, and you'll see some of the exact code I had to follow to make this connection. But let's focus on the redis connection here. In your text editor, navigate to any ReportBolt. I'm using lesson three, stage three and we search for redis.publish. Here I use the Redis Publish syntax of channel that I've named WordCountTopology. And our message string. Now that we have the publish side of our connection using Lettuce and Redis, let's explore the subscribe side of our connection. We navigate to the subscribe command and we see the general syntax of subscribing to a given channel. I do this in flask by following by a really cool Redis flask snippet. You can search snippets or tutorials using the search command, and I search for basic messaging queue and Redis. Click on the snippet. Next, my basic approach was to follow the basic messaging queue with Redis snippet, and also searching for Python documentation. And using the Python documentation for redis-py. In the Python documentation, if you search for strict Redis, you find the syntax that I used to make the connection with the conventional port 6379. And in your text editor, navigate back to app.py for a fly server. And you see the StrictRedis function with port 6379 by convention. I used additional Python documentation on Redis, searching for pubSub, which stands for publish subscribe. You see the publish subscribe section to create a pubSub object. Back in Flask, I create my pubsub object. And finally use my pubSub object to subscribe again to the WordCountTopology. We navigate back to the ReportBolt and just confirm that we have our WordCountTopology. Now that we've seen the ReportBolt using the publish and subscribe functionality commands of Redis in flask. It's time to look at how this combined drives our D3 visualization to create the word clouds that we've been using throughout this course. We'll cover this in the next video. So I see that one layer is sort of computing the the individual. Tweet. Tweet value on a, on a particular dimension. Mm-hm. Panic dimension or misinformation dimension. I mean they, they may not be compared this way. Yeah. I mean, I inc, this kind of gives me a model design, that's all it is. [LAUGH] Yeah. In, in this, in this sample topology, and then the second one sort of aggregates them for each tweet. Mm-hm. And then the third one computes some statistic. Oral statistics, globally. Is this a, a reasonable way to approach topography design or- Yeah, yeah this is a way to reasonable to, I mean this is a reasonable way to approach topography. I mean typically people, the way you do is like, you kind of split the work. Mm-hm. And do some local, what do you call simple first stage filtering, like more local joining or whatever it is. Mm-hm. Then finally, you can find like so that is typically, this is the final application, right? Like, that is, typically all the [INAUDIBLE] tend to do. Mostly like sometimes, like, [INAUDIBLE] happens at this stage, then followed by some kind of counting happens, partially counts happen this stage, then the global aggregations, analytics, happen at third stage, right? Right. So, yeah, so that is a typical, design of a [INAUDIBLE] Right. so, I'm a very strong believer that given time and open source like analytics stack and the pieces of that will emerge as the standard in the valley. And it's really determined by like, companies and the problems that they're aiming to solve. When I think of analytics I think primarily of- [CROSSTALK]. Mm-hm. So, I've spent a lot of time looking at, like, what Twitter's architecture looks like. And, I know for at least processing, stream, for stream processing, you guys are a very big storm shop. Mm-hm. In terms of scaling out your infrastructure, I guess, I know you guys use like, me cells a lot for, like, resource management. Mm-hm. Resource scheduling. I'm kind of curious about the query side of things, and, and I guess maybe the event the event handling side of things- Mm-hm. where, oh, sorry, event delivery side of things. So, how you guys are solving that problem. Yeah, I think like, I mean if you look at all of the [INAUDIBLE] I mean some of the talks that they have given you here. Right. So I see you all know like the [INAUDIBLE] plane. Yes As well as [INAUDIBLE] plane Yes. And, the [INAUDIBLE] has turned into a lot of [INAUDIBLE] which is more of a scribe, which kind of taps the data whenever the data, the moment, instant data is produced. And, from Scribe the data moves to a Scribe application systems, from a Scribe application system it goes. Okay. . Into data loop. It also goes to two cluster and cluster. Andm at Kafka, we use it quite a bit. Mm-hm. And it has been there for historic reasons. Okay. And, the apologies kind of feed on those us data. Mm-hm. In order to produce the analytics and the similarly passes bunch of back jobs. And, the bad jobs are able to turn in the peg or even scholarly because scholar is very popular. Mm-hm. So, we use one of those two major things. And, you can make your jobs like that or on the real time you either directly express your data. Sorry, you analytics using EPA, all of them are using some Hummingbird. Again, Hummingbird is open source as well. And these analytics are done in parallel or in tandem, and will read the difference between the between the two. And, once analytics is done, then the output of these analytics are either turned into. Some kind of key values [INAUDIBLE] cache, depending upon the requirements of how long that [INAUDIBLE] has to be. Either it has to be persistent or it's okay for it [INAUDIBLE] then it's up to the [INAUDIBLE] writer to choose that. Once it is written, then, then whoever is interested in consuming analytics from what he called from an application point of view. Then, they used appropriate API in where to get the data from. Then, they had to obviously refresh the logic cache port. I had a ton of fun implementing the real time visualization in D3, and I hope you explore and extend this functionality in your final project if you're interested. Also, remember that Udacity's course visualization using D3 could be super useful to take if you want to expand on this brief introduction. Let's explore the basic D3 implementation by navigating to the the app.pi file. Scrolling down note that the decorator function, @app.route(*/basic*). And function shom_basic calls the render_template method, on the basic.html file. Navigating to the Templates folder, you see a few files that you should be familiar with if you completed the optional personalization exercise in Lesson 1. We navigate to the basic.html file, and inspecting further we see that our D3 files are loaded via this source file, and our second source file is under static/app.js. We scroll down further to the body and script tags, where I've defined a variable colors, and variable svgContainer which stands for scalable vector graphics container. This uses D3 methods to select the body, and appends an svg element with the given attributes. This is the svg container that is updated in our app.js script, to visualize our tweets. Remember, if this HTML and JavaScript section is interesting to you, you can always explore this in much more detail in an HTML and CSS class. JavaScript course or our data visualization, with D3 course, here, Udacity. We then navigate to the static folder, and open the file app.js, sourced in our basic.html file. Well, since here, a familiar stream path that was defined in app.pie. And if we navigate quickly to app.pie, we see the stream path, and function response and event.stream. And finally, the event.stream is connecting to retis and the pubsub.subscribe function to the WordCountTopology, connected to our publish function in our storm.report bolt. Navigating back to app.js. Notice that I'm using an event source constructor. You can read about event source further by clicking on the Mozilla link provided in the instructor notes. And you can search for the on message, attribute. This takes an event which is again a message received from our redis subscribe method, and splits the method by a pipe character. Let's again link this back to the syntax, that we used in our report bolt Storm topology. Navigating to any report bolt, we see that the we have a word separated by the pipe character and the count. Navigating back to app.js, we see that at position 0 we define our word, and at position 1 we set our count. Next using the JavaScript associative array which I've named hash, I simply set the word =count. Notice that I have another skip word check but be removed after completing lesson three top and words filter, and by now you should see that filter at this late stage in our data flow is very inefficient. Remember to always filter as early as possible in your topology. Scrolling down further, we finally come to the real D3 syntax, that I've entered into an updateViz function for timing purposes. We define a new variable text, that takes in the svg container and the D3 selectAll, with text attribute. Notice there are none to begin with, but the will be created by D3. We then bind our data, using the D3 entries function on hash. This entry function is great in that it takes a word count association, and transforms it into a keyword value count association. And this is the form that we are using throughout the rest of our D3, functions. Additional information, along with a great tutorial, can be found in Michael Bostock's selection page, included in the instructor notes. Next, we enter this data by calling the text.enter function, and append text with the given attributes. Again following Michael Bostock's example, I followed the selection.text with value. Syntax by calling the text with an added word by using the d.key notation, and remember that key maps to word coming from Storm. Now that our text is the word coming from Storm, I added a few fun transitions, delays, and explicitly tried to demonstrate how the x and y positions, can be set using the d.value, which is again our account coming from storm. Along with the position order that the word count comes, in our associated array. Next, again for fun, I updated the font size, to be linked to again the value which is the count. And finally the fill color, using our colors variable, again linked to count and now position. A fun demonstration of this, can be seen by going to the forward /basic location. And I'm running the solution from lesson two, stage four. Notice that in the random sentence word count, as each word count is updated. We see them moving in the y direction, changing colors with transitions, and growing in size. We can play along with this more by going into our app.js, and commenting out the previous line and uncommenting out, a different fill function. We save our app.js file, but we don't need to recompile our storm topology, and refreshing the page we see that the color palette has changed, to be a smooth transition to a dark red. We can play along with this further by removing the dependencies in the order that the words show up in our associative array by deleting, our I variables. And we see now that x and y position is controlled only by the count, and the two words in the lead are and and seven which are overlapping. Notice that it's really simple to simply save and refresh in chrome, and we don't need to rebuild our Storm topology. This makes iterating on visualization super fast and easy, so. Play along with this if you like, and maybe use some of these ideas in your final project visualization if this interests you. I'm just asking cause the contrary, my company should not, it varied in putting an as star me impression, but our concern is about the, to run in, in the crass turf that we know in the, in from the stump documentation that we, every time you want to change the topology, you shut it down and bring it up. Mm-hm. It seems it's not very practical for production. Well we do that in production all the time. I mean see the thing is like here whenever you from the stomp apology whoever wants to likes the apology you never shut it off for days after that. I know to that have been running for two or 200 days without even being touched. The last one I looked at, it was 152. That is not, that hasn't been touched at all. Mm-hm. So, so, so, if that is, typically, since these stipolis are long running. Mm-hm. You don't have to keep stirring what do you call it? Kill it and submit it again. So once in a while you do that, right? So, it's a very now what do you call it? Where you less frequent operation. So that way, like even in production topology, your down time is so low, as long as essays are mad, right? Mm-hm. Then you're fine. So if we don't need to take the technology, if the, we feel the class is not enough to deal with the, the, the, the, the, the data. Mm-hm. The running data we can't a, add more machine and- Yeah. That's all it is. It's not do a run time Yeah. [INAUDIBLE] With that? [CROSSTALK]. Yeah, but not like, for example, so there is two type of data that you have to look for. When you, when you provision the proposal first, right? You will have looked at all this data that you are getting and you would have calculated some x number of instances in all the various things, right? Then when you deploy topology, you also would have seen the forecast of how much that is has going to grow. And you would have accommodated for that, right? So that's what we call and also you had to look for spikes. Spike right because sometimes the data like going in like three comments especially super human from like a 10k per second to close to 140k per second. So it's hard to wield provision for that. So, so these two are the key things that you look for and when you approximation of the product and it goes to the production. You would based on this calculation, you would of calculated some kind of and all the instincts and you would have launched it. Now if the topology is misbehaving during that time. Mm-hm. Then you will know because of the way it is data that you get. Even the open source com also has this notion of UI. But you can see how much topo is [INAUDIBLE] every stage and what amount of time took at every stage. Even though it might not provide a CPU [INAUDIBLE] but you get a lot of data. And using that you can figure out [INAUDIBLE] that I'm [INAUDIBLE] is going evenly good or not. For example if there is a data queue in some board. Then if you look at the bolds and the corresponding image counter log and immediately will know from the count itself. You know oh, this is so high on that guy so in between there is a skew inside right? Hm. So, so you can figure out those issues right away. Then so typically it starts calculations typically it will be time to look for a year. And according to probation. So that we don't have to kill them again and again, right? So now once the data grows beyond your certain limits your reproduction And that's all. And, and this is another link related to like the dashboard. So we had the situation where if we were pushing too much data out, then the website that was displaying it wouldn't update in time. And it would sort of be a, the data was coming in so fast that the browser was constantly updating, rather than, sort of rendering, because it was too large a render for it. Oh, okay. And I just wondered if there was a trick for that. That depends on, how much you want to pick it up on all frequently refreshing the webpage too. Okay. Because, the thing is like, if you could, I mean, take the example of a radius, right, and if you have a scan into a radius, like, let's say your display is going to, display a brand trend, over like ten days or 20 days, or whatever, right. So after a five minute interval, then you might want to do, fetch 24 hours divided by five minutes multiplied by ten days, right? So that many data points that you would pick up every time. I see. So what I would suggest is like, the page has data for all the, the time and trouble that you have, and then you fix that a lot. Yes. In the last five minutes only, and then update it internally. Then throw away that last one. . And then begin different importing. Like a role model. Yeah. Once you have a good understanding of how Storm is driving the basic D3 visualization, you can find inspiration starting in Michael Bostock's blocks page. This page, and blocks.org is an awesome place to start if you're looking for really cool visualizations. I wanted a word cloud visualization for this course. So I searched word cloud blocks.org and clicked on the top link to find Eric Coopley's awesome implementation of D3 Word Cloud. The main difference between most D3 implementations I found compared to what we are trying to do in Storm, with real time visualizations, is that most examples use a static load once data set. Here we see the frequency list that actually extends quite a bit to the right. Inspecting further shows that Eric Coopley's implementation uses the format text and size. Therefore, our general process is mapping our data to the D3 format first, then updating this dynamically to visualize. So let's navigate to cloud.html and, in addition now to D3, we pull in the app-cloud.js file and the d3.layout.cloud.js file, both located in the static directory. Scrolling down to the body and script tags, we see that the original static frequency list has simply been updated to take in our d3.entries and our hash function which is our associate array. We then define our own variable frequency list that takes in the d3.entries of the variable hash which is in java script in associate array. Again, similar to the basic implementation, we select the body and append an svg. But here we have a d3.layout.cloud function that uses a function words to bind the data to the frequency list. In our own modification, we use the same words function, bound to the frequency list. And finally map the font size from d.size to fontSize d.value. Navigating to our new app-cloud.js file we see the basic structure has not changed but now our updateViz function updates the same frequency list to d3.entries of hash, and calls the same d3.layout.cloud function with frequency_list and d.value. We then navigate away from the basic path to the cloud path, ending in only /, and we see that with a few modifications, we can use the same structure, the same data engineering pipeline. And use open source D3 visualizations for dynamic real time visualizations of data. During our Hack-a-thon, Team A, or Team Awesome, used a similar approach to find an open source map, link this starting from the basic D3 implementation I provided. And used color defined by Sentiment to visualize top tweets per county. You'll see a few of their team members discuss their approach later on in this lesson, along with their actual code provided as part of lesson four. It sounds like being able to decide on a good topology involves being able to analyze what the performance of one topology is. So does Storm provide like real-time analytics. Real-time analytics. For when you want to run a topology. Yeah, I mean we certainly have that. Okay. But not in using Storm, but we have this separate stack that allows you to know how much resources each instance is taking. I agree. Memory as well as JVM, and how much CPU is being used, how many tuples are emitted and consumed, how much time is spent on each one of them. We have a whole can of metrics that you can look at it. Okay. So that if there are some problems, then it's an issue, yeah. But then you're saying it's external to Storm itself. That is external to Storm, yes. Okay. Yeah. And that is a, that is a good way to see barely work, especially we use for topology. Right. Because, I mean, a lot of teams like yours, when you write the topologies and evaluated we have some issues here we don't know how to figure out. Then we look at these huge humongous charts to figure out, okay, there it is. This is the issue. Yeah. And one other problem that we face often is distributed debugging. Especially with this case of running on so many nodes. Then if one bolt is misbehaving how do you figure out which bolt is misbehaving, right? So that's very hard to do. So, one of the things that we, and Storm architecture also makes it more complex, because of the fact that the Storm workers are running multiple tasks in a single worker. So when I see the logs that you get, because each one can have a log, a JVM based log, right. All the logs are kind of an aggregation of logs coming from these separate tasks. So I don't know which one is spewing what logs, right. So it's very hard to debug that. Yeah. So and, so because of that, we have come up with an analytic system called log indexing, so that all the logs that we produce about these topologies are indexed separately so that I'm, each task is given a unique ID and the topology name is unique. And you have [INAUDIBLE] topology name, this task, this task, then to bring up the log that is pointing to that. So at a one single place I can go and take a look at it. Otherwise it becomes very hard to even debug anything. And you can spend hours and even days to debug a particular issue. And sometimes, you are not able to even figure that out. So in terms that are a streaming pipeline,. Mm-hm. Do you guys leverage Trident at all? So, Trident we were using originally. Okay. But then there, due to some production issues and and we decided to kind of phase it out. Right. The main reason is like he targeted some kind of issues in terms of how often we can sustain the right, especially because is a [INAUDIBLE] system, not a tuple-oriented system. Yeah- So on that writing of those tuples inside, any of those batches are leading had some constraint. And then we also found out, like more than 98 [INAUDIBLE] jobs are using regular Storm as compared to Trident. Right. Then when we look at use cases for Trident when we went deeper and they, they no need to do exactly once. Right. So it so but thing of merging these [INAUDIBLE] doing multiple, maintaining those systems means, that we decide to do only one system. So that kind of converged everything went to strong [INAUDIBLE] Okay. So, Team Awesome, thanks so much for participating in our hackathon last week. We really loved your [LAUGH] project and we'll be providing it to our students in the course. So, I just wanted to know your process and how you arrived at your final typology. And how did you break up the work between your group? You had a group of six or seven. Basically, the idea is we were just thinking about what we can do that's nice or useful. And then, after we came up with a few ideas, we thought of something that would fit well together. I guess we started kind of brainstorming on what kind of cool things we could visualize. We first went to the D3 repository and saw various things. And, we saw a map and thought, well, you know, Twitter, we can perhaps correlate location based information. And, we wanted to have something really pretty to look at at the end. so, I guess that's kind of how we started and kind of, looked at what the tweets kind of, contained and see any emoticons, hashtags. In short sentences, so thinking what one can do with that information. We kind of took the original spell and we did some, we added some functionality to it. One thing that we did is we did a sentiment analyzer. We put a sentiment analyzer in the spell to give a sentiment for the tweet, original tweet. We also put in a few filters in the spells. So, one thing we did is we are only, since the map is only for the United States, so we added a filter only located in the United States region. And, we also have another filter. Since we're very interested in, you know, putting this on a map, we only get, we're only getting tweets that has a latitude and longitude information. So, you checked whether or not it existed and then made sure that it was in the region. And then, ran sentiment on the entire Tweet coming out of the spout. Yeah. Did you think of putting the sentiment analyzer in to its own bolt? Or, was it just more efficient or, was that something that or Chris, recommended? So, we originally did try that. And then, I think that to initialize the sentiment analyzer it requires I think a couple hundreds of megabytes of memory. And, since we're using a virtual cluster and we have, you know, maybe 10 instance of the bolts we don't have enough memory to actually do that. So, we put in the spout instead. I know it's kind of like a bottleneck now because sentiment analyzer, it takes time to give, score each, tweet. But, you know, given, given the constraint that's, that's where we end up. The parse tweet, bolt, we're doing the filtering on the skip words, so we took out all the pronouns, and, you know, articles that we are not interested in. So, we filter them out, and then send them to the next next spout. And, the next one next bolt is called info bolt and this is the bolt that does the emoticon analysis. So, this one takes in the tweet from the previous bolt and then it then it maps each emoticon it sees to cinnamon score so a happy face would be like, a large number. And then, a sad face would be a pretty small number. But, that's kind of the scale that we went with. Now, the next bolt is called top-words. So, starting from this bolt, we are looking at information that are only grouped by the county ID, county_id. So, all the, just the tweets that go to a single bolt, they have a sort of pretty consistent set of counties and, that way, we can aggregate the stats for them. And, in the top-words bolt, you did aggregation on the stats for, you know, the most frequent tweets, say we're also doing some manipulation with the sentiment score, where we combine the emoticon sentiments and the tweet sentiments. And, the report-bolt, that's the last bolt that we have. For this bolt, I did more processing on the final sentiment, you know, the moving average for the sentiment score, and also send the sentiment score to the D3 map. So the general idea is that there's two types of kind of emotions, one from emoticons and one from emojis. And so the emoticons would be like, you know, smiley face or stuff like that and there were five different classes that we had. One was, you know, like happy, medium happy, neutral, being sad and you know, unhappy and then the other ones were emojis and they were based on Unicode characters that I found online. You know there was this kind of a list of all the Twitter emojis. Yeah, I think this might be it here with the Unicode. Oh, no here's the list. Here's the happy list. So, as we see here, here's your happy list and the various Unicode characters associated with them. And so here's happy, mediumHappy, neutral, mediumUnhappy and unhappy, and these were just, you know, based from my own feelings of what constituted happy versus mediumHappy, which is a little arbitrary, I suppose. Great. How do you think the approach in general worked? I think it was okay. I think next time, perhaps we should get rid of these mediumHappy and mediumUnhappy, kind of category, because I think that added a little bit more to the noise. It was a little bit arbitrary in terms of well, is this happy or kind of medium happy? So it was a little more difficult. And then how do these map onto the colors for the D3? Oh yeah, so, this is, happy is the darkest yellow. This one's a light yellow. This one is the pale green. MediumUnhappy is a light blue, and unhappy is a dark blue. So, happy is yellow like a happy face? Yeah, exactly. So, happy is, you know, the yellow happy face, and unhappy is like I'm feeling blue. So, that's why it was blue. This map is taken from these three. We found that previous side has example of you know, there is these three visualizations and we already decide that you know, visualizing the sentiment across the United States is a very cool idea. So we took you know, a map that we can make some interactive display. This map is broken up by counties, and each county has a color and the color is a sentiment of those tweets that we got for that county. And also if you see, when you hover over the county you'll see the tweet message of the county. Other grey area are the places where we haven't seen tweet yet. And so yellow is very happy and dark blue is unhappy and so with the World Series going on right now we see the some yellow around San Fransisco. Okay, well, let's take a look at your code. And so and a lot of the logic here is basically you've modified the app.py that we gave you. You created a map.html, so how about we start with the app.yi. Okay. So Juan all ready got this. There are already a couple, you know, applications all ready filled out for us, you know, the basic and the stream. So what we did is we added a map.html and the way to send any data to it is by adding this app.route. And that's the only thing that'll add to the app.python. So it's pretty simple. Yeah, very simple. Mm-hm. So this is a map.html. And we originally took a template that already exists. And we- That's from the datamaps.github. Yeah. The datamap.github. Yup. That's the place. And this JavaScript file has the map or data. So the java script renders all the information in this file and create this map that you see on this display. And we added a JavaScript that physical just does a county lookup. It, it a reads the county id and then give us the id that corresponds to this map we see here. And I see it's in the static county, the static\countyLookup. There it is. Cool. We have a couple colors on this map and we're trying to really show, you know, the sentiment pretty continuous scale. Well, of course, we cannot put all the colors on there so chose ten levels. And those ten levels are, you know, kind of a gradient from blue to yellow. And that's, you know, how we created that, those color code. Over here we have the source.onmessage, and this is the part that takes the information, the Storm pass to Redis. So it reads the event data, and split it by the delimiter, and gets the county id, the sentence, the tweet and then a sentiment. So that's the three things that we decided to pass to the map. Of course there used to be a lot more fields but that's the thing that we ended up using. How about we skip really quickly to the report bolt just to make that connection. So we've basically, I see that you've, you've changed the bar type character that I was using. And provided the class. And you have created a delimeter. And so basically what we're doing is Redis.publish to the word count topology stream and you're publishing the count id, delimeter the tweet, the delimeter, and the sentiment with another delimeter. And So you've stuffed this into a large string. Right. And then parsed it out. Right. Very cool. Yeah. RIght, so, we had to up, because we had to update the map, the color in real time, so what we did is that we created a hash, kind of an associative array in JavaScript. And in here we update a counting id with the most frequent sentence that we pass in. And also the kind of running average sentence that we pass in. So every time there's something new, we pass into this hash structure. And then we have this window.setInterval function. This is, this is saying, let's update a map every second. This is millisecond unit. And we call this function updateViz. And in here we are looking at okay, let's look at all the keys in hashSentiment and we're going to update. All the sentiment that we received was in that one second interval and here we're seeing if the sentiment we get the color, put the color into this data of structure and the map will take this data and update it accordingly. So this, [INAUDIBLE] you know by, in the D3 package. So we didn't have to do much with that. I know it's very complicated. Yeah. And then we reset hashSentiment. Cool. So we can see it, you know, updating in real-time. I think this is super cool. Yeah. We're getting only 1% of tweets, so the map is not, you know, populating very quickly. Right. But if we have a lot more data, then we will be seeing a lot of, you know, blinking. Mm-hm. And. A lot more colors. Cool. All right, thanks a lot. So team Awesome thanks so much for walking us through your really cool hackathon project. And, I was wondering if you had any advice for students as they are walking through their own final project. I guess we benefited by working with other people. So it always helps if you, have friends that could help on the, you know, your project via, you know, first, we had some developers, and modelers, and just a wide array of people. So it he-,. helped that everyone had different skill sets, so. Also, just by the nature of Storm, VPI is actually not too complicated. You only have a couple of things you have the spout, the topology, the bolts, and then, you got your shuffle grouping, your fields grouping, and maybe plus a couple of other things but that's basically it. You have a bunch of little steps, that by themselves, are trivial, and then you just add ten of them together and you come up with something that's very nontrivial and actually looks pretty cool. So, in the end my advice is basically to just learn what all those ten commands do, and have some good design ideas of how to put them together. [LAUGH] Karthi, thanks so much for your time introducing our students to Apache Storm. I had a great experience, and I really hope our students learned a lot. I had a great time as well. Thanks to for putting together this course. Follow us on Twitter.