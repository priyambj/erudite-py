Hello, and welcome to this unit on file systems. We'll talk about what a file system is, about how application developers interact with it, and then see what an OS is doing behind the scenes and discuss some common optimizations that make file systems faster. I hope you enjoy. Most any time that you use a computer, you're going to interact with a file system. Here you see me navigating the file system on my computer to find this presentation. There it is. I started off at my Home folder, inside of there I went to Pictures. And then the filesystems, and there I found this image that I was looking for. This is really convenient. In my case, the data I was looking for was on the hard disk of my local machine. But I didn't have to remember physically where the data was, i.e., the platter, the track, the sector number, et cetera. Instead, I was able to access the data through a very intuitive interface provided by a file system module of the operating system. There are at least three key abstractions at work here. The first is the notion of the file itself. In reality, the data may not be placed next to each other on a disk orl in the right order. But an application developer, the author of the sketchbook program I'm using for instance, gets to create the data of a file as one big, long contiguous string of bytes. Second is the filename. I don't have to remember any of the physical information about how my file is stored, the sector number where it starts, for instance. Instead, I get to assign my file an intuitive name, in my case, Image001.tif. Third is the notion of folders, or directories, as we will call them. These are containers for other files or directories that help keep things organized. The idea is that we have an electronic version of what was once the more common physical filing system. We put folders withing folders to help us keep things organized. So in this machine then, I don't just have a pictures folder but also a documents folder. And within there I have a folder for advanced operating systems. And maybe there's another one for software development process, etc. And even this home directory sits inside a larger directory for all users, which then sits inside of a directory called root. Which then contains folders for the operating systems and other things. This directory is called the root because if we think of this directory structure as an upside down tree, then this all encompassing directory would be the root or the base of the tree. Indeed, the structure will always be a tree, just by the nature of directories or the nature of physical file folders for that matter. We call the directory immediately above another directory its parent. File systems, for instancce, is the parent directory of image001.tif. Pictures is the parent director of File Systems, et cetera. Together, these three abstractions provide a convenient and almost universal interface for storing and retrieving data from mass storage. In this lecture, we'll talk about how this interface is implemented and some of the common optimizations. We'll focus on the disk medium, but many of the ideas we discuss apply to other media as well. As soon as one thinks about letting multiple users access a file system, it becomes clear that we will need some notion of access rights. Looking at our directory tree here again, it should be clear that Alice should not be able to see guest's or Bob's file unless they say it's okay. And we certainly don't want to let just any user access the operating systems files. Ultimately, for each user file pair, we need to know what permissions that user has. That is, which of the basic actions of reading, writing, and executing the file is he allowed to do? Conceptually, we can think of this as a giant matrix with users for the columns and files as the rows, for instance. And we could store the information that way, but it would be pretty inefficient. Think about how many files would have the exact same set of values. UNIX like operating systems, achieve efficiency by assigning an owner and a group to each file. Here I'm displaying this information as the OS command would show in the terminal. The owner, or user of the file, is typically the creator of the file, and the group might be faculty, or staff, or students or something like that in a university setting. We have separate read, write and execute bits for the owner or user, the group, and then another set for everybody else. This gives us a total of nine bits instead of 3 times the number of users on the system. Permissions on directories make the system even more complicated. The basic rule is that reading affects your ability to see what's in the directory. Running affects your ability to create, delete and rename files in the directory. Both of these are intuitive if you think about a directory as a file itself, containing the names of the files and subject directories in it. Execute permissions are a little more confusing, as they control whether you can pass through the directory and access it's contents in ways other than just manipulating the file names. For cases where more detailed permissions are needed, access control lists are sometimes used on various operating systems. These make it possible to create arbitrary row values in our permissions matrix by granting permissions to individual users or other groups defined in the file system. I encourage you to explore some more on your own. Some good places to start are provided in the instructor notes. Let's suppose that when I list the current directory, I see the following - a directory dir, owned by me with these permissions and a file foo.txt owned by some other user with these permissions. And if I look inside this dir. director, I see that there's a file bar, owned by me, and having these permissions. Notice that I used the super user privileges to list the files in this directory. Access to this metadata requires passing through the directory in a way that is only allowed if we have execute permissions on. So my question to you is, which of the follwing Unix commands will result in a permissions error. Cat I should say is just reading the file and printing it to a standard app. And touch is trying to create a new, empty file. Check all that apply. The answer is these last two. We'll go through these one by one. In the first case, the owner of the file is another user, and I'm not in the group either, so I fall into the other category. The other category has read permissions, so I can read the file, and the command succeeds. Now let's see what happens when I try to read dir/bar.txt. I am the owner of bar, and the owner has read permissions, but to get there I have to pass through the directory dir, and since I don't have execute permissions on dir, this generates an error. The same thing happens for touch. Writing to the directory file is okay, but actually creating a new file in the directory requires the ability to pass through. It is interesting to note what would have happened if I had reversed the permissions on dir. Then cat dir/bar.txt would have worked, since I just need to pass through the directory. The touch command would not have worked however, as I would need to add the new file name to the directory, and I don't have write permissions. To have a good understanding of a file systems, we need to know how application developers interact with it. There are two main ways of doing this. One way, keeps track of a position or cursor in the file, and as we read or write to the file, we move the cursor around. The other way allows us to treat a file, as though just a block of memory. Let's cover the position-based strategy first. We begin by opening the file, with the open procedure, specifying how we intend to use it, with some flags. Notice that I'm using the lower-level system calls, rather than C's built-in procedures like fopen, which sometimes use an extra layer of buffering. It's not that the extra layer's a bad idea, I just want to interact more directly with the operating system. Having opened the file, we can then call read or write as we wish. Write, I should say, overwrites instead of inserting, as the cursor does by default in most editors. And as we read and write, we move the cursor through the file. If we want to move the cursor without reading or writing, we use the lseek command. Which repositions the cursor in the file. And finally when we're done with the file we should close it, so that the file system knows we're done. Now I've skipped over most of the details here, because I thought it would be a lot more fun for you to learn them on your own, with a little programming exercise. Here's the scenario. We've obtained access to a file belonging to some enemy, and we want to do some sabotage. The files stores a table of key value pairs, both of them ints, and the whole file is in binary. It just goes key, value, key, value, key, value, all the way through. Your task is to write a program that will change the value associated with a particular key. So the perimeters to your program will be file name, the key who's value we want to change, and the new value we want to associate with that key. Now our enemies engineers weren't very clever. So the keys in the file are in no particular order. You just have to search there. Consult the man pages for open, read, write, Lseek, and close. And Good luck. Here's my solution. I begin by opening the file, with read and write permissions, and then I read from the file. Remember, it goes key value, key value. So, this first read, will represent a key. I check to make sure that I read in as much as I intended, and assuming that that goes okay, I enter this loop here. I first check the key to see if it's the value that I'm looking for. If it's not, then I simply move the cursor past the associated value and then read in the next key. If it does match, however, then I want to overwrite the value with the new one, close the file descriptor, and exit with success. If I ever reach the end of the file or if a read fails for any reason, I can simply print that the key is not found and then exit with failure. So far, we've seen how a developer might interact with the file system to the positioned based or cursor based aisle. It's also possible, however, to interact with a file in a way that lets us treat it just like memory. As before, we begin by opening a file with the open procedure. Instead of using read and write on the file, however, we call mmap. This returns a pointer to a region of memory that we can manipulate in our code. And eventually, whatever changes we make to the memory, get reflected in the file. When we're done with the file, we call munmap, which syncs the contents of memory with the file and then frees up the memory. And lastly, we can close the file as before. Again, I'm skipping over most of the details here so that you can learn them through a little programming exercise. The scenario here is that you're given a binary file, with arbitrary contents. And the goal, is to break it up into pieces of some fixed size, and then shuffle those pieces around, in some random order. So the parameters will just be the file name and the size of the pieces. The code to do the shuffling in memory is provided. You just need to open, close, map and unmap the file in order to use it. Consult the bin pages and good luck. Here's my solution to the shuffle program. I begin by opening the file with read and write permissions, and then to figure out how long the file is, I call lseek, putting the cursor at the end. This then will return how much the cursor was moved, which I store in this variable len. Then I put the cursor back to the beginning. Knowing how long the array needs to be, I can now end that with these flags here. And then check to make sure that, that succeeded. If it failed, we'll just exit. So now that I have the contents of the file in memory, I can call memshuffle on it, that'll rearrange it as desired. And then I can unmap it, close it, it'll get written to the disk, and I can exit with success. Next, we will discuss strategies for allocating space on mass storage devices to a file. Just like caches work in terms of units of cache lines or cache blocks and virtual memory works in terms of pages, file systems work in terms of blocks also sometimes called clusters. In the case of a disk, a block address would need to identify the physical location. There are different vocabularies here. We'll refer to platter, track and most specifically, sector. The block address identifies the starting sector and the block itself might cover several more sectors, all blocks are of the same size. There are several strategies for keeping track of which blocks are free and which are used, one easy way is to keep a list of free blocks. Another is to use a bit vector where each bit indicates whether a block is free or not. There are more complex strategies as well. Which is more appropriate will depend on the strategies for assigning new blocks to a file as it grows. We won't go into those details here. Instead, we will focus on how a file system keeps track of which blocks belong to which files and of the ordering of these blocks within the file. Ideally, we would want a file system that allows for simple and fast creation and deletion of files. The simplicity here will help maintain consistency in the face of crashes. We want flexibility on size, that is, we ant to be able to handle file sizes large as well as small, and of course we want an efficient use of disc space, and we want fast access. Sequential refers to reading a file in sequence, from start to finish, and random access refers to jumping around in a file. Specifically, we'll look at the file allocation table strategy, which was widely used in the DOS era, and is still used on many USB devices. And we'll also look at the extended file format, which is the long time Linux standard. The File Allocation Table format was originally used in the late 1970's on floppy disks, and became the standard in DOS and early Windows machines. It is still commonly used on solid state and flash memory devices like this one. As these storage technologies have evolved, it has gone through many iterations, but the two key ideas have remained the same. The first is that each filed is represented as a linked list of blocks. Normally when we think of a link list we think of the links as being part of the nodes in the list. May the last part of the block would give the ID of the next block. Here however the links in this list are represented in the final allocation table, which is indexed by block number. Given the starting block number of the file, I can find the next block with a constant time access to the file allocation table. A special value, negative one in this example, indicates that a block is the last in the chain. This file allocation table also contains a bit to say whether a block is free or not. In this example, our file starts with block two. Looking at the table, I see that the next block is six. Looking at six in the table, I see that the next block is three. Again going back to the table, I see that the next block is five. And when I look at five in the table, I see that its next has the special value indicating that we've reached the end of the file. It's important to realize that the file allocation table, or FAT for short, stores all the links for all the files. For instance, there appears to be another file occupying block four and only block four, since that's the end. And also one occupying block one. So the file allocation table tells us how to glue the blocks of a file together. But it doesn't tell us where to start these chains of blocks. That is the function of the directory tables. Directories as I should say are indeed treated as files. And they use something called a Directory Table Format. Entries in the table all have the same width and contain the essential information, including the file name, the starting block, and any other metadata associated with the file, such as permissions. The root file directory has a fixed address on the disk. So we always know how to get started. Let's say that I wanted the content of a file / foo / file.txt. I would first consult this root directory table. It would tell me that there is indeed a directory named foo, and give me the starting block of that file. I would then access that file on the disc and since it's a directory, I would look through there for the file name .txt. And it would tell me that the starting block is two. I can then chain together the blocks on disk as we did earlier using the file allocation table. To summarize then, the FAT or file allocation table serves as the glue that chains the blocks of a file together. The directory files capture the hierarchy and the starting blocks for the files and all the metadata. Here's a question to help solidify our understanding of the file allocation table. Given the values in the table, which blocks are the starting blocks of some file? You should assume that this is the whole table. Check all that apply. The answer is that the starting blocks are the busy blocks that don't have any other blocks pointing to them. Right away then, we can eliminate the blocks that aren't busy. And then I'll just read these next pointers, and eliminate those blocks as well. So this eliminates 6. This is an ending block, so he's not eliminated. This eliminates 1. And this eliminates 2. So, we are left with 3, 5 and 1 as starting blocks. Just for good measure, let's go ahead and build the chains as well. We'll start with 3 and he then would seem to point to 1 who then points to 6, when he then is done. 5 just sits there all by himself, and 7, would seem to point to 2, who then is the last one. So there are the 3 files captured by this fact. So now that we understand how the FAT format works, let's consider its strengths and weaknesses. It is easy to create a new file, so we just need to adjust the directory table of the parent directory and set the busy bit in the FAT for the starting block. Growing a file is as easy as adjusting the next field of the FAT. Space usage is efficient too. If a block is free, then it can be used and it doesn't matter whether the adjacent blocks are being used or not. This means that the system won't suffer from what is called external fragmentation. As for access, it will behave much like a linked list. It will be great for sequential access, as we quickly find and follow the links. But poor for random access, since we have to walk through half the links to get to the middle of the file and all of the links to get to the end. This isn't as bad as it might seem at first, because we keep the FAT in memory. But it's still not ideal. Overall then, FAT is a poor choice for situations where random access to large files is necessary. Hence, it has fallen out of favor. It's still a good choice for removable storage, however, where one is mostly just copying data from one machine to another, and hence, only using sequential access. And its space efficiency makes it especially attractive. Another popular system file format is the extended format, commonly used in Linux. Each file on the disk has a data structure called an Inode associated with it. The Inode is fixed length. It contains the metadata for the file, and it serves as the glue linking the data blocks together in the right order. It is this gluing function that is the most interesting. The inode stores 15 data block addresses or pointers. The first 12 of these point directly to the first 12 data blocks of the file. This makes the strategy efficient for small files. The thirteenth of these addresses points to a block that consists of a table of addresses for the next blocks in the file. This vastly increases the number of blocks that we can use in a file. The downside is that we've introduced a layer of indirection. If this doesn't give the file enough space, then we use this fourteenth pointer and not one but two layers of indirection, giving us even more space. And if this isn't enough, we have a fifteenth pointer, which uses triple indirection for more space still. Juts like in FAT, directories are treated as files. Only instead of mapping a file name to the first file block, they map a file name to its inode. That path for accessing slash foo, slash file.txt, would look something like this. We'd start at the inode for the root directory and then following its data pointers we find the data for the root directory. There we find that foo maps to another inode, following that address we consult that inode and find that address for its data block. Looking in there, we find the contents of slash foo. That's a directory. So it's going to map the file name file.txt to the appropriate inode. And then using that inode structure, we're able to peace together the data that we need. Now for a question to test your understanding of inodes. Suppose that the disk blocks are of size 4 kilobytes, and that an indirection block can store 1024 entries. What is the maximum file size that does not use the double-indirect pointer in the inode? Give your answer in kilobytes. Having understood inodes and the basic characteristics of the extended file system, let's now consider its merits and compare it with FAT system. File creating means grabbing an inode and updating the parent directory. This is not substantially different from FAT. File growth, just means grabbing a new data block and updating the inode, no big deal. We are still efficient in terms of space. There's a little waste in potentially unused fields in the inodes, and the indirection tables are space that would not be used if the file were stored in a FAT system, but this is usually going to be small compared to the whole size of a file. As for access time, it is true that the inodes add an extra layer of indirection as we go through the directory tree. A directory points to the inode for a file, rather than to the first state of the block. Because inodes are cached in memory, however, this performance cost is minimal. Much more important is that the inodes link the data blocks together in a treelike structure, with a high branching factor, as many as an indirection table can hold. This makes for much better random access than one gets from the linked list chaining of the FAT format, at least for large files. Now that we've seen how file systems work, we turn to several optimizations that help make it fast, since most storage devices tend to be slow. We'll first discuss some software optimizations and then turn to hardware. If you're first reaction is, let's use a cache, then you're on the right track. Indeed most operating systems do use free portions of main memory as a cache for the much slower mass storage device. Statistics vary on this sort of thing over time as technologies changes, but by most measurements, memory is as much as 100,000 times faster than disk for random access. Would use the portion of memory used as a cache for disc, the unified buffer cache, a name it earned for some obscure historical reasons. The terminology here can be a little confusing. So I'm going to emphasize that we're talking about in memory caching of the contents of the disc, not the RAM on the disc controller that the device controller here might use. When data is read from disc, is stored in this unified buffer cache so that subsequent reads can find it there. Instead of having to bother the disk again, because disk access is often sequential. It is common also to read ahead in a file, loading up subsequent blocks to main memory so that is there when the application needs it. Writing to disk is usually done with the write back policy. The change is made only in the unified buffer cache at first. And the page is marked as dirty. The slower operation of writing the data to the disk is postponed until some more opportune time. New files can also be created and stored in the unified buffer cache. If a file has a very short life span, writing to the disk might not be necessary at all. The existence of the end memory cache. Means that a call to write isn't a guarantee that the data is changed on the disk and will persist. The changes are only reflected in memory. The advantage is that the program gets to resume faster and get on with its work. The downside is that if the system crashes before the write has been made to disk then changes will be lost. If you really do want to make sure that changes are reflected on disk, need to call fsync or msync to flush out the unified buffer cache. The write-back policy of the buffer cache is also the reason that operating systems will warn you not to move storage devices without ejecting it first. It needs a chance to flush the buffer cache and any dirty pages so that changes will be reflected on the device. There are some dangers that come with the write-back policy of the buffer cache, a sudden system rash or power failure would mean that all the changes made in memory only would be lost. If the storage device is removal, it could just be that the user unplugged it before ejecting. It would be a nice idea if we could periodically copy all the changes reflected in memory to the disk. Maybe we could do this when some fixed number of dirty blocks is reached. The trouble is that the dirty blocks will likely be scattered all over the disks and we have to spend an inordinate amount of seek time just putting the right head of the disk in their correct place. Indeed it's the seek time, that makes random access to a disk so slow in general. Sequetial access can be thousands of times faster than random access. It'd be so much easier if we could just write everything to one contiguous block. This is the idea behind journalling file systems. They reserve a contiguous portion of the disk for this express purpose. We write the changes to the journal quickly, and mark the dirty box in memory as clean. Then, at a more opportune time, we apply all these changes stored in the journal to the appropriate files on the disk. The name journal comes from the analogy to a diary that one might write in every day to record the changes in one's life, in time sequential order. Here we are recording the changes to the file system. Of course, this does complicate reading from the disk somewhat. Any time we read from the disk, we need to check the journal to see if there are any changes to apply. Total time spent on writing is slower. After all, we are doing two writes, instead of just one. The key is, that the write to the journal is faster, and that is the one we're doing at the critical time when we found that we found we had too many dirty pages, or blocks, in memory. Journalling also has the advantage that it helps with crashes. Problems typically occur when the right to summon important data structure, like an inode, is interrupted by some system crash or power failure. Let's consider what happens, when a write from the journal to the rest of the disk is interrupted first. Well, in that case, the change that we need to apply will still be in a journal and so we can just reapply it on restart. No problem. The other write to worry about is the write to the journal itself. A failed write to the journal, however, can be detected by a checksum. And then the change can simply be ignored, again, leaving the important data structure on disk in a consistent state. There's one last optimization that I want to mention, and this is a hardware one. One might expect that the CPU had to execute all of the store instructions, needed to move data along the bus from memory to disk, and vice versa. This would work, but it would be very slow. Instead, streaming devices, like a disk, have their own controller that is capable of sending along the bus itself. Through the bus, the CPU tells it the length of the chunk it's supposed to copy, the device address on the disk, the memory address, the command, whether it's a read or a write, and then tells it to go. The CPU then goes on about its business while the device controller uses the memory bus to either read or write the data. Now the CPU and the device controller are in competition for the use of the bus here. But because the CPU is going to find most of its data in the cache, it's not using the memory bus all that often, leaving it for the device controller. This phenomenon is sometimes called cycle stealing as the device controller is able to steal cycles on the memory bus away from the CPU. Now let's do a question, to help summarize the optimizations we just talked about. Check the box if you think the optimization given, has the merit provided in the column. Most of these should be pretty clear. The tricky one is whether journaling helps improve overall performance. And it really depends on what one considers the alternative to be. If the alternative is leaving changes in memory for longer. Then no, it doesn't improve performance. But if the alternative is spending lots of seek time trying to backup in memory changes to the disk, then obviously it does help. Let's assume the latter, so that this one is true. Let's first consider which of these optimizations reduces the number of writes to disk. The use of the buffer cache does because short lived files, which are common in when compiling code, for instance, need never be written to disk. Journaling actually increases the number of writes to disk. So this is false. And direct memory access only gets involved when there is a write, so it doesn't change anything. Next we ask, which of these improve overall performance. And the answer is that they all do. The basic caching principle was the main motivation for the buffer cache since main memory is so much faster than disk and other mass storage. The advantage of journaling, we already went over. If the data must be written to disk quickly, better to do it sequentially. And lastly, we saw how DMA sped up performance through the cycle stealing phenomenon, giving more time for the CPU to do other work. Lastly, we have the improvement for recovery. And the only optimization that had any effect here was the journal. Which reduced the recovery problem to just applying the changes from the journal to the rest of the disk, something done routinely anyway. We've covered a lot in this lesson, from a high level view of why a file system is useful, down to implementation details and optimizations used in today's sophisticated systems. If you keep these ideas in mind, you've become a better programmer, and appreciate the complexity behind the systems we use every day. So long for now. Hello, and welcome to this review course, which is a prep for my graduate course on operating systems. When I teach graduate students on campus at Georgia Tech, I often find that there's a gap between the knowledge they have of operating systems at the start of the course and what they really should know in order to get the full benefit of my graduate class. This is not being judgmental. But it is just that, not all undergraduate operating systems courses are the same. And you may have taken a course on undergraduate operating systems a long time back. Or maybe you were not that enthusiastic about operating systems at the time that you took the course. This review course is not meant to be a substitute for an undergraduate education in systems. Rather, it cover the topics many students have expressed that they wished they had it before starting to take my class. Helping me with this review course is Charles Brubaker, a PhD graduate from Georgia Tech, now turned into a course developer for Udacity. Hi there. With this course, I'll try to give you concise explanations that will quickly put you into a better position. To fully understand the vectors and complete the projects of the advanced operating systems class. With these goals, let's get started. The most important resources in a computer are space and time. Space in memory for storage, and time on central processing unit for computation. We may tend to think about these things separately, but there are important interactions between the two. And just as understanding the relationship between space and time is important for any good physicist. Understanding the relationship between efficient use of memory and then the CPU, is important for any good program. This interaction will be the subject of this lesson, as we look at cache systems and that all important abstraction of virtual memory. In your first programming class, computer memory may have been explained to you with a picture that looks something like this. We have the CPU, where your instructions get executed, and these instructions may read or write to either main Memory, or to Disk. Main memory is Fast, and you can access any element in the same amount of time. That's what we mean by Random Access. And it is Temporary in the sense that when the program exits or if power is lost, all the information stored in it disappears. The hard disk, by contrast, is slow. It uses Sequential Access since you have to actually spin the disk to the right place. And what you write to the disk is permanent. It will still be there when the program exits or if power is lost. In this lecture, we're going to refine this picture in two ways. First, we'll look at Cache Systems, which allow the CPU to access the same information that is in main memory, only faster. Understanding how cache systems work will help you optimize your code when you need to. Second, we will explain the Virtual Memory system. As you write your application and compiler compiles it to generate addresses for all your variables and your code, you don't have to worry about other programs and what memory addresses they might be using. In fact there's often an illusion that having even more main memory then exists in the system, and that your application has them all to yourself. W'll see how the operating system maintains this abstraction to the virtual memory system. The details are essential to understanding operating systems optimizations. And how the need to share resources affects program performance. To help us see the need for caches, we'll examine this little snippet of code. From an application programmer's perspective, a statement like this one, the sum plus equals a of i, would seem to happen in one step. Even in assembly, it would only take a few instructions. In terms of time on the clock, however, not all of these instructions are the same. Sum, being a local variable, is most likely stored in a register in the CPU, therefore there is almost no cost to retrieving its value. a of i, however, is in main memory, and retrieving its value can take 100 CPU cycles. And even then, the bus bandwidth might only be able to pass back two bytes a cycle or thereabouts, to get the data back. Trips to main memory are expensive, and this trend has only become worse over the past few decades with hardware advances. The solution to this problem has been to place a smaller, faster set of memory made with faster technology, closer to the CPU. We call this memory a cache. And here, we're looking at a latency of about ten cycles. So, when the CPU wants the contents of some memory address, it looks in the cache first. If it finds the data it needs, great, we've just achieved a factor of ten speedups. We call this a cache hit. If the data it wants is not in the cache then we have to go to main memory. We call this a cache miss. The key is keeping these cache meshes infrequent, so that on average, time spent accessing is more like the time it takes to access the cache, than it is like the time to access main memory. To illustrate the power of the cache, as well as the importance of high hit rates. We'll do a few calculations in the form of a question. Suppose that retrieving data costs 4 cycles on a cache hit, but 100 cycles on a cache miss. Then how many cycles does it process with a 99% hit rate, spend on average? And then we want to compare that to one with only a 90% hit rate. For the first calculation, we have that 99% of the time it only costs four cycles, and 1% of the time it costs 100 cycles. For a total of 4.96. And for the second, we have that 90% of the time, it costs four cycles, and 10% it cost 100 cycles. Giving us a total of 13.6. This may be a little bit counterintuitive to see that a relatively small change in the percentage can have such a dramatic change in the speed. And the key is to think about the fact that it's actually really the miss rate that matters here. We're improving it by a factor of ten, with having a 99% hit rate. Probably the most important consideration in cache design is the trade off between speed and capacity. To help understand the impact of this trade off, I'd like to visualize requests from the CPU to Main Memory like this, where the orange lines represent the requests to the CPU. We would like the cache hit rate to be high and the speed to be fast. These goals, however, are somewhat contradictory. We can have a small but fast cache and just accept that this would have a low hit rate, being only able to intercept quickly a few of the requests from the CPU. Or we could have a large cache, but this would be slow there. Either because the physical limitations like the speed of light, or because of the high cost of fast hardware. So we intercept more requests from the CPU. But, we aren't able to return them as quickly. What is the right trade off? Well, it's hard to say. But, fortunately we don't have to make just one choice. We can trade off side versus speed multiple times at various levels within something called the cache hierarchy. What is not in the CPU registers, we look for in an L1 cache. What's not in an L1 cache, we look in the L2 cache. A multicore processors, there might be even a third level of cache. What's not there, finally we look for in main memory. Even main memory, however, we can think of as a cache for what is on disk. Or we can also think about maybe main memory as a cache for what's on the file system. Notice here that the top is smaller, faster and costlier per byte. And as we go down the hierarchy, we get things that are bigger, slower and cheaper per byte. And if we do a good job of keeping the data that we're going to access next in the higher levels of the hierarchy. That is, if we can keep our hit rates high, then making at the speed of the top part of the pyramid that has the capacity in the overall cost per byte of the lower end of the pyramid. As we've seen, in order for a cache to be effective the hit rate must be high. That is, we need to be able to anticipate what data the process will need next. Ideally, we would be able to look ahead in the code and plan ahead, but so far this has proved impractical. Instead ,we used the heuristic of locality. That is ,we assume that if an application accesses one memory address, then it will need nearby addresses soon. Sort of like, how a psychologist says that the best predictor of future behavior is past behavior. Experts like to distinguish ,two types of locality. Temporal and spacial. Temporal refers to the tendency to refer to the same memory close and time. The spacial, refers to the tendency to access memory that is close together in terms of address. Typical cache policies exploit both kinds. It exploits temporal locality by putting the data in the cache right after you use it, thinking that you're likely to use it again soon. It exploits spatial by putting not just the memory address that you access, but putting the whole block into the cache. Blocks, I should explain, are a division of memory by relatively small amounts, often 64 bytes. This one block is from zero to 64. Another from 64 to 128, etcetera. If we access the memory location, 87. Then we reason that we are likely to use addresses close to this one. And so ,we put the whole block, from 64 to 128, in the cache. Now, you may ask yourself, why do we put the block of memory into the cache instead of something more consistent? Say, maybe, the previous 32 bytes, and the next 31 bytes, or something like that. After all with blocks, sometimes most of the data we pull in will be higher in the address space, and sometimes most of it will be lower. Well the answer is that, it makes it much easier to figure out what is in the cache and where. The size of the cache block is always a power of two, so we can read which cache block we belong to just by looking at the higher-order bits. The lower-order bits are called the offset And tell us where within a given cache block the data we want is. Next we turn to the problem of mapping. That is given an address, how do I find out whether the data is in the cache, and how do I retrieve it? We'll start with a direct mapped cache and a little notation. Let's suppose that we have two to the m addresses that our cache block or cache line is two to the n bytes. And that our cache has space for two decay entries. All of these are always the power of two. With these definitions, we'll call the lowest n bits the offset. They tell us where within a given cache block, the memory we want, is. And the rest of the bits, tell us the block number. But how do we know where to look for this block in the cache? We need some kind of hashing function. Well, the easiest hashing function just looks at the lower order bits and that is exactly what a direct mapping does. The next k bit's the address tells the right index to look for our data in, in the cache. Of course, this means that multiple addresses will get hashed to the same location within the cache. Given that our cache had to be small, this was inevitable. But we need a way to distinguish them. The solution is to use the tag, or the higher order bit, to the address as a kind of confirmation. Let's do a quick example. Suppose that we have an m bit address space. And our cache block is two bytes long, so we just have one bit for the offset, and that there are four entries in our cache. So we want two bits for our index. Then a direct mapping would look something like this, where all the blue addresses would get mapped to the blue block of the cache. The green addresses would get mapped to the green, and so forth. We can tell which of the color blocks is actually in the cache by consulting the tag associated with the data in the cache. In this example, the tag for the blue is zero, because it's the 0th blue block that's in the cache. Similarly, the tag for the green block is one. Because it's the oneth green block that's in the cache and the same principle holds for all the other colors, as well. Okay. So let's do, quick exercise here. Suppose a 5 bit address space and the cache contents, given in this table here. Fill in this table, for each address, indicate whether it's a hit or miss in the cache, and if it's hit, provide the value, provided by the cache. Okay, here are the answers that I came up with, let's go over this together. As in our example we have one bit for the offset, and then the next two are the index. So this here, I'm going to look at these two bits, see index 00, and then go over and look at the cache. I see there that the tag is 0 1, and oh that does not match my 1 0 tag of the part of this address here, so that's a miss. For the next one again I begin by consulting the index, that's 11, so I go over here, and I find that the tags part of my address and of the cache match. So that looks like a hit. Which of these two's values is it? Well, for that I'd look at the offset, and I'd see that it's the second one. So I write the value 99. For the third one, again, the index is here, 1 0. I go and look at that in my table. I see I have a tag,1 0, and oh, that matches the tag part of this address. So it's another hit. Which of these two values is it? Well I go look at the offset, that's zero, so it's this first one, 5 7. And lastly, again we'll start by looking at the index That's one one, and up, the tags do not match, so that's a miss. Okay, here's another question. Suppose a 22 bit address space, a 512 byte cache, and a cache block size of 64 bytes. And I want you to tell me how many bits are used for the index, and how many bits are used for the tag. Alright? Here are my calculations. I saw that we had 512 byte cache and the block size is 64 bytes. So, how many entries could there be? Well, we just do the division. We find there are eight. So, that's 2 to the 3rd, which means that I only need three bits worth of index. And then to calculate how many bits are used for the tag. I start out with all 22 bits. I remember that three are used for the index, and since we have 64 byte cache lines, I say that there are six used for the offset, and that leaves me with 13 left over. It's possible to get unlucky using direct mapped caches, in that different blocks that are important to your application might get mapped to the same index in the cache. For instance, if an application happens to access two cache lines that have the same index, maybe this one and this one here, then they will constantly be evicting each other from the cache. And it, it will be essentially useless. This, of course, is rather silly given that there are all these other entries in the cache that aren't getting used at all. The fundamental problem is that the address, any one of these here, is only associated with one location in the cache. We can mitigate these effects by associating an address with more than one block in the cache. One strategy is to treat all the red lines as blue ones, and all of the orange lines as green ones. If we do this then we'll have twice as many places in the cache to store a blue line of memory. Actually, it's more convenient to think about the blue lines in the cache being together. And in this way, we can say that we have simply decided to ignore the most significant bit of our previous index, but to allow a cache block to now be stored in two possible places. Hence, this would be called a two way associative cache. The downside is that we have to check two tags now to see if we have a cache hit. But we are less likely run into a problem, where we are constantly evicting the memory we need from the cache. If we decide this isn't good enough, we can halve the number of indices again to create a four way associative cache. Returning to our general notation, we now say that we have 2 to the j associativity. This means that we only have two to the k minus j indices into our cache, and the index is really only k minus j bits long now. This greater associativity also gives the cache some choice about replacement. Whereas with the direct mount cache, there was only one block in the cache where the data could be stored. We now have a set of possible locations. Just two in our example but maybe more. Typically, we choose to evict the entry that was least recently used. Now let's see a two-way set associative cache in action. We will assume a 5-bit address space with a 2 byte cache length. Notice that I've added a valid bit to our cache here, so that we can tell whether data has ever been loaded up to a line in the cache. Uninitialized caches are called coal. Let's say that 0 is the first address. In most systems that's not allowed, but it's fine for our example. The second bit here tells us the index for the cache. So we'll need to put this in the 0 set somewhere. We put it in here, flipping the recently used item to say 1, because that would be our preferred place to write the next cache line with index 0. Maybe after this, I have an address that looks like this one. The index here is 1, so we want to put it in this group, and again I'll put it in the first line, setting the valid bit, writing the data, and marking the least recently used bit. Our next address has index 0, so it gets written there. Notice that I chose this slot because the LRU had been one, and I flipped it back to zero because this is now the least recently used item in this part of the cache. Next we have an address with index 1. We checked the tag and we see that it's a hit. Oh my goodness. Notice that this address is a hit, even though we didn't read the exact same address before. But we did read an address belonging to the same cache block. Next, we have another address with index 0. So, he comes in here and evicting one of the previous entries. This one, because of the LRU bit. Actually, let's make the next address a question. Is this address a hit or a miss? The answer is that it's a miss. We find the index one, we're look in here and up, the tags don't match. It does match this tag up here, but that has the wrong index. Now we follow the idea of associativety to the extreme and let any cache block be put anywhere in the cache. Returning to our example, if this was a direct map cache, where every block has only one place to go. And this was a set associative cache, where there was a small set of places that a block can go. Then this is a fully associative cache where all cache blocks are treated alike and can be put anywhere in the cache. Looking at a more general notation, then we see that full associativity means we don't interpret any of the address as index anymore. It's all tag. This means that when we ask if a particular address is stored in the cache. The hardware has to check tags for every cache entry. Or at least, do it sequentially really, really fast. This fact, generally limits fully associative caches to sizes between 64 and 256 entries So far we have focused mainly on reading data. But what happens when we want to write to memory? Here we'll consider a single core system. I'll refer you to an architecture class for a shared memory system where keeping a cache consistent becomes a challenge. We need to consider both what happens on a hit when the block we want to write to is already in the cache. And what happens on a miss, when it isn't. On a hit, the most obvious thing to do is to write-through to memory. That is, we'll write to the cache, and then also write to the main memory to copy the data, and keep the cache and main memory consistent. As long as I don't have to worry about some other entity needing an updated version from memory however, there's actually no need to write everything this instant. This is the insight behind the write-back policy which only writes to the cache. After all, that's where we'll look first if the same processor tries to read the data or write to it later. Only when the cache block gets evicted sometime later does the data really need to be written to main memory. And then it'll be the whole cache block. This lazy approach can be beneficial when the same cache block is written to many times before [INAUDIBLE]. For cache misses we also have two strategies. The first is write-allocate, which first reads the current value in memory into the cache. Then it behaves as though it were a hit, using either of the two hit strategies. Lastly we have the no-write allocate policy, where we just write the data to memory and don't bother with the cache at all. The interactions among these policies can be subtle, particularly when there are multiple cache levels. It is worth pointing out however, that some of these policies share certain assumptions, and are therefore often paired together. The write back policy is betting that there will be subsequent writes to the same cache block. That's why he is bundling up all the changes he is making for one trip to memory, when the block gets evicted. Write allocate is also betting on subsequent writes to the same block. That's why he loads the data up into the cache. On the other hand, write-through is betting on the cache block being evicted sooner rather than later. He does his writes to memory up front, so that he can quickly evict the cap block from the cache later if he has to, not having to worry about any writes to the disk. Similarly, the no-write allocate policy is also betting on that same cached block not being used again soon. It doesn't even bring the data into the cache. The chief goal of a virtual memory system is to provide a process with the abstraction that it has an address space all to itself, typically organized like this. We have addresses for the program code. We have addresses for variables that are initialized, literals, constants, things like that. Space for uninitialized global variables that we expect to change over the course of the program. Space for the heap which is dynamically allocated and might grow or shrink over the course of the program and space for the user stack the procedures, local variables, all that sort of thing. There are also addresses reserved for the kernel, we'll see how this improves efficiency later in the lesson. The advantage of this abstration are best seen from the compiler's perspective. The compiler can choose an address for a local variable or code for the body of a procedure without having to worry about what other processes might be running on the computer at the same time as the application is compiling. And what physical memory those applications might be using. He gets to pretend that the application and the OS will have the computer entirely to themselves. The key to maintaining the virtual memory abstraction is a layer of indirection. Without this layer of indirection, two processes couldn't use the same address for a variable, because then they would end up overwriting each other. This would make life very difficult for programmers and compilers. So, we play one of the classic computer science tricks and introduce a layer of indirection. Inside the operating system, each process has something called the page table that acts like a phone directory or a dictionary. Giving for each virtual address, the physical address that it corresponds to. Usually this will be a place in memory, but when space in main memory is low it might also be on part of the disk, called swap in many systems. And when a program is first starting up, you usually don't load the whole application into memory first thing but rather leave part of the program, that might not always be needed, on the disk. This indirection allows us to accomplish several things. First, it allows us to have an address space that is bigger than physical main memory. This is convenient when main memory itself isn't big enough and we need to use the disk, or when we want to map other storage devices and treat them like memory. Second, it provides protection between processes, as mentioned earlier, without indirection the applications might find themselves overwriting each other's data. With the operating system properly managing the page tables, however, these two virtual addresses can be pointed to different physical addresses. Thirdly, it can allow for sharing. Even different virtual addresses can be made to refer to the same physical address, again, just by properly maintaining the page tables. Lastly I should point out that because kernel addresses are the same for each process, they get their own page table that is shared among the processes. This is called the global page table. Recall how for caches we divided memory into blocks, or cache lines. And this's the granularity at which caching operates. Similarly, for a paging system we divide up our physical address into pages, often 4k long. Pages, are the granularity at which we grant memory to processes. The higher order bytes determine the page number, also called the physical page frame. And the lower order bytes determine the offset into the page. Correspondingly, the virtual address space is similarly divided into pages. Again, lower order bytes determine the offset within a page. The higher ones determine the virtual page number. The number of bytes in the offset always match, that's because virtual page sizes and physical page sizes aren't necessarily the same. But, the number of bits involved in the virtual page number need not match the number of bits used to identify the physical frame number. That's because, as we said before, it's possible to have more virtual pages than physical pages. Since we're mapping page for page between virtual and physical addresses, the offsets stay the same. We do need to translate page numbers however. And for this, we use a data structure called the Page Table. Here's a quick question to make sure we're on the same page, so to speak. Consider a 16-bit virtual address space and a 512 byte page size. How many virtual page numbers are there? You can assume byte addressing. Here's how I write to my answer. The fact that we have a 512 byte page size implies that we have 9 bits used for the offset. This leaves us with 7 bits for the virtual page number, and that gives us 128 possible pages since two to the seventh is equal to 128. Now we consider the implementation of the all-important page table data structure, which needs to translate virtual page numbers into physical page frames. Virtual page numbers start at zero, and then go up by one. So the most straightforward way to do this would be to create a giant array that is the number of pages long. Let's see how this strategy might work. Suppose that we have a 48-bit virtual address and 32-bit physical address and let's say pages of sizes 4K. We've used up 12 bits for the offset, leaving 2 to the 36 possible virtual page numbers. That means that we need an array 2 to the 36 long, each entry probably having 4 bytes to specify the physical address, which brings us to a total of about 256 gigs. Which is more memory than most machines have in RAM. And of course, almost all of it is wasted since most virtual addresses aren't used. Even with just 32 bit virtual addresses, we would still need a page table with 2 to the 20th entries. And that would put us on the order of several megabytes. And of course we need one page table per process. So instead, page tables are done in a hierarchical way. In our 32-bit example, the highest order 10 bits are used as an index into a top level page table. The contents of this page table then give us the base address of another page table. We use the next 10 bits as the index into that lower level page table. And the contents of that page table then give us the physical page number. Why does this help us use less memory? Well recall our virtual address space. We use addresses at the bottom, and we use addresses at the top. But there's a vast empty region in the middle. Many of these unused addresses will have the same 10 higher order bits. And for these, we won't even have a secondary table. Thus, we end up using not too much more memory for the page table than we really need to. The particulars of the actual page implementation vary greatly. Some have three levels, others have four. But they all use this basic hierarchical strategy. So far we've described the address translation process like this. The offset from the virtual address, simply gets copied to the physical address. And in order to translate the virtual page number, we do a look up in the page table, which is in memory, to find the physical page frame. Now you may be asking yourself, won't going to memory, for the page table every time we access a memory address be slow? In fact, we'll have to go multiple times for our hierarchical page table. Well indeed it would be. Thankfully the page tables themselves may end up in our cache, which can help considerably. But architects have also created a special purpose cache for storing page table entries, called the table lookaside buffer. Or TLB for short. Instead of going through these slower traditional memory mechanisms, we're taking a special shortcut. Whereas the caches we talked about earlier are usually indexed by physical address, the TLB is indexed by virtual address. After all, its job is to translate virtual page numbers to physical ones. This is extra tricky, because one virtual page number may be mapped to a physical address in one process, but it might be invalid in another or mapped to different location. There are two ways to handle this problem. Some systems simply flush the TLB, on every context switch. That is, every time the address space switches. This is a main reason that context switches are considered so costly. Alternatively, the TLB can use an address space identifier field in the table, to make sure that the translation in the table is meant for use in the current context. I should point out, too, that the kernel sometimes get's special treatment in the TLB. Because the kernel addresses our constant across processes, there is no need to flush them from the TLB on a context switch. Sometimes, too, part of the TLB is reserved for kernel addresses. Now I want to consider the contents of the page table entries, at least the entries at the leaves of the hierarchy. In our example where we had a 32-bit physical address space, it would be typical to have a page table entries also of 32 bits. With 4k pages, only 20 bits would be needed for the physical page frame. What do we do with the other 12 bits? Well it turns out that there's lots of other information worth storing in the page table. Some of the more important are things like access control. Do we have read, write, or execute permissions on this memory? If we don't, then when we come to this page entry, an access violation or segmentation fault will occur. Another essential piece of information is whether the information associated with the virtual address is actually in memory at the moment. It may be the data that we need was not loaded on the application's start-up or it may have been swapped out to disk. In either of those cases, the valid or the present bit would be zero. When a process tries to access a page with a valid bit of zero, it creates a page fault and lets the OS try to bring the needed data into memory. We'll look at the page fault handler next. One of these bits also indicates whether the page is dirty. That is, has it been written to since it was loaded for disk? If not, then if it becomes necessary to evict this page from memory, there is no need to write it to disk if it's already there. A nice optimization. Sometimes a bit or two will also control how caching of this page is done. Page table entries corresponding to virtual addresses that were never mapped to a physical address by the OS usually have physical frame numbers of zero. The physical frame number of zero is rather like a null pointer. Everybody knows that that's an invalid physical frame number and a segment fault will be generated. Next we turn to the page fault and ask the question: What happens when an application requests a virtual address that's not mapped to physical memory, but is somewhere on disk? I'll represent a user's process like so and at some point, he uses a virtual memory address that's not actually mapped to physical memory at the moment. Yikes! Actually it's not that big a deal. An exception is raised, which sets the running process back to be ready to repeat the instruction that created the, the fault. Meanwhile, the operating systems page fault handler takes over to address the problem. He checks to make sure the request is actually valid and sends, sends a segfault if it isn't. Then he loads the desired page into memory, gets everything ready and then tells the scheduler that the process is ready to resume. The process will execute its memory move or read or whatever it is again, and this time it will succeed because the page has been loaded into memory and it can continue running. That's the high-level picture. Let's talk a little bit more about what the page fault handler does. So assuming that the page we want really does reside on disk, the first thing the page handler does is to find a physical page frame in memory to load the data into. The best scenario is that one is free. The handler keeps a list of free pages. If this is not empty, then he can just pop one off. Okay? And that makes us very happy. If he doesn't have a free page, however, then he must choose a page for replacement. There are many possible page replacement algorithms. We won't go into them here. For our purposes, it suffices to say that a page handler will pick a page that it doesn't think is likely to be used soon. This page is saved to disk and then its physical memory can store the page that was created by the page fault. So, one way or another, we now have a free physical page in memory to which we can write the data we need from disk. Then we update the process's page table and any other data structures that are page fault handler needs to keep track of and then we're ready to restart the process and let it continue on to a At this point, we've covered all of the essential pieces of the memory system. Now, let's step back and put it all together. To help us do this, I've written some pseudocode that captures the strategy for loading data into the CPU. You can imagine that we're writing an emulator if you like. The first thing that needs to happen to a virtual address is for it to be converted to a physical address. We'll postpone that until later. Then, if the data is in the cache, we simply read from the cache otherwise, we read from memory. Note that whenever we read from memory, we also write to the cache evicting some block if necessary. This process is, is what we talked about in the first half of the lesson. The second half of the lesson was largely concerned with address translation. The first place we check is the tlb. If the physical page frame is there, then we're done. If not, then we need to consult the page table. Notice that none of these procedures that access the page table should use the load procedure as it's defined here. The top level page table cannot be in a paging system itself. Otherwise, load would call translation, which would then end up calling load again, which would end up calling translation and so forth. We can imagine a fixed page being assigned to store the top level of page table, another good reason for making this small. I've made the check for the access violation explicit here, because it's in the page table that this is most likely to happen. It's possible that we would get an access violation in the tlb if we try to read a kernel page. But for the most part, the page simply won't be there. If the present or valid bit isn't set in the page table entry, that means that the page we want isn't in memory. So, we raise a page_fault. This will then be caught by the page handler, up here, and then we can re-run the load procedure, and this time we will find that it's in memory. And so we won't raise the page fault again. Obviously this pseudocode has its limitations. By using just software, we can't capture the interaction between the OS and the hardware and we're abstracting away many of the details. Nevertheless, if this makes sense to you then you have a good high-level idea of how a memory system operates. You can't resist talking about one final optimization. Looking at this load method, notice how the translation to the physical address and cache look-up happen in sequence. We could speed up our implementation if we could find a way to do this in parallel. At first this might seem impossible. After all, the cache is indexed by the physical address We don't know this until we've done the translation. Recall, however, that only the higher order bits are changed when we go from the virtual address to the physical address. The offsets within the page stays exactly the same. Which bits are important for extracting data from the cache? Well it's these middlest bits that come after the offset for the cache block. As long as the index bits for the cache don't extend beyond the offset for the page, this little red line here, then we can extract data from the cache. Before we know the full physical address. Conceptually, we're going to do something like this where the virtual page number goes to the TLB and the page offset goes to the cache. The tag, or tags in the said associative case, then gets compared with the page frame number from the TLB, and the correct data is returned. Here's a question to test your understanding of the virtually indexed, physically tagged cache. Suppose 32-bit virtual and physical address spaces. Pages are four kilobytes, and cache blocks are 64 bytes. What is the maximum number of entries, in a virtually indexed, physically tagged cache? Here's my answer. The 4k page size implies a 12 bit page offset. The 64 byte cache block implies a 12 bit cache block offset. Therefore, we have 6 bits left for the index, leaving us 64 in this use for the cache. Actually, the size of the virtual and physical address spaces don't matter at all. This completes our lesson on memory systems and if you followed along, you now have an advanced understanding of how memory systems work. This should make you a better programmer and put you in a good position to think about some of the complex trade-offs involved in CPU scheduling and maintaining cache coherence, especailly in the context of shared memory machines where there are multiple CPU's. There is always plenty more to learn. Hello and welcome to this lesson on multi-threaded programming. This lesson begins with a brief review of parallel programming in general and moves on to discuss multi-threaded programming in particular. It uses the POSIX threads library as the conical example, though the same principles apply to most all threads libraries. To motivate parallel programming, consider the following task. You're given a set of webpages, some of which link to each other. We'll draw this using a standard abstraction of a directed graph, where outgoing edges represent links. So webpage A here contains a link to webpage B. And our goal is to figure out, how many links our website, say U, has pointing to it. This is something that a search engine might do to help figure out how popular a website is. Now, there's no way to know whether another web page has a link to our web page in it without looking at it. So, in one way or another, we're going to need to read all of the web pages in our collection. And if we try to do this with a single processor, this could take a long time. First, I have to read webpage A then webpage B, etc. Thankfully, this task is highly paralyzable. That is, there is nothing about my processing of A ie, counting how many links it has to website U that affects how I'm going to process website B or website C. So, I can easily divide up the web pages among, say, three CPUs. Let's get these over here to CPU one, these to CPU two, and these to CPU 3. Now, we can let each CUP count up, in its sub group, how many links it has to you, and then we can just add these units up to get our final answer. Returning to our time graph, we see that with three CPUs, we only need 1 3rd of the time, potentially, with n CPUs we would only need 1 nth of the time. It's important to realize that we're doing the same amount of work here. Suppose that in our original thread, we first process the website assigned to CPU one, then those assigned to CPU two, and then those assigned to CPU three. Well, in parallel programming, we've cut this one long strand of computation into three shorter strands and arranged them in parallel by giving them to three seperate CPUs. This, of course, is the ideal case for parallel programming. Now of course, in real life, we don't always get as many processors as we want, they aren't always available. And given that our application will likely be running on the same machine as others, we don't know exactly how many processors we will have access to. But by expressing our algorithim in this way, we can take advantage of the processors that we do have and thus make our algorithms run faster. This leads us to our first question, which of the following tasks is parallelizable? Mowing the lawn, playing the piano, cooking a chicken, rice, peas dinner, or reading a novel? Check all that apply. And the answer is, the first and the third. Mowing the lawn is easily parallelizable. I can take one part, you another, and we'll get it done in half the time. Playing the piano, I would say, is not parallelizable. Sure, I might be able to play the right hand and you the left, but the song isn't going to finish any faster. Cooking dinner is clearly parallelizable. We don't wait for the rice to cook before putting the chicken in the oven. Lastly, reading a novel in parallel would be pretty difficult. You might be able to read one part and I another, and then we could explain it to each other, but I don`t think that counts Even if you know that your application will only run on a single core single CPU, it can still be useful to express parallelism in your programs. In fact, almost every time you load a web page, your browser is making something called an asynchronous request back to the server for more information that exploits a certain kind of parallelism. You see, the web server rarely sends all the information for a web page in response to your initial request. Instead, it sends a kind of outline for the page. That includes other URL's where your browser can find the details. To get these details your browser then sends other HTTP requests back to the server, for example, we might need a style sheet and also an image to help fill out the website. Now if we were to implement this in a naive synchronous way our program might run for a while, but when it discovers that it needs a CSS file from the server it would make the HTTP request And then pause part way to get the response back. After receiving the response it could then, process the CSS file, and then continue with work, until it discovers that it needs image file. Make that request, wait til it receives it, and then process that image file. Clearly this is inefficient. There is no sense in making the browser, and hence the user, wait for a response from the server While there was more work to be done in reading and rendering the original request. This is why requests back to the server are made asynchronously. And this is really just a fancy word meaning that we get to continue our program without having to wait for a web request or sometimes a system call to complete. So an asynchronous approach, we would begin by processing the HTML profile just as we did in the naive approach. But when we find that we need to request a CSS page, we do so in a new thread. This allows our original thread to continue processing a HTML file and discover that he also needs to request a PNG file, again he will do this in a new thread. When he gets this CSS file back from the server the requesting thread can process it. And when he gets back the p and g file, the requesting thread can also process that. And the end result is that we finished loading the page much faster. Now remember, we're still assuming that we only have one CPU. This means that the computation that the naive approach did, say this red portion here, can't run in parallel with the other parts of the processing. They still have to take turns. Nevertheless, because we weren't idle while we waited for the web server, as we were in the naive approach, we've made our page load faster by always giving the CPU something to do. And it's the programming abstraction of threads that makes it possible to achieve, both, this behavior, as well as the true parallel processing that we discussed earlier. Here's a questin to test your understanding. Which of the following are indicators that your application might benefit from the use of threads? You have multiple cores and part of the program can be run without knowing the outcome of another part. Code can be broken down into procedure. Or, your program asks a slower entity, may it be a database, web server, or a file system, for information while there's other work to be done. Check all that apply. And the answer is, the first and the last. The first case is our original example, by counting how many webpages point to our site. The third, is like our second example. There's no sense in having the process wait on some blocking call, while there's other work to be done. Option two however, merely being able to break code down into procedures, is not a good indication of parallelism. To understand threads more deeply, it's important to have a clear picture of a traditional process and its address space. Typically, at the lower addresses, a process will have the instructions followed by literals, that is values in the code, then statically allocated memory, really, global variables, and then the heap. Which will grow upwards. And then at the top of the address space and working its way down, we have the current stack of a process. Remember that the stack includes all the information local to a procedure. Parameters, local variables, and the address of the instruction to return to when the current procedure is over. In a multithreaded environment, each thread has it's own stack, but it shares everything else with the original main thread of the process. So, from a certain perspective, a thread acts like an entire process unto itself, and hence threaded programming isn't much different from a traditional programming. The difference comes from the fact that part of the memory is shared. On the one hand, this makes it very easy for threads to communicate. But it also means that extra care is needed to make sure that the threads coordinate as they use this memory. I should point out as well, that one can do parallel programming without having multiple threads. Each parallelizable task could have it's own process with it's own address space. And it could communicate with the other processes through message passing. In fact if we wanted to take advantage of a distributed system, where each processor has it's own separate physical memory, we would be forced to use this approach. As we think about multi-threaded programming, however, the paradigm explored in this lesson, is more appropriate to think of a shared memory system, where we have multiple cores sharing a common piece of memory. Here's a very simple threaded program that uses POSIX threads, or p-threads as the library is also known. We'll discuss POSIX threads for the remainder of the lesson, but most of the concepts will apply to all thread libraries. The entry point for our process is main, as always. And main will spawn two new threads, t1 and t2 which will enter at the procedures f1 and f2 respectively. That's why we passed the function pointer as we create these threads. These functions become like the main of our other threads. And as we see, these threads are going to print out a message over and over that says that they are the better thread. Notice that main we have this join method. We'll discuss this a little more later, but basically this mean that the thread is going to wait for thread one to finish before it goes any further. And then join with thread two means that it will wait for thread two to finish before going any further. So, actually this is a quiz. What will happen when we run this code? Is it that thread one will seize control and print that thread one is better forever? Will thread two seize control and print thread two is better forever? Will the threads print their messages alternately, or is the order of prints indeterminant? And answers that the order of prints is indeterminate. Exactly when we switch from one process to another is determined by the threads library in the OS. We haven't exercised any control over switching here in our program, so we can't make any assumptions about the order in which these things will run. It ma be that thread 1 will print 5 times and then thread 2 will print three. Then thread one will print once again, or thread two might print two times, then thread one print fours times, etc. We don't know and go programming here means assuming the worst. Now, suppose that instead of letting each thread have its own local variable, we create a single global variable for who's better. How does this change affect the program's behavior? Will it create a compilation error? Is it that a seg fault will occur when the second thread accesses this global variable? Will the threads eventually agree on who's better? Or, does the behavior of the program not change at all? The answer, is that the threads will eventually agree on who's better. Before, each method had its own copy of the variable in its stack. Now, the threads share the variable via the global memory. In fact, this is a common way for threads to communicate. Because who's better is a shared variable, it must have the same value in all the threads. Looking back at the code, we see that this sets off a race condition, only in this case, the winner is the second thread to reach the assignment statement. Suppose that thread one reaches the assignment first. Then he sets who's better to one, and it's possible that he gets to print out that he's better a few times. But eventually thread two will seize control, change the value of who's better to two, and then from then on, they will both agree. That thread two is better. Now I've treated this as all fun, and games, but in reality this situation where we have two threads trying to write to the same memory can create some very serious [INAUDIBLE]. And it's particularly bad when the order in which rights happen is indeterminate as it depends on the choices of the schedulers, and the library library, and the OS make, and maybe let that affect the output of our program Now if you've been actively thinking about how threads work so far in this lesson, you may be wondering how does the memory for a thread get cleaned up after it is finished, or where does its return value go, or how can I make sure the thread has finished its work before I let the main thread terminate? We'll answer all these questions by looking at the distinction between joinable and detached threads. Let's start by talking about what makes a thread terminate. We'll draw the main thread here on the left, our thread here on the right. And maybe there are some other threads in between. There are really two cases. First, the entire process will terminate if any thread makes a call to exit or if main reaches the end of its code, it's or executes a return statement. Again this will kill all threads associated with the process. The second way our thread might terminate, is if he himself calls either pthread-exit or executes a return statement. The distinction between joinable and and detached threads is important for the second case here. A thread can either be created in a detached state, or detached later with the procedure call. When the detached thread reaches the end of its execution, its memory is cleaned up right away, and any return value just disappears. At least, no other thread has a way of getting at it. Moreover, we have to be careful that main doesn't reach the end of its execution, and execute a return statement, before the thread is done with its work. As seen in point 1, if main returns, then the whole process terminates. So if we want to keep this detached thread going, we have to terminate main with a special call to pthread exit. Joinable threads, y contrast, don't get destroyed right away when they finish. Instead, they stick around until another thread joins them. This other thread makes a call to thread join, specifying the thread. And also an address where the return value of the joinable thread will get stored. This call blocks, until the joinable thread is finished. Though it's okay too if it's finished already. So it's okay for the joined call to be made first. Before the join thread is finished. Having main join with other threads is a common way to make sure that main doesn't finish before the other threads, and kill them all via exit method one. By default, threads are joinable in the p threads library. To illustrate how joinable threads work, let's take a look at a very simple program. We first create a thread, and then we can't be sure what's going to happen next. Perhaps the main thread will keep control and print his hello message, or perhaps the created thread will get control and print his hello message. It's indeterminate. In any case, however, the main thread is going to execute this join procedure. It specifies which thread it wants to join and gives an address where the return value can be passed. Only when the created thread is finished will the pthread_join call complete. So we can be sure that Done will be printed last. Let's take a look at another example program. Look at the program carefully and tell me what its output will be. Will this program create a segmentation fault? Will it print out null, the empty string? Will it print out a message from the created thread? Or will it complain about how a joinable thread is not joined? The answer is null. The program creates a second thread, but this thread first sleeps. Hence we can count on the main thread printing out the current value of msg, which we had just set to null. And then, we can also count on exiting and terminating the whole program before the thread_proc call here gets to do anything. Threads can have many different relationship patterns. For much of the rest of the unit, we will look at an example of the producer consumer pattern. Before going into that however, I want to put it in context by taking a broader view of possible thread relationships. To give our thread something to do, we'll imagine that we are servicing some requests that come through a queue. In the team model, we have a pool of worker threads that all look in the queue, fetch the next request, and service it. This idea of a worker pool is common to many patterns. The costs of creating a new thread are fairly high, so rather than spending time creating and destroying threads, applications will simply let the threads wait around until there is work for them to do. A model similar to the team model is the dispatcher model. Here instead of having a whole team of threads constantly looking at our queue and possibly creating some contention, we have a single dispatcher thread looking at the queue. He finds an available worker and passes the request off to the worker to be serviced. Instead of dividing up the work by requests, it's also possible to divide it up by subtask. This is the strategy employed in the pipeline model. One thread DQs a request, does part of the work, and then passes it on to another thread who does part of the work, passes it on, etc. So, all three of these models are commonly used. One of the most common design patterns for multithreaded applications is the producer consumer model. This can be viewed as a special case of one of the models discussed before, if you like. To motivate our discussion, we'll use a tracking application. Here's our camera. Maybe it's monitoring the number of people on a subway platform to help route trains, or something like that. You can then break the computation down into two pieces. One, is to get the picture from the camera into the memory. We'll call this the producer task. And the other task, is to analyze the picture to figure out where all the people are, and pass this information on to the training planner, or whoever may need it. We'll call this, the consumer task. Now for this scenario, suppose the images are available from the camera every 33 milliseconds. We want to get all of them, so we're going to have a thread whose job is just to copy the frame from the camera, into computer memory. And let's say that the image processing to find the people is variable, taking somewhere between 20 and 40 milliseconds. But on average, less than 33. If it were more than 33, we could put more consumer threads here to do the work. Because of the variability in the time it takes to process the image, we need a place in which to store images that our consumer thread is not yet ready to handle. And for this, we use a ring buffer. Each division here represents memory for our frame from the camera. The ones that haven't been consumed yet, I've colored in red. Now we should imagine the producer and consumer threads chasing each other around this ring, with the producer one writing frames, making them red, and the consumer one running behind, turning them back to white. That is, empty or processed. Though of course the consumer will never pass the producer. We'll start with a naive attempt to implement our tracking application. This digitized procedure here, represents the producer method and the tracker is the consumer one. This if statement is intended to stop the digitizer from wrapping all the way around the queue and overriding a frame before the tracker has had a chance to process it. This if statement is intended to stop the tracker to work all the way around the queue to a frame it has already tracked again. Try to see if you can find the problem with this code. Is it that we can't write to the same memory more than once in a threaded program? Is it that the threads will go into an infinite loop? Is it that this bufavail isn't updated atomically? Or is it that each thread has its own bufavail variable. The answer is c. We are indeed allowed to write to memory more than once in a threaded program, so its not that. The threads will indeed go into an infinite loop, but that's intentional. We want our tracker to run continuously. Actually since bufavail is a global variable, it's shared among the threads. So that eliminates this option. The real problem is that bufavail might get out of sync. Going back to the code, observe that these two lines might get executed at about the same time. Our fear is that something like the following might happen. The Incrementing line might take the value currently in memory 3, load it up, but then gets suspended. And then control goes to the Decrementing line which loads the value 3 into memory and then decrements it, turning that value in the CPU register here to 2. And then copying that back to memory, making this 2 as it should be, and when the increment finishes, it should restore it back to 3. But it doesn't, because it's already done its load. So instead when it goes to do its increment, it changes the value to 4 and then writes that value into memory, meaning that we have a value one higher here than we should. And as you can imagine, this might through our queue out of sync if this happened too many times. Now, in real life if I want to keep other people from taking my things or messing with things they shouldn't, I use a lock to stop them. In threaded programming, we use a similar concept to protect memory. Let's say that inside some procedure, there's a block where I manipulate some data, in our example, the variable a. And I need to be sure that no other thread touches this variable while I am working on it. Whether executing this procedure or another one. Well then, I can declare a lock variable, which should reside in shared memory, and put the lock on before I have it do my manipulation, and unlock it afterwards. And I should do the same wherever I manipulate this variable a. If some other thread running this code simply accesses the shared resource without using the lock, it won't be able to stop him. But if he does try to acquire the lock first, then this call to mutex lock will block until the lock variable shows that has been unlocked. And only then will this code proceed. And, of course, if the schedule had worked things out so that this call had been called first, then he would get the lock. And this call to acquire the mutex lock would block until this data manipulation was finished, and the mutex variable had been unlocked. And this way the threads are able to mutually exclude each other from using resources, hence these are called mutex locks. A section of code that shouldn't be executed by more than one thread is called critical. This diagram shows the states of four threads running a single process, all running the same code, with the green arrow indicating the line of control, that is, where the tread is in it's execution. The boxes denote the critical sections. Check the treads that could be running right now. And the answer, is all of them, expect the one that is right before the critical block. This first one is in the middle of the critical section, so he's running. This next one is before the critical section, so he has some work to do before he needs to stop. This third one is past the critical section, so he's fine continuing on. Only this fourth one, only for him would the next statement to execute be in this critical section. So only he needs to pause and wait for this first thread to finsih the critcal seciton so that he can then enter it. Let's use this new knowledge to add mutual exclusion to our digitizer, tracker program. Hopefully now we won't have the same problem but before, were we could end up reading from a frame that was only half filled with the data we wanted, because bufavail got out of sync. So we'll create a new mutex lock variable. And we will set this lock Before we do our manipulation and unset it afterwards. We'll do that both in the digitizer part and then also in the tracker part. What's wrong with this program? Does it not have any concurrency? Will we end up with deadlock, where there's no thread to run? Is it that two threads cannot use the same lock? Or is it that two procedures cannot use the same lock? The answer is, that the program doesn't have any concurrency. The two threads can't run at the same time anymore. One of them will always be able to run, so there's no dead lock. And of course, two threads can use the same block. That's kind of the whole point, and two procedures can too as we've seen in a previous example. We do have a major problem with concurrency. The lock makes essentially the whole body of the code critical, while the digitizer is doing his work, the mutex lock will prevent the tracker from doing his work and similarly, while the tracker is doing his work, the lock will prevent the digitizer from doing his work. All parallelism is lost. Okay, okay. So we put too much code in the critical section. Remember, that our original problem arose when both threads were trying to modify bufavail. If we're able to stop that, then we achieve mutual exclusion on the buf frame array. Let's just try putting the lock around our changing and accessing of the bufavail variable. What's wrong with this program? This time, the answer is dead lock. We now have the potential for all the threads could be waiting for a condition that will not happen. Specifically, if the tracker were to find that bufavail were equal to MAX, then the thread would stage in this loop, waiting for bufavail to change. But it wouldn't be able to change because the other thread is waiting for it to give the lock back. It might be stuck on this mutex lock call right here. One way to solve the problem is to simply stop using the mutext lock on the while statement. This sets off something called the sync race where one thread is quickly reading a variable and a while loop, or spinning on it, as it waits for another thread to change the value of a variable. Because the spinning thread is just checking the valuable of a variable over and over. It can't possibly interfere with anything the other thread is doing. So we can be reasonably confident this is correct. This turns out to be a fine way for threads to communicate, this sync race, that is. Actually, if this spinning thread has its own processor or core to run on, then it will resume its execution very quickly, which can be a good thing. However, if processing time is at premium, then all this spinning is just wasted. Hence the motivation for another important feature of thread libraries, conditional wait variables. Before talking about conditional weight variables, I want to make a distinction between mutual exclusion and synchronization. Mutual exclusion, as we have seen, is about preventing two threads from accessing a resource, often a piece of memory, at the same time regardless of exactly where they are in their execution. For example, if Thread 1 is accessing this memory, then the lock should block Thread 2 from accessing it or vice versa. If Thread 2 is accessing the piece of memory then the lock should block Thread 1 from accessing it. Synchronization, by contrast, is about controlling where the threads are in the flow of their executions. For instance, it might be important that Thread 1 completes some task A before Thread 2 can start task B. In effect, we want the message that A happened to get sent to thread B. Mutual exclusion and synchronization are related. But it is important as a program to know which of these goals you are after so that you can use the right constructs. Most simply, conditional wait variables allow you to take one thread off the scheduling que until there is a signal from another thread, that it should be put back on. This is useful if we don't want one thread to continue it's work until the work of another thread is finished. To give an example, let's oppose that we have a thread A over here, and a thread B over here. Thread B wants to stop until thread A's work is done. Now to use a conditional weight variable, we need three things. A variable indicating whether the work is done, or some other condition that can be tested in memory. A lock that we can apply to this variable... And a special conditional wait variable. Now after finishing the work, thread A is going to acquire the lock, change is_done to true and then send a signal to anyone waiting on this condition, a signal that they should wake up, and then he'll unlock the lock. Thread B, for his part, will first acquire the lock, and then check if the work is done. If it isn't, he will wait. Now, it's important that this cond-wait not only makes thread B wait, but it also unlocks a mutex. This will allow thread A to acquire the lock and eventually send the signal. The call blocks until the signal is received and thread B acquires the lock again. Hence the need to unlock it at the end of the code. At first this might seem unnecessarily complicated. Why do we need this if statement, before the cond_wait procedure call, for instance? Well, if it weren't there, then it's possible that the signal will have already come, and we would never see it. So it's important to check that there's something to wait for, before you actually wait. Given that there is such a condition to check We better also have a lock for it. Otherwise we might decide that we need to wait. But before we actually execute the wait function, the signal might come. And of course, the cond-wait method had better unlock the mutex. Otherwise thread A would never be able to acquire and change the condition that we're waiting on. Returning to our digital tracker, remember that we were still left with the problem of busy waiting. We were wasting CPU cycles, just waiting for another thread to change the value of the variable. We're going to solve this problem, by creating an additional weight variable. So the code will look like this. Instead of having a wow loop and whether the condition is met that does nothing, the wow loop now makes a call to cond_wait. CPU's cycle save. The second thing to notice is that the new text variable is back, and it gets passed in along with the conditional wait variable into the cond_wait procedure. We do this because we want to make sure that the condition, in this while loop still holds when we put the thread onto the wait variable Q. The nightmare scenario is that in between this check and the call to cond_wait, another thread changes the condition and sends the signal. Then we could be waiting for a signal that might never come. So it's a good practice to lock variables that might cause the condition to change before you make the cond_wait call. The cond_wait method itself puts the active thread on a waiting queue, and then unlocks the new text so that another thread can change the condition we are waiting on. When the conditional signal comes, the method then reacquires the box before returning, that's why we need to unlock it again down here. Now, somewhat surprisingly, we need to recheck our waiting predicate that is, is bufavail equal to zero again before moving on. That's why we have the while statement here instead of an if. The reason for this is that we might not have been the first thread to acquire the lock. Another thread might have acquired it first and changed back the condition to something where we would need to wait again. In this digitizer example I guess maybe that would be another tracker. Putting this in a while loop so that we recheck the predicate after the signal comes is called rechecking the predicate. Now let's make sure that this program has all the properties we want. Namely, concurrency, absence of deadlock, and mutual exclusion of the important shared memory. Does it have concurrency? For this, we just need to convince ourselves that the critical sections of the code are short. This critical section here is just decrementing a variable and sending a signal. Not much work to be done there. And the tracker does the analogous thing. So we're okay on that front. The top two blocks, just this one and this one, these just check the condition and then call the wait procedure. And then they immediately unlock afterwards. So this also is short. Okay, so let's check off concurrency. The absence of deadlock, we can convince ourselves that both threads are never blocked. We can see that the mutex_lock can't be responsible for a deadlock because both the producer and consumer give up the lock writing after acquiring it, just as we argued about concurrency. There isn't much work to be done here. Nor is there much going on up here. It's just a matter of checking the value of the bufavail variable. That leaves the possibility that we get blocked on one of these cond wait calls. Let's suppose that a digitizer is blocked on this line. Because he's waiting for bufavail to be greater than zero. Let's suppose that the digitiser is blocked because he's waiting for a signal saying that bufavail is greater than zero. We know that right after condwait was called bufavail was zero. Remember we had the lock at the time. This means that the tracker can't possibly be blocked on its wait because that would imply that bufavail were equal to max. We also know that our digitizer unlocks the newtext variable. Again, that's part of the cond wait call. Hence, our tracker can't be blocked on this while he tries to acquire the lock either. Eventually, the tracker will get around to incrementing bufavail and then sending the signal back to the digitizer thread. And so we can be confident that eventually, this signal will arrive and we'll exit this procedure. The argument is analogous if we suppose that the tracker is waiting on cond wait. That means the digitizer will be able to make progress and eventually decrement bufavail and send the signal. Alright, so hopefully we've convinced ourselves that there's no deadlock. Lastly, we want to convince ourselves, that we have achieved mutual exclusion of the necessary shared memory. Well, the use of this mutex box, around all the, accesses and rights to bufavail should convince ourselves that we don't have a race there. That leaves the frame buff variable. But the logic of the program, that is the logic of the ring buffer that we talked about earlier, prevents the head from ever catching up with the tail, or vice versa, as they chase each other around the ring buffer. In fact that's the whole point of the buff avail variable. So mutual exclusion is achieved, and our program has all three properties that we desired. Phew, some of that was pretty technical, but if you followed along, you now have a solid understanding of the semantics of threaded programming. You are now familiar with the notions of joinable and detached threads, locks, synchronization, and many of the details of the p threads library. Of course, you will learn much more if you get involved in developing an application that uses threaded programming, but now at least you have a good foundation to start from. Happy programming. Welcome to this unit on networking. It covers how computers communicate, from the very simple connections in a machine cluster, to the architecture of the internet, which makes it possible to reach most any willing computer around the world. We will assume a basic familiarity with technologies and terms, such as anyone working with or programming computers would acquire today. And we'll focus on how all these pieces fit together. To make network communication possible and so simple for its end users. The Internet is an extraordinarily complex creature, using a wide variety of hardware and communication protocols. To help make sense of it all, most any discussion of networking begins with the notion of interconnection layers, an idea that captures the most important relationships among the Internet's components. Several attempts have been made to formalize these, most notably the seven-layer OSI model, and the four layer Internet protocol suite. We use a hybrid model here and try to keep the discussion intuitive. The physical layer refers to hardware that actually creates and transmits the electronic and optical signals, as well as the protocols for interpreting these signals as bits. This would include the network interface card inside your computer, the modem that your Internet service provider gave you or recommended that you buy. For ethernet cable connecting your modem to your computer, and even submarine fiber optic cables that stretch across the world's oceans. Protocols are interpreting the signals sent across these media include physical ethernet and sonnet. The link layer covers the communication between peers on a local network. The most trivial topology for a local network is just point to point, where there is a permanent link between two end points. We can also have more complex topologies, however, where multiple nodes are all connected to the same medium, and feel all each other's signals. The challenge here becomes making sure that they don't all talk at the same time. The link layer also encompasses switching, where connections between nodes are made on an as-needed basis to reduce the potential for crosstalk. The network layer is responsible for end-to-end communication on the internet. When you request a page from the Udacity website, the data doesn't come directly from our servers, but goes through several intervening machines. This process of routing the data, or figuring out what path the data should take, belongs to the network layer. Unlike other layers, which have evolved to include many different technologies, the network layer is governed almost exclusively by the Internet protocol and the system of IP addresses. Now, once data reaches your machine, there has to be some way of knowing which application it is intended for. Also, communication isn't perfect, so we need some mechanism for detecting when data goes missing, arrives out of order, or when the connection is overloaded. Coping with these situations is the job of the transport layer. And lastly at the highest level, we have the application layer, which includes application specific protocols, such as HTTP for web, SMTP for mail, etc. The relationship between these layers is mostly one of using. The link layer uses the physical layer to accomplish its goal of enabling point-to-point communication across a shared medium. The network layer uses the link layer to accomplish its goal of end-to-end communication between two machines on the internet. The transport layer uses the network layer to accomplish its goal of reliable end-to-end communication between applications. And the application layer uses the transport layer to accomplish its goal, whatever it may be, whether it's fetching a website, sending an email, or streaming a movie. The plan for this lesson will be to start at the bottom layer and move our way up. Since our concern in this course is with operating systems, I'm going to leave you to explore most of the physical layer on your own, and focus on the part that has implications for the operating system at the edge of the network, and end users like you and me. The most important piece of hardware for us, then, is the network interface card, or NIC. This typically is a direct memory access, or DMA device. Meaning that once connected to the memory bus, it has the ability to read and write from memory, on its own, independent of the CPU. Typically, the CPU specifies through the bus, the range of memory it wants to be sent, whether it's a read of write, and then sets the go bit, to let the NIC begin. The NIC then tries to copy the data, as requested. Because the NIC controller puts all of the copy instructions on the bus, the CPU doesn't have to. He can go about his business, to the extent that he finds the information he needs in the CPU cache, there won't be any contention on the memory bus. Only when the NIC finishes either sending or receiving a packet, need he bother the CPU with an interrupt. The NIC controller is also responsible for putting hardware specific headers and footers on the data. For physical ethernet, this would include an inter-frame gap of silence, and a special preamble, to indicate the startup of frame. Next we move onto the link layer. If every link between computers were a dedicated permanent point to point connection, then most of us decided that two machines should communicate it would be relatively straightforward to arrange for them to do so. The challenges here are mostly solved by breaking up larger chunks of data into more manageable chunks and putting them inside a frame. Which in addition to the data payload, would contain some metadata, its length for instance. And probably some kind of check zone as well to see if the data was corrupted in transmission. The situation becomes much more interesting when multiple nodes all share the same medium. Think of the cable line that carries the internet traffic in your neighborhood. In that case, we have multiple nodes, the modems in your houses, all connected to the same wire. This is called a bus topology. Actually, the original ethernet had this topology, with all the cables all connected to a common hub, making it look like a start topology. In reality, the hub would simply relay whatever signal it got to all the other nodes effectively making all the cables one medium. In terms of physical hardware this is attractive because we don't have to run wires between every pair of machines on the local network. It does however mean that messages themselves have to specify who they are for and who sent them. In the case of ethernet on a local area network like a home, office or a server cluster, nics are usually identified through a unique 48 bit MAC address. MAC standing for media access control. If you have a router or a modem at home you can probably find the MAC address printed on the box. On a Unix-like machine, you can find your MAC address of your network interfaces with ifconfig than pipe it out to grep for ether. The MAC source and MAC destination have designated spots in the Ethernet frame. When the frame is sent through the medium, all the nodes will receive it. But only the one whose MAC address matches the frame destination should pay attention. The rest simply ignore it. Another important consequence of having several machines share the same medium is that two messages can't be sent at the same time, if they tried the data on the line would likely be corrected. We say that there is a collision when this happens. These collisions can have an important role in the performance of distributed systems where multiple machines, connected by a local network, are used to solve a problem. So understanding how and why these collisions occur is important for the study of operating systems. We'll discuss three solutions to the collision problem. The first is Carrier Sets Multiple Access with Collision Detection, or CSMA/CD, which was used with early ethernet. When a node sends a frame, it also measures the signal on the wire. If it finds what it sent, then great. If not, then it assumes another node was trying to send at the same time, causing a collision. Therefore, it sends a special jamming signal that helps ensure that all other nodes detect the collision. It then waits for an amount of time that is part exponentially based on the number of recent collisions it has detected but also random so that the nodes don't all just mirror each other and keep sending messages at the same time. Another solution to the collision problem is token ring or token bus. Here, the nodes in the network are arranged in a ring, either through physical connections or logically. They then continually pass around a token that is empty by default that determines whose turn it is. If this node here wants to send a message, he waits for the token, then sends out his message, and once he's received an acknowledgement, he then continues to pass the token on around. The next one wanting to send a message will grab the token, send out his message, wait for an acknowledgement, and then pass the token around some more. This system is fair, and it does not fail under heavy loads, but it suffers from more latency than the more aggressive CSMA/CD. Where possible, both traditional ethernet and token ring have been largely replaced by switched ethernet. A switch is physically connected to the nodes like the hub, but instead of broadcasting every frame that comes in like the hub it looks at the destination MAC address and routs the frames accordingly. The advantage is now that we have fewer collisions. For instance, two pairs of nodes can communicate simultaneously whereas this would have been impossible before. Naturally, a switch will need a table mapping MAC addresses to the physical port it should send the frame out to. One convenient way to populate this table is with learning. Whenever a switch sees a frame, it examines the MAC source and puts that MAC address along with the port number in the table. Then to figure out where to send it, it looks in the table and if it finds the appropriate port, great. If not, then it just broadcasts the ethernet frame to all the nodes. Once we run out of ports, we can begin to arrange switches in a hierarchy like so. In our top level switch, MAC addresses for this subtree will all map to port one. Those in this subtree will map to port two. And those in this subtree will map to port three. Lower down in this switch, the MAC addresses for these machines will map to their ports. And the MAC address for all these other machines in the network would map to this switch's uplink, since that's the path along which the frame would need to travel. Switches dramatically increase the number of messages that can be sent over a local network. In the simplest topology where all nodes are connected to a single switch, it's almost as good as having a dedicated link between each pair. The only way that server one wouldn't be able to send to server 1 is because server 2 itself is busy, never because the link is busy, and even when server 2 is busy the switch can cue up frames to be delivered later. Now, I have a question for you. The rows here, are the various collision avoidance strategies that we have discussed, and the columns, are the merits that these strategies might have. For each column, check the row, if you think that strategy, has the given merit. Let's go through these one by one. Under a light load, CSMA/CD will be efficient, because when a frame is ready, it will be sent. And then you shouldn't expect many collisions. The same holds for switched ethernet. Token ring on the other hand, is not efficient, as a node might have for the token to pass all the way around the ring before he can send his frame. CSMA/CD suffers under heavy load. As we are likely to see repeated collisions and long back offs, much of which will be wasted. Token Ring, on the other hand, will continue to operate as usual because of strict rules on whose turn it is to access the medium. Switched Ethernet is resilient because the collisions are either avoided altogether at the port routing or the switch can queue up messages and send them when the machine is ready. As to being fair, CSMA/CD is susceptible to hogging. From one node continues to succeed and sending its frames, while the others backoff longer and longer, because of the exponential backoff. Token is designed for fairness, that is basically everyone gets asked before someone can send another frame. Switched Ethernet is fair, because pretty much everyone gets what they want all the time Having covered how machines communicate across a local area network, we now turn our attention to how they communicate across the internet and the network layer of our hierarchy. Whereas other layers use a variety of technologies and protocols, the network layer really only uses IP, which is short for internet protocol. Every machine on the internet proper gets a unique 32 bit address. Which is usually written out as four decimal numbers between 0 and 255. So, for example, 72.14.248.239 is currently the IP address for Udacity's home page. Ranges of addresses are allocated by regional internet registries. All of which, are under control by IANA, the International Assign Numbers Authority. The range of themselves are commonly specified by a 32 bit IP followed by a slash, and then the number of bits understood to specify the network ID. This is referred to as CIDR notation. The first, number of bits of the 32, specify the range and then the rightmost bits specify the particular host. For example, MIT was allocated long ago the range, 18.0.0.0 slash 8. Meaning that any IP starting with 18 belongs to MIT. There are about 2 to the 24th of them. Georgia Tech has the range 130.207.0.0/16 meaning that any IP starting with 130.207 belongs to Georgia Tech. There are about 2 to the 16th of those because there are 16 unspecified bits in this range. At the time of this recording a New Jersey company called Linode has been allocated the address range 50.116.48.0/20. So this gives them 12 bits of host, and the range 50.116.48.0 to 50.116.63.255. Again about two to the 12th IPs. IPs have become a precious commodity these days, as most all of them have been allocated. And not very efficiently. For instance, tech savvy as they are, MIT is probably not using all 2 to the 24th, or 16 million of it's addresses. One solution to this problem is simply to expanded the address space to use more bits. The new internet protocol, called IPv6, the old one is call IPv4, uses 128 bit addresses. The problem is that IPV4 is so universal, and so many other systems depend on it, that adoption of the new protocol has been slow. And I encourage you to read more about this on your own. Now for a question to solidify our, our understanding of the CIDR notation. What is the highest IP in the range 130.58.0.0/17? Here's how I came up with the answer. Since the first 17 bits are fixed, that also means that the first 16 are fixed. So I can go ahead and write 130.58 in here. Now, there's one more bit that's fixed. So, the one that represents a 128 here must also be 0. So the biggest that I can make this eight bit address while sending that top bit to zero is a 127, so I'll write that. And then these bits are free, so the biggest I can make that is 255. Actually, this IP would never get assigned to a particular computer, because the highest in the range usually indicates broadcast. That is to say this IP means send the data to the whole subnet. Similarly, the lowest address doesn't get assigned either. This is used to refer to the subnet as a whole. One solution to this problem of the scarcity of IP addresses is Network Address Translation. This also helps mobile devices quickly join and exit Local Area Networks, or LANs, as would happen as you walk by a row of restaurants and coffee shops with your phone. To be concrete and simple, let's suppose that you have a combined modem/router connected to the internet in your home. Now within your home, you can create a private IP network using the network ID reserved for such a situation. Specifically anything that starts with 192.168. Typically, the modem/router will allocate IPs through a protocol called, Dynamic Host Configuration or DHCP. Let's say that your computer gets the address 192.168.1.100, and your printer gets 192.168.1.101. Your neighbor might also have a local area network with a private IP address space, and maybe his printer and computer get the same addresses. This turns out to be okay. Remember that the network layer is dependent on the link layer and everything ultimately get sent through the link layer protocol. So, let's say that your computer knows the IP address of the printer but not it's MAC address. To find it the computer broadcasts an address resolution protocol or ARP package to the whole linked network asking who has the IP address 192.168.1.101. The printer will respond with its MAC address and then we're good to go. Because the ARP is sent by broadcast over the link layer network, it never exits your private network in your home. So there's no possibility of your pictures ending up on your neighbor's printer or some such embarrassing circumstance. Of course, you want to be able to do more than just connect to other things around the house. You want to be able to connect to cool web sites like Udacity. Lets connect your modem to your internet and give it the IP 98.251.119.249. When you ask your computer to send a packet to Udacity's IP, your computer will not find the matching IP in it's routing table. So, it will send the packet to the default gateway, in this case your router. The router will also not find the IP in its table. It will really only know about the private IPs. And so it will forward the packet onto its default gateway on the wide area network, the internet. As a parenthetical note here I should say that packet is a term we use, the network layer, for discrete chunks of data that get passed around. Eventually the packet will reach the Udacity site. But it's not clear how Udacity should send the information back. It can't use the local IP address because there's no way to tell whether it should send information to your computer or to your neighbor's. So instead, your router actually swaps out your IP address for its own. This way, Udacity can just swap the source and IP addresses and then send that packet back along with the data. The modem then just needs to change the destination IP back to your computer, and then route it along the private network in your home. This is a little trickier, and involves changing parts of the transport layer which we'll discuss later. Okay. Here's a question for you. Is the IP packet the payload of the Ethernet frame, or is it visa versa? The answer is the ip packet is indeed the payload of the ethernet frame. The ethernet frame needs to be the outer container, so that the recipients of the frame on the network can know how to act. Remember, other computers on the network need to know whether they are the recipient and the ethernet switch needs to know how to wrap the frame to the proper recipient. Next, we turn our attention briefly to internet routing. That is, what hops a packet will take to get from one endpoint to the other. Let's say Georgia Tech student is curious about what is going on at the MIT Media Lab. Of course there is no direct link, so data is going to have to take several intermediate hops. The essential data structure here is a routing table that translates the IP address to the next hop i.e. the address the package should be sent to next. Every node on the internet should have a different routing table. There is much that is interesting in the shortest path like algorithms used to figure out what the next stop should be and also in the implementation of the routing table. Should it be a hash table? A tri? Etc. Here, however, I want to address the question of the size of the table. In a very naive approach, the routing table would require almost two to the 32 entries, one for each IP address on the internet. Fortunately this isn't necessary. Because IPs are largely allocated based on region, similar IPs will often take similar routes. For instance, consider traffic between Georgia Tech and MIT. As mentioned earlier, MIT owns the 18.0.0/8 address space. Meaning that every IP that starts with 18, needs to go to Boston and MIT campus. All the traffic from Georgia Tech to MIT will likely follow the same first hop or two, then only later split in Boston. Therefore, Georgia Tech routers don't need 16 million entries for MIT. One entry that matches anything starting with 18 suffices. All the traffic will be correctly routed up towards Boston. This principle can be applied more broadly. To keep the size of routing tables manageable. To see the routing table on your machine, run nestat-nr. Besides the need for routing tables, the most important thing to understand about the Internet is that it is not run by any single entity, but by a collection of thousands of autonomous systems that share information. Examples of these autonomous systems or AS's for short include Google, MIT, Georgia Tech, Comcast, and so forth. Each of these autonomous systems is responsible for routing traffic within itself. This is called intra-domain routing. And as you might imagine, there are some interesting algorithmic problems here, that have the shortest path-like character. You can explore this further by taking a networking class, or consulting the links in the instructor notes. Inter-domain routing, which is essential to connecting these often regional AS's together, gets a little more complex because of the various business interests among these entities. They don't share all of their routing information with each other, but they do advertise their ability to reach public IPs with a protocol called BGP, for Border Gateway Protocol. The essentials of such an advertisement are the IP address that they're advertising that they can reach. The Next Hop or the address of the entry point into the advertising AS. And the AS Path, which captures the sequence of AS's that a packet along the route would need to travel through. And AS receiving such a message would incorporate the information into its own routing tables, so that it knows where to forward packets. To take an example, suppose Comcast wanted to advertise my IP address. Then it would send an advertisement to its partners, which would include let's say a level 3. The AS path on the advertisement would just be Comcast's own AS number. And the next hop would be the desired entry point, or gateway, into the Comcast network. One of Comcast's partners, let's say level 3, then might want to advertise my IP address to say, the Apple AS. And so it would send another BGP packet, with my IP address, their gateway as the next hop, and the AS path of level 3 and then Comcast. That's where all this information has been incorporated into the routing tables. All the routers in the Apple network know how to send the data back to my machine. They have their own internal routing before sending it to layer 3, who does its own internal routing, before sending it to Comcast, who then does its own internal routing to get the message to my house. So far we have referred to machines on the internet soley by their IP address and then its largely how the machines address each other. But IP addresses are not very convienant for users like you and me. We much to prefer to use host names. The domain name system that makes this possible amounts to another layer of indirection within the network layer. The host name, like www.udacity.com, gets translated into an IP address, which is then used in the ways we've talked about. This translation is accomplished with the help of domain name services sprinkled throughout the internet. Suppose I try to send a packet to www.udacity.com from my computer. To get the most authoritative answer, my computer would then ask. The local DNS server, not too far away, probably on the Comcast network. This in turn would ask the root name server where it might find a .com name server which would help it with host names ending with .com. Having received this response, our DNS server would then ask this .com name server for information about udacity.com. When this IP comes back The server then asks where it might find www.udacity.com, and when it receives this answer, it then forwards it back to my computer. This should be somewhat reminiscent of how a directory system works. Importantly however, high levels in the tree are to the right of the host name. Hence. www.udacity.com is sort of like / com/udacity/www. Now, of course, we don't actually make all these requests every time we need to find the IP for www.udacity.com, my computer will cache the answer and so will the local name server, not necessarily for my sake. But for the sake of other clients on the same local server who might also need an IP for www.udacity.com, and they would cache the IPs for the root server, and for the dot com server as well. You can read more about how domain names are acquired and registered, and how the severs are kept up to date, in the links provided in the instructor notes. Now we turn our attention to the Transport layer, which sits between the Network layer and the Application layer. The key contribution of this layer, is the notion of ports, which tells the OS which process the data is intended for. On a Linux machine, you can see this mapping using LSOF-I. Having multiple ports allows more than one application to receive data from the network at once, something called multiplex. Port numbers are 16 bits long, and many of them are reserved for special purposes. Port 80, for instance, is typically the one used for receiving HTTP requests, like a browser would send. In fact, we'll use this as an example. Let's say we're running Chrome, and that we visit the Udacity homepage. Then Chrome will ask the operating system for a port number that it can use, or it'll pick one that has been allocated already. Let's say the port number is 55804. It sends, then sends the request in a packet with a source port of 55804 and a destination port of 80. When the Udacity server responds, it will send back a packet whose destination port matches the one that we sent. That way, the OS will know to route the packet to Chrome, and not to some other program. It is important to realize that the transport layer is really only active at the end points at the route. The intermediate routers on the internet need only look at the IP address to know where to forward the packet to. So they never examine the port number. Control starts at the Application layer, moves through the Transport layer and the Network layer and the Link layer and the Physical layer. At the first node. Then through these intermediate steps, of course, we need to interpret the signal. Pull off any sort of ethernet frame, or the like. And then look at the IP packet to figure out where the packet should go next. We never need to look inside the IP packet at whatever comes in the transport layer. And then forward it and the same thing happens in the next node, and then only when we receive it do we need to. Unpack the transport layer. Route it to the correct application, and then let the application interpret the data. To review the purpose of MAC addresses, IP addresses, and Ports, I want you to match the description on the right to one of the numbers on the left. Okay, here's the answer. Determines which process should receive a packet. Well that's the job of the port, so I'll write one here. Determines the last step on an Internet route. IP addresses are made for Internet routing. So the destination will indeed be the last stop. So I can write two here and then for the last one it is indeed the MAC address that determines which node should listen to data sent on a shared link. We are now in a better position to understand the details of Network Address Translation. The trick whereby one public IP address is able to serve many computers on a private network, be it a home, a business or a coffee shop that we discussed earlier. Recalling our earlier scenario, your combined modem/router has this public IP address and your computer has this private IP. When you send a packet to the Udacity site, the router switches out your IP address for its own. This way the Udacity site will know where to send the response. The router also however, changes the port number and the transport layer packet and it remembers the translation between. Your private IP and port number and this public port number that it sent as part of a message. This way, when Udacity sends a packet back, notice how he flips the destination and source IP's and the destination source ports. Your router then can change the IP and port number back so that the packet gets routed to your computer. Instead of to your printer, and to the right application. Like this. So the destination IP becomes yours and the port number becomes the same one that you originally sent, so the packet gets routed to the correct application. Ports are an important addition to network communication. But if that were all the transport layer did, the application layer would be left to cope with some common problems. Given the terrific complexity and scale of the internet, it's no surprise that packets occasionally get lost or delivered out of order. It sure would be nice to have some kind of acknowledgment that the packet got through, that way we could resend the packet if necessary and achieve more reliable communication. And for larger messages that need to be broken up into lots of smaller packets, it would be nice to have some kind of numbering system so that if they arrived out of order, they can be re-assembled. Moreover, we would like to know if we are overwhelming the recipient with too many packets too fast, or if we are causing congestion on some link in the route, so that we can be a good citizen and slow down. All of this functionality is provided by the transmission control protocol, or TCP inside the transport layer. TCP doesn't just start off firing packets towards some destination without warning. The conversation begins and ends with a polite handshake to mark the beginning and end of an exchange. Once the connection is established, data can go both ways from initial data to recipient and visa versa. Let's take a look at an example. I'll use some unrealistically small numbers to keep things simple. And I'll rename the initiator G T and the recipient udacity. At this point, the connection is pretty much symmetric. So the vocabulary for the handshake becomes confusing. GT issues a request and sets a push flag, meaning send me your data, then Udacity begins transmission back. Great, but what if one of the packets is dropped, that is to say they don't make it across the internet? In our example, let's suppose that the first packet doesn't make it. We would like a way to detect the situation. Here's TCP solution. After the initial handshake, packet or segments as they are called in TCP parlance, indicate how much data the other side should have received already, and also how much data the sender has received. The former is called the sequence number, and the later the acknowledgement number. During the handshake both of these numbers get incremented by one. In this scenario, TCP will be able to detect the dropped packet, because the next message thinks that nine bites is the amount that should have been received. But in reality GT has only received the initial acknowledgement. You might think that GT would send the packet saying, could you resend that please. But actually the system works by positive acknowledgements instead of request or re-transmission. If the packet had gone through, the traffic should have looks like this. So even though GT doesn't really have anything to say to Udacity, he should have been sending empty acknowledge packets to Udacity, indicating that the packets were received. To make an analogy to human conversation, this is like saying uh-huh periodically to let the other person know you are listening. Going back to the case where the first packet is dropped, we now see that upon receiving the second packet, GT would see that a packed has been dropped, and simply won't acknowledge the first one or the later one. After enough time has passed, Udacity will notice that it hasn't received an acknowledgment for that first packet and will re-transmit it. When GT receives the re-transmitted package, he can now send an acknowledgment for both packets, saying that he has received 17 bytes worth of data. Together, this system of acknowledgments and sequence numbers helps achieve the first two goals. I'll have to refer you to a networking class for a more complete discussion of the third goal, but I do want to briefly discuss the idea of window size. In this example, udacity has sometimes sent out a packet before it receives an acknowledgement of a previous one. This is typical for TCP And how much one side is allowed to get ahead of himself in the conversation is controlled by a window size parameter. A good way to visualize this, is by drawing the data that needs to be sent as a long bar and then dividing it into the packets that actually get sent. We have the packets that have been sent and acknowledged, the ones that have been sent but haven't been acknowledged yet, and the unsent packets. This middle part, representing the packets sent out but not acknowledged, represent the window. And as the, during transmission, the window will slide across to the right. The window size puts a limit on how a wide this window can get. If the window is too small, then it can slow the connection down as the sender has to stop and wait for an acknowledgement for every packet. If the window size is too big, then there's a risk that it will over flaw the buffers of the recipient, or of one of the hops along the way resulting in packet loss. So it's important to get the window size right. You can explore other aspects of TCP's flow and congestion control by following the links in the instructor notes. Now TCP serves many applications well, but others want to handle the problems of liability and flow control in their own way. For this, they often use the User Datagram Protocol or UDP, which is much simpler. The header that gets attached to the payload includes only the source and destination ports, the length of the data, and a check sum to help detect data corruption. There's no initial handshake or enforced mechanism for the order of packets, they're just fired across the network. When is UDP used? Well, if you have a very reliable local network, then there's no need for the reliability or out-of-order protections of TCP. All those acknowledgements would just be wasted bandwidth. You would rather have the lower overhead of the smaller UDP header and protocol. Streaming applications, where it is more important to be on time than to be right, prefer UDP. Think of voice over IP. A user would much prefer a temporary degradation in the quality of sound to incurring a delay, which makes conversation difficult. If a packet is dropped, we might want to simply say that it is too late and not bother trying to incorporate its data. Similarly for video, a user would much prefer an occasional low quality frame to having the video intermittently start and stop, but with a perfect picture. UDP tends to be more popular for these types of applications. Based on the description of UDP and TCP given so far, check the box if the parameter is included in the Protocol's Header. Recall that UDP is really bare bones, of these it only includes the port numbers. TCP on the other hand is deluxe. It includes information about the desired window size, and also uses sequence and acknowledgment numbers to detect drop packets and out of order delivery. And of course it includes the port information as well. Now to put everything current in this lesson together. Let's see how a browser running in your home computer, might establish a TCP connection to the Udacity website. And send along an HTTP request for the home page. Along the way, I'll ask you to help figure out the next step, a few times. First the browser needs to find Udacity's IP address. So it will make a system call. The OS we'll use, well what will it use to convert the host name www.udacity.com to an IP Address? Give the three letter acronym here. And the answer is, DNS, for Domain Name System. Okay, so let's suppose that the domain name service says that 72.14.248.239 is an IP we can use for Udacity. Great. Now the OS is ready to establish the connection with Udacity. The actual application data will be empty at this point, so I'll just leave this slot empty. This gets the TCP and header attached to it, which includes the port numbers. And this then gets the IP header attached to it, which will include the source and destination IP addresses. The routing table on my machine will point me toward the IP address of my modem/router. If my computer doesn't have the MAC address for the router cached, it could acquire it through an ARP request. But this MAC address It is now ready to put the IP packet in an ethernet frame, and send it over the wire to the modem/router. When this router receives the packet, he unpacks it from his ethernet frame. Looking at the destination IP, he recognizes that this packet is meant for the wide area network. So he strips off the source IP and the source port, and replaces them with his own IP. And a new port number. This changing of the IP address and port number, that the router does, is called what? There's a three letter acronym for this too. Network address translation, or NAT, is what the router is doing here. He is allowing all of the machines on the private home network to share the public IP address to this kind of translation. Next in our scenario, the modem wraps the packet in the appropriate link layer frame and passes it on to the wide area network. The packet will then make several hops across the Internet. In each phase, the IP packet will be wrapped in a link layer and sent on. A routing table will be consulted to figure out where the packet should go to next, and then the packet will be wrapped up again in the appropriate link layer protocol before being sent along again. Eventually the data will reach the Udacity site. Udacity will say, of course I'm happy to establish a TCP connection. So, it will send an acknowledgement. And my question to you is, what is the destination port for this acknowledgment packet. The answer is 61208. The old source port becomes the new destination port. So to give a response then, you'd actually just swap the source and destination ports, and the source and destination IP addresses. The path back will feel just like the path forward, though it's possible that it will follow a different set of hops. And of course it will be in the reverse. When the packet gets back, to the mo, router at your house, it will swap out the IP and port for the correct ones as part of the network address translation. It's routing table will then tell it the MAC address of your computer, so that it can be put in a packet in a link layer frame and passed back to your computer. Now, to complete the handshake, and establish the TCP connection, your computer needs to send back an acknowledgement to Udacity. This will be almost exactly the same as the first packet with just a small change to the TCP header. Indeed all the communication will feel much like this, only instead of sending packets with no contents, Udacity will send each other real data, with the TCP headers, IP headers, and then sometimes on and off during the transmission, the link layer headers and footers as well. Congratulations. If you followed that last example or even better, were able to anticipate the next step, you now really know how the Internet works. No small achievement. With this knowledge, you'll be able to understand strategies for congestion control, giant scale services, and content delivery neworks. All of which are becoming increasingly important, in the world of computing. Good luck to you.