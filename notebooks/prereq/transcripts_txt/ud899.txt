When the user opens Wittr, we want to start by showing the latest post the device received, before going to the network. Then we make the web socket connection, and we start receiving new posts one by one. When we receive these posts, we want to display them of course. But also, we want to add them to the set of posts we already have stored. We also want to remove entries that are too old to be worth keeping. A database is the best model for this. It means we can add and remove individual posts as needed, iterate over them, and query the data. The web platform has a database called index DB and it's fair to say it has a bit of a bad reputation, but hopefully we can break through that in this chapter. And we're not going to touch our service workers' script for a bit, we're going to concentrate on learning IDB sharpen how it ticks. Then we'll integrate it into a whole offline first experience. If you've used NoSQL databases before, IDB will seem familiar. If you've only used relational databases before, this might seem a little weird. But if you've never used databases before, well, that might even help. As you can come to index DB fresh, without trying to translate it to similar but ultimately different things you've encountered. First up, here's the shape of indexedDB Databases. With IndexedDB, you can have multiple databases with whatever name you give them. But we're only going to be creating one. Generally you'll only have one database per app. That database contains multiple objects stores, generally one for each kind of thing you want to store. An object store contains multiple values. These can be JavaScript objects, strings, numbers, dates, or arrays. Items in the object's store can have a separate primary key or you can assign a property of the values to be the key. The key must be unique within an object store. It becomes the way to identify a particular object. Later you can get, set, add, remove, iterate over items in object stores as part of a transaction. All read or write operations in IndexDB must be part of a transaction. This means that if you create a transaction for a series of steps and one of the actions fail, none of them are applied. The state of the database would be as if none of the steps happened. You can also create indexes within an object store, which provides a different view of the same data ordered by particular properties. The model here is similar to a lot of databases which makes a lot of sense. The browser support is good as well with every major browser supporting it. So why does IndexDB have such a bad reputation? Unfortunately the API is a little horrid, and often creates spaghetti code. It's all asynchronous which is fine, but it predates promises. So it kind of invented its own event-based promise system which creates really confusing code. Now, I'm a true believer in teaching the web platform rather than libraries, but IDBs APIs is so clumsy, I'm going to chicken out of doing that. We're going to use IndexDB Promised. This is a really small library that mirrors the IndexDB API but uses promises rather than events. But other than that, it's the same as IndexedDB. So anything you learn using this library, you can apply to using IndexedDB directly. In fact if you want to spoil your day really early, feel free to avoid the library and write IndexedDB code directly. But all the examples will be using the library. In the next chapter we'll take a look at how you'd create a real life index database in the browser. Okay, in this chapter you're going to get a crash course in IndexedDB. So hold on tight. Let's get through this as quickly as possible, and hopefully in one piece. I'm going to start by going to the browser and opening idb-test. This is just a blank page with a script tag. This script is in public, JS, idb-test, index.js. All it contains at the moment is an import for the IDB library we saw before. So okay then, let's create a database. We can see from the library's code that the function signature is .open, and then the name of the database, the version, then a callback to set the database up. So let's do that. I'm going to call idb.open, then name the DB test-db, version one, and pass in my setup callback. This function will be called if the browser hasn't heard about this database before. Or if the version it knows about is less than this number here. This function gets a special upgradeDb parameter which we used to define the database. To ensure the database integrity, this is the only place you can create and remove objects stores and indexes. The API for upgradeDb is in the library. But it's mostly just a mirror of the real IndexedDB API, except for a few minor differences which we'll cover. The docs tell me createObjectStore is the same as IDB. So MDN can give us the full story. And there's the full detail of the API. So I'll create an object store called keyVal. Because we haven't told it otherwise, this store has a key that's separate to the data, which is what we want for a keyValStore. I'm going to add some content here, too. We can see from the library docs that an object store has these methods which behave the same as IDB, except they return a promise. This is the thing that makes the library way more usable than plain IDB. So I want to put an item into the store. So, let's look at the docs for that. MDN shows as .put takes an item and a key, and returns a request. These request objects are the thing that makes IDB really difficult to deal with. So whenever IDB would return a request, the library returns a promise. And yes, the signature here is value key rather than key value. Which is a real gotcha, but you'll see why it's like that later on. In fact, most of the IDB API is stupid, but for sensible reasons. So I'm going to put the value world, and set the key to be hello. With that, we've finished setting our database up. And .open returns a promise that resolves with a database object. So we'll store that for later. We can use that database object to get and set items in the database. But for now, if we refresh the page and run the code. We can pop over to the resources panel in dev tools and yeah, and right click and refresh. And there we can see our database. We can see the object store. And we can see the value inside. So now, if I want to read from the database, I need to create a transaction. The function to do this is db.transaction. Passing any object stores, I'm going to be using just keyval. Then I call object store, passing in the name of the object store I want, keyval. This seems a bit repetitive but it's possible to have a transaction that uses multiple objects stores. I called .get on the object store, passing in the key I'm interested in, hello. This returns a promise, which resolves to the value I'm looking for. Which I'll log. So over in the browser, if I hit refresh, there's the console log. But say, I want to add another value to the object store. To do that, I create a transaction just as I did before but I specify that I want to read write this time. Now, I can get my objects stores before but this time, call .put on on it to set a value. I'm going to set the key food to be value bar, .put returns a promise but this promise doesn't necessarily mean the operation worked. As we saw, if the transaction fails. All the operations are part of it are undone. This means that you can do a lot of work in a transaction and be sure that it won't be left or accessed in some kind of half finished state. Either all happens or none of it happens. Transaction.complete is a promise that fulfills if and when the transaction completes, and it rejects if it fails. Once the transaction completes, I'm going to logout a success message. Now I can refresh the browser and see the console log. And in the resources panel, there it is. There's the value we set. Now, I wanted to talk a bit more about IDB features, but it looks like we're getting rudely interrupted by a certain, Mike Wales. Mike, what do you want? You've been talking for quite a while there. I think it's time we let the student have a go. Okay. Yeah. I think you're right. Take it away, Mike. If you completed the previous task, your copy of the project is in the right shape to take on this task. Otherwise, you can run git reset --hard, to remove any local changes you have. Then, git checkout page-skeleton to get up to speed. As you saw, weâ€™re working in different script for this exercise. So head over to public/js/idb-test/index.js. Here, you can see all the code that Jake was writing. And you can see it working in the browser in local host port 8888/idb-test, by working, I mean you get a blank screen. But also a couple of messages in the console. And you'll see the database appearing in dev tools. You've already seen how to add and remove items from the key val object store, but your task is to set the favorite animal key to the name of your favorite animal, whatever that is. Once you've changed the code, refresh the page and check dev tools to see if your favorite animal has appeared in IDB. To double check everything is working, head to the settings page. And type idb-animal into the test field and press enter. So Jake, what's your favorite animal? Well, let me show you. I'm going to create a read write transaction with access to the keyval store. Then I'm going to get the store and call.put on it. Now I know it's a cliche these days, but cats are actually my favorite. So I'm going to pass in cats as the value and favorite animal as the key. I'm going to wait on completion of this transaction. Then log out a little confirmation message. Over in the browser, let's refresh. And there's the Console Log and over in resources, there it is as well. Okay, so if you're still alive at this point, you're doing really well but take my hand as we dive deeper into this API, it's time for another crash course. So far we've created a key value objects store but what if I wanted to create a different store with objects all of the same kind? People, for instance. To do that, I'll need to create another object store, as I mentioned earlier, to maintain database integrity. You can only create objects stores and indexes within the upgrade function. But this upgrade function isn't getting called, because it's already been called for version one of the database. So, I'm going to start by bumping the version up to 2. Then I'm going to create a new object store called People. And this isn't going to have separate keys. I'm going to make the name property of the objects inside the stall be the key. Okay, sure in the real world you can have more than one person with the same name. But we're going to assume names are unique for the sake of this demo. Now we've actually created a bug here. And we can see it if we try loading the page. We get all of these nasty errors. Because the browser hasn't seen version 2 yet, it will call this function, and unfortunately it will fail. This is because the browser hits this line. It tries to create the key file store, and errors because it's already created it. Thankfully the upgradeDb object has this old version property telling us the version the browser already knows about. We can use a switch statement to make the browser skip the bits it already knows about. So we switch on the old version number, if it's 0, we set up the key file store. If it's 1, we set up the people store. Usually with switch statements you have a break after each case, but we specifically don't want to do that here. So with this code, if the browser hasn't set up this database at all, it'll start here. Create the key file objects store but it will continue and create the person object store. If the browser already has version 1, it'll start here and only set up the new person object store. If we refresh the browser now, the errors are gone. And the store has appeared in the resources panel. Let's put some stuff in it. Now we have this object store set up, I'm going to create a transaction for the person store and I want it to be read write. Then I get the object store and I'm going to add a person to it including the name, age, and favorite animal. I'm really having to fight spelling favorite the British way here so I hope you appreciate that. Anyway, in this model a person is just a JavaScript object. Notice that I'm calling put but I'm not providing a key this time. This is because we've told the store to treat the name property as the key. I'm going to wait on the transaction completing and logout a little success message. One person isn't enough of course. So here are a few more that I prepared earlier. If I refresh the browser we can see in the console the operation completed. But also in the resources panel, you can see all the people. If I want to read the people in the store, I create a transaction for people. I get hold of the object store and I call get all which returns a promise for all the objects in the store. And then I'll log those out. If we run that code in a browser we can see all the people are logged in alphabetical order of their name since that's the key. But what if I wanted to group them together by their favorite animal. This is where indexes come in. Like other modifications to the database, indexes can only be created as part of a version upgrade. So we bump the version number and start adding to our switch clause. Now we're going to add an index to the person store. And at this point, we need to get ahold of the person object store. In other places we've called transaction.objectStore to do this. And there's no exception here. Database upgrades have their own transaction object found @ upgradeDB transaction. So now we have to store. I'm going to create an index called animal. Which sorts by the favorite animal property. To use this index, I'm going to modify this code. Where I'm listing all the people. I'm going to get the index from the object store by name, animal. An index has a very similar API to the object store. So instead of calling get all in the store, I'm going to call it on the index. If I refresh the browser, the people are logged out. But this time, all the cat people are logged first, and then the dog people. In def tools we can see the index as well. I can even execute queries on the index. For instance, I'm going to pass cats into get all just to return the cat people. If I refresh the browser, there they are, just the two people who love cats. And I think there's a task coming your way as the micphone is ringing. Hey, mike. Got some IDB for the student to do. I certainly do. Brilliant, take it away. In this task we're going to make changes to the code Jake just wrote. So you want to get your project into the same shape. You can run get reset hard to remove any local changes you have. Then run get checkout task IDP people, to get the project into the same shape as Jake's. Once again, we're editing the IDB script so head over to public/js/idbtest/index.js. We just saw Jake creating an index to sort people by favorite animal. But your job is to create an index where they're ordered by age. So in the upgrade function, you'll need to create your index. Then at the bottom of the script, write some code to log out all the people in that order. Once you're done, refresh the browser. You should see the names log out in order. You can also confirm in dev tools. To double check everything is working, head to the settings page and type IDB age into the test field then press enter. Jake, did you manage that one? I did. And here's how. I'm going to bump the version number of the database and add a new section to the switch statement. Then I'll get the people object store just as I did before. I'm going to add a new index called age that sorts by the age property. Getting people by age is almost the same as getting people by animal as we did before. I create a transaction. Get the object store and get the index. Now I called get all on it and log out the results. If I run that in the browser, I can see all the people being logged, but this time, ordered by age. Also as confirmation, the index appears in the resources panel. Right this is it, the final bit of the IDB crash course. Deep breath, here we go. At the moment, we're getting all the items out of the store. But we can actually go through them one at a time using cursors. Taking the age code from above, rather than calling get all, I'm going to open a cursor. This returns a promise for a cursor objects representing the first item in the index or undefined if there isn't one. So if it's undefined, I'm just going to return. Otherwise, I'll log it. The first person in the index is in cursor.value. Then I'll call cursor.continue to move on to the next item. This returns a promise for a cursor representing the next item or undefined if there isn't one. Now here's a neat trick. I'm going to name the function we're in log person. And Iâ€™m going to call at once cursor.continue resolves. This sets up a kind of asynchronous loop until cursor is undefined. Meaning we're at the end of the list. So when we hit this next step in the promise chain, we'll have gone through the whole object stall. If I refresh the browser, there's all the people being logged. At the moment this is just a complicated way of calling getAll. But cursors become really useful when you want to modify items as you're looping through. You can use cursor.update to change the value or cursor.delete to remove it. You can also skip some items. Here I'm going to call cursor.advance and pass 2 in to skip the first two items. Now over in the browser if I hit refresh, yep, we just get the last two logged. Okay, you've just taken in the majority of the index DV API in a very short space of time, but in next chapter, will put some of that knowledge into practice on Witter itself. Don't worry if you can't remember all of the syntax we've gone through. You can always come back to this video and check out the instructor's notes for links to the IDb API and the promise base Shim that we're going to be using. Or better still play around with the code I've just written and use it as a reference. It covers all of the patterns we'll be using in the rest of the course. You can check out this code by running git reset hard to remove any local changes you have. And then git check out idb cursoring get the latest code. Once you get to grips with IDB, you may even start to like it, that's what happened to me. But I can't decide if that's a good thing or if I just have Stockholm syndrome or something. Anyway, in the next chapter we'll create a database for Witter posts. The plan here is to create a database for Wittr that stores these posts. When Wittr loads via a service worker, it does so without going to the network. It fetches the page skeleton and assets straight from the cache. Now, at the moment, we have to go to the network for posts. We're going to change that. Instead, we'll get posts from the database and display them. Meaning, we're now showing post content before we go to the network. Then we'll connect the web socket to get updated posts. As shown here, web sockets bypass both the service worker and the HTTP cache. Then as new posts arrive, we'll add them to our database for next time. First thing we're going to do is populate the database. We'll deal with displaying its contents later. If we take a look in public > js > main, IndexController.js, we can see a method is called to open the web socket. And in that method, we can see a listener for the message event. And that hands off to onSocketMessage, passing in the data it received. OnSocketMessage parses the data with JSON, then passes it to addPosts. Let's take a look at that data by putting a console.log here. Over in the browser, I'm going to check Force updates on page load so I can get that JavaScript change straight away. And there are the logs. We can see that posts have an ID, a time property, data on avatar, and a message body. We want to store these objects straight into IndexedDB. There's an obvious primary key here, id. And we'll want to be able to display them in date order, so we'll need to create an index on time. And it looks like I get to take the next few minutes off, because the Mike alarm is ringing. Mike, one for the student to do? Yep, it sure is. Let's dive in. In this task, you're going to set up a database for Wittr, and add posts into it. Some of the methods have been stubbed out for you, so you want to check out that code. To do that run, get reset hard to remove any local changes you have. Then, get check out task-idb-store, to get the stubbed methods. We're back to editing Wittr, so ahead over to Public/js/main/indexcontroler.js. In the constructor we're creating a promise for our database by calling openDatabase. This method is incomplete, your task is to return a promise for a database called wittr that has a objectStore called wittrs that uses id as its key, and has an index called by-date, which is sorted by the time property. The idb polyfill has been imported at the top of this script, so it's ready to use. Once you've done that, you need to add messages to the database. Down in _onSocketMessage, the database has been fetched. Your task, is to add each of the messages to the Wittr store. Note that we're not using the entries in the database yet, we'll do that in the next chapter. This is a lot to cover, but you've done most of this already earlier in the course. The code in public/js/idbtest/index.js should act as a good reference. Also, if your database gets into a bad state, go into dev tools and run indexedDB.delte-database, and pass it a parameter of Wittr. This will remove the database and let you start afresh. Once you've created the database, refresh Wittr in the browser to run your code. Developing gets a lot easier if you have developer tools open and force update on reload checked. So you only need to refresh once to see changes. If everything's working, you should see the Wittr database and dev tools. A Wittr's object stored, a by date index, and some messages. To double check everything is working, head to the settings page, and type idb-store into the test field, then press enter. Jake did you work this one out? Yeah, I did. I'll walk you through it. To open the database I'm going to call idb.open. Passing in Witter is the name, one is the version number. And now I'm going to write my upgrade function to set it all up. Since I'm on version one, I'm not going to do the whole switch statement thing. I'll introduce a version two if I have to. To set up the database, I'm going to create an object store named witters with a key path of id. That means it will treat the id property of objects in the store as the primary key. I'm also going to create an index on the store called by-date which indexes on the time property of stored objects. Over in the browser, I'm going to use force update, reload the page, and now in the resources panel, I can see the new witters database, along with the witters store and the by-date index. Now, we need to get items into the store. I'm going to start by creating a read write transaction for the witters store. Then I'll get hold of the store. And then loop through these messages and put each one into the store. Back to the browser, I reload the page and then the resources panel, I can see messages in the database. So we've got stuff going into the database now, but stockpiling these isn't really useful. We need to show them. We want to get posts that are in the database and display them before connecting to the web socket that gets us newer posts. And lucky you, I think you're going to take it from here because we have an incoming call from Mike. Think you're up to this one? Definitely. You should now have stuff in the database. But we're going to get it on screen. Some of the methods have been stubbed out for you, so you want to check out that code. To do that, run git reset --hard to remove any local changes you have. Then git checkout task-show-stored to get the stubbed methods. Refresh the wittr page, this should populate the database with posts which you can verify in dev tools. Once you've got post in the database, head over to public/JS/main/ index controller.js. Previously, we were calling _opensocket in the constructor. But now we're calling _showCachedMessages, then opening the socket. _ShowCachedMessages isn't fully implemented yet. That's your job. Get the messages out of the database and pass them to this method. Index controller._postview.add post. Make sure they're in date descending order which may not be the order they come out of the database. Once you've made changes to your code, you need to bump the cached version number in the service worker so the changes are picked up. Also if your database gets into a bad state, go into dev tools and run indexDB.deletedatabase and pass the parameter witter. This will remove the database and let you start afresh. If everything's working, you should see posts being displayed even when you load the page offline. To double check everything is working, head to the settings page and type IDB-show into the test field and press enter. Jake, did you sort this one out? Yep, and here is what I did. Over in showCachedMessages I start by creating a transaction for the Wittrs store. Then I get the objectStore. Then I get the by-date index. I use getAll, which returns a promise for everything in the index. And I pass the results to addPosts. However, the messages here will be in date-ascending order by default. So, I reverse them so the latest post appears at the top. I want to see how this update works the natural way. So I'm going to bump the service worker cache version. Then over in the browser, I'm going to disable Force update. Then I'll refresh the page. There's our update. We'll take it. And now, if I take the server offline and refresh the page, we get the last post we saw. So things are looking great. But at the moment, we're only adding things to a database. At some point, you'll end up with thousands and thousands of items in there. And the browser's going to go, hey, you know what? I've had enough of you doing that. You're not allowed any more storage. So we need to make sure we're only keeping what we need. And it looks like this is going to be your job. Mike? This is kind of a tricky one. Yeah, but they've played with all the commands and tools. They can do this. In this task, you're going to ensure the database only has 30 Witter items in it at any single time. Some of the methods have been subbed out for you, so you want to check out that code. To do that, run git reset --hard to remove any local changes you have. Then git checkout task-clean-db to get the stubbed methods. Now head over to public/js/main/IndexController.js. Down in underscore on socket message, we're already adding items into the database. But after that, you need to ensure there are no more than 30 in the object store. The code in public/js/idbtext/index.j.s should act as a good reference. Again, if your database gets into a bad state, go into dev tools and run indexedDB.deleteDatabase and pass at the parameter wittr. This will remove the database and let you start fresh. Once you put the code in place, refresh wittr in the browser to run your code. I recommend shift refreshing to bypass service worker until you've got everything working. Then bump the version of the service worker static cache to pick up your changes. If everything's working, you should see the wittr database in dev tools. And in there, you should see no more than 30 messages. To double check everything is working, head to the settings page. And type idb-clean into the test field. And press Enter. Jake, did you figure this one out? I absolutely did. Here goes. We can see the problem here. I've had Wizard running for a while now, and I've got almost four hundred messages in the database. The code for dealing with this is a little bit tricky so, well done if you did it. Over in on socket message, after adding that into the database, I'm going to continue using the same transaction and the same object store from above. I'm going to get the by-date index, because I want to remove the oldest posts, and I'm going to open a cursor, but pass a null and prev, so the cursor goes backwards for the index. Starting with the newest post. I don't care about the first 30 posts, those are the newest ones, they can stay. I'm going to advance past them. With the posts after that, well if the cursor is undefined, we're done. Overwise I'm going to delete the entry, then continue the cursor Calling the same function again to loop through the remaining entries. This keeps the news 30 post, but deletes the rest. Now i can reload the page using force reload. And in the resources panel, I've got the latest 30 posts, but no more. This is something we want to ship, so I bump the static cache of the service worker, disable force update, refresh the page, and there's our update. Things are looking great. We've absolutely covered continual update of pages and this is huge. Once again, this is something we could launch. Here's what we've achieved. Things haven't changed much when the connections perfect, but yeah, perfect doesn't really exist. On a slow connection we get to render content, much quicker, way quicker in fact. Life our users get content rather than an infinite white screen. Even offline users get content. Although, as I'm sure you've noticed, things are still a little slow, or broken when it comes to images. That's all we've got left to fix, then we're done. You're on the home straight now, just those images, and we have a full offline first experience. Some of the posts on which I have photos along with them, we want to cache those too. At the moment we're only caching resources at install time. Whereas appear over the lifetime of the app with the posts. So we want to cache photos as they appear. We could put these photos in IDB along with the rest of the post data. But that means we need to read the pixel data and convert it into a blob that's kind of complicated. It also loses streaming, which has a performance impact. When we get an item from a database, we have to take the whole thing out in one lump, then convert it into image data, then add it to the page. Whereas if we get the image from a cache, it will stream the data. So we don't need to wait for the whole thing before we display anything. This is more memory efficient and leads to faster renders, even if the data is coming from the disk. For that reason, the cache API is a much better fit. But since we're into the advanced stages of the course, I haven't made this totally straightforward. Here's the code for the image, it's a responsive image, because images can appear at a variety of different widths, this responsive image lets the browser decide which image to load based on the width of the window and also the network conditions. So when the post error arrives through the web socket, which version do we cache? Well we wait until the browser makes the request. Then we hear about it in the service worker. We go to the network for the image and once we get a response, we put it in the cache. But the same time we send it on to the page. Note that we put the image into a separate cache to the rest of the static content. We reset the content of our static cache whenever we update our JavaScript or CSS, but we want these photos to live between versions of our app. Next time we get a request for an image that we already have cached, we simply return it. But here's the trick, we'll return image from the cache even if the browser requests a different size of the same image. Post on are short lived, so if the browser requests a bigger version of the same image returning a smaller one from the cache isn't really a problem. Returning a bigger image then the one the browns asked for that's perfectly fine too We're not wasting bandwidth by doing that. In fact, getting a smaller version of something we already have cached, that would be a waste of bandwidth. Also this resizing windows back and forth, it's only really something web developers do. We've covered most of the APIs you need to be able to cache images, there's only one thing left to cover. You can only use the body of a response, once. As in, if you read the responses json, you cannot then read it as a blob, this is because the original data has gone, keeping it in memory would be a waste. Also respondWith uses the body of the response as well. So you cannot later read it again. In most cases, this is great because if the response was like a free gigabyte video going to a video element on the page, the browser doesn't need to keep the whole free gigabytes in memory. It only needs to keep the bit, it's currently playing plus a little bit extra for buffering. However, this is a problem for our photos, we want to open a cache, fetch from the network, and send the response to both the cache and back to the browser. Using the body twice like this doesn't work. But we can fix this by cloning the response we send to the cache. So now a clone goes to the cache, and the original gets sent back to the page. The browser keeps enough of the original request around to satisfy all of the clones. So, let's start coding this up. First thing we need is to set up our image cache in the service worker. I'm going to create a variable to hold the name of this new image cache. And I'm going to create an array to hold all the cache names we care about. In our activate event that we wrote earlier, we're deleting any cache that isn't the static cache. That isn't good enough anymore as we start losing our image cache. Instead, we want to delete any caches that aren't in our array of caches that we care about. Right. Now, to handle those photo requests. Over in our fetch handler, I'm going to handle URLs that have the same origin and have a path that starts with slash photo slash. When I see one of those, I'm going to respond with whatever returns from serve photo. All we need to do now is implement serve Photo. I'm only wanting to store one copy of each photo, and photo URLs look like this. They have width information at the end. So, I'm going to create a storage URL that doesn't have the size info. I'm going to do that using a regular expression matching on dash, some digits and then px.jpg. And I'm going to replace that with nothing. Now, I have the URL, but missing the size-specific stuff. This is the URL I'm going to use in the cache. And I reckon you can handle the rest. Mike? Come in, Mike. You rang? Yes, we need to cache these photos in. I figured the student could manage this. I agree. Let's do this. To start, you'll want to catch up with the code Jake just wrote. Do this by running Git reset hard to remove any local changes you have. Then, git checkout task-cache-photos to get the stubbed methods. Now, head over to public/js/sw/index.js. The service worker script. Your task is to implement serve photo. The aim is to serve photos from the cache if they're there. Otherwise, get them from the network, but put them into the cache for the next time. Remember to use storageURL when matching and putting stuff into the image cache so you only end up with one photo in the cache no matter how many different sizes are requested. Once again, developing gets a lot easier if you have Developer Tools open and Force Update on reload checked so you only need to refresh once to see changes. You know things are working when you see a Wittr content images cache in dev tools where the URLs are missing the width and the ,jpg extension at the end. You should be able to take the server offline, reload the page, and still get images. To double check everything is working, head to the settings page and type cache-photos into the test field and press enter. Jay show us how you did this one. Sure thing, it went a little like this. So first I'm going to open the cache for our images. Once I've got the cache, I'll look for a match for storageUrl. If there is one I'll return it, done. Otherwise, I'm going to fetch the image from the network using the fetch API. Once I get the response, I'll add it to the cache using the storageUrl and a clone at the response. Then I'll return the original response to the browser. Remember, you can only read a response's body once. This is why we have one copy going to the cache and the original going to the browser. Now I'm going to use Force update and reload the page. If we check in the Resources panel in dev tools, yep, we can see the new cache and it has images in it. If I resize this panel so we can see the whole URL, you can see the URL ends before the bit with the pixel width, confirming the cache URLs aren't site specific. Let's test the serving. We'll bring the server down, go offline, and reload the page. And the images are still loading but here's the ultimate test. If I change the width of the browser and reload, yep, the images still load. This means the service worker is returning the image we have cached no matter which width of image the browser requests. Caching the photos is great, but like we saw with the database earlier, we can't just keep adding stuff. We need to remove stuff that we don't need anymore. This involves a couple of cache methods that we haven't seen yet. If we want to remove specific entries from the cache, we can use cache.delete, passing in the URL or the request of the thing we want to delete. There's also cache.keys. That returns a promise that gives us all the requests for entries in the cache. What I haven't mentioned so far, is all of this is available from pages as well as service workers. In the next chapter, we'll use these methods to clean the image cache. Let's keep our image cache under control. We're going to do this from a index controller in public JS mainIndexControl.js. I'm going to start by creating a new method called cleanImageCache. When the page loads it starts the controller so in there I'm going to call a new cleanImageCache method but that means the cache can still go out of control If the user keeps the page open for ages. So will also call it every five minutes. Now we need to do is implement this new clean image cache method. Implementing clean image cache is going to bring together index TB and the cache API. A combination of things we've learned in this course. And we've got a Mike on the line. So Mike it looks like this is the end of course boss fight. Yeah. This is a toughy but I bet they can do it. To start you want to catch up with the code Jake just wrote. Do this by running git reset --hard to remove any local changes you have. Then, git checkout task-clean-photos to get the stubbed methods. Now head over to public/js/main/indexcontroller.js, the main page script. Your task is to implement _cleanImageCache. It involves getting all the messages from the database, looking at what photos they need, then going through the images cache and getting rid of ones you don't need any more. Remember that the photo's property may not exactly match the URL in the cache. Once you've written the code, head over to the browser and refresh the page using the force update on reload option. So you only need to refresh once to see changes. Looking at dev tools, you should see the cache only contains images that are on the page. An easy way to test is to open a new tab and navigate to the URL here. We can see in the cache that this image has been stored. Now when I reload witter the cleanup code kicks in and the image is gone from the cache. To double check everything is working, head to the Settings page and type cache-clean into the test field and press Enter. You now have eight seconds to trigger your cleanup code. The best way to do this is to just refresh the app. Jay, how did you implement this? Well, it went a little like this. We can see the problem building up. I already have over 60 photos cashed to solve this. I'm going to start by creating a ray of images that I want to keep, then I'll create a transaction. To look at the witches objects store. Then I'll get the object store and get all the messages. Now we can take a peek into the database. For each message, I'll look to see if it has a photo property. This contains the photo URL, but without the width bit at the end. So I'll add those to the array of images that I want to keep. Then I'll open the images cache and get all the requests that are stored in it using cache.keys. Now, the URLs on request objects are absolute. So they'll include the local host port 8080 bit, whereas the URLs was storing an index Db, they don't have that. So for each request I'm going to pass as URL. So now if the path name of the URL isn't in our array of images needed, I'll pass a request to cache.delete. So now with force update enabled, I'll refresh the page. And I've only got seven images cached. Job done! We're caching photos now, we're so very nearly there. The last thing we need to deal with is avatars. One more chapter before we achieve total offline first. Compared to what you've done so far, this last bit should be a breeze. We need to cache the avatars. This is almost the same as caching the photos but there's one small difference. Some people like to change their avatar a lot and we don't want people to be stuck with some old version. So when we fetch a particular avatar from the cache, we'll also fetch it from the network and update the cache. Avatars are also responsive images, but they vary by density rather than width. Pretty much the same as photos but a slightly different URL pattern. We're going to cache the avatars in the same cache as photos. So the first thing I'm going to do is edit our clean up code so it includes the avatar URLs in the images that we want to keep. We don't want those getting lost in the clean up. Since I get a call from across the Atlantic, I'm guessing the rest is down to you. Hey, Mike. This is it the final task of the course. To start you want to catch up with a code Jake just wrote do this by running git reset --hard to remove any local changes you have. Then git checkout task-cache-avatars to get the stub methods. Now head over to public/JS/sw/index.js, the service worker script. There are two things for you to implement. You need to call serveAvatar for avatar URLs. Then you need to implement serveAvatar. It should return them from the cache if they're there. If not, get it from the network and put it in the cache. But here's the difference. Even if you return it from the cache, you need to go to the network to update it for the next fetch. Once again we're removing the size specific parts of the URL so use this URL to put and match in the cache. The solution will be similar to servePhoto, but not exactly the same. Once you've written the code, head over to the browser and refresh the page using the force update on reload option so you only need to refresh once to see changes. Looking at dev tools, you should see avatars arriving into the cache. If you go offline and refresh, the avatar should still load. To double check everything is working, head to the settings page and type cache avatars into the test field and press enter. So Jake, did you manage to finish the project? Yes, yes, it's all done, completely offline first. Here's how the final bits came together. So over in our fetch event I'm going to react to URLs that start with /avatars/. For those I'll call respondWith, passing in the result of serveAvatar. Over in serveAvatar, I'll start by opening the image cache, then I'll look for a match for the storage URL. Right, here's where things start to get a little bit different. I'm going to do a network fetch for the avatar. And if I get a response, I'm going to put a clone in the cache using the storage URL, then I'm going to return the original response. Now I've got a response from the cache. That might be undefined if there's no match for this particular request. But I've also got a promise for the network response. So I'll return the cache response or the network response, and that's it. If I go to the browser and ensure force update is enabled and hit refresh, I can now see in the resources panel, if I hit this update icon here, things are entering the cache both photos and avatars. So if I go offline now and refresh the page, we have content, photos, and avatars, the full offline-first experience. And that's it, job done. We've taken an entirely online only site and made it an offline first progressive app. By making it to the end of this course, you're ready to take on any offline first project. Wittr was specifically designed to cover most of the gnarly edge cases you'll encounter in the wild. For example, cache versioning for big updates like theme changes, interactive updates to get the user onto the latest version, dealing with server rendering by serving a page skeleton, responsive images, multiple caches, you even had to master indexedDB, the most feared API in the platform. You did it all. But to spell out exactly what we achieved, for the very last time, we dial into America land. Mike, are you there? I am. Show us where we started and where we are now? With pleasure. If the user's on some kind of non-perfect connection, they used have to wait for the page to appear, and imagery took ages. Now, stuff appears on the screen instantly. Lie-fi users used to be left staring at a white screen forever, but now they get content instantly. Offline users used to get a browser error page. Now they get content and a non-disruptive custom error. This is a huge improvement in the user experience. If your site works offline first, users are going to notice that it loads reliably and fast whatever the network. They'll have a less frustrating experience with your site than the rest of the web, and that's what matters, the user experience. And with that, it's goodbye from me. And goodbye from me. We look forward to a web that works whatever the connection is. So, the service worker think is pretty good. I mean, it's really good. But all it is a simple JavaScript file that sits between you and network requests. It's a type of web worker. Meaning, it runs separately from your page. It isn't visible to the user. It can't access the DOM, but it does control pages and by control. I mean, intercepts requests as the browser makes them. >From there, you can do, well, whatever you want really. You can send the request off to the network, as per usual or you can skip the network. Go to some kind of cache or create a custom random response, or any combination of those things. You register for a service worker like this, giving the location of your service work a script. It returns a promise. So, you get call backs for success and failure. If you're unfamiliar with promises, don't worry. We'll guide you through the bits you need to know throughout the course. If you call register when the service work is already registered, that's fine. The browser won't re-register it. It'll just return a promise for the existing registration. You can also provide a scope. The service worker will control any page whose URL begins with the scope and it will ignore any that don't. So for this example, the service worker will control a page at this URL and also anything with a deeper URL, but not anything with a shallower URL. The service worker doesn't get to control this one, nor will it control an unrelated URL. But be aware that this service worker won't control this URL, as it doesn't have the trailing slash. So it counts, as a shallower URL. You can have multiple service workers with different scopes, which is really handy for something like GitHub pages when multiple projects share the same origin. Scopes let you have a different service worker for each project. The default scope is determined by the location of the service worker script. It's basically the path that script sits in. Usually, you don't need to set the scope. Just put the service worker script in the right place, but what happens within the service worker? Well, you listen for particular events. Just like other JavaScript events, you can react to them or even prevent the default and do your own thing. We'll introduce these events one by one as the cost progresses, but what about browser support? This site is service worker ready has a breakdown of support by features. There's a link to that in the presenter notes. Every browser has expressed some interest in implementing. But as of 2015, service worker is only support it in Chrome and Opera with Firefox support soon to land. But just because a technology isn't universally supported, it doesn't mean you should wait. The service worker is entirely progressive enhancement friendly. Meaning, we can use it in browsers that support it without disrupting users of older browsers. They just won't get the benefits. In fact, using the stuff today to improve the user experience will encourage of a browser to implement it. So, their users get the benefit too. To use service worker in a safe progressive enhancing way, just make sure you write code in a simple feature detect. If a browser doesn't support service workers, navigator.serviceWorker will be undefined, which is a faulty value. So, older browsers will skip everything within the if statements and avoid trying to call functions that aren't defined. That's most of the nitty-gritty details covered. So let's all hold hands chance and summon our Resident Quizmaster, Mike Wales. Mike, can you hear me from beyond the Atlantic? I certainly can. It's time to get quizzical. We know that the service worker will only control pages within its scope. Given this registration code, which page URLs will this service worker control? The service worker will control any page URL that begins with the scope. That means if your scope is /foo/ it will not match /foo but it will match /foo/bar. Remember, that trailing slash is important. If our scope was /foo instead without this trailing slash it would match /foo here. Let's add a service worker to the Wittr project and start messing around with these requests. To make things easy the project already contains a script file at public > js > sw > index.js. And it's currently empty. So lets add a quick console log. The build system picks this up and the final result is served at sw.js at the root of the server. The extra code here is the output of Babel which the script runs. Babel lets you use some of the newer JavaScript features but I'm going to stick with ES5. But if you feel like writing newer JavaScript, go ahead. As we talked about before, the service worker receives events. Lets add a listener for one of these. Fetch. When the user navigates to a page within your service worker scope, it controls it. The network requests for its HTML, goes to the service worker, and triggers a Fetch event. But not only that, you also get a Fetch event for every request triggered by that page. CSS, JavaScript, images, you get a Fetch event for each, even if the requests were another origin. And we can inspect them with JavaScript. So in my Fetch event, I'm going to log event.request. To find out what kind of information is in that object, click through to the next lesson. That was me trying to build up some dramatic tension. I mean event.request it's clearly going to be information about the request. Or is it. Yes. Yes it is. The code in our service worker doesn't do anything yet, because it hasn't been registered, but I've been talking long enough, so this is a job for you. Once again, we pick up the video phone to our taskmaster, Mike Wales. Hello again. Now, this is the first coding task of the course Mike, so, you know, don't be too cruel. Who do you take me for? Okay, we're just saw Jake add of fetch listener to a service worker script. But now you need to register it from the page. To get your copy of the project into the same state as Jake's, run git reset --hard. And then git checkout task-register-sw. If you've already got the server running, you don't need to stop it to switch branches. Just open another command window to the project. Once you're on the correct branch, take a look at public/js/sw/index.js. This is the service worker script. And it contains the fetch event listener from the last chapter. And if you navigate the browser to http://local host.port 88/sw.js, you can see the output of that code. Along with a little extra that babel has thrown in. We want to register the service worker as soon as our app starts up, so let's go into public/js/main/indexcontroller.js. And its controls constructor is taking care of the set up of our app, including setting up the web socket for the live updates. Don't worry about this stuff. This would be the custom code for your app, whatever it is. In this case it's setting up our views, and getting ready to receive wheats. Also, JavaScript doesn't have private methods, but it's common practice to start methods with an underscore if they'll only ever be called by other methods of this object. Just notice that at the end there, it calls _registerServiceWorker. Here's the implementation of _registerServiceWorker, which is currently empty. And this is where you come in. Add the code to register the service worker here. We want the scope to be the whole origin, so the default scope is fine. Once you've done that, reload the page so your code runs, and check for errors. The best way to do this is to open Chrome dev tools. You can also use command + Alt + J as a shortcut, which is Alt + Ctrl + J on Windows. Once you're happy the service worker has registered, head to the Settings page and type registered into the test field. Then press Enter. It'll test to see if everything's working. Let's check in with Jake and see how he did this. Did you get the service worker registered? I certainly did. Here's how I did it. I'm going to call navigator.serviceWorker.register, passing in the URL of the script. I don't need to pass in the scope because the default is correct here. Of course this will throw an error in browsers that don't support service worker. I could wrap this whole thing in an if statement, but instead I'm just going to return if service worker isn't supported. And since .register returns a promise, I'm going to log a message if everything worked out. And just in case it didn't, I'm going to log a failure message. Of course if you did this differently, that doesn't mean it's wrong as long as the service worker is registered and you avoid errors in older browsers. That's it. So once that's done, over in the browser if I refresh the page, yep, my service worker is registered. We don't get any logging yet and we'll find out why later on. But if I refresh a second time, yep, there they are. I've got a log for every request made by the page. All the JavaScript, CSS, images, including requests that go to other origins such as this one for the font. The scope of the service worker restricts the pages it controls. But it will intercept pretty much any request made by controlled pages, regardless of URL. Not only that, but as we'll start playing with soon, you can mess around with these requests, changing headers or responding with something entirely different. This is pretty powerful, but with great power comes great responsibility. Because of this, service workers are limited to HTTPS, the securer form of HTTP. Remember when we looked at the path requests take through the network? Well when you're serving across plain unencrypted HTTP, any one of these things in the middle can remove, modify, or even add content. This is bad, this is really bad. You could request a news story from a reputable source and without encryption, what you actually receive could be very different from what the journalist wrote. Malicious scripts could also capture data you input, alter databases, read cookies, entirely without your knowledge. So all around, not such a good thing really. But service workers live longer than pages. So they could be used to persist an attack like this, and that would be really bad. In fact, it's unacceptable to let a potentially evil middleman get control of your service worker. And that's why it only runs on HTTPS. Thankfully, localhost is exempt from this rule, which is why things work fine on with his def server. Although it did take two refreshes to start seeing those console logs, and that's actually by design. The service worker has a different life cycle to pages. For example I'm going to change this console log here to just say hello world. And if we look back in the browser now and hit refresh, we're not getting those changes. It's okay, don't flip the table over just yet. We need to learn more about service worker's life cycle. In the previous video, we observed two oddities. When we first created our service worker, it took two refreshes to see the result. And secondly, when we changed the service worker, we didn't seem to pick up that change. The life cycle of service worker is one of the most complex parts. Once you get your head around this bit, the rest is easy. So we had our page open all ready, then we added code to register a service worker. Then we hit refresh. Hitting refresh spawned a new Window client, then the request went off to the network, we got a response back, and the old Window client went away. It might not seem like there's an overlap between the old page and the new page when you hit refresh, but there is. For example, if the response came back indicating that the browser should save the resource to disk Vajra download dialog, then the old window would have stayed around. But in this case, the response was a page, so we got rid of the old one. >From our page requests went out for our CSS images, but also a shiny new JavaScript, which registered the service worker. We didn't see request log from this page because the Service Worker only takes control of pages when they're loaded, and this page was loaded before the service worker existed. That means any additional requests this page makes will bypass the Service Worker. But then we refresh the page creating a new window client and because our Service Worker was up and running, it took control of it. Therefore, the request went to the Service Worker as did all of the subresources. So that explains why it took two refreshes to see logged requests. But what about when we change the thing we were logging yet still saw requests being logged ,which was our old code. If a page loads Vajra Service Worker, it will check for an update to the service worker in the background. And if it finds it has changed as in the resources and byte identical, it becomes the next version. But it doesn't take control, it waits. It won't take over until all pages using the current version are gone. This ensures thereâ€™s only one version of your site running a given time like native apps. Unfortunately, a refresh doesn't let the new version take over. This is due to the overlap between Window clients we saw earlier. There isn't actually a moment when the current active service worker isn't in use. For that to happen, this page needs to close or navigate to a page that isn't controlled by the Service Worker. When it does that, the new service worker takes over and future page loads will go through the new one. Let's put this knowledge to practice. So as before if I refresh the page, I'm still getting these old console logs. But if I navigate away then click back, the logs so difference. The new service workers running. This update process may sound complicated at first but it's actually the same update process browsers such as Chrome use. Chrome will download the update in the background, but it won't take over until the browser closes and opens again. It notifies the user that there's an update ready by changing the color of this icon. Later in the course we'll also notify users of updates to Witter. When the browser refetches a service worker looking for updates, it will go through the browser cache as pretty much all requests do. Because of this I strongly recommend keeping a cache time on your service workers short. In fact, I go for cache time of zero on all my service worker projects. And that's what Witter uses as well. As a safety precaution, if you set your service worker script to cache for more than a day, the browser will ignore that and just set the cache to 24 hours. That doesn't mean your service worker will stop working after 24 hours. It just means that update checks will bypass the browser cache if the service worker it has is over a day old. Okay that's the life cycle theory out of the way. In the next chapter, we're going to look at some deft tools that show you what state the Service Worker is in. In the last chapter we learned about the service worker life cycle. Thankfully you don't have to keep all of that state in your head, there are dev tools for that. At the time of recording this, some of our dev tools are still in development, but I recommend giving them a go. I'm using Chrome Canary which is google's almost nightly build of chrome with all the experimental stuff. As a developer, I recommend installing it so you get all the new features and tools before everyone else. It runs completely independently of regular chrome, so installing this won't mess with your other settings. Check out the instructor's notes for links to download Chrome Canary and once you've launched Canary open the WITTR up again. Once you've got Canary all set up, check the check box and we'll have a play with the latest dev tools. So let's look at these developer tools in more detail. We've already seen the console and any code that you run here, will run against the documents. But it's obvious what lives outside documents. So if I log self.registration which only exists in a service worker, we get undefined back. However, I can change the context of the console using this drop down here. I can select different frames, workers, including the service worker. Now if I log self.registration, it works. If you're familiar with JavaScript debugging, you'll be happy to know that this works with service workers, too. You can go to the sources panel, open the service worker, and debug it like any other bit of JavaScript. Here I'm going to set a break point in our Fetch event. Now if I refresh the page, our script gets paused. I can inspect the state of objects just as you'd expect. If you're new to dev tools, check out the instructor's notes for links to guides and tips. But away from standard JavaScript F tools, service worker has its own panel in the resources tab. This bin icon lets us unregister the service worker, which is useful if you want to refetch the service worker from scratch. And these tabs give us insight into the service worker life cycle. We can see here there's an active service worker, but there isn't a new version waiting. We can tell that because its waiting tab is grayed out. It seems like we have an incoming communication from Mike Wales. Hey, I can't help noticing you've been talking for quite a while now. We should probably let the student become familiar with all this new stuff. That is a very good point. I have been going on a bit. Take it away, Mike. So Jake's been demonstrating the service worker lifecycle and developer tools to observe it. What I want you to do is make a change to the service worker. Get it into a waiting state, then allow that service worker to become active. If you've been following the course in order, the demo app is in the correct state already. If not, you can run this, git reset hard, git reset hard gets rid of any local changes you have. And git check out log requests gets the project into the state worth registering a service worker and logging requests. Now for the challenge. Make a change to the service worker script. It can be anything. A new console log, even just a JavaScript comment. Then refresh the page to pick up the new service worker. In the resources tab, you should see the new service worker waiting. This is what it should look like, an active service worker and a waiting one. Once you've done that, head over to the settings page and type sw-waiting into the test field and press enter. This will let you know if everything is working Your next challenge is to get that new service worker active. As Jake showed us, that means closing all the windows that are using the current service worker or navigating those windows to a page that isn't in the service worker scope. Once you've done that your new service worker should be active. No waiting worker. To confirm that, head over to the settings page and type sw-active into the test field and press enter. This will let you know if the service worker is active. Having to close or navigate the tab to get the service worker to update is a bit of a pain during development. But thankfully, there are ways to make it easier. I'm going to make a change to the console look in the service worker, and then back in the page I'm going to hit refresh. Now there's a new service worker waiting. You can tell the difference here as the active service worker has this red icon and the waiting one is, I don't know is that teal? You can tell I'm not a designer. It's waiting because the tab is still using the active version, but rather than close this tab or navigate it to another origin, I'm going to reload the page while holding Shift. This loads the page but bypasses the service worker. This behavior is part of the service worker spec, so it should work in any browser that supports service workers. This is handy for two reasons. One, it's a quick way to test changes that are unrelated to the service worker, such as minor CSS changes. And two, because this tab is no longer controlled by the service worker, it lets the waiting service worker take over. We can see this in dev tools, the teal service worker has progressed to active. Now if I refresh normally, the request goes through the new service worker and we get the updated log. Now's probably a good time to talk about this ERR_FILE_EXISTS thing in the console there. That's just Chrome saying it's checked the service worker and there were no changes. Showing an error for this is a bug, but it isn't breaking anything. Hopefully this bug will be fixed by the time you go through this course. So, Shift+Refresh is an easier way to get updates, but Chrome's dev tools has an even easier way. In the Sources tab, check out this little service worker panel here. There's an option called, Force update on page load. This changes the service worker life cycle to be developer-friendly rather than user-friendly. In this mode when you hit refresh, rather than refreshing the page, it fetches a service worker, treats it as a new version whether it's changed or not, and lets it become active straight away, and then the page refreshes. That means I can make a change to the console log, say, Yo Everyone, and then back in the page hit refresh and I get those changes straight away. No Shift+Refresh, no closing tabs. This change in behavior only happens while dev tools is open, but it makes viewing quick changes really easy. However, we need to bear in mind it's not the same life cycle users will be getting. Later in the course, we'll look at optimizing the user experience, but first, let's do some more exciting things with the service worker. In the next chapter, we'll look at hijacking requests and doing our own thing. So far we've set up our own service worker, learned its life cycle, and explored the dev tools, but we haven't really used our service worker to do anything yet. Currently, the server's having all the fun when it comes to sending content back to the browser. Well, now it's our turn to have fun. So far we've seen requests go from the page, through the service worker fetch event, then onto the network as usual, through the HTTP cache. But now we're going to catch the request as it hits the service worker, and respond ourselves, so nothing goes to the network. Obviously, this is an important step in going offline first, so let's make it happen. Over in the service worker script, we can call event.respondWith. This tells the browser that we're going to handle this request ourselves. Event or respondWith takes a response object or a promise that resolves with a response. One way to create a response is well, new response. The first parameter is the body of the response, which can be a blob, a buffer, some other things, but at its simplest it can take a string. So here I'm going to respond with Hello world. Back in the browser, because I have Force update on reload checked, I refresh once, and I get the custom response. No matter what URL I enter here, I get the same response, because I'm intercepting all fetches. I could even modify the response to include some HTML. Back in the page, I hit refresh, and although the HTML is there, it's just being output as text. You can see why if you look in the network panel. The content type is text/plain, meaning it won't be passed as HTML. Thankfully, I can set headers as part of the response. The second parameter of new request is an object. And the header's property takes an object of headers and values. So I can set the foo header to be bar. Back in the page, hit refresh, and in the network panel, there it is. The phone lines have lit up and that can only mean one thing. A wild Mike Wales has appeared. Hello, Mike. Hey, fixing that request sounds like something the student can do. Fair point, go for it. To begin, you want to get your project up-to-date for this task. To do that, on the command line in the wittr project directory run git reset hard to get rid of any local changes you have. And git checkout log request gets the project into the state where it's registering a service worker and logging requests. Over in the service worker script file, public/js/sw/index.js. It's just logging requests. I want you to alter the service worker so it responds with some HTML. It can be whatever HTML you want as long as it includes the class name a-winner-is-me. So a strong tag with the class a-winner-is-me would be a valid response for this task. Once you've written the code, test it's working in the browser. Like we saw Jake do in the previous chapter, in the sources tab, check Force update on page load so you can see your changes straight away. You'll know what's working when you refresh the page and you get something like this. To confirm everything's working, head over to the settings page and type HTML response into the test field and press enter. This will double-check if everything is working. Jake, did you manage to get the HTML working from a custom response? I certainly did. And here's how i did it. Over in the service worker. I'm going to add the a-winner-is-me class to this bold element. But the important bit is setting the content type header. To text HTML. Now in the browser with force update enabled I'll refresh the page and it's being passed as HTML. We can also see that in the network panel. It's important to note that at this point we've created something that works entirely offline first. I'm going to disable the force update settings so we get the whole, real user experience here. Now if I bring the server down and refresh the page, it still works. Even if I switch to Lie-fi and refresh the page. It still works. Instantly. The network is completely untouched when it comes to serving this content. Of course, although it's offline first it's a bit rubbish and useless. Especially since we're responding to every request in the same way. In the next chapters we're going to look at hijacking particular requests in particular ways depending on information in the request and the response. Now that we know how to hijack a request and respond with something basic, let's do something cooler. Let's go to the network for the response, but not for the thing that was requested. Handily there's an API for that, Fetch. Fetch lets you make network requests and lets you read the response. Now you might be thinking, isn't this what XML HTTP requests is for? No, just no. Much of the XHR API is 15 years old. Soon it'll be old enough to drink. And i can't even bare to think of what a drunk XHR could do. Even from the outset, it wasn't particularly well thought out. I mean, just look at the name, XML, no, it deals with many different response types. HTTP, not really, it deals with other protocols such as file system. Request, actually the returned object is like a strange mixture of request and response, and why is the XML in all caps but only the H in http. But it's not just the name that's wrong. Here's the code to fetch some Jason from URL food. Everything feels like it's in the wrong order. The URL's down there you have to open and then send for some reason. It makes you declare how the response is read before you make the request. And it doesn't support lower level things like streams. But worst of all, the event system gets event to callback hell. Here's the fetch code for the same operation. Fetch foo, this returns a promise which resolves to a response. Then we can read that response's JSON. And then we've got the result. We can catch errors from either the request or reading the response, done. And as it turns out, back in our service worker event.respondwith takes either a response or a promise that resolves to a response. Fetch returns a promise that resolves to a response. So they can post together really well. So let's respond with a fetch for a GIF. Over in the browser, let's enable force update, hit refresh, and we've just served up different content using the network. The fetch API performs a normal browser fetch. So the results may come from the cache. That's a benefit in this case, as we want the GIF to cache as usual. Of course we might not want to hijack every URL. We could, wait a minute we have an incoming communication situation. It is I Mike Wales. Hey Mike, so what's the meaning of this interruption? Well it sounded like you were just going to describe a load of stuff. But I think it'd be better if the students figured this out for themselves. Great idea, go for it. We just saw Jake serving a gif in response to everything. Instead, let's only serve it in response to a particular request. If you want to get your code into the same state as Jake's, in the project folder run git reset --hard to remove any local changes you have. Then, git checkout gif-responses to get the project into a state where it's responding to everything with a gif. Take a look at the code in public/js/sw/index.js. As you can see your task is to only respond with a gif if the request URL ends with dot jpg. How you determine that is up to you, but remember Event.request gives you information about the request. Write the code and refresh the page to confirm your changes have worked. Remember to have DevTools open and use force update on reload so you'll only need to refresh once to see changes. Once you've got it working, head over to the settings page and type gif-response into the test field and press enter. This will double check if everything is working. Hey Jake, did you get the GIF responses happening for JPEGs only? I certainly did and here's how I did it. Now we haven't actually covered how to get the URL of a request yet. So this task was a little bit trickier. I've been doing service work for so long that I have the API burned into the back of my brain. But if I didn't, here's how I would have figured it out. We've already seen event.request. But what other properties does it have? One way to find out is to go to Google and search for MDN request. MDN is a great place for documentation. And there there's a result about the fetch API. In there it tells me that request.url is the URL of the request. Kind of obvious now we see it. Alternatively, I could have added a console log and logged out event.request, as we were before, and then refresh the page. And because the console is cleared when the page navigates, we're losing the log for the page request. If I preserve the log and refresh again, there it is. And inside the request object there are loads of details in there, one of which is the URL, and it's a string. So with this knowledge back in the code I can use an if statement. So respond with is only called if the URL ends with .jpg. Ends with is a relatively new string method, but it's really useful. Since service work is only run in modern browsers, we can make use of some of the more modern JavaScript features. Back in the browser with force update enabled, I hit refresh and the page loads as normal, but all the images have been intercepted. So we started handling requests dynamically depending on URL. But there's a lot more we can do here. Join us in the next chapter to discover some request hijacking patents. So far we've decided the response we should send back based on the request URL. But in the real world, we need to be more dynamic than that. For example, the page can send a request which we intercept and then send to the network. But rather than just sending the response back, we could look at it and do something else. For example, let's respond with a network fetch for the request, just as the browser would do if we left it to its own devices. The fetch method will take a full request object as well as a URL. As we saw earlier, fetch returns a promise. With promises, you can attach a .then call back to get the result, if the operation was successful. Whatever we return in this callback becomes the eventual value for the promise. This means we can look at the response ourselves, and if the responses are 404 Not Found, we could respond with our own message. Otherwise, we return the response we received. .catch is similar to .then, but .then is for success and .catch is for failure. Fetch will fail if it can't make a connection to the server at all, which includes offline. So when that happens, we can respond with our own message. So now, if we make sure our force update is enabled and hit refresh, everything is back to normal. But if we go to a page that doesn't exist, we get our custom message. If we take the server down, go offline, and refresh the page, we get our custom message for that too. We can create complex rules for requests, trying to get a response from multiple sources, and reacting to the results. And we can do that on a request by request basis using JavaScript. You can even go to the network, and if that fails, get something else from the network. In fact, this sounds like something for you to do. Mike, come in, Mike. Can you hear us? Yes I can. How about a task chaining two fetch requests together? Yeah, let's do it. We've seen a custom 404 response served by the service worker. But what if we wanted to serve an image instead? Jake's done most of the work here. So to get your code into the same state as Jake's, in the project folder, run git reset --hard to remove any local changes you have. Then git checkout error-handling to get the project into a state where it's handling 404s and the service worker. Take a look at the code in public/js/sw/index.js. Here's the code where the 404 is handled. Rather than responding with custom text, your task is to respond with the dr-evil GIF. This involves fetching the GIF from the network, which we covered in the last section. Write the code and refresh a non existing page to confirm your changes have worked. Remember to have DEV tools open and use force update on reload so you only need to refresh once to see changes. Once you've got it working, head over to the settings page and type gif-404 into the test field and press enter, this will confirm it's all working. Jake did you get this working? Yeah. It's one of the ones that takes a little bit more thinking than actual typing. First I'm going to bring the server online, then refresh the page. This is our current 404 page that we want to display a gif instead. Over in the code, now if you return a promise within a promise, it passes the eventual value to the outer promise. Now I know that sounds kind of like a proverb Mr Miyagi would say but here's what I mean. Rather than return a response I'm going to return a fetch for the gif''s URL and that's it. Over in the browser if I refresh the 404 page I get the gif. But if I load a non-404 page, everything works fine. Okay, it's been fun playing with the stuff. But it's time to start putting it into practice and do something useful. In the next chapter, we'll start doing the real work to make Wittr offline first. So far, we've seen how to hijack requests and respond to them differently. We've even created responses ourselves, meaning we can respond without using the network at all. However, if we want to be able to load Wittr without using the network, we need somewhere to store the HTML, the CSS, the JavaScript, the images, the web font. Thankfully, there is such a place, the cache API. The cache API gives us this caches object on the global. If I want to create or open a cache, I call caches.open with the name of the cache. That returns a promise for a cache of that name. If i haven't opened a cache of that name before, it creates one and returns it. A cache box contains request and response pairs from any secure origin. We can use it to store fonts, scripts, images, and anything, really, from both our own origin as well as elsewhere on the web. I can add cache items using cache.put and pass in a request or a URL and a response. Or I can use cache.addAll. This takes an array of requests or URLs, fetches them, and puts the request-response pairs into the cache. This operation's atomic. If any of these fail to cache, none of them are added. addAll uses fetch under the hood, so bear in mind that requests will go via the browser cache. Later, when we want to get something out of the cache, we can call cache.match, passing in a request or a URL. This will return a promise for a matching response if one is found, or null otherwise. caches.match does the same, but it tries to find a match in any cache, starting with the oldest. So we have somewhere to store our stuff, but when should we store it? Thankfully, there's another service worker event that helps here. When a browser runs a service worker for the first time, an event is fired within it, the install event. The browser won't let this new service worker take control of pages until its install phase is completed, and we're in control of what that involves. We use it as an opportunity to get everything we need from the network and create a cache for them. So let's do that for Wittr. Over in the service worker file, I'm going to add a listener for the install event. Event.waitUntil lets us signal the progress of the install. We pass it a promise. If and when the promise resolves, the browser knows the install is complete. If the promise rejects, it knows the install failed, and this service worker should be discarded. Looks like we're getting a call from Mike. I was wondering when he'd appear next. Mike, what can I do you for sir? Well, you've explain the cache and the install event. Sounds like something for the student to put together. Couldn't agree more. Take it away. To begin, you'll probably want to get your copy of Witter into the same state as Jake's. To do that, open a command prompt to the project folder and run git reset --hard to remove any local changes you have. Then git checkout task-install. Let's take a look at the code. In public/js/sw/index.js there's an array of URLs to cache there. Your task is to cache those URLs in a cache named wittr-static-v1. Remember to have dev tools open and use force update on reload, so you only need to refresh once to see changes. You can also verify the state of the cache in dev tools. Click on the resources panel then cache storage. Hopefully you'll see your cache in there. Once you've got it working, head over to the settings page and type install cached into the test field and press enter. This will confirm its all working. So how'd you get along with this one, Jake? Everything cached and accounted for. Here's how I did it. Event.waitUntil takes a promise and cache.open returns one. So I'm going to start with that. Then once I've got the cache, I'm going to call cache.addAll to cache all the URLs. I could just reference the array here, but I'm going to move the items across instead. AddAll also returns a promise, so I return it. So wait until it receives a promise for both actions combined. Now if I refresh the page, that new service worker will run and I can go to the resources panel in DevTools and there in cache storage, is the new cache recreated and the resources we added to it. Success, but it's no good having cached items if we're not going to use them. So let's use them in responses. To do that, wait, Mike, why are you still here? Come on, you've already covered responses and getting responses out of caches. I think the student can take this on. Well fine, makes my life easier. We haven't shown you code for responding with a cache entry, but you've seen caches.match for getting things out of the cache. And you've seen event.respond with, so time to put them together. If you've completed the last task, your project is in the correct state for this task. If not, you can go to the project directory in your command line and run git reset --hard to remove any local changes you have, then git checkout task-cache-response. Once again, the work happens in the service worker script in public/js/sw/index.js. First, you'll need to clear up any code in the fetch handler you already have. We don't need any of the special four of four handling from the previous tasks. Then, your task is to respond to the request with an entry from the cache if there is one. Otherwise, fetch it from the network, here's a hint. You need to call event.respond with, synchronously. You can't call it within a promise handler, that's too late. Once you've coded it up, reload the page. Remember to have DevTools open and use force update on reload, so you only need to refresh once to see changes. You'll know it's working because you'll be able to put the site into Offline mode and still get a response. Once you've got it working, head over to the settings page and type cache- served into the test field and press Enter. This will confirm it's all working. Jay did you get this working? No, but I can do it right now to begin, I'm going to respond with a match in the cash. For this request, I can just pass event dot request straight into cash's dot match. Of course there may not be a match in the cache for this particular request. In that case the promise will resolve with undefined. So if the request is truthy, meaning I got a match in the cache I'll return it. Otherwise I'll return a fetch to the network for the original request. I can actually make this a little bit neater by just writing as a response or fetch. With force update enabled I can refresh the page. And while it doesn't really look like much has changed. But if we go offline and refresh, we get a whole lot of content still. We shouldn't underestimate what we've done here, it's pretty huge. Here's what we've achieved. Things haven't really changed with a perfect connection, but perfect doesn't exist. On a slow connection, we're getting stuff on screen a whole lot faster. On WiFi, we're delivering stuff rather than a blank screen which is great. And offline gets content, rather than a complete failure. There are things we need to fix though. The photos and avatars aren't working offline. But also if we disable force updates. And go online. We can see this post from Sam with the picture of the car. And these newer posts above it. But if we now go offline. And refresh Sam's post is showing us the latest. We're not updating the posts in the cache. This is because we cached the HTML once and install time. So we're stuck with that set of messages in the cache. Here's a to do list that gets us from where we are now to full offline first success. We need to come out of a system for an intrusive app updates. We want to get the user using the latest version as quickly as possible. We should continually update the cache of posts so we're not left with the same content we originally installed with. Then we need to cache the photos and cache the avatars. The rest of the course is about doing this. Starting with unobtrusive app updates. You have done well to get this far. Your skills, they are strong, you are ready. It is time, time to disable Force update on page load. Now the service worker lifecycle is back to normal, which is how real users will experience it. You see, force update is great during development. Say we wanted to change the theme of the app. The theme colors are in public/scss/theme.scss. These are a set of Sass variables referenced throughout the rest of the style sheets. If I change the primary color to red then go back to the browser with force update enabled and refresh, boom, red toolbar. But this is because force update reloads the service worker from the network on every refresh and causes it to install even if the service worker hasn't changed. Let's say I change the whole theme by uncommenting this other set of variables here, which should make the theme green. But this time, I'll disable Force update on page load. Refresh the page, and still red. No matter how many times I refresh, still red. If I Shift+refresh and bypass the service worker, I see the changes. But normal refreshing gets me nowhere, because our cache still contains the old CSS. Our cached CSS is updated as part of the install step. But that isn't happening, because there's no new service worker to install. We can see from dev tools, there's no new service worker waiting. Without force update, the browser checks for an update to the service worker per page load. But I didn't change the service worker, I only changed the CSS, so there's nothing new to install. We need to work with the service worker to get it to pick up changes, because we're developers, right? We change code all the time. And we need users to get those changes as soon as possible with minimum disruption. Here's how we work with the service worker lifecycle. To get the CSS to update, we need to make a change to the service worker. The browser will then treat this updated worker as a new version. Because it's new, it'll get its own install event where it'll fetch the JavaScript, HTML, and our updated CSS and put them in a new cache. It won't put them in a different cache automatically. We changed the name of our cache to make this happen. We create a new cache because we don't want to disrupt the cache that's already there being used by the old service worker and the pages it controls. Then, once the old service worker is released and we're ready to take over, we delete the old cache so the next page load gets resources from the new cache, meaning it gets the latest CSS. Job done. We've already covered most of the parts needed to make this happen. We know that a change to the service worker will cause it to spin up a new instance. And that change can just be renaming the cache from v1 to v2. The bit we haven't covered is how to get rid of that old cache. The first piece of the puzzle is the activate event. We've already seen the install event, which fires when the browser sets up a new service worker for the first time, whereas the activate event fires when this service worker becomes active, when it's ready to control pages and the previous service worker is gone. This makes it the perfect time to get rid of old caches. Like the install event, you can use event.waitUntil to signal the length of the process. While you're activating, the browser will queue other service worker events such as fetch. So by the time your service worker receives its first fetch, you know you have the caches how you want them. You can delete caches using caches.delete, passing in the name of the cache. You can also get the names of all your caches using caches.keys. Both of these methods return promises. Well the internet lights are blinking and that can only mean one thing, we've got a Mike incoming. Hey, it's time for a coding challenge. Couldn't agree more. Take it away, Mike. Your task is to update the CSS in a way that doesn't disrupt the currently running version of the site. First, to get you started, you need to get your project into the same state as Jake's. Go to the project directory in the command line and run git reset --hard to remove any local changes you have. Then git checkout task-handling-updates. This time make sure force update on reload is disabled. We want to retain the full service life cycle here. Over in the service worker script in public/js/sw/index.js your task is to change the theme of the site. Thankfully this is pretty easy as all the colors are stored in variables. You can see those in public/scss/_theme.scss. You can either uncomment the green and pink theme or make one of your own up. The only requirement is that the primary color changes so that the site's header changes color. Then in the service worker you need to update the cache name and use the activate event to remove the old cache. Once you've coded it up, reload the page to see it working. In the service worker's dev tools, you should see a worker in the waiting position. Don't let it activate yet. Instead, head over to the settings page and type new-cache-ready into the test field and press Enter. This will confirm it's all working. Now you're ready to activate that service worker. As we saw earlier in the course the service worker won't activate until pages using the current version go away. So either close this tab, navigate it to a page out of the service worker scope or shift refresh it. When you navigate back to the page it should be showing the new theme. To confirm this, head over to the settings page, type new-cached-used into the test field and press enter. This will confirm it's all working. There are a few different ways to complete this one, how did you do it Jay? Well as the easy way, and there's the scalable way, we're going to look at both. Here's the easy way. I change the theme c.s.s to the green theme. Then I bump the version number of the static cache in the service worker. Then I use the activate event to remove the v one cache, the old cache cache. Now I can refresh the page and in DevTools, I see this new version install, and wait. If I look at caches in DevTools, I can see v1 and v2, which is what we want right now. If i navigate away, the new service worker activates. So if i click back the theme has changed. As I said that's the easy solution. But it's perfectly good in this case. If it's what you did, you got it right. But what about when we get to version twenty? We can't just remove version 19 in the activate event. Because the user might be updating from an older version, maybe 18, maybe 17. Calling delete on every old cache name is going to be a bit messy. And that code's only keep getting bigger and bigger over time. Instead, I'm going to maintain a safe list of cache names I want to keep and remove the others. To do that I'm going to store the name of the static cache in a variable. Then in the activate event, I'm going to get all of the cache names that exist. I'm going to fill up that list of cache names. I'm only interested in the cache name begins with wizard dash. But isn't the name of a static cache. That gives me a list of wizard caches that I don't need anymore. So, I'm going to map those to promises returned by caches dot delete. Then I'm going to wrap all of that in promise dot all. So we wait on the completion of all of those promises. Checking the cache starts with with wizard dash means we don't delete caches from other apps that might be running on the same origin, for example some other static v1. That isn't really necessary here as we only have one service worker on the origin. But on sites like GitHub pages you might have many service workers sharing the same origin. When updating the cache remember that the requests are going via the standard browser cache. So if one of these resources had a cache time of say a year, the update would just be fetched from the HTTP cache. So you wouldn't get any changes you made. In the development server, all the resources are set to have a cache age of zero as in they don't cache. In production, I strongly recommend having versioning as part of your resource names Like this. Where the version number is generated from the content of the file. Then you can give these resources a cache time of a year or more. So if you update your CSS, a build script could automatically update your service worker, changing the URL to this the CSS. Your cash version number could also be all generated based on the things that caches. Versioning resources like this and giving them a long cache time isn't advice specific to service workers. It's just general good caching practice. You can work around not to call caching with service worker. You pack pretty much anything. But things are so much easier if you work alongside good caching. For example, in this model if i update my CSS the CSS URL changes, therefore cache name changes. Now when the browser fetches all these, it can get everything but the CSS from the browser cache because. They haven't changed. The only thing that special the network is the new CSS. Okay, I think we can say we've successfully delivered unobtrusive updates. Next we'll look at making sure the user gets these updates quickly and painlessly. So now we have a safe way to update our static assets. But as we saw in the previous chapter, the changes would be in the waiting worker. Ideally we want users to be on the latest version as soon as possible. We want them to get the latest features, the latest design, and of course bug fixes. But as we saw when a new worker is discovered it waits until all pages using the current version go away, before it can take over. And, that could be a long time. Let's do something better. Our goal here is to tell user once an update has been found, and give them a button to ignore it or refresh the page to get the new version. But how can we achieve this? Well, first off, let's cover the update notification. Thankfully there are APIs that give us insight into the service work of a lifecycle. When you register for a service worker it returns a promise. That promise fulfills with a service work a registration object. This object has properties and methods relating to the service worker registration. We get methods to do things like unregister the service worker or programatically trigger an update. We also get free properties installing, waiting, and active. These will point to a serviceWorker object or be null. They give you an insight into the serviceWorker life cycle. They also map directly to the dev tools view we've been working with so far. This dev tools view is actually just looking at these registration objects. For example, if there's a serviceWorker instance in .installing, that tells us there's an update on its way, although it might be thrown away if the install fails. If there's a serviceWorker in .waiting, we know there's an updated serviceoWorker ready and waiting to take over. The registration object will emit an event when a new update is found. Called well, update found. When this file's not installing has become a new worker. On the serviceWorker objects themselves, you can look at their state. The state can be installing the installer that has fired, but hasn't yet completed. Installed, installation completed successfully but hasn't yet activated. Activating. The activate event has fired but not yet complete, or activated. The serviceWorker is ready to receive fetch events. And also, there's redundant. The serviceWorker has been thrown away. Redundant happens when the serviceWorker has been superseded by a newer worker and it also happens if the serviceWorker fails to install. The serviceWorker fires an event. State change. Whenever the value of the state property changes. Also. navigator.serviceWorker.controller refers to the serviceWorker that controls this page. You want to tell user when there's an update ready. But because the serviceWorker update happens in the background, the update could be ready and waiting, it could be in progress, or it might not have started yet. This means we need to look at the state of things when the page loads. But we may also need to listen for future changes. For instance, if there's no controller, that means this page didn't load using a serviceWorker. So they loaded the content from the network. Otherwise we need to look at the registration. If there's a waiting worker there's an update ready and waiting. We tell the user about it. Otherwise if there isn't installing worker there's an update in progress. Of course the update may fail. So, we listen to the state changes to track it and if it reaches the installed state. We tell the user. Otherwise, we listen for the update found event. When that fires we track the state of the installing worker and if it reaches the installed state we tell the user. That's how we can tell user about updates, whether they're already there, in progress, or start some time later. And I've got an incoming call from Mike. I guess that means you'll be coding this one up. Hey Mike, you think they're ready for this one? Yep, they certainly are. Well, you know what you've got to do. Tell the user when there is an update available. First to get you started, you need to get your project into the same state as Jake's. Go to the project directory in the command line and run git reset --hard to remove any local changes you have. Then get checkout task update notify. We're going to edit a different file this time. In public /JS/main/indexcontroller.JS. You might remember this file from earlier in the course. It's where you registered the service worker. There's a new method here, _updateReady. Calling this will show a notification to the user. Your job is to call it at the correct times. There's a series of comments to guide you along the way. Once you've coded it up, you'll need to get those changes picked up by the browser. This is a little awkward. The easiest way is to delete the service worker then refresh the page. This will refresh and cache your JavaScript. Now make a change to your service worker. Adding a comment will suffice. Then refresh the page. If your code's working, you'll see the notification. Once you've got the notification working, head over to the settings page. And type update-notify into the test field and press enter. This will confirm it's all working. So Jake, have you got notifications appearing? Yep, I mean they're useless, but they are appearing. Here's how I did it. First off, if the controller is falsely, I bail. The user already has the latest version if it wasn't loaded via a service worker. If there's a worker waiting, I trigger the notification and return. If there's a worker installing, I want to listen to its state changes. I'm going to call another method to do that, trackInstalling. You'll see why shortly. In trackInstalling, I'm going to take the worker, and I'm going to listen to it state change event. When that fires, I'm going to look at the state, and if it's installed, I'm going to notify the user. If there isn't an installing worker, I'm going to listen for updates. Once there's an update, I'm going to call trackInstalling again. You can see now why I factored that code out. Now I'm going to make a random change to the service worker and then refresh the page. And there we have a notification. If we were to deploy this change, we'd bump the version number of our static cache, so the old and new version wouldn't step on each other's toes. But this isn't really worth deploying yet. I mean, the notification is kind of useless. But this is an important step. In the next lesson, we'll let the user opt into the update. So we've got a notification appearing, but our goal is to give the user a button they can press to get the latest version. Clicking this button needs to tell the waiting service worker that it should take over straightaway, bypassing the usual life cycle. Then we want to refresh the page so it reloads with the latest assets from the newest cache. There are three new components that help us achieve this. A service worker can call skipWaiting while it's waiting or installing. This signals that it shouldn't queue behind another service worker. It should take over straight away. We want to call this when the user hits the refresh button in our update notification. But how do we send the signal from the page to the waiting service worker? Your page can send messages to any service worker using postMessage. And you can listen for messages in the service worker using the message event. So, when the user clicks the refresh button it will send a message to our service worker telling it to call skip waiting. And the final part, we've already seen navigator.serviceworker.controller but the page gets an event when its value changes, meaning a new service worker has taken over. We're going to use this as a signal that we should reload the page. Right on cue we have a mike hurtling towards us food into the pipes. Mike. And the one for the student to do. You guessed it. Here we go. To make things easier, some of the methods have been stalled out. And comments have been added to show or code is missing. To get those, you need to go to the project directory in the command line and run git reset hard to remove any local changes you have. Then, git check out task update reload. First, let's take a look. In public/JS/main/indexcontroller.JS _updateReady is being called whenever there's an update ready to show. We implemented that in the last task. But we're now passing the new worker into the method. In _updateReady the new version message is shown. And we hit this line, the to do if the user hits refresh. Here, you need to send a message to the new service worker to tell it to take over control of pages immediately. You'll need to handle this message in the service worker. Over in public/js/index.js at the bottom there's a TODO. Here's where you can listen for the message to take over page control. Back in public/JS/main/indexcontroller, there's a to do in _registerserviceworker. You need to listen for the pages controlling service worker changing and using that as a signal to reload the page. Once you've coded it up, you'll need to get those changes picked up by the browser. And once again, this is a little awkward. Thankfully, this is the last time it's awkward. The easiest way is to delete the service worker, and then refresh the page. This will re-fetch and cache your JavaScript. Now, make a change to your service worker, adding or changing a comment will suffice, and then refresh the page. If your code's working, you'll see the notification as before. And pressing refresh will reload the page. Once you've got it working, you can test it on the settings page. The testing is a little different here to the previous tests. First make a change to your service worker, and refresh the page so you have an update notification. Now in the settings page, type update reload into the test field and press enter. Then you have eight seconds to hit the refresh button in your notification. This will confirm it's all working. So Jake, have you got updates working? Yep, users will now be encouraged to update to the latest version of the site and here's how I did it. So here we are with our refresh button, but it doesn't do anything yet. When the user clicks refresh, I'm going to call post message on the installing worker. Passing a JavaScript object with property action and value, skip waiting. You don't have to use this specific object. This is just something I'm going to read in the service worker later on. So over in the service worker, I'm going to listen to the message event. Event.data is the object past the post message. So if .action is equal to skipWaiting, I'll call self.skipWaiting to make this service worker take over pages. Now, I want to react to skipWaiting being called. So back in the page controller, I'm going to listen for the controllerchange events. If the controller changes, I reload the page and that's it. Done. To test it, I'm going to unregister the service worker and reload the page so all of our latest JavaScript is cached. Now, I want to test how it picks up a change to the service worker in assets. I'm actually pretty bored of this green theme now, so I'm going to switch it back to the blue. And so, this change doesn't disrupt currently running pages. I'm going to bump the version of the static cache. So now, if I refresh the page, we get our notification. And if I click the refresh button in the notification, we got our theme change. This is just one possibility when it comes to handling app updates. You have access to all the moving parts so, do what works best. SVGOMG and an SVG compression tool. It does something very similar to what we did for Witter. Except, if the update lands before the user is interacted with the page, it just tells us so as work to take over and refreshes instantly. If the user hasn't interacted. Refreshing instantly isn't disruptive in this case. You could also ask the server for a change log between the new version and the current version. If your update only contains minor things, maybe don't bother the user at all. Let them get the update naturally. Or if the update contains an urgent security fix, maybe refresh the page no matter what the user is doing. That would be an annoying experience but it's better than the user using something with a major security flaw. Anyway, it's fair to say we're well on the way to a full offline first experience. We've got the user onto the latest version as quickly as possible. Next up, we need to work on updating the posts the user sees. So, we need to swap out the root page for the skeleton page, but continue to make everything work offline first. I guess this is a job for you, because the phone lines are lighting up. We have a Mike dialing in. Think they're up for this one, Mike? I do. Then go for it. If you completed the previous task, your copy of the project is in the right shape to take on this task. But for some helpful to-do comments, you can run git reset hard to remove any local changes you have. Then git checkout task-page-skeleton to get up to speed. You only need to edit the service worker to complete this task. So head over to public/js/sw/index.js. The root page is currently being cached and served up in the fetch event. Your job is to cache the page skeleton instead and serve that if the root page is requested. Once you've change the code, refresh the page. The browser will see the new service worker and now we get this notification. We can hit the Reload button to update to the latest version. You can confirm your changes are working by checking view source on the root page. The source should be small like this. To double check everything is working, head to the settings page. And type service skeleton into the test field and press enter. So Jake, have you got the service worker serving the skeleton page? Yes, and here's how i did it. Over in the service worker instead of cache the root page with the date in it. I'm going to cache the skeleton. I'm going to bump the cache version number as well because we've changed things. Now I'm going to pause the request URL. First, I check to see if the request origin is the same as the current origin. Remember that the service worker handles requests for other origins too. In this case, we only want to intercept route requests for the same origin. Now I checked the path name, if it's the route, I respond with the skeleton straight from the cache. I don't need to go to the network as a fallback, because I know skeleton is cached is part of the install step. So let's test this. I refresh the page get told about an update. I click refresh and now I'm running with the page show. At this point, we have a pretty viable product, this is something we could launch. Things haven't changed much when the connection's perfect, but perfect doesn't really exist. On a slow connection we get to render the top bar much quicker, we even get to render content quicker purely because our JavaScript is arriving sooner. Li-Fi is still a bit crap really, but at least something is on the screen. With offline, we're now showing our own error message using the branding of our app, rather than the default browser error message, which is something. Even at this stage, we can massively improve the user experience, but now we need to cache those winter posts. And to do that we're going to use one of the web's most hated APIs. [MUSIC] Yes, but seriously, we're going to tame this API and learn how to use all the good bits of IndexedDB without getting burned. It might even be fun, maybe. Hello, your watching the UDusky Offline first course, I'm Jake Archibald broadcasting from London. But we now go live to our US correspondent Mike Wales. Hi, Mike. How's the weather over there? [INAUDIBLE] Mike, Mike, you're breaking up. You some like two dialects having a rap battle. Can you hear me now? Yep, yep, go ahead. [INAUDIBLE] Okay, I think he's gone. We had this whole news reader thing planned, it was going to be fun. With home broadband and smart phones, the human race is increasingly reliant on the internet. But the internet isn't always with us. I'm sure you're familiar with this. [MUSIC] That kind of thing. The sinking feeling of being abandoned by connectivity. You're stuck outside while the internet parties without you. You can only imagine what you're missing out on. It could be anything, literally anything. A new video of a cat, a politician saying something stupid, world peace could be solved in 140 characters, but you don't get to hear about it. All you get is. [MUSIC]. Total failure in the face of connectivity issues is a worst case scenario. And it's all too common on the web, but no longer. In this course we're going to cover how you can build sites and apps that work great what ever connection the user has. We're going to use service worker to intercept network traffic, explore the latest service work at dev tools. We're going to tame IDB, the in browser database, and explore user experience strategies to deal with varying network conditions and updates. Overall we want to make [NOISE] a thing of the past. First, the problem. Here's a simple website containing a live stream of insightful gibberish from multiple users. It's a pattern you have seen on multiple social networking sites. And we're calling it, Witter. Just in case lawyers are watching. I've added it to my home screen so it looks like a native app I might get from the Play Store, except it still has all the benefits of the web. We call this a progressive app, or we would if it weren't for one little problem. If I open it when I don't have an internet connection, I get this. This is a problem, but not our biggest problem. This is. I call it Lie-Fi. Let's enjoy the fruits of Lie-Fi for a moment, shall we? This is it, just this forever and ever. You're in connection limbo. You could just put your phone in your pocket and get on with something else. But you keep waiting because it might suddenly load. Maybe within the next five seconds. No, you should probably give up now. But you know in your heart that one second after you give up, that would be the moment it would have worked if only you'd waited. So you wait, and wait. This situation can be caused by a phone barely holding on to a connection. But sometimes your phone thinks it's acing it in the connection department, but nothing is getting through. The problem is further down the line. Somewhere between you and the server, where the app and data lives. There's a lot in between your phone and the server. Your phone sends the request to the Wi-Fi router or cell tower. Then onto the ISP, through intermediate proxies and potentially, across to the other side of the world. And eventually, the request reaches the destination server. But that's only half the journey, because the server responds. And that response needs to go all the way back across the world through proxies, through ISPs, over the air and land safe and sound on your phone. But if something along the way, fails. The whole thing fails. If something along the way run slowly, then the whole thing run slow and there in lies lie-fi. Maybe you have bad signal or maybe you have perfect signal but the router or tower you're connected to is overloaded. For example, a crowded stadium or a train station that's just been hit by major delays. Everyone's on their phone trying to find another way home, and the cell tower can't cope when people need it most. Or maybe there's a faulty or busy proxy. Maybe the destination server is busy or has a bug. Loads of stuff can go wrong and we can't predict any of this ahead of time. The only way to know if we can get a response is to send a request and see what happens. And that can take a gruelingly long time and the worst thing we can leave the user with is nothing. Let's do better. So what can we do about it? How can we deal with these kind of failures and delays? We could attempt a network fetch and if that fails, get fallback content from some kind of cash, perhaps the last content we were able to get from the network. However, we have to wait for the network to fail before showing fall back content. And if the connection is slow, users still get nothing, that rage inducing life I experience. This is why the gold standard is offline first. Offline first means getting as many things on the screen as possible, using stuff already on the user's device in caches and such. We might still go to the network, but we're not going to wait for it. We'll get stuff from a cache as much as we can and then update the page if we finally get content from the network. When we get that fresh data from the network, we can update what the user is looking at and also save that new data into the cache for next time. If we can't get fresh data from the network, we stick with what we've got. Maybe it's out of date stale data, maybe it isn't. Without the network, we might not know but one thing's certain, it's way better than nothing. Taking an offline first approach means the user is happy online, happy offline, and even happy with wi-fi. The less the user has to care about connectivity, the better. This cause covers exactly how we achieve that. But first, let's see if we can get our U.S. correspondent Mike Wells on the line. Hey, we've got the connection problems worked out over here. I got a whole new set up. Great, so Mike you're a kind of end of level boss that these students must defeat in order to progress on this course right? Defeat? Well kind of, but it's not as scary as you make it sound. I'll be giving you some coding challenges along the way, so you can get accustomed to actually implementing these things. Taking a demo app from online only to a fully offline first experience. In addition to coding challenges, we'll do some multiple choice quizzes. In fact, let's do a quiz now. First up, of the following items, which do you think can either slow down or prevent users receiving data from your website? Poor signal, a misconfigured proxy, fault in the mobile network, a busy network, server being DDOSed, a bug in server code, Wifi captive portal, or the moon's gravitational pull. All of the options could slow down or prevent users getting content from you. The moon's gravitational pull is debatable of course, but maybe one of the hops between you and the user is somehow vulnerable to that. You just don't know. Given all the variables, network connectivity is a huge unknown. Let's talk about online first, the way we typically approach development. Online first means, we try the network first. If that fails, we serve some offline content like some cached data, a fancy error page, or a picture of a sad cat. So based on what Jake has been saying. If we pick an online first approach rather than offline first, in which of the following conditions will our app work well? In good connectivity? Poor connectivity or what we call lie-fi? Or offline? An online first approach works fine with good connectivity. In fact, it's no different. Things work great offline because you provide fallback content instead of a browser error page. Unfortunately, because we're waiting on the network before we do anything, things are still terrible with live with lie-fi. With Offline first, we get a good experience in all three situations. In fact, we become faster even if the user has good connectivity. In the next chapter, we'll look at how we can turn an existing app from online only to offline first. Offline first means we try offline first and we also try to get stuff from the network just after. So which of the following employs some kind of offline first approach? Deliver the page's header and content from a cache on the device, then attempt to fetch updated content from the network. Deliver the page's header from a cache, then attempt to fetch the content from the network. Attempt to fetch the page from the network, and if that fails, show cached content. Attempt to fetch the page from the network, and if that fails, show an error page from a cache. If we can get something on the screen without waiting for the network to succeed or fail, that's offline first. Delivering full content without the network is a full a star offline first experience. Delivering just the header without the network isn't as good, but still that header was delivered offline first. And getting that on screen, while you're busy with the network will give your app a better perceived performance versus a blank screen. The other two options require the network to succeed or fail before showing anything. So they're online first, not offline first. In a moment Jake will introduce you to a demo app that works online only. By the end of the course, we'll have it working lightning fast and offline first. Okay, now that we've talked about the theory, let's get some code going on. We're going to take an existing app and make it work offline first. We saw it briefly in the previous chapter, Wittr, a simple social networking timeline. It's currently slow in poor connectivity, and a total failure offline. We're going to fix that. First you need to get it working on your local machine. All of the commands I use here are in the present as notes for you to run yourself or even copy and paste. But before you run them, you'll need to install Node.js as all the server and build scripts are written in JavaScript. Once that's installed, clone the project from GitHub. This will download it all to your local machine. If you're new to Git and want to learn more, there's a whole Udacity course on it. So if you're already bored of my face and voice, you could do that course instead. I've linked to it in the instructor notes. But you don't need to be one of the Git elite for this course. We'll give you all the commands you need along the way. Right. I've checked out the app. Now, I'm ready to run it. Once again, all of the commands I use here will be detailed in the presenters as notes. First step CD into the project folder and run npn install. This fetches all the third party code the app needs, and it'll probably take some time, so be patient. Also npn does like to scare us by showing errors and warnings, but these are optional dependencies and different operating systems will show different warning, so that's something to enjoy. I love it when text flies by on the command line like this it makes me feel really smart like I'm hacking the metrics or something. Right. That's the install done. You shouldn't need to run that again. To run a project the command is npm run serve. When you run that the build script executes, building all the JavaScript, CSS, etc, and finally it tells us we have two servers running. If you ever want to shut these down it's control and c. You can also modify the port numbers of the servers if you want. Those details are also in the instructor's notes, but now we can open a browser and point at localhost port 8888. And there we go. The app is running. Excellent. Now it's your turn. Once you have everything set up, check the boxes to confirm, and we'll explore the app in more detail. Here's how the site works at the moment. When you navigate to Witter, the browser makes a request for some HTML. Like all web requests, this goes via the browser's HTTP cache. And if there's no match there, it continues on to the internet. Then hopefully, the response makes its way back to the browser. The html the browser receives tells it it needs some CSS, so that's fetched. Once that arrives, we get our first render, a page full of content. You see the initial set of posts are right there in the HTML. But at the same time the browser downloaded the CSS, it's also requesting some JavaScript. And when that arrives it opens a websocket, a persistent connection that lets the server continually stream newer posts as they arrive. This provides the live updates so the user doesn't miss out on a single piece of nonsense posted by the other users. In terms of the pages JavaScript you'll find that in public JS main. The starting point is index.js, but all this does is load, polyfill is that some browsers may need such as promises and the URL API which will use a little bit later on. The main work happens in IndexController.js. The constructor here is run for every page load. We have this PostsView which helps us update the post on the page. This ToastsView helps us display error messages, hopefully we won't get too many of those. And then we open a socket connection. The open socket method, well as you might guess opens the WebSocket and the message event is fired when data is received for a new post to display. When you post a received on socket messages called, this method passes the data is chosen and passes the result to the PostView. While the server is running, any changes you make to these files will be processed and built. I didn't want ES6 to be a prerequisite for this course, so i'm going to stick to ES5. But the build system uses babel so if you're like a cool kid from the future. Feel free to use some of the latest JavaScript syntax. And i almost forgot. There's actually another server running on port 8889 so open up. This gives you basic control over your connectivity to the server. How to play with the settings. Refresh the page and see how it responds. You'll see things are pretty bad in everything except perfect mode. Over the course of this course, we're going to fix the non perfect cases. But before we get stuck into the code, we're going to be in pixels across the Atlantic to Mike. Mike, I hear you've got a bit of an interactive task up your sleeve. Hey, yeah, I do. We want to make sure the project is working correctly. And you're able to alter the connectivity. We've already seen both servers running, one is the app itself and one has these configuration options including this little test form. When one of these challenges requires you to change the project, either by changing settings or writing code, you get it all working. Then, type the test name into this box, and press Enter. The system will run a little test, and decide if everything's working as it should. If you want to stop the animations for some reason, although I don't know why you would, who doesn't like cats? Just clear the text field. So on to the first challenge. Use the connection controls to get the app server into offline mode. Once you've done that, refresh the app to see what happens. Type offline into the test field and press Enter. Yep to experience the app offline we hit the offline radio button and refresh the at page and that's the offline experience. It's not a great experience but it's all we have right now. We're going to fix that soon. Let's try one more. Try running the site in lie-fi mode. Once you've done that, type lie-fi into the test field and press enter. We hit the Lie-fi button and refresh the app page. And yeah, this is the worst. Thankfully, we're going to show you how to avoid exactly this. And that means, learning some relatively new browser features which is what we'll do in the next chapter. So, we're familiar with the problem. Zero connectivity is a bad experience, low connectivity is, well maddening and previously there was nothing you could do about it. All you could do is make a request and then it's entirely out of your hands. It might work, it might not, it might be fast, but it might not. You have zero control until the request succeeds. If it even succeeds. Well, that story changes with a relatively new browser feature called service worker, and that's- Wait, Jake. What? Was that it? Was what, what? This is the feature that solves everything, the feature that gives developers new powers over the network. You could at least introduce it a bit better than that. How do you mean? Well. For too long, users have been left staring at a white screen. For too long, they've been let down by the cruel seas of network connectivity. And for too long we've been powerless to help. [MUSIC] We've been left waiting. But no longer. [MUSIC] A new browser feature has arrived. A total game changer. [MUSIC] A feature that lets you control the network rather than letting the network control you. Who is this new feature? What promises does it bring? Introducing The Service Worker. [MUSIC] Yeah okay, that was pretty good. I'm British you see. I'm physically incapable of positivity and excitement. Well, I still like you, Britishness and all. Well, thank you. And listen, if that wasn't compelling enough for you to stick with us for the next lesson, I don't know what is. So strap in and click through to the next lesson. Let's drag the web into the world of offline first. Yeah, what he said, let's do this.