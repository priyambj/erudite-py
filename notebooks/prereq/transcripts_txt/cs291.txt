Here are a number of terms covered in this course, match each with it's related concept. Normal transformation, anisotropic filtering, skinning, lambertian, angle axis, vertex shader. Quaternion, texture map, diffuse, model-view matrix, transpose of inverse, bones. To transform a surface normal you need to use the transpose of the inverse of the matrix you used to transform the surface itself. Anisotropic filtering is a way of improving how a texture map looks by taking additional samples and blending them. Skinning is an animation technique that uses bones to define the area of influence of the rigid parts of the model. Lambertian reflectance is the model used for the diffuse component of the basic illumination model. The axis/angle formulation in a quaternion describe the same thing, a rotation transform. The vertex shader uses the model-view matrix to transform points in view space. The dot and cross products are key for object transforms, lighting calculations, and camera frame creation. Here are some statements. You decide whether each is true or false. The dot product of a vector and itself is always 1. The dot product of two perpendicular vectors is always 0. The cross product of two normalized vectors always has a length of 1. The cross product of vector with itself always has a length of 0. Fair warning, don't assume a vector is normalized unless it says so. The angle between a vector and itself is 0 degrees. The dot product computes the cosine between 2 vectors, times the length of each vector involved. The cosine of 0 degrees is 1, but the length of the vector is not necessarily 1, so the dot product itself is not guaranteed to be 1. This statement is false. If the vector was stated to be normalized, this statement would be true. If two vectors are perpendicular, they are 90 degrees apart. The cosine of 90 degrees is 0, so this statement is true. The cross product of two vectors computes the sine of the angle between the vectors, expressed as the length of the resulting vector. This sine can vary from negative 1 to 1, so the length of the resulting vector can also vary in length. If the two normalized vectors are perpendicular, the length is always 1, but not at any other angle. This statement is false. The angle between the vector and itself is 0 degrees, so the sine will be 0, meaning the resulting vector will always be length 0. This statement is true. Which type of shader, vertex or fragment, is most closely associated with the following algorithms? Image processing, phong shading, skinning, displacement mapping. Image processing works on a pixel level so is associated with the fragment shader. Phong shading, as opposed to Blinn-Phong illumination, is where the normals interpolated across the triangle and each normal is used at the pixel level to shade, so again the fragment shader. Skinning is an animation process where a vertex is transformed two or more times and the results are weighted and added together, so that's vertex shader. Displacement shifts the actual locations of points, something only the vertex shader can do. On the show Sesame Street there's a song. One of these things is not like the other, one of these things just doesn't belong and, yeah, this is why I'm in computer graphics. Three of the subjects listed below are related, one is pretty unrelated. Which is the one that's not like the others? Object hierarchy, right-handed coordinates, cross product direction, mirror/ reflection transformation? Object hierarchy is unrelated to the other three topics. Right-handed coordinates, the cross product, and the mirror transform all have to do with handedness in some way. For example, the resulting direction of the cross product of two vectors is determined by the right-hand rule, if you're using a right-handed coordinate system. A mirror reflection switches from one handedness to the other. One method of adding indirect illumination in a scene is to capture the effect of surrounding light for various points in space. Here's a view of the idea from work by Greger Et Alia in 1998. The spheres represent the captured illumination. For a point on a surface in space, the nearby spheres are accessed by the surfaces normal and blended together. There are many different texture map types, what type of map would these spheres use? Specular map? Normal map? Diffuse environment map? Refraction Map? Displacement Map? Or Road map? The answer is diffuse environment map or a radiance map. This type of map captures the diffuse contribution from the environment for various normal directions. By having these sorts of maps for different points in space, it's possible to the diffuse contribution also vary with position. This idea is called the nuradiance volume. In practice, there is more compact representation than cube maps, but the idea is the same. Here's a pipeline for a pencil making machine. The execution time for each stage is shown. The painted pencils produced will have to pile up and wait for the pencil neck stage where the eraser gets attached. Two questions. Once it gets going, at what rate do the pencils come out? The second one is, what is it called when the previous stage has to wait? It is stalling, aliasing, swimming, Peter-Panning, starving or banding? The pipeline produces at whatever speed the slowest stage runs at, so the first answer is five seconds. The stage is said to be stalled if it has to wait for the stage to clear out after it. You're given three points of a triangle. Given these three points, two vectors are formed. What vector operation do you need to perform to compute the normal? Is it the dot product, multiplication, inversion, transposition, subtraction or cross product? Don't worry about the normal being normalized, I just want to know how you get the direction. If we move this vector down to here, we can then take a cross product. The cross product of these two vectors gives a normal, using the right-handed rule. Check all that are true for objects transformed by perspective and orthographic matrices. Lines remain straight on screen. Distant objects appear smaller. Resulting screen depth is linear with the incoming distance. Parallel lines remain parallel on the screen. Unlike a fish-eye lens, lines remain straight on the screen with both a perspective and an orthographic transform. Normal perspective matrices cause distant objects to appear smaller. Orthographic do not. The Z depth varies nonlinearly with perspective. It's linear with orthographic. The nonlinearity for perspective is connected with the fact that lines stay straight when transformed. In perspective, parallel lines usually meet at a vanishing point. Orthographic is useful in computer-aided design, because lines are made parallel. Vertex and fragment shaders give programmers a lot of power over the GPU. However, some elements of most GPUs have transistors dedicated to them. Check the elements in the pipeline that are hardwired and not currently programmable, at least not to any great extent. What's available hardwired on the GPU? The illumination model, vertex transformation, Z-buffer evaluation, image processing, such as color correction or edge detection, rasterization and interpolation, clipping. All the hardwired elements don't necessarily have to be used by the programmer. You should check off those capabilities that are available. One way to think about this problem is the inverse. Which of these processes normally is done by a programmable shader? The illumination model is evaluated at the vertex or shader level. Similarly, the vertex shader performs model view transforms as well as perspective, though not the perspective divide. The Z-buffer is hardwired in. As this is a speed critical piece that is used by most 3D applications. It's evaluated outside of the fragment shader. Image post-processing, such as color correction and so on, is something that can be done in the fragment shader. Typically by rendering a full screen quadrilateral to drive the computations. Rasterization happens in between vertex and fragment shading, so it's performed by dedicated hardware. It's what generates the fragments for the fragment shader to process. Clipping is also hardwired and happens just after the vertex shader and before rasterization itself. The vertex shader outputs vertex locations and clipping space for this hardware to process. In this scene, three viewers, A, B, and C, are looking at a point on a somewhat shiny surface, illuminated by one light source. The illumination is calculated using the Blinnâ€“Phong lighting model. Which viewers see the brightest diffuse illumination, if any? Which viewers see the brightest specular highlight, if any? For the diffuse component, the angle of the viewer to the surface or light doesn't matter. The amount of illumination depends on just the angle of the light to the surface. So the correct answer is to not check any of the boxes. With specular surfaces, the shininess is brightest in the reflection direction. This can actually vary a bit in real life materials. For example, some roughened metals can have what are called off specular peaks. Even then, the eye at location c would still see the brightest reflection off a surface. The connection between the virtual and the real one is becoming closer every day. One aspect that I find particularly exciting is how objects we design in the virtual world can be made real just a few hours later. Even if you don't own a 3D printer, you can order 3D printed objects from a business that does. For example I found this teapot model on the web at Sculpteo, a 3D print service provider based in France. I previewed it on the website in Web GL, picked out the material, and ordered it. At that point, there was no physical model, it was just data on a computer somewhere. A week later, when the real thing arrived here in California, that honestly amazes me. To learn more about the connection between 3D graphics and 3D printing. Jessie Harrington Au and I talked about these topics while at the Autodesk gallery. I'm here at the Autodesk Gallery today in San Francisco. There are some really great objects here that have been made with the aid of computer graphics. Jesse Harrington Au is here today to explain what these objects are about. So Jesse, first of all, what's your background? &gt;&gt; : So, my background's actually in industrial design. A long time ago, I was an illustrator. And then I started doing these crazy exhibits, and weird things in warehouses. Started getting involved very early on in the maker movement and then went back to school for industrial design. Started doing product design, then do a little exhibit design and that's fortunate enough to get picked up by autodesk. And where I get to make a bunch of cool stuff around young entrepreneurs everyday in a place called tech shop. &gt;&gt; So well anyway, we do have some objects here today and how about this one which is really cool coz it has moving parts, can you tell us about that? &gt;&gt; So this is just a, its the basic claw sample and this actually, it's kind of funny, this one comes with a machine when you buy it but the really cool thing about it is. That when you design something like this, it used to be that you would design something like this, send it overseas. Get a mold made and eventually around three months in, send it back to you. It would cost you about $40,000 and that's on the low scale. But now, I can design this in the afternoon, print it that night on my at home 3D printer or take it to a service like Shapewaves or any of those places. &gt;&gt; And have it back within a week for about you know, anywhere between fifty to a hundred dollars. And I can see if things fit together, I can see you know if there's thing that need slight adjustment before I go into some sort of mass production, or maybe I don't even need mass production, you know, it could just be for our home use. &gt;&gt; Is it just printed as a single thing or did you snap it together or how's that work? &gt;&gt; Yeah, so this one's really cool, it's actually printed as a single object. So what it does is if you will, it just sits in a bed and the bed, think of like an ink jet printer where a pad's coming over it. When that pads comes over the first time, it puts little droplets down to put that first layer. And on the way back it hits it with an ultraviolet light which hardens that object, and then builds it up with support material on the inside. The support material is water soluable. &gt;&gt; So at the end of it when it all comes out I can then take it and wash it off with a water pick and take all that support material out. Leaving me with a very functional object. &gt;&gt; So support material is so that doesn't sag or what's, what's support material about. &gt;&gt; Support material is, is just that. It's there for moral support. It's there for to support the piece. So if you think about inside of here, if you had to build something in space that was just going across this plane, there's no way you could really do it because you can't build on nothing. You can't just put something out into mid air, yet. And, but so that's what we do is we actually put support material. Material in there for this to build on top of and then we can rinse that out, so we can have things with a giant cliff or you know, a weird angle at it, that you wouldn't normally be able to just work your way up to layer by layer. &gt;&gt; So just about any shape can be made with 3D printing then? &gt;&gt; Yes, and in fact, the coolest part about 3D printing and the things that I think is the most untapped source of 3D printing is that you can really make anything with 3D prints. So we can get way beyond what we used to do, or take as manufacturing, I guess you'd call them design constraints. Where, those, one of those design constraints is now leaving. So we can really get into what does good design mean. &gt;&gt; So one thing I've heard about 3 d printing or catchphrase is complexity is free. I don't know if you want to say anything about that. [laugh] I think that's an interesting one. It's what, what does that mean to you? &gt;&gt; So that to me means you can get as complex as you want with an object. And you can go as crazy as you want. I've seen people do stools that were replicated sea foam. Because they could 3D print them, and there's no other way you could ever manufacturer that, now you can. And, I think really what that means also is, complexity is something that we can tend to put into something, and we tend to make things over complicated, because now that we can. And, now's kind of the time where the wave hits the end. Right? And you have to look back and say again, you know, what's the best way to make this and how do you make something that could be really complex, very simplistic, because now we have the technology to do it and the design software is so easy. &gt;&gt; Right, right. I mean I've heard of experiments where they'll like evolve designs and they'll say well, now that we can build it in any shape we want, let's just start putting out different designs and have it computer analyzed. &gt;&gt; Yeah. &gt;&gt; And see which the best one is. &gt;&gt; Yeah. &gt;&gt; So that sounds really interesting. &gt;&gt; With 3D printing, I mean, the future could be very, very Baroque. [laugh] ... &gt;&gt; [laugh] &gt;&gt; You could have your face on every single object in your house if you wanted to, ... &gt;&gt; [laugh] &gt;&gt; But that's one of those questions that keep coming up, is. Yeah. [laugh] is how do you scale that back and say, well do I really need my face on that door knob? Is that the best door knob there is? [laugh] You know, to be able to grab my head. It's probably not. &gt;&gt; So okay, so this is kind of a plastic material. You have sort of a, there's this guy who's kind of rubbery or, so what kind of materials are there? &gt;&gt; So there's lots of different kinds of 3D printing but right now you can almost three d print any material that you can think of. I've seen things that are bronze, we've done things out of different types of metals. &gt;&gt; All sorts of plastics. This one has a rubberized fading into a plastic texture. &gt;&gt; Hm. &gt;&gt; Which is kind of crazy, so this is all one print also. And it has the rubber going right into the hard clear plastic, which is pretty crazy. This was also designed in a super easy program called 123 3D design that I mean, children could do it. I mean, all the parts are in there native and you just stick them together and this is what you get out at the end. &gt;&gt; Uh-huh. &gt;&gt; You know, no other time in history. &gt;&gt; Yeah right. &gt;&gt; Could you ever put something together this quickly that of this caliber. &gt;&gt; What's the sort of design process, I mean I've made my shock absorber designed it on my three d program and now what do I do, do I hit a button and send this to a printer or how's that really work? &gt;&gt; So most three d prints work off STL files or OBJ files and almost any 3D software program can export to STL or OBJ. So once you do that, then you can decide what kind of 3D printing is going to work best for you. Or maybe it's just what's accessible to you. Something like this was built on an SLA machine, which means it printed similar to a hot glue gun when printed, right? Where it's a, a hot piece of material coming through a hot head and printing out layer by layer and building that up. Where this one's great. Worst sound in the world but, yeah. &gt;&gt; [laugh] But it's flexible. &gt;&gt; Yeah. &gt;&gt; Pretty awesome. &gt;&gt; And what's really great about it is there's a billion ways to show people on screen your design and your idea, but most people don't really understand that until they can hold it in their hands and feel it. So, that's really the advent of 3D printing is when I can take this to my boss and say, look this is going to save us so much because of this design. Look at how much material, you know we've saved or, you know this is going to work so much better this way that's, that's kind of the glory of it. &gt;&gt; How does this sort of work into the design process? I mean, I've heard of this thing rapid prototyping, how does that, what's that all mean as far as 3D printing. &gt;&gt; The real hindrance with 3D printing right now is scale. It's very cost prohibitive to print things like that are very large and scale. We have a motor cycle in the gallery. But, that's pretty big print and it took a lot of time and so there's this other idea out there called rapid prototyping and there you see products like one, two, three make things that take very large objects like this elephant. Or last year refer maker fair we did a 12 foot tall trojan horse out of cardboard. &gt;&gt; And it takes those big elements and it takes them out into folded panels or it takes them like as you can see in the octopus in the background into sliced cardboard cutouts. So we can use something like a laser cutter to make things very larger very quickly. &gt;&gt; So it's just a matter of time, so right, I usually think of three D printing because, since you're doing layer by layer by layer, that can take a while. &gt;&gt; Yeah. &gt;&gt; The higher resolution you have, I would think the more layers you have to put down. &gt;&gt; Yeah its a really interesting time too because the high end 3D printers are extremely high res and very slow [laugh] and very high cost. In the consumer grade printers, things like a maker bot or a typing machine they're extremely low res and very fast [laugh] but their cost is kind of going up and the high res stuff, the cost is kind of going down so I think the next couple of years, we're going to see a meet in the middle. And you're going to end up with this great device that almost anybody can use and give you know, a print that is very usable for multiple things. Our next unit is all about materials and the trade offs between speed and quality. I caught up with Sam Black at the Autodesk gallery. Sam is a senior engineer working on AutoCAD, a computer aided design package. Here's what he had to say about materials and performance for Interactive Rendering. &gt;&gt; I'm here at the Autodesk Gallery in San Francisco with Sam Black. So, Sam, well, first of all, tell us about your background. &gt;&gt; Well, I have bachelors' degrees in electrical engineering and computer engineering from University of Michigan, and a master's in computer science from University of North Carolina. I've worked in various parts of the graphics industry. I started out in workstation graphics, working on the first commercially available X server. Then I went to work in the games industry for a little while. Came out to California, worked for Pixar for eight years, and then came to AutoDesk. &gt;&gt; So you've really gone through what I consider sort of three pillars of the the graphics industry, which is film, games, and and computer-aided design. tell me a little bit about like, well, you know, what are the pluses and minuses of each? &gt;&gt; Well, they're all, a lot of, they have a lot of similarities amongst them and so, it's really the sort of the end product that you're looking at that's different. And everyone thinks it's the games are, or how much fun to work on but in a way they're almost the hardest because they tend to have the most compressed schedules. Although the film industry has tight schedules, but the project that I was working on tended to be a little more open-ended. I was working on the RenderMan product and some advanced rendering techniques, so I wasn't tied directly to a film, to a release. So, I didn't feel all the pressure that. &gt;&gt; No. &gt;&gt; You normally feel in those things. And then coming here to, to AutoDesk, working on AutoCad, again very similar, si-, working in the graphics field and it's just a lot of, you know, getting the, the right visuals out there with the right performance. &gt;&gt; So you're working on Auto Cad now. So what is Auto Cad exactly, is it a platform, is it a program, what is that? &gt;&gt; If you're a artist, it's sort of viewed as both. It's a product by itself, but it's also, other products are built, on top of it. There's a map product for GIS data, there's architectural products and mechanical engineering products. But when I'm working with it I actually view it as a standalone product, by itself. And it's just a tool that allows designers to really turn their internal designs into reality. Let's, let's contrast and compare a little bit here. What I find interesting is that there's the game industry where we we see that a lot, because the games out there. and there's sort of computer aided design, which is sort of a different kind of a graphics, different kind of a 3D computer graphics way of doing things. So, well for starters, what, what, what would you say is the biggest difference between sort of games programming and computer-aided design programming? &gt;&gt; I think the end result is, is the big difference. In that, if you were playing a game and something happened with the visuals, you know, a frame or two goes off. The worse that happens is your avatar dies. &gt;&gt; If you mess up in a computer aided design package, people can die. It's[LAUGH], it's just that there's, there's much more, I think I have much more invested in, in Autocad, because of the importance of the work that it does. &gt;&gt; Well with game design, it's, it's kind of a a limited field, in a sense, in that, they're in total control over the product, and there's a whole team working on that. &gt;&gt; Your product is different in the sense that you're selling it to people and they're using it for who knows what. Can you tell us some of the differences there? &gt;&gt; Well, like I say, in games, the world and the behavior is fairly static. The things happen and things can move around and change, but in general, like I said, there's a lot of control that The, the game designers and the game programmers have over what's going on in the game. Whereas with the CAD system, our users can do pretty much anything. And an interesting this is that CAD systems, you can view it as a way of creating content. So people can actually write games using a CAD system to design the assests for the game And then play the game. So, they can, they can almost interact with each other that way. But you need a lot more flexibility within, your, the CAD package or what, whatever package you're using to design the assets for the game. Once you get into the game they're a lot of tricks that programmers play and artists in order to, to fake things out and for, get whatever you need done. Again, you can also deal with the fact that if you're a little bit off one frame, it won't hap, won't really matter because the action is moving quickly enough that you'll get right past it. &gt;&gt; Right. I mean, just the fact that there's a reality to, you know, whatever you're, whatever you're building, makes sense that there's, you know, Autocad and so on. It really has sort of different kinds of constraints. well with that, for example, I mean &gt;&gt; Are their constraints as far as precision or elements like that. &gt;&gt; Well, definitely we, we have to deal with things like, we have this one model that we use internally for a lot of testing which is the model of the solar system. So we have to have auto cad be able to go all the way from the, looking at the entire solar system down to looking at a plaque on the moon that was left by Neil Armstrong. &gt;&gt; [LAUGH] &gt;&gt; And we need to be able to read the lettering on that plaque. So just the range of precision that we have to deal with can, can really blow away a lot of the graphics hardware, so again, there're some tricks that you have to play in order to be able to handle the all that different resolution with only single precision floating point. &gt;&gt; Can't break it. &gt;&gt; The nice thing is, that we're dealing with soft, we had our software implementation. Of course we had double precision, we could deal with it much better but, when you, when your end result really requires single precision. That's where it gets tricky. So yeah, as far as [UNKNOWN] go, one thing that, again, with the computer game, you can do, your, your final goal is whatever is on the screen. It's just a pretty picture, basically. you can do anything you want with materials or lighting and so on. how AutoCAD, can you contrast and compare as far as materials and lighting and so on andwhat, how does that work in that? &gt;&gt; a lot of that varies on customer because there's still a lot of work that's being done in 2D with lines and not necessarily a realistic view of, of what's going on. But the precision is still required. We, we had one customer that use to print out boat hole designs at full scale, and then use, use that to actually build his boat holes. &gt;&gt; [LAUGH]. &gt;&gt; [LAUGH] So it really varies when, when you're dealing with somebody who is working more in, in a 3D environment, then, again, there are different levels. Sometimes you want just sort of conceptual look for your design that you are going to present to a client to show them what it is you want to build. But then, at some point you want to get a realistic fully rendered ray traced image, so we need to be able to support that level also. So, it's really is a great variety of visualizations that are required in the CAD product. &gt;&gt; So people can choose among different styles for example or. &gt;&gt; Yes, yeah that, like I say, you know, we have this conceptual style which is almost cartoonish, and because it's x-ray styles are really useful to be able to see what's inside some of the objects. Of course, you know, just the standard realistic style with the very realistic lighting and shading. &gt;&gt; That, well that's interesting, I mean, like with games there's a minimum requirements, you know, you must have this DirectX 11, or dadadadada. so, do you have like minimum requirements or are there sort of tiers of support or how does that work? &gt;&gt; We have a, minimum set of requirements for your system and for graphics card, but we also have a very thorough certification process because. Even if a card says that, you know, it runs DX11, it has all these effects, there can be problems with a card of a driver. So we actually have to go through card driver combinations, certifying each one, run through a whole list of effects, make sure they can do each one well and efficiently. Because depending on the customer, some customers may want the visual accuracy and don't care as much about the performance. Some want the performance and don't care as much about the visual accuracy. So depending on what it is we're striving for. If we have an effect that looks really good but is slow, that will work for some customers but not all customers. &gt;&gt; [CROSSTALK] There are also the frame rate issues that you see, particularly in games. Where a game has to run at 30 hertz or 60 hertz. And, again, they will cut some corners in order to, to be able to do that. We don't have those strict requirements, in CAD. Although there are times when someone has a model. They need to orbit it, they need to pan around. So, we do need to get good frame rate out of that. And we'll actually play some of those same tricks, where we'll slightly degrade the visuals in order to be able to, to want, to see the model moving, and then when you stop, we, we cut off the degradation and bring back the original model. The games people have a definite constraint. Like if it goes less than 30 frames per second then they have to cut back somehow. They have to get rid of polygons or they have to go beat their artists to go put you know, less content in the scene. but since your selling to who knows who, you know, the guy could say, I wanted 50 oil derricks or, you know, 50 whatevers. You know, in my scene and so what, what kind of technique are used to sort of control that, so I want as much as I want, you know. &gt;&gt; Some of those are sort of reducing geometric complexity, where if you have, you have a complex model so as you start to orbit if it's too slow, we can desimate some triangles from that. So again, there, there are a lot of techniques that are similar between games and CAD in those situations. We tend to let the user have more control over that, where, again, they, they may care more about it. Some customers maybe fine you know, if they need to orbit something, we can turn it into a wireframe object, because that will get them to the point they need, and then we go back to the, to the full resolution. And lighting is another one where we can have some effect. We have a landscape lighter who has a thousand lights in their scene and you can't necessarily do that in real time. So sometimes we, we'll cut out some of the lights that will have a lesser effect on the scene. So instead of getting 100% fidelity with the real world, we, it may only look 80% correct. While they're moving around, but then once they stop, then we can, we can go back to showing the full scene. It's easy in 3JS to hook up the mouse, keyboard, and other devices as interactive controls for objects in a scene. We've been doing this all along with demos such as the robot arm. There's much JavaScript code to draw upon for getting inputs connected to programs, such as the dat.gui library for sliders. However, up to this point we've had little direct interaction with our virtual worlds. By direct interaction I mean using the mouse to click on the world itself and change its appearance. The main direct interaction so far has been using the mouse to move the camera. As an example of another direct interaction in this demo by Max Ogden and James Halliday I'm clicking to remove blocks or add them. Programs can be seen limiting without these sorts of direct interactions with what the user sees. Even when moving around the scene with the mouse, in many demos it says if we're a ghost, we can walk through walls or drop through the floor, there's nothing stopping us. By instead performing what's called collision detection between objects, and having the program respond to those collisions, we can create a more believable world. We can also use physics to determine how our actions and the movements of other objects change what we see. If you've played any computer game, you know how various button pushes and so on are tightly tied to what you see on the screen. How the world reacts to inputs such as these is often shown through animation techniques of one sort of another. Up to this point, we've mostly been ignoring this fourth dimension, time. Once we start changing elements in our scene over time, we're performing animation. Animations can also be run in their own right. Streams flow down a hillside. People walk past you in a marketplace. The sun sets over ocean waves. Animation, such as shown in this demo by Daniel Heat, gives a scene life and a sense of engagement even when there's little interaction. What I'm going to cover in this unit is a sampling of interaction and animation techniques. So let's get going. Direct interaction, where for example, you use the mouse to control or influence what's happening in the scene, is handled using events. An event is generated by the system whenever a user presses a key or uses the mouse. There are other kinds of events, but these two, keyboard and mouse input, are the main ones we care about in Three.js. Well, I should mention there's also a touch interface that's somewhat similar to the mouse interface, so that you can interact with scenes on a smart phone, or other touch-sensitive display. You can ignore any and all events or can listen for any you wish. Here's code for listening to a mouse click event, specifically mouse down, which means when the mouse is first clicked. If you want to wait until the mouse button was released, you would listen for mouse up. The addListener event takes a function name as an input. Whenever the mouse button is clicked this function is called. This code defines the function called on mouse down. This preventDefault line is optional. It makes your program not past the mouse event on up to the page running your javascript program. I usually don't use this call. The rest of this function is up to you. A mouse or key click could modify an object, start an animation, you name it. Enough talk. What follows is an example of using the mouse down event to perform picking. You are orbiting around a bunch of blocks. Try clicking on a block as you move past them. The demo performed 2 separate actions, selecting the object clicked on, and if any object was found, then the object itself was modified and the point clicked on was highlighted. Picking objects on the screen with a mouse is quite simple in Three.js. Here's the first half of the MouseDown function which sets up to perform picking. The bulk of the code fires a ray from the eye into the world. This is a common way to perform picking. WebGL itself is focused on rendering, so has no real support for picking in other forms of interaction. It's up to the program to figure this out. To form a ray from the eye through the screen we need two points. One's the eye's position in the world space, that's easy. The other is formed using a point on the screen, that point being locate wherever the user clicked the mouse. What we want to know is this point's location in world space. To do so we need to travel upstream, back along the chain of transformations: window coordinates to normalized device coordinates, to view, to world coordinates. Happily, Three.js makes this easy. How many times have I said that sentence during this course? The 2D point from the MouseDown event is a document object model screen coordinate. This is like a WebGL window coordinate, only y equals 0 is at the top edge of the screen instead of the bottom. The mouse vector code here converts from this screen coordinate system into normalized device coordinates, which go from -1 to 1 in x and y. The z value doesn't really matter. Any value from z equals -1 to 1 will form a point on the ray. We next create a projector object. Really, it's better to create this object once during initialization and reuse it here, but I wanted to show the code all in one piece. The final line of code does the heavy lifting. It takes the NDC coordinate and forms a raycaster object from it. Ray casting is like ray tracing except you only trace rays from the eye into the scene and no further. No new rays are spawn from intersections. Once we have this raycaster object, we can perform picking, aslo called hit testing. It's one line of code to pick. You feed a list of objects into the raycaster and it returns an array of intersections. The If statement checks if the ray has anything in it. If it does then one or more intersections have been detected. These intersections are sorted front to back, so the first intersection on the list is the closest, which is usually the one we want. This first intersection has information about what the ray hit, including the object, face and the face's index in the object. We use the object parameter in the next slide to change the material to a random color. The intersects record also includes the distance and the point in world space. To show where the object was hit, we add a small sphere to the scene using this intersection point for its location. These are just examples, you can do whatever you like with the intersection information you get back. Once you've picked an object, you might want to drag it to a new position. Here is a demo of dragging in action. It's a bit more involved, since you also have to track and respond to mouse move and mouse self events. I'm not going to walk through the codes since you can take a look yourself. The main question with dragging in 3D space is how to constrain the third dimension? There are usually two choices. One is to constrain the movement to be parallel to the screen surface as seen in this demo. The other method is to limit movement of the object to be on a world plane or other geometry in the scene. For example, if you drag a handle on a face of a box to resize it, you limit interaction to be along the face's normal direction. Beyond the various mouse and keyboard combinations, double-click, Shift, and Ctrl keys, there are also different forms of selection. In areas as diverse as real time strategy games, and computer-aided design programs, you can select a whole set of objects by drawing a rectangle around them on the screen. This sort of selection is done by finding which objects are fully or partially inside what is essentially a smaller frustum on the screen. Lasso selection lets you draw an arbitrary shape on the screen for choosing a set of objects. Yippee ki-yay, these aren't built into 3GS. I just wanted to point out some ways of doing selection. Keep in mind that you don't have to make a selectable object a visible object. For example, for positioning a directional light, you might put an invisible sphere in the scene and make it selectable. Wherever the user clicks on the sphere determines a point in space, which in turn, determines the direction the light comes from. This is a far as we're going with selection and interaction. The rest of this unit will be about animation, which is a huge topic. You've initialized your scene with lights, objects and a camera. Naturally, the next step is to render. If you look through an entire Three.js program you've seen this line, telling the renderer class to render the scene. It's possible to call the renderer once during initialization and be done, but usually we want to have the scene be updated as needed. Once the scene is set up, calling the animate function gets things rolling. The key bit of magic is the window.requestAnimationFrame call, which in turn uses the animate method itself. This method will start up a rendering whenever it makes sense to do so. The other thing that requestAnimationFrame does for you is help lower the temperature of your computer. No, really. Say you have a webGL program running in a browser window. If you switch to another tab, the browser will know that the program is no longer visible and it will not keep it running. The animate method calls the render method to well, render the scene. Actually, it can do much more than that. I like to separate out the render method itself just to keep track of what's doing what. This method is where you want to put code that performs changes before rendering the scene. For example, if the user modifies the slider, the slider's value is used here to modify the scene. We've seen that in previous programs. If you have a bouncing ball animation, you change the position of the ball here. This is where animation gets going. Let's get animating, putting something in the render method to at last make our drinking bird move. I've gotten things ready by structuring the geometry a bit. The bird has two parts, the support, which is the legs and feet, and the animated part, which is the body and head. I've put the body and head into a new Object3D called bodyhead. Next I do a thing that javascript allows. I create a new parameter for the bird called animated, which I set to bodyhead. In this case, I want to access the animated part of the bird in the render function, so for convenience I give it a separate parameter. Adding parameters at any time to an object is a feature of javascript. That said, it can also be a bit annoying at times. For example, if I try to set a parameter and make a small spelling error, I'm not told that the parameter didn't previously exist. I could also create what I think is a new name, only to find later that I'm actually using an existing parameter. Long and short, be careful. Usually you'll find the error eventually, but the word there is eventually. Here's where the animation happens, we want to rotate the body around its z axis, the axis of the crossbar. There's a variable called tiltDirection that's either one or negative one. If It's one, then the rotation will increase by a small increment, half a degree per render. When the angle reaches a maximum of 103 degrees, the object swings back the other direction by changing tilt direction to negative one. Whatever amount the bird has moved past 103 degrees is used to move it the other way. Now the rotation angle is decremented by half a degree, until a limit of negative 22 degrees is reached, which causes it to reverse again. What we'd like to see is this result, with the drinking bird rotating around its crossbar, forward and back. We get this instead, with the drinking bird rotating around the origin. The problem is that we haven't set the body of the bird to have a pivot point that is where the crossbar is located. Your task in this exercise is to set the pivot point for the body and head to be around the crossbar. Think back to the unit on transforms. We saw how the snowman's arms could go to the wrong place with a bad order of transforms. If we translate it then rotate it, the arms would swing wildly in their position similar to the drinking bird; however, it's a bit trickier than that. The birds body and head are in the correct spots, so where would be translate them to. Think about it this way, say we constructed the bird a bit differently. Instead of moving the body and head to their final location, what if we moved them so that the center of the cross bar were was at the origin. Then, the body and the hat would be right location for rotating around the crossbar. We could then put both parts into an object 3D, such as the body head object, and apply the rotation to this parent. We could then move the parent to the proper location after performing the rotation. Your job is to modify the create drinking bird method so that the pivot is set correctly. I've given you a variable pivot height which is the height of the cross bar above the origin. I'll also give you a hint. It should take about 3 lines of code inside just this method to set the pivot. This problem could be fixed in the body and head code by changing every child, such as the eyes, nose, et cetera to be at the right height initially. That's a lot of work. Instead I use the body and head objects themselves. I move the body and head down to the origin by the crossbar's height. This puts the crossbar at the origin. The body head object holds both of these parts, body and head as children. If I rotate this object around its z axis, it'll properly go around its crossbar. Since translation happens after rotation, I then translate this parent object to its proper location. Now the animation works fine. This animation works, but it has a potential problem. We're basing the animation entirely off of the number of frames rendered. Each frame rotated the bird half a degree. However, what if your machine runs this demo at 60 frames per second, and mine runs it at 30 frames per second? This means your drinking bird is rotating 30 degrees a second. Mine is rotating only 15. This might be fine for drinking birds but say we had a facial animation play and it was supposed to be synced with the sound file of the character's voice. This would be a disaster. What we really want to do is use the real world time to know how far to rotate before rendering the next frame. For example, say our target rate is 60 frames per second, meaning that the bird should rotate 30 degrees per second. We'd like to know how much time has gone by since the last frame. In a moment, I'll show you how this is done. As you can see from demo, once we switch to the timed version of the code, the rate of the drinking birds motion is unaffected by the frames per second shown. Give this demo a try yourself. All it takes it's this little bit of code to get the amount of time from the previous frame to the next one. When you call clock.getDelta, it returns a precise floating point value of the number of seconds that is elapsed since the clock created or from the last time getDelta was called. By calling this at the beginning of each render, we can know exactly how much time we should use when we rotate the drinking bird. This code is how we were rotating the bird, adding half a degree each frame. If our goal is to rotate 30 degrees per second, we replace the 0.5 degrees per frame with exactly this. As each frame passes and the delta gets updated, the object is rotated the proper amount. This works well while the browser window is visible, but an interesting thing happens if you start the demo in timed mode, set the frames per second, and then switch to another browser window, and hide the program. When you come back in a while, you're likely to see the drinking bird spinning around and around. The problem is with our function that computes how much to rotate. The assumption in this code is that the rotation angle will never increase much beyond 103 or negative 22 degrees. However, when this window is hidden, rendering stops. When the window is exposed again, the renderer gets a huge delta, which the limit testing code doesn't deal with at all. The bird spins around and around incredibly fast for a while as the huge value gets damped out. There are a number of solutions. Here is one that locks the drinking bird directly to the time itself instead of the change of time, the delta. The method getElapsedTime gives how many seconds have gone by since the clock option was created. We take this time and multiply it by 30, the number of degrees per second. We know that the drinking bird sweeps through a 125 degree arc in both directions for a cycle of 250 degrees total. By taking the modulus, the remainder after the division by 250, we get an angle between 0 and 250. If the angle is less than 125 degrees, we rotate the bird forwards. Otherwise, we rotate it backwards, adjusting for the starting and ending angles of negative 22 and 103 degrees. Now if the browser window is exposed later, the time is always going to resolve to a proper angle. The moral is, when you design any animation driven by time, make sure it's bulletproof from large time changes between renders. By the way, there's one more mode in the demo called wave. It's my attempt to make the bird swing back from the glass at a varying rate, instead of moving back at a constant number of degrees per second. This avoids a bit of the jerk that occurs when the direction is reversed. One nice thing about using the time as an input is that you can then decide what the angle should be at various times. I can't say my attempt is all that convincing, but at least it's a bit nicer. The drinking bird has just one degree of freedom. It can rotate around its pivot point. However, the motion along this axis is fairly complex, and is difficult to capture in a few simple equations. One brute force method to create animations is called motion capture. The idea is to capture the movements of a person or object by using a few strategically placed cameras. The information recorded is processed to derive the paths of various parts of the body. These motions are then applied to the model in the virtual world. To more easily track where each part of the body goes, it's common to attach ping pong balls or lights to a person while filming them. These attached objects are called markers. There's a a move toward marker less systems where no special suits are needed. In the foreground here, you can see the actors pose being capture and applied to a character. In this clip there's a lot going on. You might want to watch it a few times. In the upper right the 2 actresses are being recorded. The cameraman is also moving around the scene. In the lower right you can see a reconstruction of the actresses's motions. On the left, these motions are being applied to the 2 computer animation characters in real time. There're 2 views here on the left. On the one half is the cameraman's view. The other half shows a fixed camera view of the whole scene. Motion capture is used extensively in the film and games industry, as it's a straightforward way to quickly generate animations and is cost-effective. There are some drawbacks. The first is cost of entry. This process is currently not something you can easily do without some investment. That said, costs are coming down, and inexpensive hardware such as Microsoft's Kinect can be used for capture. You can also use more limited forms of capture. This demo, by Jerome [UNKNOWN], does head tracking using a web cam. It's using 3JS and is Free Source, so it's something you can instantly use yourself. Motion capture can only do so much. Systems that use markers can track only those markers. For example, hand and finger gestures will not be caught by a full body capture system. Motion tracking is also affected by the limitations of the actors in the world itself. There are real world constraints on real world performances, and the data is only as good as your actors can provide. Artists generating animation have unlimited freedom, at least until they run out of time or money. To get back to our humble drinking bird, the way this process could work is that we track the motion over time. The positions tracked could be turned into the amount of rotation of the Z-axis for the bird's moving part. We would then have a giant table of values, which we could interpolate between. At some given time T, the angle of rotation is Z. Apply this rotation at that time, and we have an animation matching real life. Keyframing is a basic tool in the animator's box, perhaps the first one learned. The whole idea is this: to animate an object, save its state at various times and interpolate among these. For example, I have an object I want to move from one position to another over time. At time A, I record the position in time. At time B, I do the same. When I want to run the animation, given a time, I can get the position. This is a linear interpolation between positions A and B. The process of generating these frames in between the keyframes is called tweening. We can do the same thing with the drinking bird, interpolating between its two rotation angles, down and upright. Here's a set of keyframes, A and B, and what we've shown is the times that they occur. Since we want to loop, we give two keyframes and then we go back and repeat. A is the bird in the down position about to go up. B is the bird in the up position about to go down. Here's the timeline of our animation. We start with the bird down, he goes up over two seconds and then he goes back down again over three seconds. A is our keyframe for down, B is our keyframe for up, and they have associated angles. We can use these two keyframes to form a loop for our animation. One improvement we can easily add with keyframes is making the drinking bird pause while it's drinking. We add another keyframe to our timeline, call it C, where the object is held in the down position. C and A are identical, the same down position, they're just different locations on the timeline. The bird is held in place while the time is between C and A. Here's a graph of time versus the drinking bird's angle. With our initial drinking bird animation, the end of the swing backwards is particularly unconvincing. The bird abruptly stops and moves forward again. With linear interpolation you get just that, linear motion. As you transition from one pair of keyframes to another the motion can suddenly change. To make the transition look smoother its common to use spline curves of various sorts to change the motion from keyframe to key frame. You'll also see the terms ease in or ease out which means to start or end the animation gradually from a stop. 3JS has an add-on library called Tween.min.js that lets you set up a series of keyframes. With this library, here's my attempt to improve the drinking bird by using five keyframes total. I added an extra two keyframes at the end to get a wobble. You'll get to take a closer look and critique it in a minute. Here's a more elaborate keyframe animation by Jason Kadrmas, showing multiple keyframes in different rotations and translations. This was created in Blender, a free modeling program, and exported to a file format three.js can read. I simply dropped this file into Jason's program threefab, running in a browser window, and can play through the animation at different speeds. Watching the animation play, you can see how just a few keyframes can define some fairly elaborate motions. This lesson's about texture animation. When I hear the phrase texture animation, it brings to mind 2 different ideas. The first is shown here, in a tutorial by Lee Stemkoski. The idea is the same as a flip book, where you draw a different image on each page, and then flip through them. The other form of texture animation is where you move the texture by transforming the texture coordinates, such as shown here. There's plenty of animation that can be done without a motion capture studio or even a modeler. Remember that almost anything you can set on a model, light or camera is something you can change in the render loop. You can modify material, color and shininess, transparency and visibility and just about everything else. In this exercise, you'll make a river flow. Well, it's more like a square lake than a river and really the texture should be more stretched to look like flowing water and the banks of the river - well, well never mind. This is an exercise after all so I want to keep it simple. The initial code doesn't have anything moving. Your job is to use the texture transform feature in Three.js to animate the water as shown, moving from top to bottom. The rate of movement is one second per copy of the texture. In other words, when the texture repeat is three, a point at the top of the square will take three seconds to move from top to bottom. When the repeat is changed to one, the top of the texture moves down the screen in one second. For your solution, the movement should be tied to the elapsed time in the real world. If you look just above where your code should go, you'll see how the texture repetition parameter is set on the texture. You'll need similar code in your solution. Here's the entire solution. The main elements here are figuring out how to get the elapsed time and how to offset the texture itself. About the only tricky bit is the realization that if you want the texture to move downwards, you add the time to the offset. This has the effect of moving the texture's frame of reference up, which in turn moves the display of the texture down. My solution is actually offsetting the v value a potentially huge amount as the time increases. But it doesn't matter since the texture repeats. A less sufficient solution is this. It's best to avoid creating new objects every frame, such as this vector. Eventually, javascript has to perform garbage collection, which can cause your animations to hesitate at intervals. Once I had the basic solution, I started to play with the horizontal offset too. Giving this offset a slight wobble with a sine wave made it a little more appealing. I also changed the repetition length along the vertical axis by dividing the repeat factor by 3. This gives a more stream-like appearance to the water. It looks a bit like a rushing river or waterfall now. Look in the additional course materials for my snippet of code and try it out yourself, then try other functions. It's pretty addictive. Quaternions represent the orientation of an object, how it's rotated. They can do the same thing as axis angle rotations. They rotate a model around a given axis by an angle. They get a lot of use in animation because you can easily interpolate between one orientation and another. Think of two vectors poking through a sphere at the origin. If you want to change your orientation and go from one vector to the other, there are a number of ways you can do it. Say you want to do this over four frames. You can't normally do this well with Euler angles. The change in direction is likely to need more than one Euler angle changed at a time. Changing two Euler angles at once can give you problems with Gimbal lock, as one angle is applied before the other. Euler angles are terrible for interpolation around an arbitrary axis, especially when the change is large. Using an axis angle scheme, you would go a quarter of the angle and form the matrix. Every frame would need to form a new matrix, which can get pricey. Creating each takes a number of trigonometric function calls. Quaternions are a third solution. You can set quaternions to perform the same interpolation in a clean way and in a form as compact as Axis-angle - just four values. No trigonometry calls are needed. Quaternions themselves are fairly non-intuitive as to what their four numbers mean in physical terms when compared to Axis-angle. However, just like you rarely look at a single number in, say, a rotation matrix, the meaning of the Quaternion's individual values doesn't really matter when you're using them. When you interpolate around an axis, it's called a SLERP, short for spherical linear interpolation. All this means is that you're moving from one point on the sphere to another in a direct and linear fashion. 3JS and many other packages support quaternions because of the simplicity. In 3JS, if you wanted to use angle axis rotations, you'd also have to maintain your own translation and scale matrices for the object being rotated. Because quaternions are fully supported as a separate interpolation system. You can enable these in 3JS, while still using the position and scale parameters. Quaternions make it easy to specify and interpolate between various orientations. Similar to rotation matrices, but with less values involved, you can also multiply quaternions together. Doing so produces a new quaternion that is a result of this series of rotations. Quaternions are often used for animation paths or flying controls. In fact, 3GS uses them for just that class, FlyControls.js. That said, you do have to be careful in using quaternions with cameras, in that the up vector will not be maintained throughout the SLERP, and you'll need to reorient. Try the demo that follows. It moves the bird's hat back and forth on its head in a jaunty way. You can choose to interpolate using Euler angles or quaternions. With Euler angles on, try setting the angles to their maximums. If you look from above, you'll see that the path is not straight around the head but rather takes an s-shaped curve. Say you've designed your robot, now you want to put a skin over his frame to make him look more lifelike and infiltrate the humans, become more accepted by humanity. A robot model typically has a few rigid elements that do not change over time, such as a forearm and an upper arm. At the joint the objects are clearly separate. That's where you'd like to put skin. One simple solution is to just put some triangles connecting the two rigid pieces, sort of like a cylinder. As the joint bends, these triangles deform and stretch, keeping the two arm pieces attached. This doesn't look very good as the more the arm bends, the more the joint flattens out. What we'd prefer is something more like a flexible tube that bends, but mostly keeps its shape. Say we now add such a joint, with three cylinders, one after the other. The challenge now is how to move these cylinder vertices as the joint bends. The basic solution is surprisingly simple. Each vertex is assigned a weight. This weight is how much each rigid part's position affects the vertex. The top and bottom of the tube still stays connected to its rigid body part, so that body part's influence has a weight of 1. For vertices closer to the forearm, the weight is say, 2 3rds forearm, 1 3rds upper arm. This modeling process of adding polygons and assigning weights is called skinning, or sometimes vertex blending. The rigid parts of the model are defined by bones. These bones are like a skeleton, so this whole area is sometimes called skeletal animation. As a bone moves, the skin is influenced by the bone's transform times its weight. Specifically, let's look at this one vertex on the skin that's near the forearm. The vertex location is transformed twice. Once with respect to the forearm, and once with respect to the upper arm. These two transformed locations are then multiplied by their corresponding weights, giving the interpolated point between them. The basic algorithm works fairly well, but can have problems. For example, in the inner part of the elbow, the points may bend inwards in an unconvincing way. In practice, the vertex can be influenced by more than two bones. The bones and weights are something the person modeling decides, and the weights always add up to 1. These characters were exported from Valve's Team Fortress 2 game and animated with Three.js' skinning class called skinned mesh. Skinning is well suited to the GPU, as the vertex shader can transform plates multiple times and add together the weighted locations. There are more elaborate algorithms that give better results, but for interactive rendering in particular, skinning is the mainstay. Morphing is the process of changing from one model to another. It's a fairly tricky problem to solve if the two objects are quite different. For example, if I want to morph a cube into a unicorn model, I'll have to create a bunch of vertices on the cube and decide how these move in a convincing way. Much simpler is to [UNKNOWN] between two meshes with the same number of vertices. In this case, linear interpolation between the two meshes is trivial. A more general approach uses what are called morph targets or key poses. A simple example is a smile. Say we have three poses, a neutral pose, a smile an a frown. To go from one to the other we can interpolate. Essentially the key poses are similar to key frames to interpolate between. By changing the amount of influence a particular target has affects the result. For example, I could specify a half smile by saying to have each vertex be half the neutral pose, half the smile pose. A further advantage of morph targets is that they can be combined. For example, I could have key poses for an eyebrow and use it as I wish with a smile or frown. A pose can be associated with parts of a mesh and so influence only them. For example, here I'm using the wonderful ginger demo by Stickman Ventures. They'e a slighter set on many different facial features. By changing a setting, I'm changing the influence of the morph target. Even skinned models can be exported and treated as morph targets. Morph targets are well supported by 3JS and so are a popular way to display animated models. The main drawback on the modeling end is that it's labor intensive to create morph targets. The whole process of setting up a character's animation controls is called rigging, and one that take skill and time to do properly. Such processes are almost always done with interactive modelling and animation programs. Another drawback is that morphing needs much more data than skinning while running an animation, both to download and to keep in the GPU's memory. All that said, morphing has the advantage of being generic. Any model changes can be saved into morph targets and there's no real ambigity. Skinning cannon is done in a myriad of different ways with research continuing to find new techniques. Lots of tears have been shed by many developers trying to convert skinning animations from one format to another. One area where interactive 3D graphics is used extensively is in what's called previsualization. That's where a director sets up a virtual world and sets his cameras lights and so on. Doing this can cut cost and speed delivery. What follows are some interviews with The Third Floor, a previsualization company. [MUSIC] The third is a previz studio. We specialize in pre-production of all types. working with different directors in both commercials video game cinematics and feature films. The Third Floor originally was founded up at the Sky Walker Ranch. We actually referred to the third floor of that building where we were. Sequestered for a number of years on Star Wars episode three.t At the end of our tour duty, we decided to get together and form our own studio down here in Los Angeles. And Action. We made it a mission of ours to create a room where we can have a virtual film making experience. Directors like to work with a virtual camera. So we have a system that allows them to manipulate a camera with a sensor on it. That's fed live into Motion Builder that actually lets them manipulate around the Motion Builder camera. We have actually added a Wacom tablet. And we use that as our viewer. And our input device for operating a camera inside the virtual world. From there if we do integrate the motion capture suit. We can record this information and they can actually create their own shots. It really is an exploratory process that, that opens up the directors to whole host of new possibilities. So this is a little demo that I typically do for the people that come here wondering what previs is all about. Normally, it begins with this kind of simple character that we build from various pieces and texture. We'll just take that character and rig it up with our special rigging system, but have, we have it built in segments so that there's no actual skinning going on, so that we can keep these guys running around very quickly. So then, we'll apply either an animation cycle we've created or something that we do from the, from the motion capture. And so of course, then, you can go and give this guy a friend. And this is how we begin to build up the sequence. And then the director will start to ask, okay, where are they standing, where is, what's the environment? So, we end up building this whole location to scale. So actually, when you go all the way out, you see that this is topographically accurate, it's the real scale of the real environment. So that, if the director wants to look in any different direction, they're seeing the correct heights of all of the landmarks. Then, I'll just add a few effects to this particular shot, required some pyrotechnics. So, we actually ended up going and consulting with the pyrotechnics guys to make sure that the, the scale of the, of the explosions was correct. So even though, they're just a bunch of still images, we end up getting the accurate scale of what these, the explosions would be. But, the first thing that I need to do before it becomes a shot is actually look at it through a camera. So here, the guys running through the shot, this is how we initially composed the scene. And then we decided, you know what, let's do an alternate angle. So, we just quickly threw in another, another view. And this is the same action, but played out as just a pan tilt as opposed to a dolly shot. So you can get a lot of, create different camera moves out of the same action. Next up I want to show you how we add some extra realism to the camera itself. First of all, we select the camera, and then we have all kinds of controls. in this case, I'm going to turn on the handheld control. And I can amp this up, so that now, instantly, I see the same action but with a bit more shake on it, so it feels more realistic. Especially great for these kinds of action sequences. Once the scenes are approved, we end up taking a look at this from a completely different perspective. Can we actually do this shot? And in the case of this particular shot, they decided, well, let's just start shooting this on a green screen. And so, how this actually works is, we can take a look in my at the volume of the sound stage. And we brought in of course, all of the gear, the, the lights, the, the camera equipment. So, when we're looking at the shot on set we can make sure that we have the right-sized green screen, and that all of the technical pieces are working. Now you'll notice here in the beginning that one of the actors is stepping on this kind of red dolly track that we have there. So, what we need to do is make sure that kind of move this over and we can, we can place that wherever we need it to be. And our camera rigs work in reverse. So right now, this is giving me a certain solution. It's all driven by the creative camera, but the actual rig just follows along. So if I wanted to take this rig, and in my change the trajectory of the rig, I could scoot it over this way so it's out of the shot. The, creatively, the shot is going to be the same, but, at least we know now where best to put all of the gear. So, the last part of the story here is, once we've got the creative worked out, we've got a plan for how to shoot it effectively, then we need to make sure that we communicate that to the clients. So we give them all these different views and usually encapsulated into one particular drawing. a page that we can give to the production managers or the producers, so that they can keep track of how each of these shots was created digitally. But it's not to take anything away from what the real cinematographers would do when they step on onto the set. But at least, that they've brought the right camera crane, the all of the gear that they need to make sure that they effectively have all the tools to produce the shot. We've talked about how to create particles and some of the effects achievable. You can create all sorts of animations with particle systems. Here's an abstract presentation by AlteredQualia, which includes varying the sizes of particles over time. Objects can also be formed out of particles. In this demo, the models are represented by particles instead of wire frame triangles, allowing interesting transformations to occur. Sets of particles can be used for all sorts of animations, such as this one with a firework emitting sparks as it goes. By overlapping particles, you can get fuzzy effects. Of course, you can always make fireworks themselves. Here's how animators preview some effects. Particle systems are often set up with guidelines either fixed or with some randomness. Rules include elements such as when are particles created, what path it travels and whether it disappears after a certain duration. The path could be hand-animated or controlled by the laws of physics or even a bit of both. Another common use is to have large collections of particles simulate smoke, water or other phenomena. Here, particles are shown with a fountain effect. The particles don't have to be small, they can be objects in their own right, such as these dandelion seeds being blown away by the wind. We can also create particles that are random 3D fragments, which act as if they're generated from a collision. Even effects such as sparkling logos can be done by animating particles. Here's the preview followed by the final offline rendering. Or you can simply use procedural methods to create amazing scenes. It still astounds me how many particles you can animate and render at once. In this demo by AlteredQualia, there are 150,000 rotating 3D cubes being thrown at the screen and I'm getting a frame rate of 28 frames per second. The whole particle system, evolution itself, creation, movement, and extinction, can now even be calculated entirely on newer GPUs. This allows more elaborate animations while also freeing up the CPU for other work. One important element in making a virtual world seem real is collision detection and response. Collision detection is just that, you detect whether two objects have moved such that they overlap. Even modest forms of collision detection can make a user feel that they're in a virtual world instead of a ghost that can pass through anything. For example, simply shooting a ray from the user's location and finding the distance to the closest intersection will reveal whether the user's position should be adjusted. Where the problem begins to get complex is when you have many objects in a scene. With a brute force approach you compare everything to everything else for collision. Much of the efficient collision detection coding is in creating data structures that quickly call out possible collisions that can't occur in a frame. The other half of the problem is collision response, what to do when two objects are found to have collided. This depends on a number of variables, including the velocity of the objects, and in this case, the elasticity of the materials. Collision detection is done primarily on the CPU in most programs at this point, though there have been some moves to off load the work to the GPU, or even custom chips. This is a complex subject, entire books have been written about it, but this is all we'll cover here. It's one thing to animate characters by hand, however there's a huge variety of phenomena that are governed by the laws of physics. For simple tasks such as bouncing ball, keyframing can suffice. Here's a keyframing timeline in 3D Studio Max, and the process of setting up an animation like this. What if we wanted to have the bouncing ball keep bouncing? Keyframing by hand becomes a challenge since there's no simple repetition. For these phenomena we turn to simulation to generate the animations. Here's another example. A set of bodies attracted to each other by gravitational forces. For the bouncing ball, we could have perhaps written a little program that would create the key frames for us. Here the motions are complex and are calculated frame to frame on the fly. Many other phenomena can be simulated in real time. Here's an example of a spring system. These three demos were all created in GlowScript, a system that's even higher level than Three.js. Simulating friction is a scenario that can be challenging, but critical if you want to get anywhere. A blowing wind has drag and lift constraints, cloth has stretch constraints. This is a relatively simple system that can be solved in real time as you can see. Simulations can be affected by the user's actions. As a simple example, in this demo, you can turn the wind on and off, add a ball, and so on. These are not pre-stored canned animations, but are interactive. Go the physics and engineering sections of a technical schools library, and you'll see a huge amount of information on a wide variety of phenomena. Beyond drag, lift and stretch there are many other properties we could simulate. Friction, applied force, flow, elasticity, tension, normal force, turbulence, spring force, heat transfer, gravity, trust and so forth. Computing these is done by what's called a dynamic solver. What you're seeing on the screen now are a number of different simulations done for all sorts of phenomenon. A few of these are computed at interactive speeds, some are created offline. In either case, the rendering is separate from the physical computations. Typically, the way solvers work is that a tiny timestep is chosen for the simulation, often smaller than the time represented by the frame. The effect of forces in the scene are computed for this sliver of time and objects are modified. This process is done again and again, time slice by time slice. Small steps help avoid various types of errors. The area of physical simulation is wide ranging and users can have different goals. An engineer or scientist may be making a visualization of a particular process and so the simulator is aiming to be as realistic and true to the physical world as possible. If interactive speeds are not needed, then time slices can be exceedingly small, and mostly dependent on how much compute time the user wants to spend. The other major area that uses simulation, is for generating what I call plausible animations. These animations are not required to be physically correct. Rather their goal is to look reasonable to the viewer. Just as we don't pick up on reflection maps not having perfect correct reflections, there are plenty of errors we don't notice or can rationalize away. Though using physical principles for the creation of effects normally gives the most realistic results, for interactive rendering we often have to make significant simplifications. There are also simulators for phenomena that are not based on physics. For example, flocks of birds in flight, traffic in a city, and crowds of people walking inside a building are all systems that can be simulated with some basic rules. The artist can also guide the simulation as desired, perhaps having characters run towards a particular location and jostle for position. Here's the final render of a piece of cloth. It's a combination of the artist determining where one point on the cloth has moved, then physical simulation being computed. What you're seeing is a final batch rendering, but this type of simulation can be created and viewed in real time. Simulations that are more complex can be run in the object location stored for all frames. These stored sets of locations can be treated similarly to morph targets, with the animator being able to blend among two or more as desired. There are plenty of other combinations and possibilities of previewing and influencing the course of a simulation. Let's take one last look at our drinking bird. This version is just key frames and if you compare it to the real thing it's not very true to life. One element we haven't animated at all is the fluid level in the bird itself. The distribution of fluid matters to how the bird swings in the real world. Simulating this effect would be a bit of work but once done properly would provide a true to life simulation of the physics of the situation. Perhaps the discoverer of the drinking bird equation will be you. What kind of system would you use for animating the following phenomena? Pick the most relevant algorithm. Some could use multiple techniques, so use each algorithm only once. The things to animate are, a frowning troll, avalanche, a gymnastic flip, or showing the right-hand rule with a hand. Let's start with the clearest match. The avalanche is certainly most easily done with simulation. Tracking all the small bits of snow is extremely difficult with the other techniques. Of the remaining three, morph targets are best for making a troll frown as just two key poses would be needed. That leaves skeletal animation and motion capture. Both tasks could be done with both. However, it's a fair bit of work to get the center of gravity correct for a gymnastic flip. Motion capture would make this easy. Motion capture might be able to capture a hand, but this can be tricky. Skeletal animation could clearly handle this task. This lesson doesn't quite fit in with this unit, but the topic is important enough that I'll cover it here. As we move the camera or objects through a scene, models become larger and smaller. For very large scenes with many objects, rendering everything in the scene at full detail can make us lose any shred of interactivity. We've seen how there are limits to how large a texture needs to be, in fact it's usually best if the texel to pixel ratio stays at about one to one. More than that and the texture can just become a source of noise. Choosing the proper data representation is part of a much wider topic called level of detail. In this area of computer graphics, the focus is on how to change an object's representation as its size on the screen changes. Methods must balance tradeoffs between performance and quality. The two major areas where level of detail techniques come into play are with meshes and textures. Textures can be down sampled to create mid maps so that proper texture level can be used for a scene. As a textured object gets farther away a smaller texture on the mid map is needed. There also limits for how much memory a GPU has available for texture storage. For very large data sets such as a city or world map, it's impossible to save all the high resolution textures in memory at the same time. Since we know that when an object is small on the screen, it will need only a lower resolution texture. We can design our system to load only the low resolution textures out of the mip chain. As we zoom in on a particular object We can load the higher and higher resolution textures. At the same time, we can be deleting the higher resolution textures for objects that are now receding from view, or have been clipped off the screen entirely. As an object gets smaller on the screen, we can also consider simplifying the illumination equation. In other words using shaders with fewer instructions. For example, if we get far away enough from an object, its displacement map may add very little detail. Displacement maps help in particular by giving a more realistic silhouette to an object. At a distance, these fine details are less important. As an object gets smaller, switching from a displacement map to a normal map then to no bump map at all saves considerable amounts of memory. Mesh geometry itself can be thought of in the same terms. As a mesh gets smaller on the screen, fewer vertices are needed to represent its details. For objects that are tessellated into triangles, this means a smaller tessellation factor can be given. For more complex objects that do not have a simple tessellation factor, simplified meshes have to be either created by hand, or by what's called the decimation algorithm. Such algorithms try to find the least important triangles and patch up their areas with fewer triangles. Here's an example by Paolo Cignoni using mesh lab, a powerful and free mesh manipulator. This automatic techniques are effective but can run into problems with objects such as faces, where the lips and eyes should be subject to less simplification because of their importance. In mipmapping we interpolate between two textures in the mip pyramid. With level of detail techniques for geometry, we can do the same with what are called geomorphs. This technique can provide a smoother transition between levels of detail, but at the cost of additional processing and memory, which is in opposition to much of the point of using of level of detail techniques in the first place. It's much more common to simply replace a model with its simplified version when it becomes smaller on the screen. You sometimes see a pop if the two versions of the model differ considerably. 3JS supports this form of level of detail. In this demo, you can see different versions of the sphere model being used depending on the distance from the viewer. These were explicitly chosen by the programmer. If you look closely, you'll see four different levels of detail. A common method of simplifying a mesh is the edge collapse, where one vertex is moved along an edge to be at the same location as another vertex. If I move vertex A to be at the same location as vertex B, I can then get rid of vertex A in memory and store just vertex B. This saves me from storing one vertex but there are other benefits. If I'm drawing a wire frame, I'll draw fewer edges. If I'm drawing triangles, I'll need to save fewer faces. The question to you is, how many fewer edges and faces are drawn when you do an edge collapse? The answer is 3 edges and 2 faces. The two triangles bordering the edge formed by A and B will disappear. The previous problem brings up a useful little formula from topology called the Euler-PoincarĂŠ Formula for connected planar graphs. V is the number of vertices. F is the number of faces. E is the number of edges, and g is the genus. The genus is the number of holes in the object. As an example, a sphere and a cube have genus zero. A doughnut and a coffee cup with a handle have genus one. There's an old joke about topologists not being able to tell their coffee cup from their donut. An edge collapse is a reversible process, so let's show how it affects the formula. First we know we're going to have a vertex created, so that's one. We can see that as the point moves up, two faces are created, so that's two faces. We can also see three edges are created. The genus doesn't change. There are no holes created or destroying. We can see this sums up to zero, the delta. In other words, this formula applies to this mesh and this mesh, and they both must come out to be the answer too, so the delta must be zero, the difference between them. So this equation says we subtracted a vertex in two faces. To balance the equation, 3 edges must also be removed. I'm going to give you just a tiny glimpse of this great terrain demo by Stavros Papadopoulos and you should definitely go check it out yourself. It shows a wonderful terrain rendering system with a level of detail. I'm going to turn on the wire frame just to show you what I mean. As we get closer to various pieces of terrain, a more detailed version is popped in. Notice that flat areas don't need much detail but hilly areas do. Anyway, this is just a taste. You should definitely go check out this website and read it over and try out the demo. What do you think is the biggest challenge facing the field of interactive rendering at this point? I think it's mobile graphics. Everything seems, everything is going in that direction right now. Things are moving away from the desktop onto the pads and the phones. And when you go from a standpoint of trying to get performance per dollar or performance per, in the, in the film industry it's performance for BTU of cooling capacity. [LAUGH] And now we're going into, performance per watt of power that you can have, on your device. It, that's a really interesting challenge for how to get what you want on your device for that little tiny bit of power. &gt;&gt; Managing complexity. We have so much high definition data, like. Sometimes you just want to render a house but the bolts in the house all contain a million polygons. How do you turn that rich information into something that is just the most important for rendering a pretty image? I think that's the big thing. &gt;&gt; I think in my field, coming from that, that maker, everyone is a designer, standpoint, is, is that, that play between 3D printing, and physical objects, and rendering, and the, the hardship that is when I create something on the screen, I can do it in whatever I want. When I create that reality, sometimes the reality doesn't [LAUGH] match up with the rendering. Every since my first online Java program, back in 1997, I've been excited by the potential of the Internet to teach 3D graphics. Interactive 3D graphics, with speed and quality equal to that seen in AAA title games and high-end CAD packages, is now possible on the web. I'm also thrilled that we can now share programs in a straightforward way. No installs, no compilation, and anyone can look at the code being run. Udacity's motto is Learn, Think, Do. I find that perfect for interactive rendering. You don't need to buy a DNA sequencer or array of radio telescopes. You have everything you need at hand. Experimentation is easy and quick with 3js. The only danger is the risk of staying up till 4 AM saying, what if I twiddle with this parameter. I ran across a great quote from Confucius while making this course. Every truth has four corners. As a teacher, I can give you one, but you must provide the other three. This suits my view of computer graphics. You can be given a firm foundation, but you're the one who really needs to pursue knowledge and learn by doing. Confucius probably meant something different. Don't forget you have a giant reference library at your fingertips. Much of what I found for these lessons consisted of me typing in 3JS collision detection and seeing what the search engine popped up. Developing any sort of interactive 3-D graphics program was out of reach of almost everyone 20 years ago because back then graphics accelerators cost the price of a luxury car. Now they're the price of a large pizza. Even up until a few years ago, creating a 3-D application involved compilers and downloads and installs and once you were done, it would usually work on only one operating system. If you ran someone else's glorious demo, you were usually less Scratching your head. How do they do it? The fact is you can now make a wonderful 3D interactive that anyone with a browser can access with just a click and can read the code that created it with just a few more. How amazing is that? There's so much you can do, figure out what someone else's code does, write your own programs, write your own libraries. Even contribute to 3JS itself, and if you do, document it. Go off and do great things, and whatever you do, send me the link. I'm Eric Haines and I'll be teaching about something I love, computer graphics. What's not to like? In this area of computer science, you will write code to create cool pictures. I've been programming computers since 1974 and I've been working on computer graphics since 1983. With computer chips constantly getting faster and with graphics accelerators, in particular evolving at a rapid clip, the rules about what's possible change time and time again. There's always new things to learn. I guarantee that the computer languages and hardware we will use will change. The good news for you as a student is that I'm going to focus on what makes computer graphics tick. The underlying principles I'll cover here will give you a firm foundation in the field. I'm particularly excited that I'll be able to teach this class now. A few years ago, such a class would have been extremely difficult. With the rise of a web technology called WebGL, you can simply click on a link and immediately run a 3D demo for yourself. In some trivial sense, everything you see on your computer screen can be called computer graphics. Word processors, videos, such as this one of me, paint programs, and much else result in images on your monitor. I'm going to focus on Interactive 3D rendering in particular. Let's break that phrase down starting from the end. This last word, rendering, means depiction. Well, it can also mean translating to another language, or turning animals into meat, but let's ignore those definitions. An artist can render a scene by painting it, an architect can render a building in perspective. And so on. To render means to create an image. For the phrase 3D, this means defining things in a three-dimensional world. In the field of 3D computer graphics, you mathematically define objects, materials, lights, and a camera. And then use these together to render onto a 2-dimensional screen. Your monitor becomes a window into this world that you've created. Even newer smart phones have 3D graphic accelerators in them. You've undoubtedly seen video games and special effects in movies. These are two of the most obvious uses of computer rendering techniques. There are a vast number of other uses of 3D rendeing that effects how the world works. Architects commonly create 3D graphics presentations for their own benefit and for clients. Engineers use rendering to help show potential problems with their designs. Doctors use 3D reconstructions from medical scanning devices to understand patients problems. Even lawyers will employ 3D animations to help explain accidents or other events. Interactive means you can instantly influence what you see on the screen. Video games give us obvious examples. You turn the wheel left in a racing game, and go that direction. Compare this to say, playing a DVD. There's perhaps, some minimal control that could be considered interactive, pause, fast forward and so on. But you're not affecting the world itself in any way. Drawing these lines is interactive, I'm affecting what I see, but it's 2-dimensional. By Interactive 3D, I mean that the users actions affect something in a 3D virtual world At the least, you move the camera to the world, possibly affecting objects and lighting as you go. This program by James Gow shows MRI data of how the brain is stimulated when exposed to different objects and actions. On the right is the 2D system for picking an object, on the left you then see the area stimulated. You can also change the view and with the slider, control how the data is represented. I asked Jack Galiant, who helped with this program, about how much of the brain is dedicated to processing what out eyes see. The answer is, that more than a quarter of our brain is involved in interrupting visible signals. I should note that there are other areas of computer graphics having to do with images but are not truly 3D rendering. For example there's a complementary field called image processing. Where you use the computer to analyze photographs or other images and try to extract a mathematical description of the world from them. Though graphics hardware can definitely accelerate image processing operations. We'll mostly be focused on 3d rendering in this course. So, your first quiz. Which of these can be described as using interactive 3D rendering? Feel free to search the internet for any of these titles and terms to learn what they are. The computer games Call of Duty and World of Warcraft, the Toy Story and Ice Age movies, recording a 3D image for medical purposes, such as a CT scan of a patient's head, the mobile phone games, Angry Birds and Pacman. Only the first answer is correct. In Call of Duty and World of Warcraft, and many other video games, you move through a 3D world and interact with it. Pixar's Toy Story was the first film made completely with computer graphics back in 1995. It and many other animated films, such as the Ice Age series, use 3D rendering techniques. However, the final product is not interactive. It's a linear film that you have no real control over. Each image in a computer animated movie can take from seconds, to minutes, to hours to compute depending on the complexity of the scene. That said, artists and technical directors working on these films depend on interactive rendering techniques to set up lighting, design materials, and create animations. Computerized tomography or other 3D image techniques gather slices of data. However, gathering the data by performing the scan is not the same as rendering this data. Pacman is interactive and is rendered, but the scene described is entirely in 2D. Games such as Angry Birds are also in this category where an illusion of 3D is achieved by having 2D layers that move at different speeds. Mobile phones and tablets can certainly perform 3D rendering, but these particular games don't need such capabilities. Now, before we go any further, you should make sure your computer can run WebGL programs. Gl stands for Graphics Library and it's what this course uses to teach 3D Graphics. Webgl is built into a number of web browsers so you don't need to install anything. Unfortunately there's one major exception, Internet Explorer. So, you'll need to use a different browser such as Chrome or Firefox. On Macs, Safari supports WebGL but you'll have to do a little twiddle to enable it. Linux has it's own issues. You may also have problems with older graphics cards and drivers. The easiest way to test and debug your computer is to visit the following url, get.webgl.org. If you see a simple rotating cube on the screen you're all set. If not follow the instructions on this webpage to get your computer properly set up. If one browser doesn't work for you try another. My personal preference is Google's Chrome as it has a java script debugger built in. If you get stuck, see the additional course materials for more tips and suggestions. Ultimately, you have to get WebGL to work, in order to fully enjoy this course. Once you have it set up, you can go to a huge number of web pages that have graphical demos running on them, like this one. Look at the links provided for some of the better programs out there. In this unit, we're going to look at some of the core ideas behind interactive rendering. First, I'll discuss interactivity, and explain concepts such as the refresh rate and frames per second. I'll next talk about the human eye and how a camera works. These both relate to the idea of defining a view frustum as a way to see into a virtual world. I'll discuss how light travels in the real world and how we simplify this physical process in our virtual world in order to rapidly make pictures of it. In the latter half of this unit, I'll focus on what the graphics pipeline is and how graphics hardware is able to interactively render complex scenes. I'll end by discussing two rendering algorithms, the painter's algorithm and the Z-Buffer. If we want to play a video game or design a new tennis shoe, or try out different color paints on a building, we would ideally like the experience to be interactive. When we take an action we would like to see response from the computer quickly enough that we don't feel slowed down. How fast is fast enough? I'll give you a real number. It's about six frames per second. Let me show you. So in this demo we're showing six frames per second for this frame right here. And I can adjust that and make it look slower. So, now it's just going to update at one frame per second and you can see that that's, that's pretty unusable. It's sort of like you're trying to walk through molasses or something. And you'll see up in the upper left hand corner how long each frame is taking. So if I adjust it to say, somewhere around 19 or so. Now it's pretty good, you can still see some jerkiness. But if I go to sixty, it becomes nice and smooth. So now try it for yourself. So, we saw that six frames per second was usable, but not very fast. In fact for video games, the minimum rate is often, 30 frames per second or even 60 frames per second. These numbers are not arbitrary, but are tied to the hardware itself. Most monitors these days, at least in America, refresh at 60 Hertz, which means display the image 60 times a second. This is called the refresh rate. This value is part of the monitor itself, not something that varies. This value of 60 Hertz gives an upper limit on the frame rate, the number of new images per second. However an application can certainly have fewer frames per second. So, for example, say it takes us a tenth of a second to compute and display each new image. This would mean our frame rate is 10 frames per second. The display itself may refresh, that is, send it's stored image to the screen 60 times a second. But most of those images will be the same. In fact, each computed image will be refreshed 6 times on the screen. In the film industry, the frame rate is 24 frames per second. However, the refresh rate can be 48 or 72 hertz with each frame, repeatedly projected two or three times. It surprises people that the frame rate of film is so low compared to computer games with rates like 60 frames per second. I won't go into the details in this course, but the key reason that film can have such a comparatively low rate, is that film captures what is called motion blur. When a frame is captured by the camera, objects moving will be slightly blurred. This gives a smoother experience and so a lower frame rate is possible. A fair bit of research has been done in the field of computer graphics to rapidly create frames with motion blur, since they look more realistic. See the additional course materials for more on this topic. However, a film is not interactive. A faster frame rate in an application also allows a faster reaction time, giving a more immersive feel. In Europe, televisions refresh at 50 Hertz. How many milliseconds go by between each frame update? So we compute the answer by taking 1 second, which is 1,000 milliseconds, divide it by 50 hertz, and we get 20 milliseconds per frame. So we found for European television that you have 20 milliseconds per frame update. This is an incredibly small amount of time and isn't even the fastest time strived for by games. If you have a CPU running at say 2.8 gigahertz, that's 2.8 billion instructions per second. Turning 20 milliseconds, this comes out to be 56 million cycles for that single CPU. This sounds sizable, but if there are million pixels on the screen, this leaves just 56 cycles per pixel per frame, not a lot of time. Later in this unit, we'll see that the CPU usually takes care of creating and positioning objects in the scene, these objects are then sent to a graphics processor which is particularly tuned to graphically display them. So, how do we see anything? Well, clearly, we use our eyes. But lets talk a bit more about that. Light travels through the pupil, through the eyes interior. It makes contact with the retina in the rear which is covered with rods and cones. The rods are for low level lighting, such as during night time, and also for peripheral vision and motion detection. The cones provide us with core vision. The key idea here is, light coming from a particular direction is detected by cones in a particular spot in the eye. This description barely touches on the complexity of the eye. The iris around the pupil opens and closes to let more or less light in. The back of the eye is a curved surface. The density of cones is highest tin the fovea. Which is where our eyes focus for reading or other activities. And we haven't even touched on what happens to these signals when they're processed by the brain. An easier model for rendering is the Pinhole Camera. With a Pinhole camera, you put a piece of photographic film at one end of a box and poke a hole in the other end, then you cover up this hole. You do this in the dark so that the film was not exposed to light. Of course, just about no ones uses film anymore, but I'll assume you have some idea how camera film works. To take a photo, you point the camera, open the hole for a short amount of time. Then go back to the dark room and develop the film. With the Pinhole camera, the light coming in from one direction is detected by a single location on the flat rectangle of the film. Put these differing light contributions together, and you get a picture. Most cameras work the same way. They may have more complex lense systems, but they gather light in a similar manner with the image projected behind the lense. We often simplify and think of our eyes as elaborate cameras that send images directly to our brains. It's better to think about the eye as part of our visual system, since ultimately it's the brains interpretation of the data that matters. So here is one of my favorite optical illusions by Edward Adelson at MIT. So, the question to you is, there's A and there's B in those squares. Is A darker than B? Is A brighter than B? Or are A and B the same, as far as intensity goes. The shocking truth is that the squares marked A and B are actually the same shade of gray. If you don't believe it, here's a bit of proof. An image where two vertical bands with the same shade of gray are overlaid on the image. So, you can see here and here that these gray bars, they're all the same intensity. And that they're also the same intensity as this A and this B square. So, these are all the same gray, believe it or not. If you still don't believe it, you can print out the original image and cut out squares A and B and hold them up next to each other. See the additional course materials for where to get these images. As professor Adelson notes, this illusion proves that the visual system is not a very good light meter, but that's not its purpose. Our visual system tries to compensate for shadows and other distractions so that we can properly perceive the nature of the objects in view. I've talked about eyes and cameras and how they both work. The eye is part of our visual system. The brain interprets what the eye sees. How is this visual system different than how a camera produces an image on its film or sensors? In other words, how does our brain interpret what the eye itself records? Is it that the eye's sharpness changed with lenses? Bright light can blind us? We can control what our eyes focus on? Or we see images correct side up not upside down? High quality lenses can be used to improve clarity for the eye or for a camera, the brain cannot influence this area. Too much light can temporarily or permanently blind eyes but will also blind a typical camera as the image produced will be overexposed often appearing all white. It's the eye that's damaged, not the brain's interpretation of what the eye is seeing. Muscles in the eye control the eye's focus but the brain does not interpret the image differently. The answer is that we see images right side up. Look at our pinhole camera model. The image is actually formed upside down. The simple idea that the image on a retina goes directly into our brain in that form is clearly false. There is in fact a considerable amount of processing that occurs in the brain beyond just rotating the image 180 degrees. For example, our eyes have blind spots where the optic nerve is attached. Our brains fill in this area with the surrounding elements in the scene. A pinhole camera captures an image upside down. In computer graphics we think of a camera a bit differently with the image being formed in front of the viewer. This is a more convenient way to consider the task of image generation. For starters, now the object projects onto our screen right side up. This pyramid-shaped view of the world is called the view frustum. Frustum means a pyramid with its top chopped off. It is one of the most frequently misspelled words in computer graphics. So, lock it in your brain now. There's no letter r in the last half of that word. I suspect it sneaks in there because of the r in the word frustrate or maybe the word fulcrum. Anyway, enough about that. The point here is that we want to know how much light is coming from each direction. Each light emits photons which bounce around the world and may get absorbed or otherwise changed. In a camera we gather photons from the world and these create a photograph. In computer graphics, we simulate the real world as best we can and try to determine how many photons are coming in each direction. Whatever amount of light we gather from a direction, we record at a pixel in the screen. Another way to think about rendering is as if we're looking through a screen door or a wire screen in a window. The screen is there to keep out flies but for my purposes each little square on the wire screen is like a pixel on a computer screen. Through some screen squares we see a bright red. Through others a deep green, and putting them all together we get a rose. When we look in our computer screen, we can think of it as a window into a 3D world. However, moving our head in the real world, normally does not change what we see through this window. Our task in 3D computer graphics is to define a world, and then figure out as best we can what light comes from each direction and so forms an image. The computer renders a 3D scene by determining how much light comes from each direction. There are a few elements that go into this calculation. First, we have to define a world. For example, take this scene. Here, there are many different objects defined in different ways. Each object is described by some 3D geometry. It also has some material description that says how light interacts with it. Objects can also be animated over time. The red cube is obviously spinning and if you take a closer look at the metallics sitting man on the left you'll notice he moves. There are also lights in this world which in this case are not animated. As the view changes, some materials look different as the light reflects off of them at a different angle while others don't change. Finally, a camera is also defined for the scene and interactively changes under the user's control. To sum up, a scene consists of objects, which are typically described by geometry and material. Along with lights and a camera. So, let's get back to computer graphics and see what a graphics processor can really do. Well, guess what, computers are fast, a graphics processor consists of specialized computers working in parallel. Let's get some idea of how much processing is involved in rendering a full stream. My monitor is 1,920 by 1,200 pixels in size. I'd like to have a frame rate of 60 frames per second. How many pixels must be computed each second to attain this rate? So, to calculate how many pixels we need per second, we take the horizontal resolution, 1920, multiply it by 1200 and multiply that by 60. The answer is 138,240,000 pixels. By the way, the word pixel is short for the phrase, pixel element. Note that this value is the minimum number of pixels to compute. This huge number helps explain why all computers may nowadays, have some sort of 3D graphics acceleration hardware inside them. This can take the form of a $2 integrated chip on up to two or more graphics cards, each costing hundreds or even thousands of dollars. There are any number of ways to figure out how much light arrives at each pixel to form an image. In theory, you could fully simulate the world around you. Photons start from a light source, hit atoms, and they're reflected, absorbed, and so on. You would a camera in this scene, and whatever very few photons actually happen to enter the camera's lens would then determine the image. As you might guess doing so would take a huge amount computation for even the most modest scene. A back of the envelope computation for a 40 watt bulb, shows that it emits about 10 to the 19th photons per second. To get a sense of how involved it would be to track all the photons in a scene, I want you to do a rough estimate. Say we have ten 40 watt light bulbs in a room. As mentioned before, each produces 10 to the 19th photons per second. About a billion computers, 10 to the 9th are currently in use world wide. Say each computer can trace a million photons through a scene per second. This is an unrealistically high rate, since we're talking about tracing each photon until it is absorbed by an object or it reaches the camera. Your grandmother's computer likely can not do this. So, the question to you is how many earth's worth of computers would we need to trace all the photons in a scene in real time and so form an image? So, your choices are, we need just one earth's worth, ten earth's worth, one hundred earth's worth or ten million earth's worth of computers to do this in real time. So, the way you compute this is to take this 10 to the 19th. And there's 10 of those, so that's 10 to the 20th. And then we have a billion computers in the world. So that knocks 9 off of that so it's only 10 to the 11th now. And each computer does 10 to the 6th. So that whacks another 6 off giving you 10 to the 5th. And 10 to the 5th is 100,000 Earth's worth of computers. So that's the answer. That's all you need. If you went for 10,000,000 you were thinking way to high and those extra computers could run World of Warcraft, or something. We've seen how expensive it would be to track all the photons in a scene. To avoid this level of computation. In computer graphics, we make a number of simplifying assumptions. First, only those photons that reach the camera are the ones needed to make the image. So, these are the only ones we would like to compute. We know which objects we can see. But how do we know in advance which photons will matter? Each photon is generated by a light source, but we don't know where each ends up. So, we reverse the process. Instead of sending photons from the light, we essentially cast a ray from the eye through each pixel and see what's out there. When a surface is seen at a pixel, we then compute the direct effect of each light on that surface. Add up all the light contributions and you have a reasonable proximation of what the surface looks like. One more approximation can be used for these lights, nothing blocks them every surface is considered visible to a light unless it faces away from that light source, in other words no objects casts shadows. This simplification saves us a lot of computation. Rendering can get even simpler than this. For example, you could do away with lights all together and just draw each object with a solid color. However, I'm drawing the line there as lights add considerable realism and perception of 3D to a scene. For example, look at this scene with, without lights. So there are two extremes. At one end photons are carefully traced from light emitters through a scene, potentially giving a highly realistic image. At the other end the camera determines what gets viewed and a rough approximation of what light reaches each surface is determined. This process can be unrealistic but can be computed extremely fast, We'll start with a second extreme and, and in later lessons show how we can remove or modify various simplifications, so the more photon based computations are performed, such as casting shadows. You may have noticed that there have been a few teapots in these lessons. I hope you like them because you'll be seeing lots more of them as we go. The teapot model was created by Martin Newell, one of the pioneers of computer graphics. Among other accomplishments, he co-developed an early rendering method called the painter's algorithm and did seminal work in the area of procedural modeling. He also founded the company Ashlar Incorporated, a computer aided design software firm. The model was inspired by a real live Melitta teapot, manufactured in 1974. Back then, there weren't many good models to use for rendering, especially for displaying materials. Objects made of cubes, cylinders and spheres and so on, can only take you so far. In 1975, Martin Newell, at the University of Utah at the time, was talking about this problem with his wife, Sandra. They were just sitting down for tea, so she suggested that he model a tea set. He made a sketch of the silhouette on graph paper. The drawing represented a recognizable profile of a teapot. It doesn't match the real world teapot. The spout and the knob on the lid are a bit different, for example. He described these curves by using what are called "cubic bezier splines." A spline is a fancy word for a type of curve. These spline curves are formed by carefully placing a few points called "control points". A surface is then formed by using spline curves in two different directions to define a patch. See the additional course materials to learn more about this process. To render these patches interactively, we often convert them into grids of triangles. This type of modelling is useful for creating all sorts of curved surfaces, such as car bodies. When Newell created the teapot, he also made some other models to create a whole set. Not half as well known, the tea cup and saucer make up a pretty nice little model. At least they look good from this angle. Well, not great though, there's no shadow. So, the cup looks like it's floating over the saucer. If you rotate the view, its secret is revealed. There's a large hole in the middle of it. There's also a teaspoon model, which is pleasing from any view. I was surprised by how easy it was to display these models, once I had my teapot demo working. The text files of patch data for the teacup and teaspoon are available on the web. The time stamp on these files is from 1991. Digital data doesn't rot. A nice feature of using cubic beziers for the data description is that the text files were each around 8,000 bytes. For a lot of applications nowadays, that's less than the size of an empty save file. Let's get back to the teapot. Like the tea cup, the teapot also didn't originally have a bottom. After all, a teapot is usually sitting on some tabletop, so doesn't really need one most of the time. Four patches defining a bottom were added later by Frank Crow in 1987. Some graphics practitioners consider adding a bottom to be heresy. So, be careful about what choices you make and whom you show your teapots. One other obvious difference between the real world and virtual teapots is the vertical scale. Martin Newell shared his teapot data set with other researchers. His original sketch's scale matches the real world teapot, but the computer graphics model evolve to be squatter. I've seen two stories in print about how this happened and being in print it makes it true, right? The more reputable one is that Jim Blinn, one of the early users of the teapot data set, decided that the teapot simply looked more pleasing when squished. The other story I've seen is that the display Blinn was working on didn't have square pixels. Rather than mess with the camera's transform, he decided to scale down the teapot vertically to compensate. I like this version a lot, since Blinn has a well-known quote of: "All it takes is for the rendered image to look right." I asked Martin Newell about these two stories and he thinks the non-square pixels may have been the original reason, but that Blinn's aesthetic sense could well have kept the model that way. Martin himself prefers the squat version. Speaking of looking right, there is a noticeable gap between the lid and the body of the teapot. I've taken the liberty in this course of widening the lid a bit, so that this gap is not noticeable. Shhh - don't tell anyone. Blinn's use of the teapot in his research popularized it and the size change stuck. The teapot is a great object to render for a number of reasons. First and foremost, we have a good mental model of what the teapot should look like, so can easily identify bugs. The curvature of the surface varies considerably, especially on the spout and handle, causing the shading to change in interesting ways. Some of the spline patches form a dense set of triangles, others less so, which gives some rendering algorithms a workout. Since the teapot is not some simple convex form, such as a block or cylinder, it casts a shadow on itself - another good test case. My small contribution to the cause was an open-source program that generated the teapot. This was back in the late 1980s, long before the term "open source" even existed. I ported this code to three.js for this course. Many people consider the teapot a standard model. Back in 1987, Jim Arvo created this classic image, showing the teapot as the 6th platonic solid, next to the tetrahedron, cube, and so on. Really, I've seen a lot more teapots rendered than I have dodecahedra. The teapot is an iconic object that is the symbol of 3D computer graphics. There are other famous models out there - the VW Bug, the Stanford bunny, the Sponza Palace. But the teapot is by far the most celebrated. Keep an eye out for it in the first "Toy Story" movie and in the Simpson's 3D episode. If you want to see the real teapot in all its glory, it's on display at the Computer History Museum in Mountain View, California. Now that you're aware of the teapot, you'll start to notice it popping up when you least expect it. So, here's a question for you. Say, you render an image by the extreme of finding what object is visible at each pixel and finding how light effects just that surface, which of the following objects can be rendered with this simple system? Here's a hint, exactly two answers are correct. So, could you make the object polished metal surface reflecting everything around it, unpolished wood showing a grain, a light bulb, or an object made of glass reflecting light through it? Here are your four materials. Pick two that could be made with this simple rendering system. Let's look at each of these materials in turn. The problem with the highly reflective ball, is that you're picking up light that's reflected off of this surface. So, you're not just looking at the surface, and seeing how the light affects it, you're seeing how other surfaces affect it. So, this answer is no good. This one works just fine. This wooden ball isn't really doing anything as for reflecting other objects, it's just getting directly illuminated. So, that answer is correct. Light bulb, oddly enough, is something that you can just render very simply, there's not really much to it. You basically are looking at the surface and the filament inside the light is Illuminating it. So, this answer is also correct. Finally, this glass ball is no good because it's picking up light contributions from other objects in the scene. We now have some sense of how we can make lighting calculations, not take an incredible amount of time. In this lesson, we're going to take an extremely high level view of how objects get rendered by a graphics accelerator. This graphics hardware is usually called the GPU, for graphics processing unit. A GPU uses a rendering process called rasterization or scan conversion, which is optimized to use the simplifications I've outlined. Let's look at one rendering pipeline from end to end. The idea of a pipeline is to treat each object separately. The first step is simply that the application sends objects to the GPU. What objects? Typically, 3D triangles. Each triangle is defined by the full locations of its three points. An application converts a cube into just a few triangles. A sphere gets turned into a bunch of triangles. So, in the first step, the application decides what triangles to send down the pipeline. In the second stage of the pipeline, these triangles are modified by the camera's view of the world along with whatever modelling transform is applied. A modelling transform is a way to modify the location, orientation and even the size of a part. For example, if you were rendering a bouncing ball, the ball's modeling transform each frame, would move the ball to a different location along it's path. The effect of the camera view is clear enough. After the object is moved to it's location for the frame, is it still in view of the camera? That is, is the object inside the view frustum. If not, then we're done with this object, since it won't affect any pixels on the screen. In this case, we say the object is fully clipped. So here, this cube is fully clipped since its outside of the view frustum. The camera and modelling transforms compute the location of each triangle on the screen. If the triangle is partially or fully inside the frustum, the three points of the triangle on the screen are then used in a process called rasterization. If a triangle is slightly off the screen, it gets clipped and it's turned into more triangles. This process identifies all the pixels whose centers are inside the triangle. In other words, it fills in the triangle. So, rasterization here has found that this triangle covers the pixel centers, in these various places. These locations are then used to show the image of the triangle on the screen. The idea of a pipeline is that every object is dealt with once. Say, we have a list of 52 parts for a car, each part represented by a set of triangles. Each of these parts is sent down the graphics pipeline and is rendered to the screen. Once an object goes through this process, we're done with it for the frame. If we stop the process half way through and looked at the image formed, we'd see only the first 26 car parts in the image. The advantage of a pipeline is that each part of the pipeline can be worked on as a separate task at the same time. Say, we take a trip to the local cardboard box factory. I won't describe what each step is because it's a box factory, that's pretty boring. All you need to know is that each step takes the same amount of time. So let's say each stage takes 10 seconds. If you want a single box, it'll take 30 seconds from start to finish to make it. However, if you want a lot of boxes, a new box will arrive at the end of the assembly line every 10 seconds. Let's see how that works. The red box gets cut, that's 10 seconds. While red box is at a folding station, the yellow box follows behind and is now being cut at the same time. Each taking 10 seconds. In the next 10 seconds, the red box is taped, the yellow box is folded, and the blue box is cut, that's 30 seconds. After this point you can see another box will come off the assembly line every 10 seconds. Our previous box factory pipeline had just three stages. Say, we were able to split each of these stages in half, and each takes 5 seconds. So, instead of 1 cutter we have 2 cutters, 2 folders and 2 tapers. The time to take a single box is still 30 seconds. At what rate do boxes come off this pipeline? Boxes will come off the assembly line at a rate of 1 every 5 seconds. This is because we have 6 separate stages, all of which can operate at the same time. So, having one pipeline is effective in giving you extra production. But it can only go so fast. So, we can produce even more boxes by buying more factory space and creating more assembly lines. Gpus are designed to use these techniques of pipelines and parallelism to get extremely fast rendering speeds. So here's another pipeline. Don't worry about what each stage is doing. I'm going to let you figure this one out, what happens if stage B suddenly starts to take, instead of one nanosecond, it take three nanoseconds? I'll give you the easy answer. An object will take a total of one, two, three, four, five, six nanoseconds to travel through this whole pipeline. The harder question is How often will an object come out of the pipeline being fed with objects? So our original pipeline had one object per nanosecond. Think through how often the objects will come out of this pipeline once it has settled down under these new conditions. An object will come off this pipeline every how many nanoseconds? For the answer, imagine you have objects entering each stage of the pipeline at the same time and suddenly stage B takes 3 nanoseconds. After 1 nanosecond, the object entering stage A is done processing, but stage B is still busy. So, the object coming out of stage A is blocked and has to wait. With stages C and D, the objects move on through just fine. However, the object in stage B does not enter stage C until 3 nanoseconds have passed. And that is in fact the time that this whole pipeline will take for every object to come off it, is 3 nanoseconds. So, given this pipeline that was in the quiz, we found that B is the slow stage here and B is, in fact, called the bottleneck. It's the slowest stage and it's the one that determines how fast anything is going to come out of the pipeline. And there's always a bottleneck in the pipeline. In fact, there's always some slowest stage. Maybe two or three that are tied for slowest but there's always some stage that's the slowest. Objects 3 and 4 move on through the pipeline and object 2 is still being processed by B. So, what this is called is when stage C is waiting for input and nothing's coming because we're waiting for B to finish, that's called starvation. That's where C is not getting anything until B is totally complete. On the other hand, what's happening here is stage A has finished processing object 1 but object 1 can't move on because, stage B is still processing object 2. This is called stalling. With the rendering pipeline, these same sorts of principles apply. Having the application run efficiently and keep the GPU fed with data is a common bottleneck for example. The bottleneck will change over time. With sometimes one stage being the slowest sometimes another. Gpu designers use different techniques to perform load balancing, such as first in and first out queues. Where you sort of stack up a bunch of objects so that if the stage suddenly gets faster, a bunch of objects can move into it quickly, or other techniques such as unified shaders. We wont be describing these techniques in detail in this course, but it's good to be aware that the GPU has a lot going on inside of it. Various performance debugging tools let you see what parts of the pipeline are bottlenecks and so, can help you improve the performance of your application. With our simplified pipeline, the application sends down triangles to be transformed to, to screen locations and then filled in. What happens if two triangles overlap on the screen? How is it decided which one is visible? One way to render it is what is called the painter's algorithm. The idea is to draw each object one on top of the other. To make this work, you sort objects based on their distance from the camera back to front. You then render the most distant object first. Render the next closest object. Render the next closest object than that and so on. The closest object is drawn last so it covers up the objects behind it. This isn't really how most painters actually paint but it could work in theory for, say, oil paints. I guess it makes some sense for computers to use it since it's a somewhat mindless way to paint a painting. It turns out the painter's algorithm has a serious flaw. Below are three drawings. Which of these three scenes cannot be rendered by painting one object on top of the other? The problem with the Painter's Algorithm is that you can get into cycles where each object is in front of the next. Here, the red triangle is in front of the blue, which is in front of the yellow, which is in front of the red. There's no way to fully draw these objects in any order and produce the proper image. So, this is the only one that's impossible to draw. These others have a defined order that is just fine. The way the GPU, usually solves the visibility problem, is by using what is called a Z buffer or if you have British meanings, it is pronounced zed buffer. The artist Lennart Anderson once said, landscape painting is really just a box of air with little marks in it telling you how far back in that air things are, it's not the[UNKNOWN] of quotes, but this is a pretty good description of the Z buffer algorithm. The Z in the Z buffer stands for distance from the camera. Buffer is just another word for a data array like an image. So, for the image, in addition to storing a color at each pixel we also store a distance which is called the z-depth. The z-depth is often considered as a floating point number that ranges from 0, meaning close to the eye to 1, meaning the maximum distance from the eye. The z-buffer is initially cleared by filling O with 1s. When an object is rendered into an image, at each pixel, the distance from the object to the camera is used to compute its z depth, a value from 0 to 1. So, here for example, I've rendered a sphere. And at this pixel, I found that the depth of the sphere in 0.6. This value is then checked against the current value stored in the z buffer. Which you'll recall, we've initialized to all ones. If the object's distance is lower than the z depth value stored, the object is closer to the camera. And so the color of the object is then stored in the image's color buffer. So, in this case, the sphere's depth is clearly closer than the maximum value can store in the z depth. So it's visible at this point. Lets say we draw a cube. The cube's depth at this pixel is 0.3, which is closer than the 0.6 that we have storage for what the sphere did. So, since its closer, we use the cube's color at this pixel and replace this value, 0.3 replaces 0.6. Say, the last object we draw is this ground plane. At the pixel, it turns out the ground plane's depth is 0.8. This is higher than the current z depth 0.3. So, the ground plane fails, which is as it should be because the ground plane is obviously farther away than these two objects. The final conclusion of this process is that the current z depth stays at 0.3 and the color that's stored at the pixel, this blue cube, is indeed the color that gets displayed on the screen. By storing a depth at each pixel, the GPU can mindlessly keep track of what object is closest at any given moment. What's interesting to realize is that decades ago, when this idea first came up, of the z buffer, it was considered absurdly expensive because you had to add a big chunk of memory of z depth buffer that was just an outrageous amount. You'd have to add a whole mega byte of memory which would be outrageously expensive. So, this algorithm originally was a terrible idea. And memory costs came down just a tad and now it's the predominant way of rasterizing triangles. Say, it costs one cycle to read the Z-depth in a pixel in the Z-buffer. One cycle to write a new value into the Z-buffer, and one cycle to write a new color into the image itself. Ignoring all other costs, what is the fastest way to draw objects? Your four choices are draw objects from front to back, that is closest to the camera to farthest away, draw objects from the largest to the smallest object, randomize the draw order to avoid the worst-case behavior, or draw the objects from back to front, farthest to closest to the camera. The answer is going to depend on how each Z-def comparison goes. When the object is closer than whatever is already in the Z-buffer then the cost is 3 cycles, read the old z-def value, write the new Z-def value and then write the new color. We call the new value coming in the source and the old value the destination. When the object is farther away, only 1 cycle is needed. We'll always have the cost of leaving the destination of reading the Z-Buffer depth. When we compare the source to the destination, we get 3 cycles due to this path of having to actually replace the old color with the new color. And the old depth with the new depth. However, if we have the case where the source is greater than the destination, we're done. The object's hidden, and we don't have to do any further processing. So we only need 1 cycle. So let's look at the answers in reverse order. Back to front order, in fact, is the worst case possible. Since each object in turn is the closest object so far. So all values read in the z buffer will also be written. Randomizing might help avoid worst case behavior, but it's not the fastest way. Largest to smallest has no predictable effect as the testing process is happening per pixel. So size doesn't really matter. So this first answer is the correct one. Roughly sorting and drawing objects from front to back is indeed a way in which applications optimize GTU performance. In this course, we're using WebGL and developers library I chose for this course called the Three.js. WebGL is an Application Program Interface, API for short, that controls the GPU. It is based on OpenGL ES, an API used in mobile devices which in turn is based on OpenGL, an API for desktop computers that dates back to 1992. The other major competing API is called DirectX or Direct3D, which is commonly used for games on the Xbox console and Windows PCs. This is almost the last time I'll mention DirectX. It's not all that different from WebGL. In fact, Google Chrome on the PC turns WebGL commands into DirectX calls. See additional course materials for more information. WebGL itself is a state based API. You set up exactly how you want the GPU to do something, then send a geometry to render under these conditions. This gives a pretty fine grained control over the GPU that can result in hundreds of lines of code that draw even the simplest scenes. This is why we'll be using Three.js, a free development library with many useful features. Just a few lines of code in Three.js can create objects with materials, set up lighting and a camera, and even perform animation. You'll now run 3 demo programs that show different rendering modes. In true computer science fashion, we'll call them number 0, number 1 and number 2. When you move the mouse, you'll change your view of the world. The objects themselves don't change position. Look at each demo carefully, as you will have to answer a tricky question about the last two demos. You just ran three demo programs. The first program gave the correct view of the scene. The next two demos were a bit more mysterious, call them mystery demos number 1 and number 2. The question to you is, which of the following is true? Number 1 draws objects in a fixed order, number 2 draws objects front to back. Number 1 draws objects front to back, number 2 draws objects back to front. Number one draws objects front to back, number two draws objects in a fixed order. Or number one draws objects in a fixed order, number two draws objects back to front. The correct answer is the first one. The 1st mystery demo gives a precise draw order. First the blue object, then red, then green. The 2nd mystery program sorts the objects in the opposite order that we'd want to use for the painters algorithm. It orders them so that the closest are drawn first. While this order is good when the z buffer's on, in that less shading computations are then needed, it gives a consistently wrong look. You now know about the fundamentals of interactive rendering. We've covered a lot ground. The basics of lighting and cameras, pipelines and bottlenecks, pixels and the z-buffer. I'd estimate you now know more that 99.44% of the population, about what make interactive rendering tick. In the lessons ahead, we'll learn more about these subjects and related fields. You'll also begin to program and create virtual world to yourself as we delve further into the key elements of interactive 3D graphics. If you experimented with the frames-per-second demo, you may have noticed something odd. If you slid the slider from 30 frames per second up to 59 frames per second, the frame rate shown in the upper left was a pretty solid 32 milliseconds per frame. If you went all the way to 60 frames per second, this drops to 16 or 17 milliseconds per frame. This happens because a new frame is not computed until the previous frame is displayed. For example, say an image takes 12 milliseconds to compute, this is well within the 16.6 milliseconds per frame. So, each frame will display a new image. If the image takes, say, 21 milliseconds, this is longer than the 60 frames per second rate. It's sort of trains leaving the station at a constant rate. If you don't get your work done in time to catch the current frame, you have to wait until the next one comes around. The fastest we can display frames is 60 frames per second. So, as soon as an image takes more than 1 60th of a second to compute, it will miss the first frame. This makes the effective frame rate 30 frames per second, since only every other frame is updated with a new image. I should say this doesn't always happen with every application. It depends on the way updates are done. This problem assumes that we don't begin to compute a new frame until an old one is displayed. The question to you then is, what is the effective frame rate if you just missed 30 frames per second? In other words, say your image compute time is a tiny bit greater than 1 30th of a second, at what rate can you display these images? If it takes you slightly more than a 30th of a second, and the monitor updates at 60 frames per second, this means you'll miss 2 frame updates and be able to update only every third frame. 60 divided by 3 is 20, so 20 is the answer. I'm fine with either using logic or experimentation to figure this out by the way, but logic gives you a deeper understanding. I have a rendering algorithm question for you. Here are 4 different optical illusions. Assume each object, in each of these scenes, is drawn separately. For example, each block or square, each brick, each ring, and each dot is a separate object. Which of these scenes most definitely could not be rendered with the basic painter's algorithm? The painters algorithm works by drawing each object fully, in back to front order. It depends on there not being any cycles where one object is in front of another, which is front of a third, which in turn is in front of the first. This image has a red ring that is front of the green. The green is front of the blue and the blue is in front of the red. Given this cycle it certainly cannot be rendered with painters algorithm if each ring is drawn as a whole. It's possible to process the scene and break the objects into pieces that could be drawn in the proper order but the basic Painter's algorithm clearly fails here. So this is the answer. By the way, these are called Borromean rings. No two rings are linked to each other, but still all three are attached. Measuring frames per second is a common way to talk about an application as a whole. If an application runs at 10 frames per second on one system and 20 frames per second on another, we can truthfully say the second system is about twice as fast in this instance. However, graphics programmers themselves have been moving towards using the inverse of frames per second. Miliseconds the seconds per frame. There are number of good reasons for this. The most appealing one to me is that milliseconds give you a solid number for use in comparison of different algorithms. If you talk about an algorithm's implementation in terms of milliseconds per frame, you can then treat the effect on speed as a single number. This user interface is costing us five milliseconds per frame is a useful fact in its own right. This user interface slows us down by 10 frames per second. It's not all that helpful, since we don't know what the frame rate was before. Reading individual parts of the rendering process by frames per second, doesn't make a lot of sense, since it's the sum of the cost that's important. You can add millisecond costs together to see how much a frame will typically take. Games for example will often have a budget of say 33.3 milliseconds per frame, which means 30 frames per second. If different effects cost various amounts of time, it's easier to add these up and see if we're over budget. If an application aims for 60 frames per second, the budget is halved. Another reason to avoid frames per second is that they're difficult to work with mathematically. So, of course, that's what I'm going to ask you to do, to get a feel for this sort of problem. You measure your application. Say it's a walk through of a building. Here are the rates you find. 25 frames per second for 100 frames. 50 frames per second for the next 100 frames. 25 frames per second for the next 100 frames. And then 10 frames per second for the last 100 frames of your benchmark. What is the average speed of the application in frames per second? I'll give you a hint, simply averaging the frames per second values is not going to work. Think about how many frames are rendered and how many seconds passed during the entire benchmark. You might want to translate this problem to thinking about miles per hour. Usually, we just add numbers up and take the average. This doesn't work with frames per second. And that's the problem with 'em. If you add the 4 values together and divide by 4, you get 27.5 frames per second. That's not correct. however. Say we think about this question in terms of miles per hour. Instead of 400 frames, say you drive 400 miles. If you drive 25 miles per hour for the first 100 miles, 50 miles per hour for the next, and so on. How long will the trip take you? Divide 400 miles by this number of hours, and you'll get your average number of miles per hour. Using this idea, we see that the first set of frame takes 4 seconds to view, the second; 2 seconds, third; 4 seconds, and the fourth; 10 seconds. A total of 20 seconds. 400 frames divided by 20 seconds is 20 frames per second. This answer is a little surprising in that it's lower than 3 of the 4 frame values mentioned. The duration of the 10 frames per second portion of our test is so long, that it pulls the average down. Here are four renderings of the same scene. One is trivial to render. The others take more computation. It's up to you to think about how expensive each is to render. Put on each line and number from one to four. One means the quickest to render, four means most complex. The quickest scene to render is the one without any lighting at all, right here on the lower right-hand corner. These objects just use a solid color and so little detail is visible. The second fastest to render is here in the upper left, as the objects do not cast any shadows. Note how details get lost near the front edges. It's hard to tell where the floor ends, and the top of the outer wall begins. The next fastest to render is here in the lower left. This one has shadows, which take longer to render. Shadows help us a bit in telling how surfaces are related to each other. However, everything in shadow looks the same. Since we're computing only light that directly hits these surfaces. The slowest rendering is here in the upper right. It's also the highest quality, as light bounces off the walls and into the shadowed areas, so giving them some definition. Here's a thought experiment. Imagine you're a firefly. Now, imagine you're flying around in a closet with the door closed. You're the only light source. Finally, imagine your light is quite bright and that you have human eyes. Horrified yet? An easier way to say it is this. Imagine there's a single light source in the scene and you're viewing everything from its location. Which of the following are true, if any? Objects are visible, but get dimmer the farther you are from them. Objects are visible, but only in shades of gray. Objects are visible, but are only solid colors without shading. Objects are visible, but not shadows can be seen. The first choice is true. Objects definitely get dimmer the further they are from a light source. The number of photons hitting a surface drops off proportional to the square of the distance from the light. The second choice is false. There's no reason that objects wouldn't have colors. Well, maybe if all the objects in the closet were gray, but that's not a fair assumption. The third choice is also false. Even though you're at the light source's location, the objects can receive different amounts of photons, depending on their distance from and angle to the light. The fourth choice is true. This is the main point of this question. What the light sees, by definition, is not in shadow. We'll use this fact later in the course to generate shadows on objects. When we look at a scene, we can't always tell how far away objects are. Our eyes, well at least my eyes, are not laser scanners. They don't return z depths directly to my brain. Maybe another way to say it, is that we're not bats and don't use sonar. Our eyes detect what is called the radiance coming from each direction. Our brain figures out distances from a variety of visual cues. We're not even particularly aware of these cues. For example, it wasn't until the renaissance that painters figured out that distant mountains had a blue tinge, and so started to paint them that way. However, the actual amount of light from those distant mountains is a given amount of radiance. We don't store depth. One way to think about radiance is in terms of a function called the light field. The idea of the light field is that for any point in the universe, and for all directions from that point there is an incoming radiance value that we can record. Say we're sitting at a desk. For our location we can look around and see a color in any direction. In computer graphics terms, we're recording a radiance for each different, specific direction we look. To form an image, we find the radiance coming through each pixel. Imagine we have a sensor floating in space that can move anywhere and look in any direction. I like to think of the sensor mounted on a turret, with the turret rotating as needed. The question to you is, how many numbers, how many variables do we need to specify our imaginary sensor's location and direction so that we can record the light traveling through any point in the universe from any direction? The answer is five. Three for the location and two for the direction. You might be tempted to answer six with three for the direction. If you think about how a [UNKNOWN] works the sensor can be given in altitude and a swivel angle to specify direction. Just two values. A third value would be needed for the distance in a given direction, but our eyes detect just radiance, not distance. It's an interesting way to think of the universe. Just five numbers specify the location and direction, and only need then is the radiance. Put together enough radiance readings for a given location and set of directions and you get an image. It's also informative to think how the radiance values change as you move around. For example, if our sensor is in deep outer space, moving even large distances will cause only small changes in the radiance values received. Having insight into how a scene's light field changes can help us figure out how best we can compute and store these changes to simulate it For you first encounter with actual WebGL and 3js code, I'll give you a simple program that creates a scene with one object, a cube. And a few grids surrounding it. To see it, however, you'll have to get your hands dirty and fix a few syntax errors. In the code, to the left of the lines that have problems, you'll see little marks that denote that you need to fix those lines. Make the corrections, and then try doing the test run, and you should see this. Once you fix the three syntax errors in the program, you should see something like this on your screen. Note that you can interact with it. You can move around, zoom and pan. Try the various mouse controls to see how it works. If you get things into a broken state, just refresh the page on your browser. Once you've got the program running, try changing numbers in the code to see the effect. Don't worry about what the code's doing at this point, I'll be covering that in the lessons ahead. More importantly, I'll be explaining the underlying physical principles and mathematics behind this code. In this unit I'm going to talk about how geometric objects are define for rendering. Along the way I'll discuss points, and vectors and how these get used in computer graphics. Let's dive right in, there are two distinctive mathematical objects that we use in computer graphics all the time points, and vectors. I'm going to focus on the 3D aspect since that's what we commonly use 3D computer graphics. Working in 2 dimensions is similar, 3D is just 50% more interesting. To start, we'll define a 3D point as a location in space. A 3D vector defines a direction in space. A location in space has to be defined with respect to something else. For example on the earth we define a location in terms of its latitude,longitude and altitude. This is a spherical coordinate system, in that the lines of latitude and longitude wrap around a sphere, the earth. In 3D computer graphics, we usually use a Cartesian Coordinate System. In this system, we have an origin and three direction vectors called x, y, and z. The origin is some fixed point in space whose location everyone agrees on. This point can be anything. I could build a world based on the location of our surveyor's marker, a dot drawn on a tablet, or a reference point is simply imagined. For example, if I'm designing a great new teapot, I don't really care where the rest of the world is located. So, I can state that my origin is just a point in empty space. I might then build a the teapots model so that this origin point is in the center of the bottom of the teapot. Here's a prettier view since I want you to see the 3D coordinates better. The three direction vectors define the axes of the coordinate system. They are normally each perpendicular to the other. That is, the angle here, here and here are all each 90 degrees. It's possible to define a coordinate system with axes that are not perpendicular but this is rarely done in practice. For example, finding the distance between two points becomes a tricky concept when the axes are not mutually perpendicular. Given an origin and three vectors, we can then define the location of any point in space with a triprelative numbers. For example, this point has a triprelative 2, 3 and 4, as it's two units along the x, three along the y, and four along the z. 3D vectors are described by a similar coordinate system, except that the origin is not needed. A vector describes a motion, which again can be described by three numbers. That is, it describes how far to move to get from one point to another. However, this movement is not fixed in any particular location in space. To understand the difference, think about time. A specific time is like a point. An amount of time is like a vector. It specifies a duration, but no starting time is given. In these statements, is the thing described with a number more like a point or a vector? So the first one's, let's meet tomorrow at eight am if that's not too early. The continental shelf is generally no deeper than 150 meters from sea level. This film is almost three hours long. The statue is 200 yards north of town center. This first statement gives a definite time, which is like a point on a timeline. So, point is the correct answer. The second describes a distance, which is like a vector. No specific point at sea level is given, so this does not describe a location. So, this statement describes a vector. This third sentence describes a duration and no specific time, so it's like a vector. Finally, the fourth describes the location, given another location, and a motion vector, so it's a specific point. Just like you right hand and left hands, both have 5 fingers but look different, coordinate systems also have an orientation to them. We need to find a set of axes. You also need to specify one other fact, whether the axes are in right-handed or in left-handed system. Here's an example of a right-handed coordinate system. Say, I want to build a building. I define the x-axis as going east and the y-axis as going north. Now, my floor plan shows x pointing right, y pointing up. This is the standard 2D Cartesian coordinate system. If I had to find altitude as the z-axis with the arrow coming out of the diagram towards the viewer, this is a right-handed coordinate system. Why is this right-handed? Well, if you place your thumb along the x-axis and the index finger along the y-axis, the middle finger is the z-axis and points towards you. You'll notice if you try to do the same thing with your left hand, you can't match up all 3 axes, at least not comfortably. This is a left-handed coordinate system. The key thing to realize is that the left-handed coordinate system is equally valid. It does not change the underlying reality of, of the situation. For example, intelligent rabbits might agree that x is east and y is north, that's fine. However, they might decide that the z-axis points down into the page. The rabbits care more about how far down a tunnel is dug from ground level and want to say 5 feet, instead of negative 5 feet when they're digging. This is a left-handed coordinate system. The underlying reality is the same in both cases, nothing has changed place, just how we and the rabbits refer to location differs. When we say 12 meters, the rabbits would say, negative 12 meters and vice-versa. To convert between these particular two systems is simply a matter of negating the z-coordinate value. Is longitude, latitude, altitude a right or left handed coordinate system? Consider going east to increase the longitude, and consider going north to increase the latitude. Order is also important. I list longitude first here, since we tend to think of that direction as the X direction. In reality, locations are usually listed as latitude first, then longitude. So the question to you is, is longitude, latitude, altitude left handed or right handed? With the X axis east and the Y axis north and the Z axis up like this, that forms a right handed coordinate system. So, this is the correct answer. What's interesting is that if you swap the latitude and longitude, and so, list the coordinates as latitude, longitude and altitude, the coordinate system is now left handed. It also swaps if you consider west to increase the longitude or south to increase the latitude. I happen to love the game, Minecraft, along with a few other million people. Yes, I built the teapot in the game. I even wrote a program to help me print 3D models in Minecraft. But really, no, I'm not addicted at all. One confusing thing in the game is the coordinate system. In the beta version of Minecraft, the coordinate system was as follows, the X-axis points south, Z pointed west, Y points up. You could tell these where the axis directions because in the game, their sun rose in the east. When the game was released the developers changed the direction the sun moved. The sun started rising and what I used to think of was the south but was now considered the east. In other words, in release, the X-axis now points east, Z points south, and Y points up. So, what handedness are the beta and the release coordinate systems, right-handed or left-handed? The way I solved this puzzle was to first align my right hand to the directions given in the beta, X south, Z west, and Y up. If I could do it, then I know the system was right-handed. Easy enough, my right hand aligns just fine. If I try the same thing with my left hand, I can get 2 out of 3 axis to align but the third is always in the opposite direction. So, we know the beta version of minecraft used right hand coordinates. I could solve the second half of the question by reorienting my right hand and again, noticing that the fingers can properly align with the axes. So, it is also right-handed. However, you don't really need to use your hands to figure this out. What happened within the world itself was simply that the sun started to rise form a different direction. None of the world's data had changed at all. The only thing that had changed was our perception of which direction was east. Since the world was the same as before, the underlying coordinate system itself had not changed. Realizing this, I could conclude that the released version is still right-handed, just like the beta, but it never hurts to check with my right hand. There is no correct answer for which is better, a left-handed or a right-handed one of the other. May be more convenient for people or intelligent rabbits, depending on a situation. One other area in computer graphics where opinions vary is whether the y-axis or the z-axis should be on the up direction. Architects tend to think in terms of floor plans, so the x and y-axes, our directions on the ground, and z is then the height. However, another common way to think about coordinate system is with respect to our display. Looking at a monitor and starting on the lower left-hand corner, the x-axis goes to the right, the y-axis goes up. If we want to present a right headed coordinate system with respect to the monitor, the z-axis then points towards us. We'll see this type of definition in a later unit where we cover how to define a camera. The difference between these two coordinate systems is just a 90 degree rotation along the x-axis. So, it's easy enough to fix. We'll talk about rotations in a later unit, too. However, you've now been warned. If you read in some models some day into your program and the object is sideways, it's probably because the models up direction is different than what you're expecting. Which way is up to you, the y or z axis? I mean when you're working on a workstation and your screen is vertical, then I think of it as y up but people working with pads now you sort of have this case where the z is coming out of your pad, your tablet, because you're working in this direction. I would say, negative y Because that's how my old VGA card on my PC worked. For some reason the Y axis points downwards on them so when you started making the first 3D effect which was like, you know, the star scroller, you're like flying toward in a some kind of galaxy with stars. You ended up with, you know, when you increased Y the points were moving downwards. So I would say - negative y, that's up. For me, because I'm in a shop all the time, z is always up. And then that has to do a lot with like CNC machining, you know. If I'm in the shop it's always the Z, if you're doing 3D printing it's always the Z axis. To work out your brain a little, say, we have two coordinate systems with the same origin. The first uses the architect's scheme, X goes east, Y goes north, Z goes up. The second uses minrcraft's current system of X goes east, Y goes up, and Z goes south. If I have a point 10, 24, 13 in the architect's system, what is this point's location in the minecraft's system? Here are both coordinate systems with the architect's scheme in blue, Minecraft in red. We're in luck with the x axes, in both systems we know the x axes match. Whatever happens with the y and z axes won't affect the x axis value. So in the second system x will also be 10 . In the architect system, z goes up and in the mine craft system y goes up. That's simple enough. The height of the point in the architect system is 13. The height is still 13 in the mind craft system so y will be 13. Finally in the architect system, y goes north and z goes south in mind craft. The y coordinate in the architect system is 24. Since z is going in the opposite direction. Z must then be negative 24. So the answer is 10, 13, negative 24. There are only three basic primitives we ever send to the graphics processing unit, points, line segments and triangles. Of these, triangles are by far the most important. For example, just about everything you see in a 3D game is ultimately made of triangles. Sometimes it's obvious where the triangles are used, such as in Minecraft. Sometimes it's not, as in this shot from battlefield 3, captured by Duncan Harris at dead end thrills. We're going to touch on points and lines only briefly so we can focus on triangles. Defining a point is simple, you just specify a 3D location. For example, this point is at 0.4 in x, 0.2 in y and 0.0 in z. By the way for all these examples I'm going to make the z values 0 but it can certainly have a different value. In 3D computer graphics you tend to set all three coordinates x, y, and z even if you are mostly working in 2D. By defining a second point we can create a line segment. The order of the points does not matter. A triangle is specified by three points: one, two, and three. We have progressed from a zero dimensional point, to a one dimensional line segment, to a two dimensional triangle. That is even though the coordinates of each object are specified in three dimensions, the objects themselves have these lower dimensions. Two dimensional surfaces are usually as far as we need to go. We normally care about how light bounces off an objects surface, we don't care that much about representing a solid volume. That is, we don't need to represent what's inside the volume in any detail. Finally, you get to program. Most of our exercises will be in JavaScript. Even if you don't know JavaScript, you should be able to get through the first few exercises without too much difficulty. Your goal in this exercise will be to draw a square. To start with, you have to create a geometry object which allows you to add vertices to it. Which lets you make triangles and squares and so on. So, this geometry object will consist of vertices along with a face. Vertices are the corners of a polygon, usually a triangle. Once we've created our geometry object, we add vertices by pushing them onto the array. Basically, push means add to the end of an array. And JavaScript keeps track of how large the array is and so on, you don't have to worry about allocating space for it. And these are our three vertices. We use Vector3 for both points and vectors within 3JS, within this library. So, these are floating point numbers in fact. And define three points in x, y and z coordinates. A geometry object also has faces in it. By using a Face3, we define three different vertices that we're going to make a single face out of. In other words, we're going to define a triangle. The way that face indices work is they point to the various vertices in the array. So, this is vertex zero. We always start counting from zero. Vertex 1 and vertex 2, and by listing out those indices, 0, 1 and 2, so these are integers, that says this triangle has this, this, and this vertex. We now have the triangle's geometry defined. An object in 3D graphics needs both geometry, and a material. So this the glue code that you need to actually display the thing on the screen. You add a material, and we'll talk about this next unit. And you put the geometry and the material together into what's called a mesh in 3JSm, and then you add that mesh to the scene. In the exercise you won't be doing this part. You'll be filling in a function that creates the square. The function you'll be implementing is called draw square and takes a lower left hand coordinate and upper right hand coordinate. What you'll return from this function is a geometry object which then the program will draw. It turns out there are many ways to specify a square. 1 solution with triangles is a bit brute force. What you do is you create 2 triangles independent of each other. So these four lines of code give you a triangle that goes from index 0, 1, and 2. These are the first 3 points, right here. And then you have a second triangle 3, 4, and 5. That are these 3 points. Now you'll notice there's duplication of points 2 and 3 and 5 and 0 and we can save space and time by merging these. A more compact and efficient way to draw the square is to define the 4 vertices needed and then define 2 triangle faces reusing these points. So for example, here we're reusing these points, 0 and 2 for this first triangle, and for this second triangle. It turns out that the 3JS library happens to support a 4 point face. So you could just use one face like this. That said, the way that web GL, DirectX, and other low level APIs work is to define only triangles. When the 3JS library defines a 4 sided polygon this way, the quadrilateral is rendered by 2 triangles being sent to the GPU. If you happen to solve this exercise by drawing a single quadrilateral, that's great that you read the 3JS documentation, bonus points for you. But it's worth your while to try this exercise again using just triangles. I'm going to start this lesson by noting that tessellate is the most misspelled word in computer graphics. So give it a good look and think of doubling every letter that you can when you spell it. Yes, 2 Ls looks weird to me too, but that's how it works, and I force myself to spell it correctly. The word, tessellate is from the Greek word Tessella, which means a small stone in a mosaic. In English, breaking an object into polygons of any sort is called tessellation. In computer graphics, a curved surface is often represented by triangles by placing points on surface and connecting these together. For example, a geodesic dome is 1 tessellation of a sphere, such as the biosphere in Montral. When it comes to making surfaces the GPU understands only triangles. To draw any polygon, we break that polygon into a set of triangles. This process is naturally called triangulation, usually we use the existing vertices as the polygon when performing triangulation. The major rule is that when we add an edge, it has to be entirely inside the polygon. Triangulation is a particular form of tessellation. Here's the same pentagon tessellated in a fairly arbitrary way, with new vertices added inside the pentagon and connected together. Once upon a time some older graphics hardware from the 1980's supported objects such as polygons or even more complex objects. For example Nvidia's first chip, the nv1 supported ellipsoids as a basic primitive. Graphics hardware manufacturers quickly came to realize that the triangle is a fine building block for almost everything we want to draw. For example one problem with a quadrilateral is that the surface is poorly defined if all four points are not in the same plane. A triangle's three points are by definition always in the same plane and, so, have no such problems. By the way the graphics coprocessor I used to develop on in the late 1980's cost about $35,000 at the time. It was perhaps at best 1-1 millionth as fast as the fastest GPU's are nowadays, at only at the price of a BMW. Here's a quick quiz. Using triangulation or tessolation, what is the minimum number of triangles you can use to represent this hexagon? General tessellation won't help. Adding vertices will just mean more triangles to join these up. The answer is 4 triangles in fact the rule is a polygon with n edges will need n minus 2 triangles to represent it. We don't care all that much in interactive rendering which triangulation is used. Here I've made a fan of triangles, all radiating from 1 vertex but there are other ways to do it. The shape of the triangles is important in other fields such as stress analysis program that use meshes but isn't so important in interactive rendering. If you're a puzzle buff here's a bonus question for absolutely no credit and little use in computer graphics itself rather just for the sake of it. How many different unique triangulations of this hexagon are there? Once you solve that, is there a general rule for polygons? Feel free to discuss this on the forums. Here's a slightly different question. Given this pentagon, how many ways can we triangulate it without adding new vertices? Surprisingly there is only one way to triangulate this figure. Because of the deep concavity, we can't add edges here or here, as these edges are outside of the figure. Instead the only place to add edges is here and here, giving the only possible triangulation. The good news is, that no matter how complex the form, any polygon can be triangulated into triangles, so it can be displayed using a GPU. 3-D computer graphics uses an interesting concept to speed up object display called backface culling. Imagine you're looking at a box. Here's the side view in 2-D. Only the sides of the box that face toward you are visible. The backfaces here and here do not need to be rendered. The fastest polygon to render is the one you don't ever have to render at all. Backface culling can throw away about half the faces in an object, so it's a worthwhile optimization. For example, here's a real live box. If you look at it from any angle, you're only gong to see a few of the faces of the whole box, and the rest of the faces can be thrown away. Backfaces can be culled for any solid object. This V-shape seen from the side has only two sides facing towards the camera, outlined green. These other two faces can be thrown away without further processing. It turns out this green face here is hidden from view by this front face, but we let the z buffer take care of hiding that. We can set whether backface culling is enabled or disabled, but how do we determine if a triangle is a frontface or a backface? There are three points defining each triangle, and there is no flag stored for additional information. What determines front or backfacing is the order of the vertices after they are protected on to the screen. In WebGL, if the order is counterclockwise, the triangle is frontfacing. If its clockwise, its backfacing. Going counterclockwise like this is often called the right-hand rule. The fingers of your right hand wrap around the edges of the triangle in order. If your thumb points at you, the triangle is then frontfacing. This is a fairly simple exercise. You're given a program that draws two triangles to make a square. Similar to how this was done earlier. Back-face culling has been turned on, which has caused one triangle to disappear. Your job is to determine the problem and fix the triangle's representations so that both triangles appear. Here's the method you need to fix. You can ignore the material culls at the top and the mesh and screen culls at the bottom. The problem has to do with the geometry culls here in the middle. By the way, in Three.js Back-face culling is turned in by setting the side variable in the material. So, here in the Material Creator we set side to Frontside, which means that only the front faces are visible. In this exercise, the vertices themselves are not the problem. The first triangle is the one that appears on the screen. You can verify this by commenting out this triangle and seeing that nothing then appears. The second triangle is then the one that's causing problems. We've been told the triangle disappears when culling is turned on, so it's probably the vertex order. By swapping any two of these indices. You reverse the order of the triangle. Here I've swapped the first two. Doing this solves the problem and gives us the right answer. Creating individual triangles by hand is pretty painful, as you probably noticed from the exercises so far. The most common way to generate geometric objects is to use a modeling program. Writing such program is the bread and butter for the company I work for, Auto Desk. Here, I'm using 3 studio max to create a number of basic objects including the tea polyhedron. There are plenty of other ways to acquire models. For example, laser scanners can be used to find a cloud of points. This data is then processed to make a mesh. Another method is to take a series of photos and have a program derive the shape of the object from these. Whatever process is used you normally end up with the geometric mesh of triangles. Within Web GL on 3JS, there's a whole process where you can take a mesh in some 3D file format and convert it to a form that Web GL can read. I could spend the next few units discussing this process, but it's not all that relative to interactive rendering itself. Take a look in the additional course materials for it to learn more about this. You've now got down with sometimes the hardest thing in 3-d computer graphics, getting a single triangle to appear on the screen. In the problem sets that follow, you will get a chance to do a lot more programming using triangles and other primitives to create different objects. Your task in this problem is to write a regular polygon creator. A regular polygon is one that has equal angles and equal length sides. When you create the polygon, sucessive points will be counter clockwise. Your job is to form the minimum number of traingles that triangulates this surface. Your draw function is called polygon draw function in the code. It takes as an input the number of sides desired. I'll provide you the basic trigonometry function for generating the polygons points. This code the produces x and y creates the polygons vertices. For example when the loop variable point is at 0, the 2D point at x and y is going to be the point 0,1. Your job is to save the vertices, then use these vertices in a loop that generates the faces for the polygon. Here's what your aiming for, a regular polygon centered around the origin. There are 2 parts to this problem, the first part is saving the vertex in the geometry object. This is pretty straight forward and you've seen it before. You add a point to the vertices. The trickier part is generating the faces. There are a number of ways to do this, here's mine. You may recall from the triangulation exercises that a polygon with some number of points generates points minus 2 triangles. The loop here uses this fact with face looping through this many triangles. My loop makes this kind of fan with all the triangles including this top most point and radiating out from there. Your next problem is not too tough. The PolygonGeometry function has been extended to include a location. This is where the center of the polygon is to be located when it's created. Note that the location is passed in as a Vector3. So, you'll access the location as shown with location.x, and location.y. You can ignore the value of location.z. I'm going to. Your program should look like this when running correctly. The solution is to add the location to the x and y-coordinates being generated. Doing so moves all the vertices by this amount. Since all the vertices are generated relative to the origin, this essentially moves the origin to this new location. There's one more useful thing we can add to our polygon creator, changing the radius of the polygon. By default, the distance of all the points from the polygon center is 1. Now, modify the program to use a radius to make the polygon generated larger or smaller. Here's your new interface. Notice that we've changed the order of arguments here. In the previous problem, the location was the second argument. Now it's the third. This is usually not a great idea, especially in JavaScript, as this language does not really check if the number of arguments passed in matches the number of arguments expected. We did it this way because we felt the location change is a bit easier than the radius change and wanted to save the trickier problem for later. When your work is complete, this is what you should see on the screen. You again have to modify the x and y values computed. By multiplying the original point by the radius, this expands or contracts these points around the origin. After that, you add the location to move the whole polygon elsewhere, same as before. This gives you a little taste of the unit on transforms, where we will be talking about these sorts of operations in depth. Here's your chance to build a stairway to, well, not heaven, but at least to a gold cup, a virtual gold cup. This exercise gets you familiar with how most of 3 JS's, geometric objects work. You'll be spending your time working inside the create stairs function. We're going to use the cube geometry method to create 2 pieces for steps, a vertical piece and a horizontal plank. Cube geometry's not a perfect name since a cube is defined as having all its dimensions be the same size. Box geometry would have been better but no big deal. Here's the code for defining the geometry for two steps. The way cube geometry works is that you provide it a size in x, y and z. It then creates a set of triangles for you forming a box with these dimensions. Once you have this geometric object, you can use it as many times as you wish to create 3JS meshes, each of which consists of geometry and a material. You can use the position parameter on the mesh class to re-position the box, and in fact you'll need to do this for the exercise. When you create a box, it is centered around the origin. For our vertical part of the step, we want this piece to be resting on the ground. To do so we take half of the height of the piece and move it this distance upward so that the bottom is now at ground level. The horizontal piece making up the flat part of this step is created with this code. In this case, we first push the step up half its thickness so it rests on the plane, then push it up the rest of the height up the vertical piece, so that it rests on top. The z position has also changed. Pushing the plank so that one edge is at the origin and that pulls back a bit so that it rests firmly on the vertical piece that's shown here. Got all that? Once you run the program, you'll see the two step pieces with the vertical piece resting on the ground plane. You'll also notice a cool program feature. There's some controls available. The grids can help you get a sense of distance along each axis. They have lines every 100 units. Each is labeled with the axis that are in that grid plane. For example, since the y axis is up for this scene, the x, z grid is on the ground plane. Your goal is to build a number of steps, one on top of the other going forward that just reaches the gold cup. Here's what things will look like when you're done. Don't build any more steps than are necessary or we'll charge you for extra time and materials, plus you won't pass the problem. I recommend that you first play around with the position parameter and see its effect on the model. Setting the position to zero shows what the block looks like centered at the origin. You should also consider using a loop as its a lot easier in the long run than brute force. Being a smart programmer can save you a lot of groundwork. Using programs to generate geometry such as a number of stairs is called procedural modeling. Personally, I'm pretty bad at using modelers so I like modeling this way. It's great that with a few lines of code you can generate pretty complex models. Here's a procedural model called sphere flake that made more than 25 years ago. Around the same time Pixar created a short film called Red's Dream. Primarily using procedural modelling, since there simply weren't great modelling tools easily available back then. See the additional course materials to download the program for this model and others, along with pointers to many of the related resources, but don't get distracted right now. Go ahead and build the steps. Congratulations, I'm guessing this problem took you a bit to figure out. My teaching assistant Congard de Kana came up with this problem so it was fun for me to solve in my own way. The key insight for me was that each step needed to be a constant distance up and forward from the previous step. From the second step I started by adding the height of the first step to its location. This first step's height is the vertical step's pieces height plus the step's thickness. The other piece of the puzzle is how far forward the second step is from the first. From the diagram we see that this is the horizontal step depth minus the step thickness. With these two values in hand I could create a loop to generate all of the steps. Here's my code. At the top I computed the height and depth I needed to offset between each step. Through experimentation I found I needed to make six steps with my loop. On each loop iteration I modified both the vertical and horizontal parts of the step by adding in the height and depth offsets. Your goal is to make a simple model to manipulate in future units. You'll create a basic drinking bird model, very basic. At this point he won't even have a beak. We'll get to that in a later unit. The drinking bird is a little toy that drinks from a cup. Google it. The objects you'll make are the legs, body, head and jaunty hat. Here's the blueprint showing front and side views. Well, it's more of a back-of-the-napkin drawing, but sometimes that's all you get from a client. Me, my favorite client comment I heard that one designer received was, could you make that sphere rounder? The three objects you'll use are the cube, well, box, sphere, and cylinder. Here are the typical calls for creating these three objects. For the cube, the numbers passed ten are the width, height, depth of the box. The sphere needs just a radius to describe it's geometry. The additional two numbers specify how much tessellation happens along the equator and from pole to pole, respectively. You might try changing these tessellation numbers to see the effect on the body, but please use these numbers, 32 and 16, when you submit your answer. The cylinder gives an upper and lower radius, followed by a height, and by the amount of tessellation along its equator. Again, please use the number 32. See the additional course materials for documentation for these various methods. There are three functions you'll be working on. The function, CreateSupport, will make the drinking bird's support frame, base, legs and feet. To get you started, I've laid down a base that the legs connect to, along with the left leg. You'll have to add the right leg and feet. The function, CreateBody, will create the bird's body and spine. The function, CreateHead, will make the head and hat. You have to create the objects in the correct location and add them to the scene. When you've done that, you'll see the model of the drinking bird, just like it's shown here. Well, modeling that was certainly a pain, wasn't it? Now you know why you should always buy Autodesk brand 3D modelers and help pay my salary, or use a free alternative like blender. Well, I do want to put a little plug in here. Which I had absolutely nothing to do with, but am very happy about. All Autodesk software I know of is free to students, teachers, and veterans, and unemployed professionals. See the additional course material for information. Really though, there's not much to say about this exercise. Either you got it or you didn't. The drinking bird is starting to look sort of vaguely like the real thing if you squint a lot. Modifying this model a bit more, I made the legs white. When I do this and run the program, this causes what is called Z-fighting as you can see here. What is happening is the two polygons, one from the foot and one from the leg are overlapping attempting to occupy exactly the same location. At one pixel, one surface's Z depth will be infinitesimally closer than the other due to precision limits. At the next pixel over the situation maybe reversed. While this is fine mathematically, it can't happen in the real world and curling makes for unconvincing images. Examine the problem yourself by running the program. Find the link in the additional materials. The question to you is, which of these solutions will fix the problem? Make each leg a bit thinner so it doesn't overlap with the foot. Use a newer GPU with a higher-precision z-buffer. Switch to another rendering algorithm, such as ray tracing. Remove the bottom part of each leg so that no part of the leg and foot overlap. I should not that two of these answers are correct. The first solution is acceptable, for the most part. It solves the problem of two surfaces being in the same location. However, it does change the geometry of the model, which might not be acceptable. also, even surfaces that are near each other can still have Z-fighting problems, since the Z buffer has limits on its precision. So, this solution works, but it's not optimal. The second solution sounds nice. A new graphics card is always fun, but it doesn't help at all. Since both surfaces are in an identical location, which one happens to be closer is entirely arbitrary. Another GPU can and will give you a different view of Z-fighting. So if you like the Z-fighting effect, you can control it if your users are on different machines with different GPUs. The third answer is switching to another is switching to another rendering algorithm, again, will just cause different precision errors to show up. The underlying problem of two surfaces being in the same place is a modeling error and can't be solved by rendering. The forth answer, removing the bottom part of the leg that overlaps with the foot, is certainly the cleanest solution. This is also the most efficient answer, as making the object smaller will mean it covers less pixels and so causes less work. In this unit I'm going to teach about colors and materials. The application uses the GPU to compute a color for each pixel and displays it on the screen. We'll examine how this color is defined and what limitations there are in displaying them. In the second half of this unit, we'll show how to create and control different types of materials. We won't go quite as far as this demo does in it's selection of materials. It has some interesting styles that we'll discuss more in the unit about shader programming. However, by the end of this unit, you'll have a fair sense of how these images are formed. You should have a good start on color theory and how materials are used. To get a flavor for the sorts of materials you can apply to a surface give this demo a try now. You'll see the link in the additional course materials. To get back to the rendering Pipeline, our simplified view is this. The Application sends a triangle to the GPU. The GPU determines where the triangles vertices are on the screen, including the z-depth. Each pixel inside the triangle is shaded. If the pixel passes the z-buffer test It is then saved to the image and displayed at the end of the frame. Modern GPUs have parts of the Pipeline that are Programmable. The transformed screen part of the Pipeline is done by what is called a Vertex Shader. This Programmable element, essentially a little computer, processes each Vertex of the triangle. The Vertex Shader uses information provided to it to manipulate each Vertex in some way. For example, the color of the triangle at this point could be computed, or the vertexes position could be modified, if, for example, you wanted to have an object inflate or explode. One operation the vertex shader always does is to output a location of the vertex on the screen. The second half of our modern GPU Pipeline, we represent here by two stages. Triangle set up in the Fragment Shader. Triangle set up uses the three screen locations generated by the Vertex Shader for an incoming triangle. This forms a triangle in screen space. Each pixel covered by part of the triangle has what is called a Fragment generated for it. This process is called scan conversion. The Fragments generated are sent to the Fragment Shader. Well if you used Microsoft's Direct X API, this is called the Pixel Shader instead. The Fragment Shader is provide information by the triangle being processed. Some to the Vertex Shader the programmer can also feed in any other data desired. The Fragment Shader runs a program that typically a Color an a Z Depth value. This Z Depth value is then tested against the Z buffer as usual. If the surface is visible, the color is saved for that Pixel. The shade Pipeline is designed to compute the Color at each Pixel the surface covers. That's it's ultimate purpose after all, creation of an image. Everything done in the Pipeline comes down to this. How DO WE EFFICIENTLY CALCULATE THIS COLOR? That's what the last half of this unit is about, materials and how they work. Given a material and some incoming light you want to compute a Color off that material. In interactive rendering systems, a color is defined by three values: red, green and blue, abbreviated RGB. Each color value is often called a channel. These values are usually in this order in most systems, RGB. That said, once in awhile you'll run into data in the order BGR, blue, green, red. Which is RGB's evil twin. Without properly swapping the channels, you'll see some odd-looking images, such as this one on the right. The green stem looks fine, since the green channel is in the same place, but what was once red is now blue. Okay, now you've been warned. We usually define colors in one of two ways, as floating point numbers or as integers. We'll start with floating points, since that's generally the easier one to understand. For floating point, each color channel is defined as values from 0 to 1. 0, 0, 0 is black, 1, 1, 1 is white. Setting just the red channel gives red, green to green and blue to blue. Where things go off the tracks for some people is when we set a color as follows, RGB of 1, 1 and 0. This is to make a color using the red and green channels at their maximum and with no blue. If I take red paint and green paint and mix them together, I get at best a murky brown. This happens because paint colors are formed by absorption of various wave lengths of light. The color model for mixing paints and inks is called subtractive color. The three primary colors used in printing are cyan Magenta and yellow, with black added as its own dark ink. This printing system is called CMYK with K standing for key and essentially meaning black. As different colors are mixed, the pigments become darker over all. In computer graphics a monitor, such as this one, emits light of different labelings. Our eyes see light in wavelengths between 390 and 750 manometers. Longer wavelength light is red, shorter is blue with the rainbow in between. If we specify some red and some green light then these contributions are added together giving a brighter color overall. This color model is called additive color. See the additional course material for more articles about this area. If you want a particular color, the additive color RGB values are what you will need to determine. To get familiar with this way of mixing colors try out the demo that follows. In Three.js there are a number of ways to set a color. See the additional course materials for a link to the documentation. We'll cover four of them here. You start by defining the material. We'll talk more later about what this material does. For now, just know that you can set a color for it. If you don't set any at all, the default is white. The MeshLambertMaterial object that you've just created has several different parameters. This can be set when you first create the object or later in a number of different ways. One of these parameters is the material's color. These parameters have type THREE.Color and you can use all the methods associated with it. One method is to set the r, g and b channels separately. Here I'm setting the color to red. This directly sets the variables r, g, and b in the color attribute to the desired values. I can also use the setRGB method on the color class, setting the color to orange. A more compact way to set a color is to use hexidecimal numbers. Here I'm setting a bright blue. The value 0x1280FF is a single number holding the red, green, and blue channels all together. Let's break it down. The 0x prefix means that this is hexidecimal number, base 16. Where the letter a means 10, b means 11, on up to f meaning 15. If you don't know about hexidecimals, see the additional course materials for a link to more information. 3 channels, r, g, and b, are each encoded in order. Trading each 2 digits as the channel's value. For example, the red channel is one two in hexadecimal. This means 1, 16 plus 2, to give 18 in decimal value. Green is computed in a similar fashion. 8 times 16 plus 0 gives the level of 128. Blue is at the maximum, ff, adding up to 255. Each of these values can range from 0 to 255. The range 0 to 255 is used, because most computer monitors have 8-bits of precision for each color channel. 2 to the 8th power is 256. So the range 0 to 255 is the entire range of possible color channel values. So, once you have a color you like, in Three.js, you can conveniently just paste it into place. In fact there's a way of initializing the material to color it when you create it. When you put something in braces like this, you're initializing that variable in the object to the given value. This particular hexidecimal value gives a yellow color. Note that the hexidecimal letters, such as the f's here, can be upper or lower case. It doesn't matter. I tend to use upper case as it makes it more readable. In the additive color system, red is 1, 0, 0, green is 0 ,1 ,0, and blue is 0, 0, 1. Using additive colors, what color is magenta? Magenta is red and blue added together, so the answer is 1, 0, 1. This question is not graded. I'm just interested in what you think of when you hear this color's name. Ready? Chartreuse. What color does that word make you think of? Don't worry about your answer. I got this one wrong too. I probably should have answered I don't know, which is a correct answer. But I thought I did know. While I don't know would have been the correct answer for me a few years ago, a more useful answer, and the correct one, is a yellow-green color. Chartreuse is named after a liqueur in France. I have a theory that women know this color more often than men, since it's a fabric color used in dresses. Me, I thought it was a red-orange color. Crayola thought it was a yellow color for a while and revised it later. Here is a visualization of the RGB color space. The axis to the left shows the red channel increasing. The vertical axis is green. And the channel to the right is blue. We can't see the interior of the cube showing intermediate values. All the values we can see in this particular have one or more channels at the maximum. Here's a view inside the cube with the red and blue axes swapped. Next, this direction doesn't really matter. The point inside the cube shows how sky blue is formed by the RGB values of 1 5th red, 3 5th green, and 4 5th blue. Another way to visualize the RGB color space, is to take a slice of the cube at these three corners. Each corner represents a single color channel at full intensity, with the other two channels off. For example this vertex is where green is set to one and red and blue are 0. By slicing a plane through these three corners, we'll be able to see where red plus green plus blue equal 1. This slice gives another view of how these colors blend. This is what the next exercise is about. Up to this point a vertex has been defined by just a 3D coordinate, in fact you can attach a large amount of data to each vertex. Data attached in this way is called an attribute in WebGL. To visualize a slice through the color cube we'll attach a different color to each vertex. To start you'll add a triangle to the scene. To then add a color per vertex use the following kinds of code after defining the triangle's face. This last lines adds the RGB color attribute by specifying a color at each vertex of the face. Your task in this exercise is to make a triangle with three vertices at these locations and with these colors attached. Instead of these colors in the example code, add a full red color to the first vertex, a full green to the second, full blue to the third. The GPU will interpolate the colors across the surface, giving a blend of these colors in the triangle's interior. To interpolate means to find a value in between two or more other values. In this case the original data are the three corner colors, red, green and blue. For example, this point on the polygon is closest to the green so it gets more of that color, less of the red and less yet still of the blue. When you're done with this exercise the result should look like this. The code itself is pretty straightforward. We set the vertex positions then make a triangle face. Nothing fancy there. We then create the three colors red, green and blue. Finally an array of these three colors is assigned to the face itself. One thing worth noting is that these vertex colors are actually multiplied by the materials color when displayed. Since the material color is by default a solid white it doesn't have a noticeable effect in this particular case. When we interpolate the red, green, and blue colors across a triangle, we get this result. It's important to understand how the triangle rasterization part of the pipeline actually interpolates across the triangle. Essentially, whatever is put at one vertex fades off across the triangle. For example, say, instead of putting rgb, we put green at the top corner, and black at the other two corners. You can see in this case that the green is the brightest at the corner, and then fades as we approach the other two corners. If you think of green as represented by one, and black as zero, we linearly interpolate across the triangle and fade from one to zero. This works for the other two corners in a similar fashion, with blue fading to black as shown here. For comparison here's the red component. Since light is additive, the GPU uses all three of these components to display the triangle. The GPU will interpolate any data put at its vertices in this way, effect we'll use later for material shading. The color slice in the previous exercise gives a sense of the limits of what colors can be displayed on your monitor. This is a different kind of color space. We won't get heavily into this diagram, which shows what is called the CIE XYZ color space. But I want to point out a few key bits. Along this curved edge are the pure spectral colors. The values here are the wavelengths of light that produce various colors. You can in fact, see the colors of the rainbow as you move along it. Red, orange, yellow, green, blue and so on. You might also notice a funny artifact along here and here. I find these lines will show up more strongly if I look at my computer screen from a different angle. These bands which are called Mach Bands are something you might run into every now and then in computer graphics. It turns out that the pure spectral colors on this outer rim are not something a monitor can actually display. The pixels in a monitor each have a red, green and blue component, and none of these components is a pure spectral color but rather a mix of differing wavelengths of light. What this means is that our monitors have limits on the colors that they can display. What is happening outside of these dashed lines is that the illustrator is just faking it, by picking the best possible color and repeating it on out to the edge of the curve. What the diagram of displayable colors looks like is this. This triangle is called the device's color gamma. The vertices are the same, RGB vertices we used to form our own color triangle. And there's a slice out of the color cue. This whole area reminds me of seeing advertisements for TVs on television. The announcer would talk about how their new TV's image was brighter, sharper and with richer colors. On the screen they chose some bright, crisp video. But of course I was seeing this video with my old TV, so there wasn't much point in them showing me anything in the advertisement at all. The key take-away in all this is that the RGB color's base is directly connected to the monitor. And that monitors have limits as to what colors they can display. The idea of a color gamut is also important in printing. For example, for high-end printing, custom inks can be used to increase the extent of the gamut. We now know a lot about color, but nothing yet about materials. Say, we have a red ball. How do we make the GPU compute how the lights in the scene and the ball's material interact, so that the ball looks real. When performing interactive rendering, the standard way to think about a material's appearance is as a few different components added together, emissive, ambient, diffuse, and specular. The idea of the emissive term is that it's for coloring glowing objects. For example, a lightbulb has its own color and other light sources don't really affect it. In reality, the emissive term is simply a way to add in some constant color regardless of the lighting conditions. The second component, ambient, is a fudge factor. Something added in so objects look better especially in areas that are not directly lit. You'll see it in various systems controlled in various ways. When all is said and done, the ambient term helps compute a constant color value that is added to the fragment's final color value. The emissive and ambient terms are kept separate, as they fulfill different functions, and often have different controls. For example, three.js allows you to set an ambient light that affects all materials with an ambient term. Remember that we made a simplification with lights in a scene. We assume photons come only from lights, and don't bounce around. In reality, photons would reflect from the floor to the bottom of the sphere to our eye. However, with our assumption, the lower part of the sphere doesn't get any light and so would look black. The ambient term gives us at least some color in these types of areas instead of being entirely black. The diffuse and specular components are computed based on the lights in the scene. While the emissive and ambient are essentially independent of these, the diffuse term can be though of as a flat, matte finish, and specular can be thought of as the shininess of an object. In terms of computation, the specular term is also affected by the viewers location. While the diffuse term is determined only by each lights location. Let's sum this up. The surface color is then computed with the emissive component, plus the ambient component, those are both solid colors. And then for each light, there's a diffuse component which depends on the light's direction and the specular component which depends on the light's direction and the view direction. If you haven't seen it before, this Greek letter sigma is the summation sign. It means to sum up all the diffuse and specular contributions for each light. Putting math in any lecture is often considered the kiss of death. The good news is that you can mostly ignore this equation here. It's just another way of showing all this previous text. I put this math notation here mostly because you'll run across this summation term a lot when looking at material reflection models. The emissive and ambient terms are pretty simple. So our focus for the rest of this unit will be in looking at the diffuse and the specular terms in depth. Let's flip this around, keeping the light fixed and showing the effect on the sphere. In fact, throughout this unit, let's assume each light is extremely far away like the sun. So, we can just look at its direction instead of its position. That way, we don't have to think about the light's direction changing as the surface changes position. Here, the sphere's normal changes with respect to the light. The light's effect is at a maximum at the top of the sphere, goes to 0 at the equator, and stays that way since the bottom of the sphere faces away from the light. This is what the diffused term looks like when the sphere it is lit from above. This next point is a little subtle but I think it's worth going through. A diffused material is defined as one that scatters light in all directions above the plane according to this cosine law. So, this green shows how the amount of light gathered actually drops off as the angle gets steeper. That is, the amount of light from a single point on a surface drops off with the cosine of the angle, the surface compared to the eye. So, you might think less light would hit the eye when the surface is viewed from a shallow angle. However, a balancing factor is that, as the angle of the surface to the viewer increases, the area of the surface visible through the pixel proportionally increases. That is, you see more of the surface even though less light per unit area is reflected towards the eye. These offsetting effects give the net result we want. The viewers location doesn't matter at all. As the shade of the surface depends only on the angle to the light itself. Not needing the eye direction makes the diffuse term faster to compute, which is always important at an interactive rendering. To shade a diffuse surface, we need the cosine of the angle between the direction to the light and the surface's normal. We can perform a vector operation called the dot product to directly compute this cosine. First, you must normalize the surface normal and the vector to the light. Normalizing means rescaling a vector so that it has a length of 1. Normalized vectors are the norm in reflection models. As we'll see, if a dot product of two normalized vectors gives a value between negative 1 and 1, which will prove useful in computing the effect of lighting. For example, say, I have the vector 3, negative 4, 0. To normalize it, I find the length of the vector. This is simply the Pythagorean theorem. Take each component of the vector and square it. 3 squared is 9, negative 4 squared is 16, and 0 squared is 0. Take the square root of 25 and you get the length of the vector, 5. It's lucky that turned out so easily. By dividing the vector by its length, you get to normalize vector. So, 3, negative 4, 0 normalized is 0.6, negative 0.8, 0. Looking at this vector, it goes the same direction, but only travels one unit. Note that normalizing a vector that's already normalized leaves it unchanged. Try normalizing this vector again and you'll find the length is 1. Dividing the vector by a length of 1, of course, does nothing. It's very handy to normalize vectors in this way. Usually, we store the surface normals as normalized vectors. We'll see exactly why in the next lesson. For now, a quick quiz. What is the length of this vector? The answer is the square root of the sum of the squares. So, 6 squared, negative 15 squared and 10 squared equals 361 and the square root of that is 19. So, the answer is 19. Given two normalized vectors, the cosine of the angle between them is computed by the dot product operation. So this is the dot, it's a big dot, and these are denoted as vectors by putting arrows over them. The dot product is equivalent to multiplying the x value of A times the x value of B and so on with y and z and then adding these up together. Say I have these two normalized vectors, and I want to find the cosine of the angle between them. The dot product is then 0 times 0.8 and so on, next to values multiplied and so on. This comes out to be these three terms added together, which equals 0.6. So this is the cosine of the angle between these two vectors. Out of curiousity, if we take the Arc cosine of this value, 0.6, we get 53.13 degrees. However, we don't really care about the angle, we truly want the cosine. As this is how much we'll scale the light's contrubution to the surface. The dot product is a little mysterious, and is so fundamental to a large part of computer graphics, that it's worth buliding up some understanding of what it really means. Why does this operation have any connection with cosine? So, here's our x-y coordinate system and let's just forget about z for now. Let's start with a normalized vector 1,0 pointing along the x-axis. If I draw a normalized vector here at 0.8,0.6, the dot product between these two vectors is simply the x component, 0.8, with the vector 0.707,0.707, a 45-degree angle here. The dot product is again, the x component, 0.707, with the vector of 0.6,0.8, the x component is 6, and that's the dot product. It's always the x component. Continuing on with the vector 0,1, the dot product between this vector and this vector is 0. Vectors at right angles to each other always have a dot product of 0. If you work your way around with more vectors, you'll form a circle. As you may recall from Geometry, the cosine is often defined as the distance in x of a point on the circle. So, as this vector angle increases, the cosine of this angle is computed by taking the dot product. I've known this fact for 30 years, but it's still slightly magical to me that taking a dot product of two normalized vectors, which just multiplies and adds them together, gives you a trigonometric function. If you're still not convinced, what you can do is plot out the various angles and what dot product you get out of them. And you'll see, you actually get a cosine curve. So, here are two normalized vectors. When you take the dot product geometrically, what is happening is that one normalized vector is being projected onto the other. That is, the dot product shows how far one vector extends with respect to some given vector. So, for 0.6,0.8, the vector extends 0.6 along our initial vector. Another way to say this is that one vector projects onto the other vector to give this distance. This works both ways. Our initial vector can also be projected on to the second vector and get you the same length, 0.6. In other words, the dot product operation is commutative. A dot B gives the same answer as B dot A. Projecting one way gives the same answer as projecting the other. That's it in a nut shell. The dot product projects one vector onto another vector. If both vectors are normalized, the dot product gives the cosine between them. We'll see in the next unit on transforms, what happens when one or both vectors are not normalized. For computing the shade of the material, however, we have the tool that we need, so let's use it. In this exercise, you'll set up a diffused material. The sphere on this scene starts with a solid color, unaffected by the lighting. To give this object a diffused material, the first step is to change the material from three.js's basic material to Lambert. The Lambert material has a few separate color components. The equation that the Lambert material uses is, the color is equal to ambient plus the material color times the sum of the normal dotted with each light source. We're going to ignore the emissive component, since in practice, that component is rarely used. In the exercise, you'll see code like this. This code sets up the solid color. By switching this material to Lambert material, the color set here is for the diffuse component. You could also explicitly set this color after creation with the following code, as usual. If you want the value of one channel of the color, you can access it like this, for example. This first term of the equation is the materials ambient color setting. The ambient color is called, wait for it, ambient. The ambient color is multiplied by the ambience light color, which I've set to white. Your task is to set this ambient color to be the material color times 0.4. At the start of the exercise, you'll see this, a flat shaded sphere. When you're done, you should see something like this. This exercise is pretty straightforward. You change the material, you set two colors and you should get this result. Notice that when you rotate around the finished scene, the effective lighting on the sphere doesn't change as the view changes. The other thing you'll notice is that it looks pretty bad. The sphere is not smooth. We see the facets, the underlying triangles. This is truly how a sphere made out of triangles would look in the real world. How do we make it look good in our virtual world? To have a tessellated surface made of triangles look smooth, we introduce the idea of a shading normal. For a tessellated sphere the geometric normal's for the triangles look like this. Each triangle has geometric normal's that are all the same along its surface. In other words the triangle's flat. So you get these discontinuities at the vertices. Instead of using geometric normals, we can store a shading normal at each vertex. These shading normals are created by the application in some way. Sometimes you can get this normal from the underlying surface. A sphere is a good example, as the normal at a point is always the direction from the sphere's center. However shading normal is entirely made up. You can set it however you want. In fact, there are different ways of averaging the geometric normals of a triangle that share a vertex. None of these ways is perfect, but each has its own strengths and weaknesses, so there's no right answer. Shading normals are different from geometric normals. If you're given a triangle, you can always get its geometric normal. That's a property that's derived from the triangle's points. The first step we did to use a diffuse material was to change the Mesh Basic Material to Mesh Lambert Material. The next step is to simply not use flat shading. I admit it, I put the flat shading into the material so that you'd have to remove it in this exercise. The reason I did this was so I could explain shading normals to you. So, to make things look smooth, just take out this shading argument, shading, three, flat shading. When you turn off flat shading, smooth shading is used by default. The resulting object now uses a smooth shading normal stored at each vertex to compute the effect of lighting and interpolate the result. This is the usual way to use this lambursion diffuse material. However, now you know what's going on behind the scenes, with the application supplying tessellated geometry and smooth shading normals. Now that you understand the components of the diffuse reflection model give them a try. In the demo that follows we'll use the terms KA, and KD in the reflection equation. These are simply numbers used to scale the effect of the ambient and diffuse contributions up and down. KA is the greyscale level of the ambient light and KD is handy way to modify the diffuse contribution. To sum up the equation we use for this demo it's fragment color equals KA times material plus KD times material times N.L. This is actually a bit simplified from what 3JS allows. In 3JS, you can inset an entirely different ambient color, though to be honest, this is a peculiar thing to do. Also, in this demo, you'll a see different color space used. Instead of RGB, it's hue, saturation and lightness. HSL for short. Hue means the color essentially. Saturation is how much the hue is used. Lightness is a scale factor, letting you easily make the color itself brighter or darker. Here's a visualization of the color space. The 3JS library lets you use HSL to set colors. For example, this code gives a mustard color. So go try out the demo. Vary the ambiant and diffuse contributions. Change the material color. Move the view around and see the effect. It's worth your while to catch your breath and play around a bit so that you truly understand this reflection model. One method of saving a GPU computations is to bake the lighting into the surfaces themselves. Say I have a scene with a bunch of a defused subjects, each with its own material color and with the surface normals stored at the vertices. If i can compute the shade of the object once and store this color instead of the normal. I can save on lighting calculations each frame. As before assume every light is very far away, but that it can change direction, just as the sun's angle to the ground changes direction over time. The vertex shader normally takes as input the position of the object, in other words, the vertex's location. And it also takes in the normal and lights that want to be applied to the surface. The output is the screen position and an RGB color. Instead of storing the surface normal, under what exact conditions could I safely compute the lighting once, and store an RGB color at each vertex? Your first choice is, if the object orientation does not change. By object orientation I mean whether the object is rotating. If the eye position and lights' directions do not change is your second choice. If the lights' directions and object orientation do not change. And finally if the eye position and object orientation do not change. So, which of these answers is the one that allows you to precompute an RGB per vertex and use that, instead of having to do lighting calculations each frame? There are three elements that can change in the scene. The light's directions, the eyes position, and the objects orientaion. In the first answer, the object does not move. This means the objects normals are the same each frame. The lights can still move. If the lights move, the shade of the object will change, so we can rule out this answer. The second answer locks down the eye position and the light's directions. Now the lights cannot move, but the object could rotate in space. If it does, the directions of its normals compared to each light will change, which changes the diffuse calculation. So this answer won't work. The third answer locks down the lights directions, and the object's orientation. The eye position is not locked down, and so can move and change the direction it is looking in. However, the two elements that affect the diffuse lighting equation are the lights directions, and the objects normals. Eye direction is unimportant, as the diffuse material has the same shade from any direction it's viewed. So this answer is correct. The fourth answer keeps the objects orientation fixed, but like the first answer, the lights can change directions. Surface shading can change. So this answer is incorrect. There are quite a few materials that are nearly diffuse reflectors such as rough wood, newspaper, concrete, and mouse pad. However, a considerable number of surfaces are shiny or glossy. We call these specular materials. Examples include polish metals, plastics, polish wood, glass, glazed ceramics, and enamel paint. These materials look different when you view them different angles, so we need to take into account the direction from the surface to the eye. One standard way of simulating specular materials is called the Blinn-Phong Reflection Model named after its inventors, Jim Blinn and Bui Tuong Phong. The full model has a number of terms in it for self-shadowing and for a shininess factor called the Fresnel coefficent. But the simplest and most common form is this. Specular equals the maximum of N dot H or 0, whichever is larger, raised to a power. N is the surface normal, same as with diffuse material. H is called the half angle vector. Say, you're given a surface, a light source direction, and a viewer direction. How would you point a mirror so that the light reflected directly toward the viewer? The answer is the half angle vector, which is the vector halfway between these two directions, so that these two angles are equal. If the surface normal and the half angle are identical, then the surface is perfectly aligned to reflect light to the eye. So, N dot H would be 1, and all light is reflective. As the normal and the half angle diversion direction, N dot H becomes smaller. Once the angle between these vectors is 90 degrees, the contribution goes to 0. The maximum function here limits the inputs so that this value is never negative. We want to avoid it being negative because we're about to raise it to a power. The S factor here is the shininess or specular power, and has a range from 1 to infinity, though anything above 100 is not too much different. When you raise a fraction to a power, the result is smaller, and smaller still the higher the power. For example, 0.5 squared is 0.25, cubed, is 0.125, and so on. By raising this term to a higher power, the object appears shinier. We can see this in the graph of N dot H versus the specular intensity. As the cosine power rises, the slope becomes tighter and tighter and gets sharper. What the half angle represents is the distribution of microfacets on a surface. A microfacet is a way of thinking how a material reflects light. For example, a fairly smooth surface may look like this. Light coming in from one direction will bounce off the surface mostly in the reflection direction. A rougher surface will a lower shininess has a distribution of facets more like this and the light will still go in the reflection direction, but with a much wider dispersal. At this point, it's best for you to try out the specular power function and see how it responds. An example program that follows, you control the ambient, diffuse, and specular contributions. Try playing with the shininess and other controls to see their effects. When we compute lighting at vertices and interpolate like this, it's called Gouraud shading. Named after Henri Gouraud. Unfortunately our eyes are very good at picking out places where the rate of shading changes. Our visual system will even perceive creases. Which are called Mach bands. To avoid this problem, we'd like to compute the lighting more frequently. One way to do this is to increase the number of triangles making up the surface. In the last demo, you can see that increasing tessellation can make lighting computations look better. However, if you turn up the shininess, you can still pick up problems. Also, along the edges of the sphere the density of the triangles is much higher than the middle. If we keep increasing the sphere's tessellation, we'll be making a huge number of triangles along the fringe, which will cost us computation time without adding much to the final image. Ideally, what we'd like to evaluate is the reflection model each pixel. This is an entirely fine solution. Instead of interpolating the RGB values that result from computing the effects of lighting, we interpolate anything we need in the reflection model. In this case all we need to interpolate is the shading normal. What this means on the GPU is that we have the vertex shader, we input the position in normal as before. For output, we get the screen position and instead of an RGB computed from the reflection equation, we put transformed normal on the output. These two normals are the vertex normals, and they're interpolated per pixel. This interpolated normal is input to the fragment shader, which then evaluates the reflection model and computes the RGB to be displayed. I should mention one important point with normal interpolation. The interpolated normals will not be normalized. That is, their length will not all be one. As you can see in this drawing, the interpolated normals are always a bit shorter when interpolation occurs. So, each normal should be normalized before using it in the lighting computation. When we interpolate normals and evaluate the reflection model at each pixel, this is called per pixel shading or phong shading. Confusingly, phong invented two cool things. One is the phong reflection model, which gives us a spectral highlight, shiny spot. The other is this phong shading, where we interpolate data across the triangle and compute the color per pixel. This is compared with Gouraud shading, where the color is computed at each vertex and is interpolated between those two. Try running the new demo and see how phong shading vastly improves image quality. There's a cost, of course, and that then the lighting calculations need to be performed much more frequently. But this is often what you need if you want to have a good specular highlight. Here's a question for you. Say, it takes roughly the same amount of time to compute the lighting at a vertex as it does per pixel. You're trying to get some idea of how much more expensive it is to draw objects using the material evaluation, using fragment shading instead of vertex shading. In other words, Phong shading instead of Gouraud shading. Say your object and application has the following characteristics. Each vertex on the mesh is squared by an average of 5 triangles. That is, any vertex where we compute the materials color can be shared among 5 triangles. On average, each triangle covers 60 pixels. Don't worry about visibility. Assume the pixel shader is evaluated for all of these pixels in a triangle. So, the question to you is how many reflection evaluations are needed for, per pixel versus per vertex shading? In other words Phong versus Gouraud Shading. In this question, the number of triangles doesn't matter. Since the number of vertices lit and the number of pixels lit varies directly with the number of triangles. But it's a bit confusing to just think about five triangles. So I like to pick a big nice round number. Let's say 100 triangles. 100 triangles times 3 vertices per triangle gives 300 vertices. We know that each vertex's material calculation will be shared by 5 triangles. So that gives 300 vertices divided by 5 triangles that are sharing a vertex. And that gives 60 evals. In other words, 60 material calculations that are needed. For the fragment shader, where we're shading per pixel, there's no sharing. So 100 triangles times 60 pixels per triangle gives 6,000 evals. In other words, 6,000 material calculations we need to do. 6,000 pixel evaluations versus 60 vertex evaluations gives a ratio of 100 to 1. As you can see it can be considerably more costly to evaluate per pixel than per vertex. Note that this doesn't mean that the program itself runs 100 times slower. There are mitigating factors and other parts of the pipeline can be the bottleneck. Which changes this ratio on practice. Up to this point, we've been working with opaque materials. Where light hits the surface, and bounces off. For a number of reasons, transparent objects are more challenging. First, light refracts, and so changes its direction when it hits a transparent surface. For example, here light comes from the surface of the pencil, travels through water, and hits the glass. The glass causes the light to go in a different direction. Such the different light rays reach our eyes and cause the pencil to look bent. This change in direction can also affect the distribution of light on a surface. This crystal ball acts similarly to a lens, focusing the light and creating various bright spots below it. In computer graphics, we call these bright spots caustics. The color and the intensity of the light can be filtered by the transparent object. The farther a ray of light travels through an object, the more light that can be absorbed. Even thin filters can change the color of the light. All of these effects can be simulated or at least approximated in a convincing way. Refraction where the light changes direction is expensive to do correctly. I'll be discussing it further in later lessons. It's also possible to computer the amount of light absorbed due to traveling through an object, though this can be costly. In the following lessons, I'm going to focus on absorption of light by thin transparent surfaces, such as these eyeglass filters. To simplify further still, I'll be talking about only the effect of such filters on objects visible through them. In other words, I won't be talking about how light passes through such a filter, reaches another surface, and then travels to the eye. I'll only be talking about light that travels from the object through the filter. This all sounds like a lot of simplification, but you can get wonderful effects from just this form of filtering alone. For example, this fantastic demo made by Alexander Roddick shows how transparency can be used to effect. There are other cool effects going on here, such as the God rays of light. But transparency is what really makes this demo shine. It's 1 of my favorite Web GL programs. . Once simple way to think of transparency is that you want to see a bit of both objects. So, say, we have these two objects on the screen and, so, opaque sphere and this red filter on top of it. This is a blow up of the pixels and this shows the sphere itself being rendered. So, if we want to add this red filter, what we could do is fill-in every other pixel with red to make it look semi-transparent. The idea here is that pixels are tiny. So ,the eye will blend the two colors together to get a magenta when the objects overlap like this. This is a very limited form of transparency, however. It only supports two objects and only a 50/50 mix looks good. This algorithm is called Screen-Door Transparency because you're treating the pixels as like they're a screen door, and you're filling in every other pixel with a semi-transparent object. What would be more general is to have the transparent filter's color affect the color behind it. In other words, we want to blend the two colors. In blending in computer graphics, the original color is called the "destination" and the color of the transparent object we're looking through is called the "source" color. We'd also like to control how much can be seen through this filter. In computer graphics this control value is called "alpha". Here it represents our filter's opacity. When alpha is one, the filter is fully opaque and does not let any light through. As this value decreases, more of the destination color becomes visible. Here I'm just showing how the red is affected; the blue would show through. When alpha is zero, our filter is entirely transparent and has no effect. A typical way to blend these colors is by using this equation. Here, the color of the source of our filter is multiplied by the source's alpha. This gives how much the source will affect the final color. The larger alpha is, the more the source color affects the final color. To counterbalance this, this same alpha is subtracted from one, and this value determines how much the destination color influences the result. This type of blend is called an "over" operator, as it gives the effect of putting one color over another. The alpha of the source, our transparent filter, can be any value from zero to one. If the value is zero, this equation simply becomes "C" is equal to the destination color, since the source filter is entirely transparent. If the value of alpha is one, the source is fully opaque and hides what ever it's being placed over. The equation simplifies as expected, with the final color being the same as the source color. The alpha value blends between these two colors. Another way to say it is that the equation performs linear interpolation. As one color's influence increases, the other color's effect drops off in proportion. This idea of having a control value, in this case, alpha, that varies between zero and one is commonly used in many areas of computer graphics. Expect to see this sort of equation elsewhere as you explore the field. The over operator is useful for transparency. However there are many other blend modes available on WebGL. For example, one blend mode is called "add" as it adds the source and destination colors together. In three.js the blend mode used is specified with the material's blending parameter. Here we see the over equation in action. By varying the alpha, we fade the effect of the transparent object on and off. Try it yourself. I'd like to have you give the over operator a try now. So you have a reddish filter of 0.9, 0.2, 0.1, red, green, blue, and a yellowish background 0.9, 0.9, 0.4. You set the filter to have an alpha of 0.7, fairly opaque. What is the resulting color when you use the over operator? To solve this equation, we substitute the source color into here and the background color into here. And we multiply by the alpha in both cases. We get these intermediate results. And when we add them together, we get 0.90, 0.41, 0.19. The demo that follows, shows the result of this particular exercise. Try varying the alpha and the colors to see the effect. The over operator shows how we can get a transparent effect by blending 2 objects. However, there's one little catch. How does the Z buffer work when you're using transparent objects. Think about what the Z buffer is dong. It stores the depth of the object that is closest to the eye. Say, we draw the blue far object first, then draw and blend in the red transparent object. This works fine. The blue object's color is taken and blended with the red fragment that is drawn. However, if we reverse the draw order, we start to get into problems. We get the red fragment and want to blend it. But we don't even have the blue object in the Z buffer yet. Even if we decide to wait for the blue object, we get into trouble. When the blue fragment renders, it is considered hidden by the red filter, since its farther away from the eye. Here are some solutions to the problem just outlined. The drawing the red transparent object first causes the blue object to not be seen by the camera and blended in properly. Which of these solutions will solve this particular case? More than one answer can be correct. First is, draw all objects sorted by size, small to large. Draw all objects sorted back to front order. Draw all objects as if they're all transparent, blending each one. And finally, draw the fully opaque objects first, transparent later. This first answer doesn't really help. Size doesn't matter, it's transparency that's causing the problem. The second answer works. The blue object is further from the eye than the red one, so will be drawn first. However, for more complicated scenes, it's a bit of overkill and inefficient to boot. The sorting order of a set of opaque objects doesn't affect the final result on the screen, that's the z buffer's job to sort out. Drawing a pick objects in a back to front order is not efficient, as fragments will be processed by the fragment shader, then covered up by the next closest object. The third answer is not particularity meaningful, and won't solve anything. The blue object essentially already has an alpha, it's equal to one meaning fully opaque. Even with blending on, an opaque object will still act like an opaque object. This solution does not fix the problem that the blue object does not being rendered early enough. The fourth answer works for this case, where there is only one transparent object. First all the opaque objects are drawn, there's only one, it is then guaranteed that the transparent object will be able to blend with the opaque objects as needed. Say we now add a transparent green object to our scene. To avoid our original rendering problem, the rule is that all opaque objects are drawn first. So the blue object is rendered. If we now render the green object, and then, the red object in that order. The green object will block the red one. We want the blue object to be affected by both the green and red filters. But the red filter is rendered too late and has no effect. Just to make life exciting, say we add yet another filter, a yellow one behind the blue object. This filter is not visible for the pixel shown. But whatever solution we choose must make sure this object is taken into account properly. My question to you is, which is the best solution? The one that will give us the result of red blend to blue. And then green blended over that. Draw all the transparent objects, sorted by their alpha values, largest to smallest. Using the painter's algorithm, with the z-buffer off. Sorted in back to front order. Or based on intensity, with the brightest objects drawn first. Ideally, we want to draw the blue object first, then the red, then the green. The yellow filter is behind the blue object and should have no effect. For this first answer, since we aren't given the alpha values of the green and red filters, we don't know how this affects our draw order. The red or green object may draw first. We can also rule out this last answer for the same reason. We don't know the intensity or even how intensity is defined so we can't use this as a way to solve the problem. The second answer works well for the green and red filter, but falls apart on the yellow filter. With the z-buffer off, the yellow filter would be drawn on top of the blue object filtering it. This is clearly incorrect. This leaves the third answer, which is correct. This answer is similar to the second answer and that the Painter's algorithm also sorts objects from back to front. However, this time, with the z-buffer on, the yellow filter is properly hidden by the blue object. The red filter is rendered next and blended, then the green is added in on top. We've derived one system of transforming transparency that mostly works. To sum up all the opaque objects are rendered first, with Z buffering on, blending is then turned on for the transparent objects that follow. Blending takes extra time for the GPU to compute, so it's turned on only when needed. Finally, the transparents objects are sorted by their destins along the view, and are rendered in back to front order. If the camera or objects are moving, this sorting has to be done in every frame. This algorithm is in fact with three j s implements, you make an object transparent by setting its material. There are two parameters to set. The opacity is the f of value used for blending. You must also set the transparent parameter to be true. This transparency scheme usually works fairly well in practice but there are still quite a few problem cases. One problem is inter penetration. Here's the demo showing one transparent block moving through the other, give this demo a try. Move the camera around and use the slider to position the block. The dot on the center of each object is what is used to measure it's distance along the direction the camera is viewing. Once you're done with the demo, you'll then answer a question as to what's going wrong. Looking at the demo, you should have noticed that when you pull the red box to be closer to you, suddenly you can't see it through the yellow object. You can see the ground playing grid through the red object, but the red object has disappeared. Which of these is true for this picture? More than one answer can be correct. The red object is rendered after the yellow object. Blending has been turned off for the yellow object. The yellow object was not drawn to the Z-buffer. The red object is closer to the eye than the yellow one. Say you look at just two polygons from the side. A red one and a yellow one. Because they interpenetrate. The draw order will always be wrong for one pixel or another. If the yellow object is drawn first, this upper pixel will have the wrong order of yellow, then red. If the red object is drawn first, the lower pixel has the same problem. Red than yellow. For the first answer, with the yellow object being rendered first, there's no red object for it to blend with. So this answer is correct. Blending has not been turned off for the yellow object since you can see the grid lines through it. So this answer is wrong. The yellow object blocks the red object at the top so it must have been drawn first and written to the z-buffer. So this answer is also wrong. The fourth answer is correct. The transparency algorithm states that the more distant object, the yellow one, is drawn first. Another way to think about it is that you can have red over yellow, or yellow over red, but not both at the same time. There are even more problems with simply sorting objects and drawing them roughly back to front. For example, notice that the back faces are not shown for any transparent objects. If you turn back faces on, you'll start to get strange rendering art effects. Some back faces will be drawn before front faces blend with them, some won't, giving this patchy look. For example, here's a view of a block from the side, along with an order that the phases are drawn. Since phase 1 is drawn first, phase 2 blends with it. Phase 3 is drawn filling the z-buffer and also blends with phase 1. Phase 4 is behind phase 3, so is invisible when rendered. Of the two back faces, in this case only phase 1 has had an effect on the image generated. Even with back face calling on, and a good sort order, you can still have problems with different sized objects. Say a ground plane and a red object resting in it are both transparent. The middle of the ground plane is closer to the camera. So by sort order, it will be drawn after the red object. This will make the ground plane appear to be above the red object. Here's one more problem with transparency. Complex objects can have two or more surfaces overlapping a single pixel. Again it depends on draw order as to what appears on the screen. If the fragment on the left is drawn first, then the fragment on the right will en-properly blend with it at this pixel. There's even research about the ordering of triangles in the mesh itself to avoid these problems. But this type of technique can be costly to apply. At this point you might have given up all hope for transparency always working right. The z-buffer was meant to store only one object at a time and unless you can guarantee the order of objects covering the pixels, there's little else you can do. There are some further tricks you can do by only reading and not writing to the z-buffer, but there's no perfect solution. If you desperately want to get the right answer, there is one technique that works fairly well called Depth-Peeling. The idea is to peel away each transparent layer until all layers are processed, by storing an additional minimum z-depth for each pixel. So for example, the first layer is all the transparent surfaces closes to the camera. The second layer is all the second closes surfaces. The third and fourth layers are processed and turned until the whole object is rendered. That's conceptually the key idea, see the additional course materials for more about this algorithm. The draw back is that each peel operation needs to have all the transparent objects rendered again. Many passes may be needed before all the layers are found and processed. Ultimately what would solve the transparency problem is storing a list of all the transparent fragments in each pixel, along with their depths. This approach is known as the A-Buffer. Once we have all this information, we can then combine these fragments in the right order. This type of storage is possible in newer GPU's for the desktop, but can use a considerable amount of memory. What's interesting is that mobile devices usually use a different architecture called Tile Based that can more easily keep such fragment lists around. Yet another approach is called Stochastic Transparency. Which uses Screen-Door Transparency width in the pixel itself, along with some randomness to get it a reasonable average result. This brings us full circle back to the original transparency algorithm I presented. There's no simple answer on the GPU. In the long term this whole transparency mess may become a thing of the past as architectures evolve. For example, ray tracing is an algorithm that solves transparency issues directly. Be ware that currently transparency is not a solved problem than most GPUs, and are simple solutions and have some serious side effects. In the meantime, mellow out and enjoy the jellyfish. Now you know how to give 3D objects a variety of interesting materials and to make them look good. The next step is to learn how to precisely position and change the shapes of the objects themselves. With these two basics in place, material and object transformation, you'll be able to really control your virtual world. What's tri three color conversions between the different RGB representations. Given the color on the left write the color on the right, for the second question just give three decimal places of accuracy rounding off. For this third question when you convert, round off to the nearest integer. For this first question we separate the hexadecimal digits like this, 0 5 is easy, that's 5 in integer. 24 is 2 times 16, which is 32 plus 4, which is then 36 total. The hexadecimal number D zero is 208 as D is decimal 13 and 13 times 16 is 208. To convert to floating point, we divide by 255. Not much divides evenly by this value, so we round off. 127 converts to 0.49803 etc, which we round off to 0.498. 200 divided by 255 is 0.784. 255 divided by 255 is easy. That's 1.0 exactly. For this last problem we start with 0x which shows a hexadecimal number 0.4 times 255 is 102. In hex this is 6 times 16 plus 6. So 66. 0.0 times 255 is 0. So in hex this is just two more zeroes. 0.11 times 255 is 28.05, which we round to 28, which is 1C in hexadecimal. Le'ts change some materials on the drinking bird to make it look a bit more realistic. Change the following materials to use the Mesh Phong Material. The Hat, Body, Leg, and Foot should all use this material. Assign the shininess, and also assign the specular color to be this. Note that the specular color is different than the material color. You should check the online documentation to make sure you're setting the right color. When you're done, the results should definitely look shinier like this. For the answer to this one the code involves setting a few parameters on each material. Here's one typical solution for the hat material changes. Here the shininess gets passed in as an initializer, and the specular color is set seperately. In the previous exercise I modified the drinking bird's geometry itself a bit from the original model. I'm going to let you use your detective skills to figure out what I've changed. Which part has slightly different geometry; the hat, head, the body bulb, or the legs and feet? Once you figure out what part has changed. Think about why I modified it. I won't test you on this. Sadly, the computer can't grade essay questions yet. But there's an interesting reason. By the way, even if you're not doing the exercises, you should be able to look at the exercise code and see what's changed. Here's a zoom in of the origional geometry, and here's a view after my modifications. I changed the legs and feet of the model to have rounded edges. So, instead of a block with perfectly sharp edges, there's a small bevel. This detail looks a bit more realistic, since most plastic parts are not razor sharp. More importantly, a bevel lets the model pick up specular highlights along its edge. Here's sharp edges again. Here's smooth. It's worth keeping in mind that specular highlights don't add all that much to flat surfaces. Which is why I use the teapot for most material demos in this unit. Let's hope the drinking bird look a bit better still by having its body made out of glass. I'll do the geometry improvements, you do the glass material. I've now split the body into two pieces, an exterior shell and an interior blue fluid. On the left, the blue fluid here is without the shell. And on the right is how things look with the shell. Clearly the shell needs to be made well clear in order to see the fluid inside. Your job is to make this material transparent. The fluid's material is called body material in the code. The glass body uses a new material called glass material. The current glass material's shininess is fine as it's set, but otherwise, needs fixing. I want you to set the glass bodies materials so that it's 30% opaque, and of course, transparent. Regular material color, fully black, in other words, 000. And the specular color, fully white, 111. When you have the glass materials set correctly, your solution should look something like this. The solution is this piece of code. Opacity is set to 0.3, which is 30%, and the transparent flag is turned on. The color variable is set to black and the specular set to white. Here's a picture of the teapot with the lid off. You can see the inside of the teapot. What's odd here is that the outside and inside surfaces have the same lighting. This is because the same normals are used to calculate both the inside and outside surfaces. That is, all the normals point outwards. Regardless of whether we're looking at the inside or outside. This is certainly not how the real world works. I call this the paper lantern effect, where the surface is sort of like a paper lantern, looking the same on both sides. However, if you think about it, the shading here is really not like anything in the real world. For example, if we put a light inside this lantern all of the surfaces would be dark. This is because they're normals all point away from the light. Say we want to fix this problem and have the surface be double-sided? Which of the following fixes will work? First if the normal is facing away from the light, don't render diffuse and specular, render the model twice. Once with backface culling on. Once with frontface culling on and flipping each shade normal direction. Make and also render a second set of triangles in the same locations, but with their normals reversed. Do not cull. Put a second light opposite the first light, pointing in the opposite direction. The first answer doesn't solve anything. It's just saying to do what we already pretty much do. Shade the surface only if it points towards the light. The second answer works. You can use a slightly different vertex shader to render an objects geometry a second time, reversing the normal before using it. The third answer doesn't work because culling is off. If you render two meshes in exactly the same location, you will get z fighting. If culling was turned on in the data set properly, this solution would work. Well, this solution doubles the size of the geometry need to store. It does have the advantage of not needing any special treatment. It's just another object to render in the normal way. The fourth answer will make almost all surfaces get some light. This fix will have the effect of properly making the dark inside of the object be lit. However, there are also areas inside that are lit that shouldn't be, and these will continue to be lit. Similarly, the outside of the object would now be lit in both sides, which gets too much light, so this answer is wrong. Say we're using depth peeling or some other advanced transparency technique. We're getting great pictures of transparent objects. Front faces and back faces are both rendered. And in the right order. Life is good. However, we then see another rendering problem when we put a blue glass cube on a wood table. What is this major problem that gives horrible art effects that change from frame to frame as the view changes. Is it that the bottom of the cube has z-fighting with the top of the table. Absorption of the light is not properly accounted for by the GPU. The bottom of the cube is black due to total internal reflection. Or the sides of the cube do not refract the light, causing black areas to occur. I'll give you a hint. It's something we've covered in this course. The problem is that you get z-fighting with the bottom of the cube and the top of the table. The other three answers may or may not be true, but they would not cause serious artifacts that would change with the view. Here's a test of your knowledge of shading and illumination models. We have Phong compared to Gouraud and we have Blinn-Phong compared to Lambertian. Say we have a scene and try out each of these four combinations. Assume that shading costs in execution time far outweigh the illumination costs. In other words, if a fragment shader is used to evaluate illumination. It'll be considerably more expensive than any vertex spaced shading approach. Assign each combination a number, rating them from least to most expensive in cost. In other words, 1 means the smallest amount of time needed, 4 means the largest. This question is something of a test of your vocabulary. Shading refers to per vertex versus per pixel. With Gouraud being per vertex and Phong being per pixel. We're given the per pixel will be considerably more costly than per vertex. So this side of the grid will be more expensive. Illumination refers to the Lambertian diffuse model versus the Blinn Phong specular model. Since the Blinn-Phong model builds on the Lambertian model, adding shininess, it must be more costly. So the top part of this grid will be more expensive. These two facts establish that Gourand and Lambert is the quickest and Phong shading with Blinn-Phong illumination is the most expensive. The tie break between these other two combinations is that per pixel shading is given to be much more costly than vertex shading. This tips the balance so that Gouraud shading and Blinn-Phong illumination is faster than Phong Shading and Lambertian Illumination. In this exercise you'll add a body to the robot this body is created by the create robot body function you'll see it in the code. This body can rotate and has the upper arm attached to its top your job is to add this body to the robots design. When your done your robot arm should look like this you might want to turn off the display of the other two arm parts while you're testing out the body itself. Simply comment out the line of code that adds the arm to the scene. If you really want to get fancy, you can add a y axis control for the body itself. It's pretty easy to do this, just three lines of code. Take a look in the user interface set up gui method and in the render method. This exercise extends the heirarchy of the robot arm by adding the body object so that the arm is attached to its top. Instead of adding the arm to the scene itself, we create the body and then we attach the arm to the top of it. And that's really it. In this exercise you'll finish adding a hand to the original robot. You'll see in the initial code I've created one grabber for the hand by using the create robot grabber function. It starts like this, with just this one piece of the hand. Your goal is to create a hand that has two grabbers that slide along the cylinder and rotate together. I've provided sliders for both of these controls. When you're done, the hand should look like this. The hand z control rotates it as a whole. The hand spread slider opens and closes the hand. Note that for this exercise you'll have to modify both the fill scene method and the render method. The code to make the right grabber is essentially the same as for the left grabber. The slightly tricky bit is what you need to add to the render method. You have to move the right grabber in the negative z direction when the slider is slid. Normally matrix order matters. If I rotate, then translate an object, I'll usually get a different result than if I translate and then rotate. However, there are exceptions. Say I give you a bunch of the same type of transforms, such as four different translation matrices. If I mix up the order of the transforms, like on the second line here, would any object still be transformed in exactly the same way by this new order, or does the ordering matter? So that's your first question. Can I do a series of translations in any order and get the same result? Your second question is, what about a series of rotations? Can I change the order and always get the same result? What about a series of scales, uniform or non-uniform? Does order matter? Finally what about a series of rotations and translations all along the same axis? In other words, rotate around z, translate around z, rotate some more around z, translate some more around z. And so on. Can I change the order of those matrices and get the same result? For all these questions ignore precision errors. Assume you have infinite precision. For this first one, translations represent movements. Just like vectors can be summed together to make a single movement vector, translations can be multiplied together to get a single translation. The order that we add the vectors doesn't matter. Likewise, translation order among translations doesn't matter. So the answer is yes. Rotation order certainly does matter. Reversing the order will normally change the effect. So this second question's answer is no. For scaling, each access scale is independent of the others. If you stretch an object along the y-axis, its x and z coordinates do not change. Because each is independent, there is no danger of the scales interacting with each other. Each axes' scale factors can simply be multiplied together. Multiplication is commutative, so the scales can be done in any order. Yes. For this last question, say you rotate around the y-axis and translate along the same axis. The rotations can be done in any order, since each is just an angle change around the y-axis. The rotation affects the x and z coordinates of any point transformed. Translation affects only the y coordinates. Since these are independent values, rotations and translations along the axis can be evaluated in any order. So the answer here is yes. Say I apply a uniform scale to an object centered at the origin and then translate it. That's case A. I start again and apply the translation first to the object then scale it. That's case B. Assuming that the scale and translation factors actually do something. I'm not scaling by one or moving zero feet or anything like that. Mark any of the following that are always true. First, neither object's center has moved from the origin. The object is the same size in both cases. The object's is in the same position in both cases. The object's center has moved farther in case B than in case A. This first answer is simply false. Both objects are translated by some amount from the origin. The second answer is true. If you scale by the same scaling matrix, the objects are the same size. They may be in different positions, but they'll have the same dimensions. The objects is not in the same position. If you scale up by a million, then move a foot, you've moved a foot for case A. If you move a foot, then scale it up by a million, its origin will essentially move a million feet, for case B. This next one could be true or false. It depends on the scale factor. If the object is growing larger, then it's true, B would move farther than A. If the object is shrinking, A would move farther than B. Before we start, this is important. Save your code away for this exercise. You'll use it as the basis for the next exercise. We don't start the next exercise with the solution for this one. At this point, you should be using a text editor for these exercises. I like Notepad ++. Others like SublimeText, and there's plenty of others out there. By saving code snippets around, you can look these over in future exercises and see what you did. In this exercise you'll make a flower, you'll start with this a stem and stamen of a flower with the single petal part, a comb sort of stuck in the stem. I start you off with this cone which is a child of the petal which is a child of the flower as a whole. Your job is to make 24 instances of this petal and form a flower. You'll need to use rotational [unknown] angles applied in the proper order. And translations. And you'll need to apply these to the cone object and to the petal object which is its parent. Note that all the tips of the cones meet in the middle of the sphere, the stamen. Here's my solution. The loop creates 24 petals. For the cylinder, I positioned it, so that its tip is at the origin. I then know that any rotations I do to the petal will keep this tip there. The trickiest bit is that I first want to rotate the petal, so that it's horizontal. Then, rotate it in increments of 15 degrees around the origin. That's why I use the z-axis for rotation, then the y-axis. Remember that oiler angles are applied in the order z, y, x. After the rotation is done, I then move the petal up to its final position at the flower's height. In this exercise, we're going to try to make the petals look better. Starting with your solution to the previous problem, make two small changes that will make the petals look more realistic. The first is to squish the petals, make them one quarter as tall as they were before. The second is to tilt the petals up by 20 degrees. When you're done you should get something like this, which looks at least a little bit more like a flower. Notice that all of the points of the petals should still be at the center of the stamen, the sphere at the top of the stem. When I first solved this problem, my petals were pointing below the stamen, which isn't what I wanted. I expect that you can do better than I first did. The code changes are pretty small though they took me a bit to get correct. The scale was easy enough. I modified the cylinder's x scale to be 1 quarter of what it was before. The z notation I originally applied to the cylinder, but that wasn't correct. That rotated the cylinder around its own center. This gave an interesting result but wasn't what I was aiming for. The proper rotation was to simply not rotate the petal itself as far along the z axis, rotating 70 degrees instead of 90. In English, the word transform is a verb and means a dramatic change. Such as I saw the cube transform into a unicorn. I won't be using that form, I'll be using it as a noun, where transform is a mathematical term meaning an operation that changes the position, orientation, or size and shape of an object. Transforms are a key part of computer graphics. In these lessons, I'll be explaining how transforms work. You probably didn't realize it, but you've already been using transforms in a few of the previous exercises. When you set the position of an object, you were using a type of transform called a translation. We use transforms whenever we animate anything, objects, lights, or cameras. We're going to spend a fair a bit of time on transforms since they allow us to do so much. The study of transforms for computer graphics is associated with the field of linear algebra. This field is concerned with vector spaces and is usually considered a prerequisite for any computer graphics programming course. If you know a bit about linear algebra, great. Having a deeper and wider understanding of the subject will help. However, basic computer graphics needs just a few small bits of theory to get things going. My goal in these lessons ahead is to teach you the tools you'll need to quickly and effectively be able to control your virtual world. A point is a position and a vector describes a motion. We can combine points and vectors in various useful ways by adding or subtracting their coordinates from one another. To start with, if you subtract the location of point A from point B, you get a vector describing how to get from point A to point B. So B minus A is calculated by taking the coordinates and subtracting them from one another. And this gives you vector v. The reverse also holds true. A plus v equals B. While we can subtract points to get in a vector. Our system doesn't produce anything geometrically meaningful if you add the 2 points together. We can also add and subtract vectors themselves. For example, S plus T equals U. We can also reverse this process. For example, U minus T equals S. And we can go the other way around. For example, U minus S equals T. One way to think about vector subtraction is that you're, instead, adding a vector going the opposite direction, as shown here. The other thing commonly done with vectors and points is to multiply them by a scalar, that is, a number. Multiplying a vector by a scalar is clear enough. You want to describe moving further in a given direction. If you multiply by negative 1, you reverse the vector's direction. Multiply by a different negative number and you'll reverse the direction and change how far to move. Multiplying the point by a number is way to make an object look larger or smaller. This type if operations called scaling, we'll discuss in detail later. Points and vectors don't require any numbers to be associated with them. For example, I can say, define a vector from the Andromeda galaxy to Chicago. That vector now exists, but doesn't have any coordinates until I define a frame of reference of some sort. I could ask, what are the vectors coordinates in terms of the Earth's latitude, longitude, and altitude? That gives one set of coordinates, the one that changes every second as the Earth rotates. Or I could ask, what are the coordinates in terms of the Sun's location and the Earth's path, and get coordinates that change more slowly. Whenever we assign a point or a vector a coordinate value, it's always with respect to some frame of reference we've defined. In 3D computer graphics, we usually talk about the location and movement of objects, lights and cameras, in terms of world space. This frame of reference is usually right handed. You can define units for it if you like, such as 1 unit equals 1 meter. Or it can be unitless, if there's no need for them. Here's a question to get you going with Vector math. In world space coordinates, given p is a point negative 7, negative 4, 14. E is a vector negative 4, 19, 18. And s is a vector negative 5, 6, negative 10. What's q? Where q is defined as p plus e minus s. To solve this one, we plug these values into this equation as shown. When I do vector math, I don't not like double negatives. So I usually fold this minus into the coordinates themselves. Adding up these coordinates, the final answer is negative 6, 9, and 42. The 3GS library has many vector operations available. See the additional course materials for a link to this documentation. My question to you is; For the vector three class, which method changes the vector so that it points in the same direction, but it has a length of one? Here's a hint. This operation was used to make surface normals on the light direction vector each be one unit long. The method's name is normalize. Note that this method changes the vector itself in place. Some libraries will return a separate new normalized vector and leave the original vector alone. Translation is just a way to say move something to a new position. In fact, in 3JS, we've been using translation for positioning parts of the drinking bird model. 3JS has translation and other transforms built into every object. In this code, we're moving the center of the sphere to the position 0, 540, 0, which is a vector, that is any point on the sphere is moved by this vector to a new position. This code does the same thing, only this line has been changed. It's pretty obvious that this y-coordinate can be the sum or product of any set of numbers. My point here is that adding values together like this is equivalent to adding the coordinates of the vectors. So here, a vector of 0, 160, 0 and a vector of 0, 390, 0 are added together to get 0, 540, 0. Translations can be treated somewhat like numbers in this respect. You can add them together in any order and you'll obtain a final translation vector that sums all the translations and tells you the final position. 3 j s also has some built in support for object rotation. The call is like this, you can specify x y or z for the axis of rotation. The object will rotate around it's center along that axis in a counter clock wise fashion. The angle is specified in radians which computers like, as a human I like degrees. So here I'm specifying 70 degrees then converting this number to radians by multiplying by PI over 180. Here we can see the direction is counterclockwise if you were to point the x axis toward you. A way to remember this direction is the right-hand rule. Even though it looks clockwise in this figure, if you put your thumb along the x axis and rotate around you'll see it's indeed following the right-hand rule. In this exercise, you'll start with a program that gives a model like this. Note that the x, y, and z axis are shown given as the colors red, green and blue respectively. Think of this as a clock hand sort of, it points to 12 and 6 o'clock currently, we'll fix that later. Your task is to rotate the hand of the clock so that it points at 2 and 8 o'clock as shown here. To solve this one, the three elements you need to determine are the axis, angle of rotation and the sign. In other words, which direction you're rotating. The green axis is the one we want to rotate around, and so this is the y axis. For the angle of rotation, there are 360 degrees in a circle. We want to rotate 2 12ths of the way around the circle. So that's 60 degrees. We also need to rotate in a negative direction, since the positive direction is counter clockwise. So the sign is negative here. All this information is encapsulated in this line of code, y rotation, negative 60 degrees. There's more than one solution to this problem. Since we can't tell which end is which. We could instead rotate 120 degrees counter clockwise and get the same answer, which would let us put a positive angle. Here this hand has rotated from one position to another. Say you rotated this hand from 0 to 360 degrees, what sort of volume would it have swept through? That is, if you've made a million of these hands, each one at a slightly different angle, what would they form? Would it be A Sphere, A Disc, A Hemisphere or A Doughnut? The answer is a disc shaped volume, a pretty flat cylinder. Whatever axis you rotate around, each point will trace out a circle perpendicular to this axis, with the center at the origin. It's useful to think about the axis of rotation this way. For example, if you have a location and want to rotate it to another location, find the circle that has its center at the origin and contains these two points. The circle's corresponding axis is the one to rotate around. Notice that there are three rotation angels on the three.js object. You can rotate around the x, y, or z axis all together. If you're controlling a plane, the x rotation is known as the pitch angle. Since it pitches the plane up and down, the y angle is known as the yaw or head direction, since that's the direction the plane heads. Z is the roll, how much you're rotating around the axis of the plane itself. In some other fields these angles are called the x roll, y roll and z roll. These angles are called Euler angles when used together in this way. They're named after Leonhard Euler, a Swiss mathematician and physicist who lived in the eighteenth century. He was one of the most prolific mathematicians of all time. If you have ever run across the number e, that's called Euler's number. Euler invented using the sigma sign for summation. Just listing out his accomplishments would take a whole unit at least, but you get the idea. There's a great quote from Euler that sums him up for me. When he lost the use of his right eye, he said, now I will have less distraction. In three.js, the Euler angels are applied to the model in the orders z, y, and x. I should mention that this order can be different in different applications. For example, the order z, x, z is seen in robotics. What happens if you rotate around two or more of these axes? The short answer is that the frame of reference changes for each rotation. This is actually easier to see than to explain. Here I'm changing the x rotation. And you can see that the plane pitches up and down. Now, applying the y rotation, notice that we're not rotating around the world's y-axis. But rather, around a plane that is defined by the x rotation. In other words, the y rotation is applied with respect to the plane's frame of reference, not the world's. In fact, all the rotations are applied with respect to the plane. The initial x rotation just happened to be aligned with the world. The z rotation is also clearly changing with respect to the plane's orientation, rolling it along its axis. Euler angles are handy for flight simulators, robotic applications, and even mobile applications, as they can be used to describe the orientation of the mobile device itself. However, Euler angles can also run into limitations, such as the problem of Gimbal lock. For example, say that I set the angle of rotation for the y-axis to 90 degrees. You can now see that the x rotation and the z rotation have exactly the same effect. We've lost a degree of freedom under these conditions, limiting how we can move away from this orientation. I encourage you to play with the Euler angles demo that follows and get a feel for the strengths and limitations of this method of setting rotations. Definitely try setting the y angle to about 90 degrees to see Gimbal lock in action. To scale something is to make it larger or smaller. This operation is somewhat different than translation and rotation. When we translate or rotate an object, we don't change it's shape or it's volume. These two operations are what is called, rigid body transforms. The name rigid body is just what it says. If you have an object and apply any number of rotations and translations to it you won't change its shape. It's as if the object is made out of something, well, rigid and will keep it's form. Scaling does change an object's overall size, so it's not a rigid body transform. Scaling in 3 JS is simple enough. Just as there's a translation or rotation parameter on the object, there is also a scale parameter. Here's the code to make a balloon object 3 times as large as it was before. Note that unlike real life. This makes the balloon 3 times larger in all ways. The thickness of the skin, the part where you seal it off, and so on. Like rotation, scaling is done with respect to an origin. Since the valve of the balloon is at the origin, the valve will stay still and the balloon will expand upwards. In fact, you can scale an object differently along each of the three axes, x, y, and z. This code is scaling the object along the y axis, but not the other two. A number less than 1 means you're making the object smaller. The scale shown here has the effect of compressing it along the y axis. When you scale a object the same amount in all directions, like we did with the balloon, this is called uniform scaling. If the scale varies it's called non-uniform scaling. Uniform scaling does not change any angles within the model itself. Non uniform scaling can modify angles. In other words, the shape of the model itself is changed. In this exercise, I want you to add another indicator to my watch like object. I've started by putting a magenta sphere in the scene of radius 10. I've positioned it correctly but it's not looking at all like a hand. Your job is to stretch and squish this sphere into something looks kind of like a clock hand but centered at the origin like this other hand. I want this hand to be 60 units long and 4 units wide and high. Use the scale parameter to modify the sphere. Once you've properly scaled this sort of an hour hand, rotate it so it points at 11 and 5 o'clock. When you're done things should look like this. The axis to stretch along is red. Which is the x axis. The one tricky bit is to realize a Sphere of radius ten is actually 20 units across. If we want to go from 20 to 60 units, the scale along the x axis needs to be multiplied by 3. That is, 60 divided by 20 is 3. To go from 20 units to 4 units, the other two axes must be multiplied by 0.2. For the rotation, we do as before, rotating around the y axis. 30 degrees corresponds to 1 hour of movement. Up to this point, we've been mostly ignoring the order of operations. That is, we've been merely performing translations, rotations, and scales, without worrying about whether one needs to be done before the other. After all, we found that translations can be done in any order and these still add up to a single translation. What could go wrong? The answer is, plenty. Order matters when rotations or scales are involved. 3GS uses the following order to apply the transforms on an object, scale, rotate, translate. In the scaling exercise, 3GS first scaled the clock hand made out of a sphere, then rotated it into position. Finally, it placed this hand to be just above the other hand. It doesn't matter what order you set these parameters. 3GS always evaluates them in the order of scale, rotation, and position. This is the default because it is often the easiest way to produce the results you want. If you have something special in mind, you may find it best to transform in a different order. That's possible and, in fact, we will later see how we need to use a different order to make our clock example look really good. For now though, we'll stick with scaling, then rotation, then translation in that order. Let's see what would happen if the order of rotation and scaling were reversed. That is, first we rotate and then we'd scale our sphere. For our clock exercise the result would look like this, the rotation seems to have had no effect. Think about what the order off operations actually does to the object. First we rotate the sphere. This has little effect of what we see on the screen. The sphere looks pretty much the same from any angle. So by doing the rotation first, we've essentially lost its effect. Stretching the sphere afterwards gives us the shape we want but the rotation's already been done. At this point, I want to show you a great program for experimentation. The Three.js Interactive Scene Editor. See the additional course materials for link to this program. To start, let's add a sphere to the scene. It comes in black as there's really no lightning of scene. So let's add a directional light, put it over here. And select our sphere. We can move our sphere around. And you can see the change of position over here. You can also rotate it. And obviously, it doesn't look very different, because it's a sphere. But you can also non uniformly scale it. So here, we've scaled it a bit. And I can rotate it, and you can see the effect. Give this program a try. It's really excellent for figuring out how rotations, scales, and positions work together. Let's build a snowman using rotation and translation. I did the first part, stacking up the snow. Your job is to position the pole through the snowman and give him arms. The center of the pole should be placed 50 units above the ground. When you're done, you should see something like this. The answer is that you rotate the pole to be horizontal by rotating it around the x axis 90 degrees, then move it up 50 units along the y axis. We saw earlier how scaling and then rotating is often the most convenient order. With our snow man the arms were positioned by rotating then translating. Here's the stick without any transform on it. I made the snowman transparent and removed the ground plains so we can see the stick's position. It's actually inside the snowman halfway through the ground. Whenever you create most geometric objects in 3JS, the object is centered around the origin. The rotation along the x axis put the stick into its proper orientation. Then the translation moved it upward in world coordinates to the proper location. Let's see what would happen if we first translated then rotated. First we move the stick upwards 50 units. The center of the stick is now in place in the middle of the body. So far so good. Now if we rotate the stick after positioning it, we suddenly see it lying on the ground. What happened? The problem is that the rotation takes place with respect to the origin. In the first step we moved our stick up above the origin. In the second we rotated the stick but it rotated around the origin. Let's watch this disaster in slow motion. The stick starts centered at the origin. We move it upwards 50 units, now it's center is at the right height. However, when we then apply a rotation, the object rotates around the origin, down here, instead of rotating around its center. Finishing the 90 degree rotation, the sticks landed on the ground. Another way to think about it is that translating an object moves its center. You're moving the object up. Relative to the stick, you're moving the point it will rotate around down. We've established that scaling before rotating is usually what we want. And rotating before translating is also generally more useful. This is why 3JS uses this order, scale, rotate, translate when dealing with a single object. We've seen that 3JS can't easily let us position and rotate the hand of a clock in place. The problem is that 3Js rotates and then positions when we like to do these in the opposite order. One simple solution that 3JS provides is to use Object 3D to make a new object that contains our clock hand. Here's the code for how to do this. What is happening here is that the block is nested inside the clock hand object. The translation moves the block so that one end is over the center of the clock face so that the hand will rotate around the clock properly. By putting this object into the clock hand, I'm saying that I want to use the block in its current position. I can then apply an additional scale, rotate and translate. In this case I just want to rotate. The final line of code adds the clockHand object to the scene. That's 1 way to look at Object3D. It adds more transforms into the list that you can then use. We now have some six transforms that we can set in this system. Here's the conceptual diagram of the whole idea. We create a block and we can scale, rotate and translate that block. Then by putting it inside an Object 3D called clockHand, we get an additional scale, rotate and translate that wen can apply. Here's a more compact way to write out the order of transforms. The object at scale, rotated and translated and then scaled, rotated and translated again. This looks a bit crazy, I've put the order of the transform from right to left. There is a reason for this and we'll talk about it when we get to matrices. For now just believe me that this is how most computer graphics text show a series of transforms. The way I remember this order is by the word trash. There's TRS in that word in the proper notational order. I guess I could use the word trees instead and not have to worry about the h, but I like trash, as it reminds me of the TRS80. Called the TRASH 80 for short, one of the first personal computers. From back when programs were stored on cassette tapes, once upon a time, this computer ruled the earth. In 1979, it had the largest selection of software for any PC. Trs then is the order to remember. In our case, just the boxes translation and the clock hands rotation are used. This gets us the combination we want. Translates so there's a new origin and then rotate around this origin. Adding a whole Object 3D to get a single additional transform seems like overkill, but really it doesn't add much code. If you to take a box or a spherical clock hand from their previous exercisers and position it and rotate it, you'll get it something like this. [unknown] is doing the standard rotate then translate in a series of operations. The hands are rotated to the proper angles but the translations don't move that much along their axis but rather move them in world space. In this exercise, you are going to make proper clock hands. The positions and scales in the exercise are all correct. You shouldn't need to change those. You need to figure out how to rotate the hands so they are properly oriented. When you're done, you should see the hands as shown here. You'll need to use an Object3D to let you rotate after translation. Here's my solution for the minuteHand. The cube is translated to the correct pivot point by the position. That does't change. A minuteHand is then added and then rotated into place. This new object is then added to the scene. It's this minuteHand object that undergoes that rotation gives us what we want. Similar code is added for the hour hand. We've used object 3D to give us access to a few extra transforms in the chain. However, object 3D is designed for another purpose that is extremely useful. What object 3D does is create a parent child relationship between two objects. Once an object is a child of another object. That child is affected by whatever is done to the parent. Here I'm sowing the fleshed out set of parent child relationships. The car has two doors and four wheels. Each wheel has a rim a hubcap and a tire. This is usually called a seam graph as it defines the graph of relationships of objects in the scene. This whole tree structure is called the hierarchy another frequently misspelled word so get it right. With this hierarchy we can do a lot. For example, we can move the whole car by applying a transform to just the parent. The other objects in the hierarchy will automatically be updated with this transform and follow along with the body. We can also affect objects within the hierarchy. If I change the wheel's rotation, the rim, hubcap and tire will also be updated and move along. Under the hood what is happening is that a series of transforms are being maintained. For example, for the hub cap, here's the series. There's the scale, rotate and translate set for the hub cap itself. Then another scale rotate and translate for the wheel. And finally a scale rotate and translate for the car as a whole. In practice, the scale matrices are not often used. As many model creators make their models to scale. The good news is that these complex series of transforms can be compressed into a single transform that does it all. It holds all the scales, rotations and translations. Plus any thing else done. We'll show how this works in the upcoming lesson on matrices. That said, fixed hierarchies in the parent child relationship they establish are extremely useful. Hierarchies allow most objects to be controlled in a natural way. I believe I've said this three times, so I'll invoke Lewis Carroll's line from The Hunting of the Snark, what I tell you three times is true. Personally, when I'm thinking through what series of transforms I want to apply, I work step by step. Perhaps the most important rule I can offer you is this Once you've applied a transform, forget about it entirely. That transforms applied, it's history and you now have some new object and a new position, possibly with some new location at the origin. Next, draw a picture if it helps. I usually do. Finally, once a transform is applied, where does your new object want to move from there? If you find you've messed up and the new object is not moving toward your goal, undo and try again. Instancing is the idea that a single geometric set of triangles can be reused again and again. For example, for a lamp, you might have a single piece of geometry for a bulb fixture. If you want three bulb fixtures, you then set up transforms for where the objects go but reuse the same set of triangles for each one. In three.js a mesh of triangles is called its geometry. Though you often hear it called no objects mesh, a mesh in three.js, is a type of object. This is easy enough to do in three.js. If the geometry doesn't change, just reuse it. Here's a simple example. I create the geometry for a cylinder, and then reuse it again and again for each mesh object that I make. Note that an object is made from some geometry and a material. This means I could've given each cylinder a different material while we're using the same geometry. Say, I have a car model with 4 tires. The question to you is, which of these are valid reasons for using instancing for tires? It allows an individual tire to be shown as our of air and flat. It reduces the number of transforms needed. It can use less memory. It allows some tires to be drawn with less geometry than others. The first reason is false. Since the single set of triangles is shared by all four tires, each tire must look exactly the same, at least as far as geometry is concerned. A flat tire doesn't look the same as the others. The last reason can also be rejected, as each tire is drawn with the same geometry. The second reason is not true, as each tire needs to have its own transforms. The third reason is definitely true. Instead of storing four separate triangle sets for each tire, we can store just one. The savings can be huge when there are many identical parts. This reuse can also increase performance on the GPU, depending on how the data is passed in. Let's see how a simple hierarchy works in practice. I'll make a robot with two parts to its arm, a forearm and an upper arm. This is kind of a classic computer graphics example, except that my robot is going to be a T850. The forearm is this piece made of six objects. I run some tests on it and it looks good. I've made it so that the point where I'll attach it to the upper arm is at the origin. I do this because I know that when a rotation is applied, the rotation happens with respect to the point at the origin. Here's the code to create my forearm, there's not a lot to it. I create an Object3D, i call, createRobotExtender, and then I add the forearm to the scene, so I can see it. My createRobotExtender function adds a bunch of geometric objects to the forearm object. A faLength is passed in to let this function know how tall to make the extender. It doesn't really matter what I add to the forearm object. The only rule is that whatever I put into this object, I need to remember that this object will itself rotate around the origin. I designed my upper arm in the same way and coded up about the same. Again, it rotates around the origin. The bit we're interested in is how these two objects get hooked together so that the forearm is the child of the upper arm. Here's the code that attaches the two objects together. We make the two arm pieces the same as before. The key difference is that instead of adding the forearm to the scene itself, we add the forearm to the upper arm. We also move the forearm to be at the top of the upper arm. At this point the upper arm can be moved around, rotated, scaled and all of its parts will be transformed in the same way. This is why we name this object to be the arm instead of the upper arm, since in fact it holds both the upper arm and the forearm. For example, if we set a rotation on the arm, the whole object will rotate. What's important to realize here is that the arm doesn't particularly know or care what's inside any of its children. When I add the forearm to the arm, the forearm becomes just another part of the arm, no different than adding a simple sphere or block. However, since we can manipulate any object by changing its transforms, we can also change the position of the forearm itself. So changing the angle with this line of code, will change the angle of the forearm without affecting anything else. The transform order is the forearms rotation and translation, followed by the arms rotation and translation. This means the forearm can be transformed however you wish, without knowing about the arm as a whole. The arms transforms are applied later. Give the demo a try to see how the two parts interact with their separate controls. At this point, you should have a pretty good sense of what the various transforms do. Three.js supports these basic transforms without you needing to understand what's going on inside the code. However, three.js can't provide every possible type of transform out there. Though it tries. More importantly, you'll need to know more about how transforms are coded if you want to program the vertex and pixel shaders in the pipeline. Here's a screenshot of the contents of the forearm object 3D from the robot arm demo. Using the debugger. You have been using the debugger in your browser, right? If not, please go look at the additional course materials right now and learn more about it. If you dig inside any object 3D, you'll see a parameter called matrix. It is a Matrix4 with a bunch of numbers in it. Ultimately this matrix parameter is what holds the transform that changes the object's shape, orientation, and location. When you set the position, rotation, and scale these all affect what gets stored in this array. This matrix represents all these transforms put together in a tidy package. We can multiply a coordinate, represented by a pointer vector, by this matrix and get a new coordinate. That's honestly it in a nut shell. Any object we make is ultimately represented by a bunch of points. These points are in the object's own sort of space. For example if we make a cube using cube geometry it's centered around the origin. We transform the cube's points by a transfer matrix to move, rotate and scale it as desired. A transform matrix is a 4 by 4 set of numbers. Three.js also supports a Matrix3 type which almost no one uses. In math in general a matrix can have any dimensions at all. Such as 12 by 38. The 4 by 4 matrix is the size that the GPU prefers. We can multiply a coordinate by a matrix. This is shown by putting the coordinate to the right of the matrix. The new resulting coordinate is put to the left of the equals sign. The mathematical notation is shown here. The coordinate times the matrix gives this new coordinate. To multiply a coordinate by a matrix you take the coordinate and perform a dot product with each row of the matrix. For example the dot product of the first row and the coordinate is this, D1 equals N11 times C1 plus N21 times C2 and so on the sum of these four terms is the first coordinate of the result. The dot product of the second row in a coordinate gives D2, the second coordinate of the result. After four dot products, you have a new coordinate. Now it's your turn. If you multiply this matrix and this vector, what is the resulting vector? The answer is 6, 6 5 and 1 for the coordinate values. For example, this first row times this column vector gives 2 times 2, negative 1 0, 3 times negative 1, 5 times 1 and that sums up to be six. Points and vectors are mathematical entities representing locations and movements. When we give them coordinates, we're saying where they are or where they move in comparison to some frame of reference. We use three coordinate values for both a point and a vector. We've been keeping track in our heads whether a coordinate represents a point or a vector up to this point. You'll notice for matrix multiplication our coordinates have four values. When using matrices, the way we tell points from vectors is by putting a 1 or a 0 for the last coordinate. This also works rather nicely as an operation check when using point and vector math. For example, if we add 2 vectors, the fourth coordinates add up to be 0, which means a vector. If we subtract 1 vector from another, the same thing works out. The fourth coordinate is 0 so it's a vector. If we had a point and a vector, we get a point because the fourth coordinate is 1. If we subtract 1 point from another we get a vector showing the movement needed to go from the second point to the first. This fourth coordinate also acts as a way to check against doing illegal things. If you add two points, this normally has no real meaning. The fourth coordinate is 2, which is a tip off that something is not right. Now that we have this fourth coordinate, we can multiply points and vectors by a 4 by 4 matrix. The usual default setting for a matrix is what is called the identity matrix. It consists of zeros everywhere, except along the diagonal, which is set to ones. If you multiply any coordinate by this matrix, you get exactly the same coordinate back out. The first draw of the matrix selects the first coordinate out of the coordinate array, the second row selects the second coordinate, and so on. Try it yourself. In three.js, we create a Matrix4 by this bit of code. By default, this matrix is the identity matrix. If during processing, we want to reset the Matrix4 to identity, we call the identity method on the object itself. If we want to change the location of a point, we can use a translation matrix. This matrix has the translation movement put in the top three positions of the last column. The rest of the matrix looks the same as the identity matrix. If you multiply a point by this matrix, the one coordinate in the last position of the vector multiplies each translation value in turn. This has the effect of taking the original point. And adding the translation to it. This may seem like a convoluted way to add a vector to a point, but you'll soon see that many other transforms can be combined and stored in this matrix, translations among them. With a single matrix multiply, you can then perform any number of transformations. Say instead you multiply a vector by this matrix. You'll get the same vector back. The fourth coordinate of the vector is zero. So for all the translation values that are multiplied by zero, and so ignored. This makes sense. A vector does not have a location so it can not be translated. The only thing mentioning at this point is that this type of matrix and vector notation Is called column-major form. There is also a row-major form for notation where the coordinates are written out in a row. The vector row is multiplied by each column of the matrix in turn. Personally, this is how I learned matrices 30 years ago. Microsoft's DirectX uses this form. The story I heard as to why row vectors became popular was due to the influence of Steve Koons. He wrote some influential papers on transformations for computer graphics back in the 1960s. He said he said he used draw vectors in those papers because it was easier for the stenographers to type the translation values along the bottom of the matrix rather than having to type them in the column. All that said, web GL uses column-major notation. In this convention is also the norm in most publications, so that's all we use here. Either form is fine, they're both just a way writing down the math. I mostly want you to be aware that there's another form out there called row major. The good news is that, internally, Web GL and DirectX ultimately store the translation values in the same locations in memory. If you go look at any matrix in memory. You'll see that the last four values are the three translation values, and then a one. Now it's your turn to make a translation matrix. You find a space pirate treasure map and it says from atop Rumbullion Asteriod, go 5, 12, negative 7 kilometers. Then, negative 2, 3, 0 kilometers. Then 9, 1, 2 kilometers and there me treasure lies. You don't have all day. You want to send your cargo droid there directly to get the loot. Your droid uses a matrix to determine what orientation and position you'd like it to achieve. What translation matrix do you give it? Translations can be added together to get a single translation vector. So, we add up the three vectors to get the answers 12, 16, negative 5. To set a Matrix4 in creation, you can pass the values for the matrix in the constructor function. These values happen to set the matrix to perform translation, moving an object to the position 12, 16, negative 5. Note that this initialization is purposely made to look like column major form. Where the translation values are on the right. However, this order is not how the data is stored in the matrix itself. Looking at the order here, 1, 0, 0, 12. You might expect to see that in the matrix itself. But instead you see 1, 0, 0, 0. That's because as we've pointed out before the translation values go at the very end of the matrix as show here. Since translation is a common transform 3JS provides a function to set a translation matrix called makeTranslation. You give the position and it fills in the matrix for you. So say we have our robot arm, in object 3D. To apply a matrix 4D to an object 3D, the most direct way of doing so is to set the local matrix on the object itself, however you also need to set matrix auto update to false. Remember that 3JS has its own transform system built in. Position, Rotation and Scale. By setting matrix auto update to false you say, don't use this, but use this matrix instead. Here's the whole meaning for the dot product. For any two arbitrary coordinates. We'll go through a piece at a time. This first part we're used to. If you have a normalized a and b vector, then the cosine of the angle between the two is the dot product. However, when a or b are not normalized, in other words the length is not equal to one, the dot product also consists of these two factors. The length of a times the length of b times the cosine of the angle between the 2. To sum up, you can think of a coordinate as moving due to a rotation or staying still, and just being recategorized by a new set of axes. To throw 1 more term at you, these 3 axes are called a basis, and this is a term you'll see as soon as you get into vector math at all. So far, the matrix class hasn't done all that much for us. I've shown you how to do exactly what you already knew how to do, translate and rotate around the x, y or z-axis. Here's a transform that isn't built in to the object 3D class, axis angle rotation. And for that, you really do need to use a separate matrix. Say I want to make a little star shaped ornament out of four cylinders. Or if you're feeling more dramatic, I want to make some carbon fiber caltrops to trip up the robo-minotaurs about to launch a final assault on our base on[UNKNOWN] Prime, it's really our only last hope. I'm better now. Well anyway, I want to align the four cylinders with the four diagonals of a cube. The hard part is figuring out the rotation matrix I want to use. If I'm stuck using Euler angles, do I rotate in y, and then in x, or z and then in y, or something else? With the axis/angle function, I need to figure out what axis I want to rotate around, and how much to rotate. We're going to use cylinders to make our object. Here's a cut-away view of the cube, showing the original position of the cylinder and its desired location in the wire frame along one of the diagonals. Every cylinder starts pointing up along y-axis. Step one is to figure out around which axis to rotate this vertical cylinder. I want to rotate the cylinder to the point x y z all equal to 1. The question to you is which axis do I want to rotate around in order to get this cylinder into its final position? Your choices are axis one, which is along the y-axis, axis two, which goes along negative x and z, axis three, which goes along this xyz-axis itself, and your last choice is axis four, which goes along the z-axis. Mark each that is correct, if any. If you get stuck, you might want to go back to the answer for how we formed a disc-shaped object out of the swept clock hands, as that might help you out. This can be a hard problem to think about, I got the axis wrong myself when I was first making up this quiz, drawing sketches helps. Remember that a vector rotating around an axis forms a circle. The cylinders axis on the y axis and the final axis thru the corner of the cube define a circle whose center is at the origin. If you think about continuing this arc the whole way, it becomes pretty clear you're cutting the cube in half diagonally when viewed from above. We can also see that the axis through this circle is going to point in this direction or the opposite direction in order to be perpendicular to this slice. Since the y axis is in this slicing plane, there won't be any y component in the coordinates for the axis of rotation itself. So this axis must be either x positive and z negative or x negative and z positive. For this reason the second answer is correct. Negative x z is the axis. There can only be one plane that contains the circle and there is only one axis direction or its opposite direction that is perpendicular to this plane. Therefore there is only one axis of rotation that will take us from our starting position to the corner of the cube. So we can rule out all three of the other answers. I have my axis of rotation. It's negative X, Z. It's a good idea to normalize this, but this will do for now. If I look at my cube, it's pretty easy to figure out the diagonal directions themselves. Here is the easiest one. It goes from negative one, negative one, negative one to one, one, one. So the cylinder's axis is (2,2,2). We can also this vector's length to find the distance from one corner to another. These are the facts we need to continue. We've got the axis of rotation. We need the angle to rotate and the length of the cylinder itself. Here's three JS code that computes the cylinder's length. Three JS uses a class called Vector3. What you put in a vector 3 a point or a vectors coordinate is up to you. Perhaps a better name might have been coordinate 3 but that' pretty bulky most graphic systems you'll encounter will call coordinates vectors but it's easy enough to understand from the context what is meant. To find the cylinders axis direction and length I set up the 2 corner locations I then subtract one vector from the other using the sub vectors method. Giving a vector from one point to another. There are a huge number of vector and matrix operations supported in 3JS, subtraction is just one of many. Once we have the cylinder axis, this last line of code computes the length of this axis, which we'll need for knowing how long to make the cylinder. At this point, we have the axis of rotation The length of the cylinder, but we don't have the angle of rotation yet. However, we have everything we need to get it. Given this cylinder's access, we can dot product it with the y axis, which is where the cylinder starts at, and then, by the dot product of that and its final position... We will get the angle that we need to rotate. Remember that the dot product computes the cosine of the angle between two normalized vectors. So we normalize the cylinder's axis here and takes its dot product with the y axis vector here. This gives us the cosine of the angle. So we take the arc cosine to get the angle back in radiants. Using a dot product is a bit of overkill, by the way. Computing a dot product with a y axis like this is the same as just grabbing the y component of the cylAxis. We could've simply normalized and done this like this code here. However, unless this is a critical loop, like we're making a billion of these ornaments, I'd recommend using this first way. If someone reads or modifies this code in the future, the intent is clearer and more general. Now we have all the facts we need to make this object, the axis and angle of rotation and the length of the cylinder. So let's get cracking. Well, I guess I should mention that the sign of the theta might be a little bit questionable here. We could use the right had rule to make sure we have the direction of rotation right. Or we could use the principle of negate and try again. I learned about this practice long ago from an article in an old trade journal called SGI Insider. It was great to realize that someone else was doing what I also do. If I put a rotation in and it goes the wrong direction, just try the opposite. So feel free to do the same, as you're in good company. You could spend 15 minutes trying to figure out the sign needed after some set of matrix operations, and still have about a 50 percent chance of getting it right. Faster, more reliable is just to try it and see. That said, this practice won't help much if there are too many things to negate or if the rotation is part of a long chain of transforms. Or if you're starting from an incorrect algorithm. But, if you're feeling the math is solid, go ahead and just do it. Using this principle with my test program, I found I needed to negate the angle to move the rod to the correct place. I guess I could've thought it through and realized this is the right-hand rule, and all that kind of stuff, but nonetheless, just trying it made me realize we need to. To have a negative theta. I don't really like negative angles, I like to think positively, so I negated both the angle and the axis direction. This should give exactly the same rotation. If you rotate clockwise around one axis, rotating counter clockwise around the opposite axis' direction is the same thing. For a tilted cylinder we were able to look at and think about what axis to rotate around. However we usually won't be able to do this. If I gave you two arbitrary vectors and said quick what's the axis of rotation? Best of luck to you, I sure couldn't do it. Happily there's an easy way to get the axis of rotation and it's called the cross product. In three.js you call it like this, it takes two vectors as its inputs, and the result is put into the vector three itself. The third vector is in fact the axis of rotation or at least one of them. The direction is determined by the right hand rule. You wrap your hand from the first vector, in this case the cylinder to the second vector, in this case the y axis. This then points along the axis of rotation. If we computed the cross product of these two vectors in the opposite order we would go from the y axis to the cylinders vector. And we would get the opposite rotation axis. Recall how the dot product gives us the cosine between two vectors, the length of the cross product result is in fact proportional to the sine of the angle between the two vectors. There is one special case I'm going to point out and its kind of a headache. If the cross product gives back a vector that is of length 0 or nearly so then the two vectors are either pointing in the same direction or in directly opposite directions. You can use the dot product of the two vectors to figure out which, if they point in the same direction, then you're done. You don't need to rotate at all. If they point in exactly opposite directions, then you need to rotate 180 degrees. However, the rotation axis you'll get back from the cross product is actually 0,0,0, which is no axis at all. At this point, you basically need to choose some arbitrary axis that is perpendicular to your vectors and use that for rotation, or just form the rotation matrix directly. Here for example I use the x axis since I know y is going to be perpendicular to it. See the additional course materials on a good way to solve this problem in general. The mathematical notation for a cross product is this, a big X. The length of the vector produced by the cross product is equal to the sign of theta, that's the angle between the two vectors, times the length of A. Times the length of B. The cross product itself is computed by multiplying neighboring elements of the two vectors' coordinates. For the x coordinate, we multiply Ay times Bz and then subtract Az times By. For the y coordinate we multiply Az times Bz minus Ax times Bz. I like to do this kind of x cross thing here as we did before. So I tend to take this and wrap it around to this side. So it's Az times Bx, and Ax times Bz. For the last coordinate we do the same thing, Ax times By minus Ay times Bx. At the end we have a vector that describes the axis of rotation from one vector to the other. And in fact this vector will be perpendicular to both of these two. Oh, and one more thing. If you want to use this vector later you probably going to want to normalize it, because its length will be pretty obscure. We're finally at the point where you get to do an exercise and do some programming. To make our ornament, or defensive device to stop the robo-minotaurs from destroying all things good and just, we need to rotate each cylinder into position. We have all the data we need for a cylinder. The axis of rotation, the angle of rotation, and the length of the cylinder. First we create a cylinder. Using the previously computed cylinder length for its height. Here's the code that actually sets up the access angle rotation matrix. For the axis angle rotation function, we need to pass in a normalized axis at an angle in radiants. This function could normalize the axis itself, just to be safe. However, for efficiency's sake, it doesn't, so we normalize the axis earlier here. Next we do an obscure thing. Set matrix auto update to false. This parameter is true by default. What it means is that, if true then our objects 3D matrix will be computed from its position, rotation and scale parameters. By setting it to false we tell 3JS that we are not going to use these parameters. But rather are going to set the matrix itself directly. And that's exactly what we do in the next line, where we at long last finally call the matrix rotation axis method. Giving it our normalized axis and angle. At long last we have added a cylinder at the proper angle and length to our seam. Now it's your turn to finish off the rest. Add another three cylinders to complete this object. There are a few ways to solve this one. Here's the approach I took. I knew that the cylinders were all the same length, so I created just one. I then, reused this cylinder's geometry to make each cylinder object. To get the four rotation axes, I realized these formed an x shape. If I could enumerate all four directions in my loop, I'd be done. So, that's what this tricksy code does here. It gives me negative x, negative z on the first iteration, negative x, z on the next, and so on. I then normalize this x and then use it as usual. You could also solve this one by using just two rotation axes and flipping the sign of the theta, rotating each pair of cylinders in opposite directions. One reason that we use four by four matrices to store transforms is that a single matrix can hold any number of transforms at once. As an example, consider object three D's rotation paratmeter. Here is a snipet of code from the oil angler demo. The airplane's three rotation axis are already set. This means that the airplane is first rotated around its z axis then its y axis, then x. Internally a transform matrix is made for each rotation. Then these are multiplied together. Matrix multiplication works like this. For each location in the resulting matrix, you take the corresponding row of the first matrix. [inaudible] And the column of the second matrix, and perform a dot product between these two. For example, to compute element n two four, I compute the dot product of the fourth row of the first matrix, and the second column of the second matrix. This gives this set of terms here, added together gives n two four, 16 dot products later and you have the resulting matrix. To multiply together our three rotation matrices. We can start at either end. Multiplying Rx by Ry or Ry by Rz. I've decided to start with Ry and Rz. Multiplying these together we get some temporary matrix U. We can then multiply together the X rotation matrix by this temporary matrix. This gives us another matrix call it Q which consists of all three rotation matrices multiplied together. This resulting matrix Q can then be used to transform coordinates when an object coordinate is transformed by this single matrix the coordinate in fact is rotated by the three rotation matrices in turn it's clearly more efficient to use a single matrix than three. The scale and translation parameters in the object 3-d class do the same thing. They create matrices and these all get multiplied together. Here's the full sequence of transforms that happen for an object 3-d when using its parameters: scale, the 3 rotations, and translate. Internally, these matrices are all multiplied together to give a single resulting matrix m. The parameter in the object 3D class, is, in fact, called matrix. You can now see why I've been listing the order of matrices as from right to left. As this is the order we use for multiplying them together. Multiplying matrices together like this is called concatenation. Better yet, we can chain together all the matrices that effect an object and store what 3JS calls a world matrix. Say we have a hubcap that's part of a wheel, that's part of a car. Each of these objects has its own concatinated matrix of transforms. If a program wants to display the hubcap on the screen, we need to multiply the hubcap's vertices by these three matrices in turn. Instead of doing three matrix multiplies, we can concatenate the three matrices to form a single world matrix and store that separately for the hubcap. This matrix is the world matrix, or it's sometimes called the model matrix. Whatever you call it, a single matrix multiply is then all that is needed to transform a vertex from its model space to world space directly. I showed how three rotation matrices multiplied together gives a single new matrix. Interestingly enough, this resulting matrix R is, in fact, a rotation matrix itself. Any number of rotation matrices multiplied together always results in another rotation matrix. This resulting rotation matrix will have a single specific axis and rotation angle. I have two questions for you. First of all, can you multiply a series of translations together and get a single translation matrix? Second, can you multiply a series of scales together and get a single scale matrix? For example, say you apply a number of scales in a row to an object. If you multiply all these scale matrices together, do you get a single scale matrix? Or do you get something else? For the first question, each translation matrix represents a vector, a movement. We can move an object by vector after vector and at the end of it, the object will have changed its location, nothing more. So we know the resulting concatenated matrix is purely a translation matrix. So the answer is yes. With scaling, each scale operation changes the dimensions of the object. Scale does not actually move or rotate the object. So a series of scales will result in a scale matrix. So the answer here too is yes. This is a pretty involved lesson. And I don't expect you to absorb it all at once. But I do want you to at least heard of the idea. I know it's taken me about 20 years to really grasp this whole concept. Let's get started. In previous lessons we've seen that the order of transforms for an object is usually rotate and then translate. For example, we start with the letter F, at the origin. If we rotate it negative 45 degrees, we get a tilted F. We then translate it by one unit vertically, and it moves up. This we write out as TR, with the matrices applied in reverse order. However, we're now able to use matrices which ever way we want. So say we reverse the order. If instead we first translate up a unit, then rotate, we get this, the notation RT. Clearly different, with T changing the origin we then rotate around. Here is another way to look at it. Say we first just rotate the F. We express this by the notation R. We could add a translation matrix before or after this rotation. If we add it before the R this means we translate the object after the rotation and so move it with respect to the world. Whatever happens before this translation was applied is said and done. The object is in whatever orientation that has already been set. Ancient history, so TR is this top case as before. If we put the T after the R, it says to apply the translation first so everything is rotated around the origin. So finally I haven't said anything new. However, another way to look at all this is you're changing the frame of reference by each of these transforms. Instead of rotating or translating the object, we can think about it as changing this frame of reference. By taking this new path, we can think of the transforms in these terms. First the rotation changes the frame of reference that any transforms to the right of it will use. Then the translation moves the object with respect to this new frame of reference. Any point of space is given some x y z coordinate only with respect to something. We started off by thinking of this something as world space, but notice that we have actually been making things in what we call model space, and then transforming them to world space. For example, when we have a parent child relationship, the child is oriented with respect for the parent, not the world. So it's all about respect. Just a little bit. When we put the T matrix after the R, we're saying to translate with respect to the view of the world of the R matrix. So putting the T here, moves the objects with the respect to the R matrices frame of reference. That is, the translation is done with respect to the R's y axis. Putting the T in front of the R means that the translation is not done in the R's frame of reference, but in the world's frame of reference. The R itself is now influenced by the T's frame of reference. Another way to say this is that the first matrix to the right of some given set of matrices is transforming the object with respect to that set. In other words, everything to the left of it. For example, on the top here, the rotation is transforming the F with respect to the frame of reference of the T translation to its left. This all may sound obtuse, but think of someone looking at a hubcap in your model, he wants to know where it's located and what angle it's at. You might describe it as well, I moved the hubcap so it'd be properly attached to the wheel, and then I moved the wheel so that. But he cuts you off, I don't care how it got there, where is it now. When you position the hubcap you weren't really moving the hubcap to its position you are positioning it with respect to wheel's frame of reference. And then you are positioning the wheel with respect to the car's frame of reference. It's more like the car's location and the wheel's location were maps for where the hubcap should go. Different frames of reference are just like you might use latitude and longitude to fly somewhere. Then use a street map, a different frame of reference, to get from the airport to the office. Then use a map of the building to find the room you want. Each frame of reference takes you to the next. Here's what a scaling matrix looks like. Set all three values here to one and you get the identity matrix. If you want to uniformly scale something to be three times as large as before, set SX, SY, and SZ to all be three. If you want to instead just stretch something up along the Y axis so that it's five times as tall, set SX and SZ to be one, and SY to be five. There's just one funny thing that happens with scaling in particular. It can mess up normals. Remember we're not just transforming points with matrices, we're also transforming shading normals. If you run normals through a translation matrix, nothing happens to them. They're vectors so they don't get touched. If you run normals though a rotation matrix, things are fine. Here's a triangle from a side view, looking along it's plane, along with it's normal. If you uniformally scale this object to be larger, look what happens. The object grows larger, as does the normal. The good news is that the normal doesn't change direction, so all we have to do is re-normalize it before using it in any lighting equations. If you non-uniformally scale an object, In other words, stretch or squish it. Things get weird. Here I stretch the triangle in one direction. The normal vector has also stretched upwards and is clearly no longer pointing in the right direction. To solve this problem, we need to inverse and transpose a matrix. Transpose of a matrix is this original matrix with it's rows and columns flipped along the diagonal. There are two diagonals but the one I mean is from the upper left to the lower right. It's where we put the scale factors for example. Here I'm showing the original matrix A,B,C,D and so on with column order. What we do to transpose is we flip along the diagonal and they're now in this order. The mathematical notation for this is M superscript T, where T means transpose. The other operation that is commonly done to a matrix is computing its inverse. The inverse is shown like this a negative 1 in the superscript. If you multiply a matrix by its inverse, you get the identity matrix. The inverse of a matrix essentially undoes the work of that matrix. For example, if your matrix moves an object five units up, the inverse moves it five units down. The inverse of a rotation matrix is one that rotates on the same axis but back the other direction. The inverse of a scale matrix scales the object back down. Let's look at the inverses of some common transform matrices. For translation, we start with our translation values over here. The inverse is the negation of all those translation values. Which makes sense. You move to one direction, and then you come back the other direction. For any rotation matrix, it turns out that the inverse is the transpose. So, for this one, for example. Its rotation around the z axis and you simply transpose these two elements and you know have the inverse of the z rotation. For a scale matrix, we divide one by the original scale factors. Or to say it more mathematically. We take the multiplicative inverse of each scale factor. Things get more complex when we combine rotation, translation and scale matrices. The inverse of a matrix representing a complex series of transforms is usually not obvious. I won't spend the time explaining how to compute an inverse that's a whole lesson of linear algebra. The good news is that every graphics library in the know universe. Has a matrix inversion routine of some sort. Three.js is no exception. Matrix4 class has a getInverse function. We have a problem with transforming normals when using non-uniform scaling matrices. If we tried to transform the normal with the same scaling matrix used to transform the vertex coordinates, we ran into the strange situation of normals getting bent away from their surfaces. How to solve this? I'll jump to the punch line. What you want to use to transform normals is the transpose of the inverse of the model matrix. You could also use the inverse of the transpose of the model matrix. They're the same thing. Either way that's a mouthful and let me explain. Let's look at how this solution applies to each different kind of transform that we're used to. First of all, forget about translation matrices. We're going to be transforming normals and translation matrices do not affect vectors in any way. For rotation matrices, the transpose is the inverse. The transpose of the inverse, in other words, would be the inverse of the inverse. This is starting to sound a little like Alice in Wonderland. To be a little more clear, if you take the transpose of the inverse of a rotation matrix, you get the same rotation matrix back. This means that for matrix made up of any series of translations and rotations you can use this same matrix to transform points and normals. This is because the transpose is the inverse as far as normals are concerned. Scaling makes it no longer true that the transpose is the inverse so, we have to take special steps. If the scaling matrix is uniform. In other words the shape of the object doesn't change. We can get away with just normalizing the normals after transforming by the original model matrix. For non-uniform scaling where the shape of the object does change we must explicitly compute this special transpose of the inverse matrix in order to transform normals correctly. Or you can do the inverse of the transpose. I'm not going to explain here why the transpose of the inverse works. You likely don't want me to talk about contravariant and covariant vectors. See the additional course materials for more information. If you keep away from non-uniform scaling matrices, you'll never run into this mismatch and can merely use the modeling matrix for both position and vector transforms. If you never use scales of any sort you won't even have to re-normalize your vectors. The good news is that if you use scales of any sort in three.js it will correctly handle normal transformation for you. It's only when you're using scales on your own that you can get pretty weird shading results if you don't pay attention. And you do want to pay attention. As while this is always the correct answer, it's costly to compute all the time. I've seen many a young programmer led astray by normal transformation. Seriously, back in the 1980s before any book ever mentioned it. This problem was fairly common. Back then, we consulted for a hardware company, who will rename nameless, writing programs using their expensive graphics accelerator. And I mean expensive, something like $35,000, the price of a nice BMW back then. I found that the graphics accelerator had this problem with polygon display. Meaning we had to avoid using non-uniform scales as a modeling operation. The company was more than a bit dismayed when they found out about this problem, since it's costly to fix properly. One transform matrix worth a quick mention is the mirror matrix, also called the reflection matrix. This matrix usually has one element here be a -1, and the other two are ones. For example, here's a mirror matrix that mirrors the Z coordinate. This transform makes the Z axis act something like a mirror. Anything on one side of the axis is mirrored to the other. You can create a mirror matrix easily enough in three.js by setting a scale value to -1. There are in fact an infinite number of mirroring matrices since by rotation and translation you can move the mirroring plain. One major problem with the mirroring matrix is that it not only flips the triangle it also flips the sense of the triangle. Using the mirroring matrix converts from a right handed coordinate system to a left handed one or vice versa. That is in a right handed system a triangle's vertices normally proceed counter clockwise around its edge. After a mirror matrix, this order's reversed. If you're using backface culling, for example, you have to reverse the culling sense from making the front side visible to the back side visible. Even with this, I find that mirroring matrices are tricky to use in three.js. By default, the reversal also affects lighting, for example, making it look like lights are coming from the opposite direction. Maybe you'll have better luck and if so put it in the forums. One place where mirroring sometimes is used is when modeling. Instead of sculpting a whole moth, for example, you can make half a moth. You then use a mirror matrix to create the other half. Sometimes you might encounter a matrix in a data file and don't know where it came from. It might be a mirror so that could really mess you up. You can use the matrix for a determinant call to see if there's any mirroring in the matrix. If the determine is negative, it contains a mirror. We had to take corrective action when we transformed normals by a scale matrix, needing the inverse transpose in the worst case. For rotation and translation we could use the same matrix for normals that we used for points. So the question to you is what happens to normals if they're transformed by the same mirroring matrix used for transforming points? Your choices are the transforms works only if the normal is perpendicular to the mirror. The transform works only if the normal is parallel to the mirror. The transform works all the time, but you must normalize the normal afterwards. Or the transform works all the time. If you need a hint, think about Alice through the looking glass. I won't formally prove it here. But the same mirroring matrix can be used to transform both points and normals. The mathematical explanation is that the mirroring matrix is like a uniform scaling matrix. However, since no scaling is actually occurring, we don't need to renormalize the normal. Therefore, this fourth answer is correct. Intuitively, if you think of going through the looking glass. Your left and right hand change position, and your body flips, and so on, but your shape and size remain the same, so you don't have to mess around with your normals. Once you start looking under the hood, you'll get some sense of the lay of the land with matrices and how they relate to transforms. Here's a map of the sort of things you'll find. The upper left area of the matrix is where rotations and scales show up. If a transform changes only this area of the matrix, it's called a linear transformation. I'm not going to spend time on the formal definition of this term. The additional course materials include resources for more on the underlying math. The definition is fairly basic stuff about how addition and multiplication are preserved. But doesn't have much effect on how we actually do computer graphics. The upper right is where the translations accumulate. These translation values will get effected by multiplication with other matrices of course. Translations only effect points since vectors have zero for their fourth coordinate. 3JS provides a function called decompose to extract the translation, rotation and scale factors from a matrix. The translation and scale factors come back as vectors, as you might expect. The rotation comes back as a quaternion. Something we'll talk about when we get to animation. The short version is that a quaternion is a compact way to store the axis and angle of rotation for a rotation matrix. One useful property quarternions have is that you can easily interpolate between them, which is useful for interpolating between two different orientation. Notice that the bottom row is always 0, 0, 0 1. The transforms we covered here are called affine transforms. Parallel lines stay parallel when an affine transform is applied. In modeling you'll essentially always use affine transforms. So we never change this last row. Since GPUs are tuned to use four by four matrices Most of us just use four by fours everywhere for simplicites sake. When we discuss perspective cameras, we'll set the values in this last row. We'll then be using a projective transform. With affine transforms, when a points coordinates are multipied by the matrix. The fourth coordinate starts out as one and ends up as one. This last row in a projection matrix modifies that fourth coordinate to be something other than one. What that means is something we'll leave til a later lesson. I'll sum up some of the major concepts we've covered. First of all WebGL and THREE.JS matrices are in column order. With column order matrices we record the transforms to the object from right to left. We apply the frame of reference from left to right. Matrices themselves are associative. You can multiply whichever two neighboring matrices you want in whatever order you want. Finally matrices are not commutative. That means matrix ordered does matter. Well I should say matrix order usually matters. Sometimes for example if you have a series of translations in a row you can do the translations in any order. Now that you know a lot about transforms I recommend poking around in documentation and reading through all functions available for vectors and matrices. More important still go and try them out. Take a three.js program of any sort and mess with it. Change things and see what happens. Having java script, an interpretive language with a built in debugger makes it easy to dig in and see what all sorts of programs are doing. My sincere congratulations to you. You've just finished working through one of the most difficult areas of interactive 3-D graphics. Between transforms and materials, you've learned two critical elements of the field. You can create and display 3-D objects in your own virtual worlds. I think of this point in the course as the top of the roller coaster ride. That slow part where you come to the top of the rise and the tracks ahead come into view. With these basics down we can now explore areas that will let you create even richer scenes and applications. It's going to be a wild ride. The transpose of a rotation matrix is its inverse, the rotation goes in the opposite direction. What sort of matrix is the transpose of a translation matrix? Your first choice is, it is a quaternion. It is a linear matrix. It is an affine matrix. It is a projective matrix. Check all that apply. This matrix is not a quaternion. No matrix is a quaternion, not even a rotation matrix. A rotation transform can be represented by a rotation matrix. A quaternion is an alternate representative for a rotation transform. The transpose of a translation matrix puts its translation values in the bottom row of the matrix. The only type of matrix that puts its values in this area is a projective matrix. So of these three types of matrices, the only correct one is projective matrix. To be honest, I don't know what sort of projection that would do. Its not what we normally run into in computer graphics. But it definitely is a projection of some sort. I'd like you to identify the types of matrices these are. Your four choices are translate, rotate, rotate then translate, and scale. When you figure out what a matrix is, put the appropriate number in the box. Each transform will be used one time. This matrix is a translation matrix. Because only this fourth column is different than the identity matrix. These values are the translation amounts. This matrix is a rotation matrix. Because the upper left three by three has been modified, and not in a simple way like a scaling matrix. I should mention that if the three by three areas change in a arbitrary way you can get what is called a sheer matrix. I see sheer matrices get mentioned in textbooks, hey, we cover them in our own book. But personally in the past three decades I've never used a sheer matrix. This matrix has elements of both translation and rotation and so is the two of these types multiplied together. This leaves the last matrix as a scaling matrix, it's a non-uniform scaling along each axis. 3JS tends to create what are called canonical objects. This is where you create an object centered around the origin and then apply a series of transforms to move it into position. This is fine so far as it goes, but is sometimes awkward when you want to perform particular kinds of modeling. For example, if I want to make a model where I want chains of cones to build a tree, I might rather say where I want each end of the cone to be located. This sort of modelling where I run a program to generate an object is called procedural model. I want to orient the cone, given the top and bottom locations. That'll be your task. In the cullen routine the top and bottom variables are vector three positions given the ends of the cone. You'll see documentation for the other variables in the code itself. Your task is to implement the internals of this method. Actually, I decided to simplify a major piece for you. Mainly because it has a special case having to do with a cross product. I implemented this function myself. So your job is to create the proper length cylinder and feed this method its axis direction and center. When you get the answer right, you'll see this on your screen. I gave a lot of test cones here, and in fact, a few of these revealed some bugs in my own code when I was writing it. If you ever design a component like this, they will be used by others. I highly recommend that you try to create as many different types of test cases as you can. The code for this one is pretty short overall. You get the cylinder's axis, get the length from it. And then get the center by taking the average of the two point's locations. Notice that this is the one time where we actually add two points together to get the center point between them. If you're up for a challenge, try this exercise again, but delete the MakeLengthAngleAxisTransform method and write it yourself. I was going to make this the assignment originally, but after taking a few hours myself fighting JavaScript and some cross product headaches, I decided against it. Once you can place cones and cylinders, another handy method to write is one that creates capsules, also known as cheese logs. A capsule is simply a cylinder with a sphere covering each end. You can build up some pretty interesting looking objects using cheese logs. Here's the interface for a capsule. The capsule function is again, fully documented in the code itself. For this exercise, I've implemented the simple part, positioning the cylinders, and I've given you the code for how I want you to create the sphere's geometry. Your job is to efficiently add spheres to the end of the cylinder. The open top and open bottom Booleans determine whether or not a sphere is added. If you need to add two spheres, one to each end, please use instancing. In other words, make a single sphere geometry object, and reuse it for both ends. When you're done, the scene should look like this. Notice how some of the cheese logs touch, and only one sphere of a particular color is rendered at the intersection. Now, on to the exercise. I implemented the solution by creating an object in which to put my cylinder and two spheres. At the top there's a little logic here to determine whether any sphere is needed at all. If so, I create the sphere's geometry. Then, each end is checked to see if it's used. If it is, a sphere object is created with the geometry, positioned and added to the capsule, which is then returned. That's it. Once you have capsules available, you can make all sorts of stringy objects. Here are some helices formed out of spheres. Your job is to make these helices out of capsules instead. Stringing together the capsules can be a little tricky, as you want to get the end conditions correct. Where two capsules touch, you want to generate only one sphere, not both. This saves on memory and increases performance. That said, you want to make sure that both ends of the helix have spheres on them. As a bonus challenge, try not to compute both points for each capsule on each loop iteration. It's more efficient if on each iteration you compute just one of the points. When you're done, the capsules should look like this. There's a parameter in the code called radial segments that you can increase to give a smoother result than shown here. There's a key technique in my solution that's very useful in computer graphics. You could compute both the top and bottom locations in the loop for capsules. However, a better method is to initially compute the bottom location, and then on each iteration compute only the top position. At the end of the loop, you then set this top position into the bottom. Now, bottom is set for the next loop iteration. The only other clever bit of code here is the realization that the bottom sphere needs to be on for only the first capsule in a string. After that, I set open bottom to true, so that it's never output again for this helix. If you want to take this exercise to the next level. Rewrite the code so that the cylinder and sphere geometry is reused throughout the helix. Right now, no instancing is done, so it's a memory hog. Every capsule creates its own geometry. Another extension is to allow the art parameter to be a floating point number so you can generate just part of the arc. If you really want to play around try factors to have the helix become a larger and larger spiral or have the radius vary with height as a sign curve. Me, I find once I start playing with procedural modeling that it's hard to stop. If you make something great consider submitting it into the 3JS library itself for inclusion as a demo. You can use get hub to make submissions, see the additional course materials for more information. We've neglected our poor drinking bird. Even though we've had techniques for a while now to give him everything he needs, he is missing eyes, a nose, and, most importantly, a crossbar to keep his body attached to his legs. Well, in computer graphics we could have him just hover in the air forever, but we're trying to make things look a bit realistic. The specifications are in the code itself, as comments at the end of the fill scene method. Don't be surprised if you have to add an object 3-D here and there to get all the transforms you need. Once you've done adding these features, he should look like this. Now he has a cross bar eyes and a nose. The solution for the drinking birds crossbar and nose are nothing special. You set the position. You rotate it by 90 degrees and you're pretty much done. The eyes are a little bit more interesting. First you have to translate to position, then rotate. To do this, you have to make an object 3D so that you can do the rotation after the translation. Our drinking bird is functional but could always be improved. He could maybe have a few helices for his tail feathers. His eyeballs could be flatter and he could have pupils for his eyes. He could have a jaunty hat and a corncob pipe in his mouth. The crossbar could surround his tube instead of poking through it. He could have classier feet. I hope you'll play around with him and improve his appearance. I'd love to see what you come up with. Say you're given a mesh like this. Some statue standing on a platform that's some distance from the origin. You'd like to rotate it in place on its base so that it's facing like this. You know that if you just try to rotate it, it's going to rotate around the origin and it will swing wildly in position. You want it to stay where it is. You have a few matrices at hand to perform the task. You're given a translation matrix that would move the center of the model's base to the origin. You also have a rotation matrix that will rotate the model or anything else around the y-axis. Finally, you can also invert the translation and rotation matrices. The question is, what series of transforms will rotate the model on its base? In other words, we want to end up with the object in the same position, but rotated around its own axis. Put the transforms in the usual right to left order, rightmost being applied to the object first. If you don't need all four slots for matrices, just leave the leftmost ones empty. For example, if you needed just a single transform, put it in this rightmost slot. To solve this one, you might want to remember the rule, once a transform is applied, forget about it and just think about where the model itself is then. The answer is to translate, rotate, and then use the inverse translation. The translation moves the model to the origin, where it's now centered. The r matrix rotates the model on its base. Finally, the inverse of the translation moves it back to where it was. The key idea here is that the object is first being moved to the origin. Once that's done, we can rotate around its axis with the r matrix. At this point, forget about that there's even a rotation matrix. The model's locations are now rotated. Once that's done, we can move it back to the same position as before, with the inverse of the translation. A 3D virtual world has three major components: objects, lights and cameras. This unit is going to explore lights. The next, cameras. These components are on opposite ends of the rendering process. Photons are generated at light sources and the photons that reach the camera are what form the image. The objects in the scene are what ties these two ends of the process together. Light makes right. I love this little play on words that Andrew Glassner made up back in the 1980s. It succinctly captures a kernel of truth about computer graphics. The more you can determine about how light moves through an environment, the better your results will be. I've talked a bit before about light, and how it bounces around a scene, eventually reaching the eye. The idea is that each light, often called an emitter, sends photons out. The fate of each photon is that it's absorbed as is, or reflected. An object can absorb light and in fact, almost everything absorbs at least a little light. Not even the highest quality mirror reflects 100% of the light hitting it. The numbers are usually more in the 90 to 95% range. Along the way photons can be absorbed by dust, water droplets, or other particles in the air. Or the inside of translucent objects themselves. This simple photon model ignores various effects such as polarization and fluorescence. But that's generally fine in indirective computer graphics. Things are complex enough. We can see objects precisely because photons ultimately travel from them to us. The reason objects have different colors is that each varies in how much it reflects various wavelengths of light. Where we do have some choice in computer graphics is what light paths we follow. The two paths we've used so far are light to surface and surface to eye. We've been a bit more elaborate with surface to eye in that we've allowed transparent filters along this path to modify the color. Light emitters can take whatever form you like since, ultimately, the effect of a light is something computed in the vertex or fragment shading. That said, there are a few different common forms of lights used in applications. The simplest is called a directional light. This type of light is defined by just that, a direction vector. A directional light is something like the sun. The direction to the sun is essentially the same for every object on earth, and a directional light is considered infinitely far away. In many modelers, such as the 3JS editor here, the directional light will be shown as some local point that you can move around. However, you're truly manipulating the direction, not the location of the light. Since we often combine colors in various ways, there's not necessarily an upper limit of 1.0 to a color channel. It all depends. For example, if I want a clear the screen, I would want to use values from zero to one. There's an upper limit of each channel's intensity, so values larger than 1.0 are beyond the monitor's abilities. However, if I wanted to make the light bright, I might set its intensity to 11. When the distance from the light to the object and the object's color are taken into account, the final color may well be within the zero to one range. In some systems, you can even set the light intensity to a negative number, meaning the light is somehow sucking photons from the surface. The light's color is just a number in an equation after all. This is true for a material's color too, that it could be set however you want, but we tend to leave material colors between zero and one, thinking of them as reflectivities. These basic illumination equations are not based on any physical unit, such as watts or lumens, but rather are set in the zero to one range for our convenience. In 3JS a directional light is created and added to a scene by code like this. The 1st value is the color of the light, in this case the light yellow. The second is the intensity. Setting the light's position is how you actually set the vector, by setting 200, 500, 600 we're saying that the light, which is infinitely far away, comes from a direction that goes from point 200, 500, 600 to the origin. Note that the length of the direction vector does not matter. We could of just as easily set 2, 5, 6, or even smaller for the so called position and we'd get the same result. I've ripped the lighting out of the drinking bird scene so that you can set it up. Your task is to put a directional light into FillScene. The direction to the light from the origin passes through the point negative 200, 200, 400. Make the light white with an intensity of 1.5. When you're done, the scene should look like this. The code for this one is just this bit. Create a directional light with a white color and intensity of 1.5, position it so that it's coming from direction negative 200, 200 negative 400, and then add light to the scene. A point light is what it sounds like. You define a position for it in space and it gives off light in all directions. You can set the color and intensity just as you can with directional lights. These lights are different from real world lights in that by default the distance from the light does not effect its brightness. Attenuation is not done mostly because it's easier to quickly light a scene with point lights if there's no drop off with distance. It's entirely possible to use a different equation for how the light is attenuated by the distance. For example, you could divide the intensity by the distance or the distance squared, which is how real lights work, or any other combination. 3JS supports only one drop-off mode, which is to define a maximum distance. At this distance, the light goes to zero. This is entirely non-physical, but is useful for the artistic control of lighting. You can put point lights in a scene and have each affect a local area without affecting the lighting in other parts of the scene. Think about being in a room at night with a lamp on your desk. You drop your pencil under the desk. The only reason you can see the pencil is that light from your desk lamp bounces off the walls, ceilings, and other surfaces. Some of this light makes it to beneath your desk and then gets to your eye. One reason we don't use lights that drop off at the square of the distance is that light doesn't bounce around in our scenes, but only effects what it directly hits. Since there's no light bouncing around it dies off very quickly within our simplified system. We compensate for this lack of reflected light, called indirect illumination, by adding a fudge factor called ambient lighting. In 3JS we set this light by giving it a color. The light color is multiplied by the material's ambient color to give a solid amount of fill color. Remember that the ambient color on the material is actually a separate color in 3JS. If you don't set it, the color is white and the ambient light will simply add some grey to the object as a whole. Grey's okay but it's better if you set the ambient color to match the diffuse color. You can then use the ambient light's intensity to change the feel of the scene as a whole. Creating a point light in 3js is simple enough and you can look at the documentation page for more details. In this exercise the code is set up so that the scene is lit by only ambient lighting. It's quite flat as you can see and details lost. What I want you to do is turn off the ambient lighting and turn on what I call a head light. Think of it as a miner's light, something you put on your head to look around in the dark. Instead of a flat ambient light, you have a light that is guaranteed to illuminate everything you see since it's located where ever you are. If you search for the word headlight in the code,you'll see where you need to modify things. There are two things you'll need to do. The first is to add the proper type of light to the fill scene method. Use the headlight variable which I've declared at the top of the code for you. Declaring it at the top makes this variable usable in any of the methods in the code. Make the headlight a full white color and intensity of 1.0. The second task is to make the light's position always match that of the camera for every frame. If you look in the render method you'll see this code. This method is called every time a new image is to be rendered. Don't worry about the delta and the camera control calls. Leave those alone. The key line here is the render objects render call. This call is what actually renders the scene. What you want to do is to make the headlight's position math the camera's position before rendering. By doing so, the headlight will then shine on everything visible, bathing it with illumination. The surfaces then are not all the same dull solid ambient color, but have some definition to them, which looks more realistic. Both the light and the camera are derived from object 3D so you can access the position of both of these in the same way. In fill scene, you need to delete the ambient light or at least comment it out. And then create a point light and add it to the scene. The position of this light initially doesn't matter, since it'll be moved to match the camera's position. In the render method, you copy the camera's position to the headlights. Now, when rendering takes place, you have a headlight giving a varying shade to the visible areas. In practice, you may want to set the headlight's intensity lower, since its effect is fairly obvious. A spotlight in 3JS is similar to a spotlight in the real world. You shine this sort of light on something to make it the focus of attention. This emitter is like a point light in that it has a position. It's also like a directional light, in that you have to point it somewhere. However, there's one important addition. Control of the cone of light it forms. The main control in 3JS is the angle where the spotlight ends. There's also a fall off exponent that's similar to how specular highlighting works. As you increase this number the spotlight becomes tighter. More elaborate spotlights are possible. For example, this is 3D Studio Max's spotlight. It uses two angles to control the outer cone, and a separate inner cone it calls a hot spot. The spotlight can also be circular or rectangular. There are no rules about what constitutes a spotlight, and with shader programming, you can create any sort of spotlight you can imagine. I should also mention, at this point, that you can put any number of lights in a scene. There's only one ambient light setting, but you can have as many spot, point, and directional lights as you want. Each light's contribution is added to the fragment's final color. Let's show off the drinking bird. Replace the directional light in this scene with a spot light having the following characteristics. Don't worry, these are in the comments, too. I recommend reading the documentation for how to set the target position, which is where the spot light is pointing. When you're done the scene should look like this. It's not a huge change, but does look more dramatic. Things will get even more interesting in a minute or two. Here's my solution. One thing to realize is that the angle must be set in radiance. The other is looking up how to access the target position. I added this information to the online documentation after I had problems myself. I recommend the same to you. If you find a problem in the 3JS documentation, fix it, and submit it using GitHub. You'll help at least a hundred other people. At this point you might want to experiment with a spotlight and play with it's various parameters. Either in this program or using the 3JS editor. A problem with adding more and more lights to a scene is the expense. Every light you add means yet another light that must be evaluated for the surface. One way around this is deferred rendering. Look at this demo. There are 50 lights in the scene, and it runs just fine. Normally, you render a surface, and the fragment color for each pixel is stored, if it's the closest visible object. This is often called forward rendering. In a deferred rendering algorithm, you instead store data of some sort in each pixel. There are many variations, with names such as deferred shading versus deferred lighting. And here's just one. You could store the position, normal, and material color and shininess of the closest surface at each pixel. You also draw into the Z-buffer as usual. I'm not going to get into the details of how you store these various pieces of data, the point is that you can do so. It's just image data in another format. With deferred rendering every point light in the scene has an upper limit as to how far its light goes. This distance forms a sphere. So a sphere is drawn in a special way for each light. Another way of saying this is that each light can affect a volume in space. Whatever surfaces we find inside the sphere are affected by the light. Each light effects a fairly small number of pixels on the screen, namely whatever area the light sphere covers. This means that a huge number of lights with a limited radius can be evaluated in this way. By drawing a sphere, we're telling the GPU which pixels on the screen are covered by the light and so, should be evaluated. There are variants on what shapes you draw. A circle, a screen aligned rectangle. Whatever is drawn, the idea is that the geometry's purpose is to test only the limited set of pixels potentially in range of the light. This is as opposed to standard lights, where every light is evaluated for every surface at every pixel. I hope this give you a flavor of how deferred rendering works. I'm really jumping the gun, here. You need to know about shader programming to implement these. But the idea is to treat lights as objects that are rendered into the scene after all object surface values are recorded. There's some problem cases for preferred rendering techniques, such as transparency, but it offers a way to have an incredible number of lights in a scene. In both Egyptian and Greek mythology, it's said that art was invented by someone tracing the shadow of a person on a cave wall. Shadows give a scene more realism and solidity. An object's shadow firmly establishes its location in relation to its surroundings. Rasterization is focused on triangles rendered from the eye, so we have to work a bit to generate shadows. One popular method of adding these is called shadow mapping. This algorithm dates way back. It was first described by Lance Williams in 1978. The idea is to render the scene from the point of view of the light. Whatever the light sees, is what gets lit. Remember our firefly with human eyeballs? This is where that answer comes into play. If a light doesn't see a surface, that means the surface is in shadow. In fact, this is exactly how the basic algorithm is done. The scene is rendered from the view of the light. And the distance of the closest objects is stored at each pixel. Because the light has to look in a particular direction, and has a limited field of view the light source used for shadow mapping is often a spotlight of some sort. A spotlight can be pointed in a direction, and has a constrained view of the world. In 3JS the only lights capable of casting shadows are spotlights and directional lights. For directional lights, you specify the limits of how wide the light extends. The shadow map algorithm creates a shadow image of the scene from the light's view, called the shadow buffer. What is this image most like? Is it like a regular scene's alpha channel, fragment shader, z buffer, or color buffer? The depths of objects in the scene are stored in the shadow map. This image is most like a regular scene z-buffer, which stores the distances to objects. On the left is an example of the shadow buffer's stored depths with black being closer to the light. On the right, this shadow map is used to detect from the camera's view which pixels are in shadow and which are in light. Turning on shadows for a scene is pretty straight forward in 3JS. You enable the shadow map on the render itself, and tell the spotlight to cast shadows. For each object you have a choice. You can have it cast or not cast a shadow, and you can have it receive or not receive shadows from other objects. For example, if you know a ground plane is never going to cast a shadow, you can save a little. Processing by not setting the castShadow flag to true for it. One feature of 3 JS is that you need to set castShadow and receiveShadow for every piece of geometry. If you have an object3D that contains three meshes, you need to set the flags for each of the meshes. Setting the flags on the object3D itself will do nothing. This is actually a constant struggle in seeing graph systems. What wins? If I set an attribute on a parent and on a child, does the child setting win over the parent or vise versa. 3JS avoids the issue by simply not letting the parent have any say. It's tedious to go through every object and have to set these flags, especially for something such as the drinking bird. Happily, 3JS has a traversal method you can call that will walk through an object and all its descendants. Children, grandchildren, and so on, and do whatever you want with them. Here's the snippet I use. This code sets the cast and receive shadow flags true on every object that makes up the drinking bird. It''s actually doing a little bit too much, as setting these flags on objects without meshes it doesn't do anything, in other words on parents. Here's a better code. This is some real live Javascript, checking if an object has a mesh. If it does then the shadow flags have meaning for it, so we set them. Give it a try now turning on shadows for the spotlight you added to the drinking bird scene. I've kindly set up the cast and receive shadow flags for the drinking bird itself, but not for the ground plane. I'm also leaving it up to you to figure out where the various call to enable shadowing go. For this exercise I've also added a little sphere representing the light source in the scene. If you move out you'll see it hanging in the air. Doing this for your lights is a handy way to get a better sense of where you're placing them. There are a few small bits of code to be added. To the spotlight itself, you need to add that the light casts the shadow. The solidGround object was not receiving any shadows, so you need to set this parameter to true. You don't need to have the ground cast a shadow since it's below everything else. Finally, the renderer itself must have shadows enabled, which is added to the list of renderer initialization settings. Take a look and examine the legs of the model. I expect you'll see some odd shading on them. If you make the headlight brighter, the effect will become more obvious. Moving the spot like this is likely to shift these patterns around. The shadow mapping algorithm works by seeing the world from the lights point of view. The depth image form is then checked during normal rendering. The depth of the surface visible at a point is compared with the transformed depth of the shadow map buffer holds. And that's where all the trouble begins. The pixels in the Light's image of the world don't exactly match-up with those in the Camera's view. These cause some form of what's called Surface Acne, where little dark dots or patches appear in areas that should be fully lit. Take this example. The Light's view of the surface says that it's 3.80 meters away from the light. The Camera's view of the surface says that the location is 3.81 meters away from the light. Since this value is slightly higher than the Light's view. The surface is considered in shadow. The surface is something that should be impossible. It shadows itself, as the depths don't compare as well as we would like. Here's a demo showing the problem, as well as a partial solution. On these surfaces, you can see patterns of acne, where the geometry is shadowing itself. One obvious solution is to add some tolerance value, called the shadow bias. If the cameras computed distance along the lights view plus this bias is less than the lights depth, then consider the surface lit. Unfortunately, a simple bias leads to another problem called Peter Panning. Just like Peter Pan the object starts to look as if it's detached from its shadow and sometimes as if it's hovering above its true location. In the 35 or so years this algorithm has been around, many different solutions have been proposed to this problem. There are other problems with shadow mappings. Such as the edges of the shadows looking pixelated. And solutions such as percentage cluster filtering and cascaded shadow maps. I won't explain these here except to say that 3JS has both of these solutions built in. So far we've examined what are called local illumination models, where an object is affected by a light, and the result is sent to the eye. Light comes only from, well, light sources. No light is reflected from other objects. There are many more light paths that can be tracked. One rendering technique that can simulate these is called ray tracing. We've seen how the GPU sends each object triangle in turn to the screen and rasterizes it. Ray tracing instead fires rays from the eye through each pixel. Objects are tested against each ray. The closest object found along a ray's path is the one that's used to compute the ray's color contribution. You can think of each ray from the eye as rendering a single one by one pixel. One way to find the closest object for this pixel is to send every triangle down the pipeline in attempt to rasterize it. Using the Z buffer to keep track of the closest object. This is perhaps the slowest way ever to perform ray tracing. In reality, researchers and practitioners have spent huge amounts of time creating and tuning algorithms to rapidly find this closest object along the ray. Where rasterization gets some of its speed is from the fact that a single triangle usually covers a bunch of pixels. Various intermediate results can then be shared when processing the triangle. At its simplest, ray tracing can give the same result as rasterization. Each ray from the eye finds the closest object along it. The effect of light on the surface is computed and the result is displayed. However that's just the starting point for ray tracing. For the light to surface pass, we've made one obvious simplification for basic rasterization. Objects don't block other objects. In other words, no shadows are cast. In ray tracing, adding basic shadows is trivial. Shoot a ray from the surface to the light. If there's something in the way, the light is blocked and can be ignored. Since all computations happen in World Space we can avoid many of the precision problems found in rasterization techniques, such as shadow mapping. Classical ray tracing also offers us the ability to create true reflections in glass effects. For shiny surfaces, we can spawn a new ray in the reflection direction. Whatever this ray hits is what is reflected in the surface, and so can be factored into the ray's final color, its radiance. Let's say our sphere is actually glass. Instead of simple filtering, we can also spot a refraction ray and see where it hits. When it encounters the other side of the sphere and exits, we spawn yet another refraction ray in the reflection ray. We can continue to spawn rays until we hit some limit. There are whole books written about this algorithm. Though at its simplest, it can fit on the back of a business card, such as this old one of Paul Heckbert's. There are even ray tracers with amazing animated effects that are written in 256 bytes, such as this demo scene program called Tube. Well, actually, this program is only 252 bytes. The last four bytes spell out the author's name, Baze. This is one of the most amazing programs I've ever seen. This is as far as I'm going to go with ray tracing, but you now have a taste of its power. Shadows, sharp reflections and refraction can all be added easily. There's been much work to make ray tracers generate images at interactive rates for complex scenes. We're not quite there yet for some applications, and there are headaches with keeping a constant frame rate in reasonable quality. But we'll clearly see more progress in this area in the future. Classical raytracing gives us a lot of paths from the eye into the environment. However, there are various paths ignored by this basic algorithm. What paths are ignored? Paths from the eye to the background. Paths from a light to an object to a diffuse object. Paths from a light to a mirror to an object. Or paths from the eye to the light itself. Check all that apply. The path from the eye to the background is pretty trivial and easily handled. An empty scene is about the easiest thing to render, so this path is not a problem. The path from a light to any object to a diffuse object is not well handled by ray tracing. If you ray trace from the eye to the object, you don't know to shoot a ray in this direction. Objects can be reflective, but when the light's contribution to a visible surface is computed, only paths directly from the light to the surface are checked. So, this answer is indeed true, it's ignored. For this third path, imagine you're viewing the world from a point on the surface. If you think about it, just about every surrounding visible object is reflecting light at the surface, not just the light sources in the scene. Determining which paths could possibly contribute more light is difficult. Any paths from the light source are ignored with classical ray tracing as there are a huge number of them. So, this case is ignored. The fourth case is easy to handle. Even rasterization typically handles lights by giving them a fixed solid color. And classical ray tracing can treat them in the same way. One solution to tracking the various paths that light can take to reach the eye is to shoot a lot more rays per pixel and bounce them around in hopes of finding light sources. This is called path tracing. It can be very noisy to start with but can give correct results with enough time. This is a real-time demo by Evan Wallace in WebGL, one I recommend you try yourself. Notice the color bleeding effect of light bouncing off the walls onto the spheres. You can also make objects have glossy reflections instead of the sharp ones we see with classical ray tracing. This demo uses what is called progressive rendering, shooting more and more rays for each pixel and blending in the results. The longer you wait, the better the image gets. You can even use path tracing to render scenes form MineCraft. Here's one I made using the free chunky path tracer, giving a beam of light effect down the scene. A basic pure path tracer is pretty straightforward to write. You generally shoot more rays in sensible directions and sum up the light contributions found. Where all the time gets burned is that you're shooting tens of thousands of rays per pixel or more. This scene took about 16 hours with 12 CPU threads and it was still a bit noisy at that point. I should mention there are many other algorithms such as photon mapping and bidirectional path tracing that work to get the best of both worlds. The general idea is to send light out from emitters using rays depositing radiance wherever the rays reach. The scene is then retraced from the camera's view and the emitted light is gathered. Algorithms such as path tracing can give unparalleled realism, given enough time. Set up properly, a rendering can be a true simulation of how light percolates through a scene. Not just an artistic approximation. I used to work on a global illumination system back in the 80s and 90s. Even back then we knew that path tracing could, given enough computer cycles, get the right answer. The running joke was that we were mostly just biding our time, coming up with optimizations for other algorithms while wait the 50 years needed for computers to get powerful enough to shoot 10,000 rays per pixel to get the right answer. Nowadays I think you probably need more like 100,000 or a million rays per pixel, but by my watch, I just need to wait another quarter century for rendering to be over, and then the singularity comes anyway, assuming people don't put more objects in their scenes. One feature of shadows that is not captured well by basic shadow mapping is how lights truly cast shadows. All lights have an area, but shadow mapping assumes the light comes from an infinitesimal point. Unless the real light itself has an extremely small filament and no diffusing bulb surface, treating it as a point of light is a weak approximation at best. The area fully in shadow, where no part of the light is visible, is called the umbra. Real lights cast shadows with areas that are neither fully in shadow nor fully in light. This area of the shadow is called the penumbra. An interesting way to look at the problem is from the point of view of a location in the penumbra, however much of the area of the light is visible correlates to how much light the location receives. For example, the green point on the ground here can see about half of the emitter above, so it will receive about half the emitter's light. You have the worry about the angle of the surface normal to the light, the light's distribution of energy and so on, but the major factor is how much of the light's surface is visible to each point. In this unit, we're covering three.js's lighting capabilities. However, computing more elaborate forms of illumination are possible. To give you a glimpse at some of these higher end techniques, I've asked David Larsen to show off some of his work. I'm here at the Autodesk Gallery with David Larsson. David was the lead developer on a product called Beast for game developers. Many of the lighting demos you'll see in this unit were developed by David. So, David, as a start, what's your background? &gt;&gt; I got a Commodore 128 as a Christmas gift. My brother and I got it together. And we started programming and exploring this basic environment, which is, which is basically the operating system, but also a programming language, which you end up in, when you boot up one of those computers. And, I was kind of reading over the shoulder, and asked my brother to, you know, type in listings from computer magazines and instruction manuals and we, you know, started making programs. And that's kind of how we got started, programming together. And when I was growing older, I got involved in something called the demo scene, which is basically, just teenagers trying to outdo each other with graphic effects and visual effects on, computers. And that was a really good boot camp for, for learning about graphics programming and and programming in general and also like, you know, running projects. It was a really effective way of, of learning all those tools that you need in order to, to become a, a good programmer. So a few years, I had a time, when I was in college I also temporarily worked for a game studio. And then, some time, when I was in my last year of college, a friend, actually another guy I knew from the demo scene, he asked me, if you, I wanted to come work for, the company they had, and that company is, was, Illuminate Labs. which is a, at that time we were developing a renderer that was kind of a general purpose rendering, renderer. And at some point we realized we have to narrow, like focus group, and what we did was target it towards game developers, and you know, things got much bigger, and eventually we ended up being acquired by Autodesk. That's where I've been working for the last two and a half years. In this session we're going to take a look at shadows. This is a little environment i built and it's being lit by a directional light. When doing trivial graphics rendering, which I'm doing here, shadowing is typically not supported. For instance we can see that light leaks in under the little roof in the back of the scene. so let's lay a little shadowing. Now we can see that the scene is being properly shadowed. Parts of the environment that were previously incorrectly lit are now dark. This happens because we are now disabling light if there is something between the light source and the geometry. By rotating the light, we can also place the shadow where we want it. But there is still something wrong with the shadows. View live shadows are typically not as sharp as what we see here. Now we have enabled soft shadows. This looks much more realistic. Perfectly sharp shadows come from assuming the light sources infinite this small. In real life, light sources have size. And by taking it in consideration, we get a soft shadow border. This shadow border separates the shadow into two parts. We call the completely shadowed area the umbra. And the soft part with the gradient the penumbra. The penumbra happens in regions of the surface where the light source is being partially occluded. Also note how the penumbra has been small when the lit point is close to the occluder, and large when it's farther away. This wraps up the part about shadowing. Thanks for your attention If you look at the shadows in this scene you can see they're completely black. This is typically not what reality looks like. As an example, the floor under your desk is typically not completely black even if no light from any specific lamp is hitting it directly. The classic hack for working around completely black shadows is the flat ambient light. It basically means adding a constant color to all pixels. As you can see, the shadows are no longer black but still they look flat. As an example, you can't even see where the edge between the wall and the floor is in the shadow. Having a constant color like this doesn't simulate how light behaves in reality. You wouldn't be able to buy a lamp like behaving like this at Ikea. Let's see if we can do better. So how do light behave in real life? When you look at the point in the scene, what is it you actually see? It's the light from the light source being reflected into your eyes. But only a fraction of the light reflected from the surface is actually going into your eyes. This bounced or indirect light is a big contribution of light in a scene. Let's enable simulation of it. Now the shadows looks much more interesting. What's really going on is that light hitting a surface is being reflected back into the scene. This light can then be reflected back again and again. Since the walls absorb light with each bounce, each secondary bounce is weaker than the previous one. [BLANK_AUDIO] You can also see how the light hitting the red wall is being reflected back into the scene as red light. This is what we call color bleeding and it happens because the red wall absorbs all but the red component of the light bouncing off it. To properly simulate indirect lighting for complex scenes can be very time consuming. Single frames of high quality rendering can literally take hours and even days. This wraps up the ambient and indirect like part. Thanks for listening. Check all of the following that require global illumination. Light through stained glass hitting the floor. A Lambertian material such as Spectralon. Patterns of light at the bottom of the swimming pool. A surface evaluated using the Blinn-Phong illumination model. Ultimately, all materials can be globally illuminated. What I'm looking for here are those situations where global illumination techniques are required. Oh, and what's Spectralon? See the additional course material if you want to know about this material. Though it's really not all that important. Global illumination is when one object affects another's appearance. Light traveling through stained glass and hitting the floor is a case of global illumination as the glass affects the color of the light hitting the floor. Similarly, the bottom of the pool has caustics placed where the light gathers due to the water rippling. Even an object casting a shadow on another object is a form of global illumination, in that one object effects the other. The Lambertian, in other words diffuse, and the Blinn-Phong equation are local illumination models. They are computed using just the information from the object itself, not using other surrounding objects. One light source in 3JS that attempts to simulate the surrounding bounced light in the scene is the Hemisphere Light. Instead of this light having a particular position, it's considered to fully surround the scene with light coming from every direction. This is also how you can think of ambient light, but with a difference. With a Hemisphere Light, you assign a light color to the top and a different color to the bottom. Each object's normals determine the color of the light it receives from this light source. If the normal points upward, the light's color is the top color. If the surface normal points straight down, the bottom color. Any direction in between gives a blend of these two colors. The idea is that you can assign the top color to be something like the sky, the bottom to be something like the surrounding terrain. It's basically a more elaborate for of ambient lighting. When we talk about textures we'll see a more involved version of this scheme called a Diffuse Map. A source of light we tend to take for granted and not really think about as a light source is the sky. Especially for outdoor environments, a lot of the ambient lighting is actually sky lighting. So let's enable it in this scene. As you can see, the shadows gets a nice blue touch here. Let's disable the sun in order to isolate the sky lighting. [BLANK_AUDIO] Areas where more of the sky is secluded, like under the roof in the back of the scene, is getting generally darker. But it's a very smooth effect that also accentuates details and geometry. The most trivial example of the importance of the sky as a light source is a cloudy day. Just like in the middle of the night, the sun is not reaching in, but on the cloudy day, it's not dark outside, because of the light from the sky. It should be said that the light from the sky is mainly light from the sun taking a different path down, but we won't cover the details surrounded here. For most application you want to treat the sun and the sky as two different light contributions. Sky lighting is a very effective way of setting the mood in an environment. We can experiment with different colors and it makes a big difference [BLANK_AUDIO] In this case, we're only looking at constant color skies, but for more realism, it's possible to use images representing the actual sky color from different directions. Unfortunately computing proper sky lighting in interactive applications can be hard from performance point of view. Different flavors of ambient ablution can take you part of the way though. This wraps up the sky lighting session. Thanks for your attention. One phenomenon that's somewhere between a light and a camera effect is fog. In the real world, the atmosphere can absorb and reflect light in many different ways. 3JS has two simple forms of fog. One is called linear fog and we've been using that in a number of demos. A minimum distance is specified for where the fog starts and the maximum for where the scene is entirely fog-colored. Anything in between these two distances gets the fog-color blended in. The other type of fog is exponential. This is a bit more like real life in that you specify a density particles. This value effects how the fog increases somewhat realistically. Both kinds of fog are useful in limiting how far we can see into the distance. There's only so much content a program can load and without other clever tricks the user will see the limits of the world. There are plenty of other types fog and atmospheric effects possible. This demo shows what are called god rays, a beams of light type of effect. See the additional course materials for more about this particular effect, which uses post processing image based shaders. To finish up, try out the fog demo that follows. You can choose from among the different types of fog and change the related parameters and fog color. There are many different types of lights you'll encounter in various applications. Programs for lighting design will include all sorts of data. Such as how particular fixtures affect the distribution of light from an emitter. The other thing you may run into, is different terminology. Here a light source, called an omni- light is being moved around the scene and modified. What's the 3JS name for this sort of light source? Is it a directional light, point light, spot light, or hemisphere light? The lights location moves around in the scene and the light emits in all directions. This describes a point light. In this exercise, you'll start with a directional light pointing at the back of the bird. It doesn't look very good. I want you to hook up a slider to the light. Use the slider to vary the light's x and z direction by the cosine and sine of the angle. In other words, x is related to cosine, z is related to sine. And by the way, save your solution. You'll reuse it in the next exercise. When you've got the code running, the program should look like this as you vary the angle. Also don't forget the cosine and sine want to be fed radians, not degrees. The code needed is put in the render method. Notice that the light position set will not be normalized, the y position is always one. But this doesn't hurt anything. In astronomy, the attitude is the vertical angle, and the azimuth is the angle around the horizon. You're job is to add another slider to also very the altitude angle of the light. To get it right you'll have to take into account that the x and z components need to be scaled down as the y component increases. So that the direction is normalized. In other words, the light direction should be x squared plus y squared plus z squared equals 1. Another handy formula you'll find is that, the cosine squared plus sine squared equals 1. When you're done, the light should now operate with altitude and azimuth angles. Another way to think about is that altitude is similar to latitude on earth, azimuth is similar to the longitude. The code first computes the altitude. It then uses this position to figure out how much length is left to share among the other two coordinates, x and z. Their positions are computed and multiplied by this length. Since cosine squared plus sine squared equals 1. We know that all this length will be used up between the x and z components. In three.js what attributes does each type of light have? The attributes are color, intensity, and direction, for the ambient, directional, point or positional lights, and spotlight. All lights have a color. The ambient light doesn't have an intensity as this is a special light meant to fill in areas of shadow and should not be cranked up. All other lights have an intensity parameter. Directional and spot lights have a direction. Ambient and point or positional lights do not. Remember the spotlight and shadow exercise? If you look at the code, you'll notice a mesh phong material is used for the ground plane. This material is something we normally need for specular highlights. However, the ground plane is not shiny. So say we change this material to mesh Lambert material. Strangely enough, the spotlight disappears. But the shadow remains, if you look closely enough. Why did the spotlight disappear? The spotlight is evaluated per vertex, not per pixel. Shadows are baked onto the surface, the spotlight is not. The spotlight is actually defining a specular highlight. A spotlight can be thought of as a negative shadow. Even though the spotlight has disappeared, the way that three [INAUDIBLE] shadow system works in this case is that it dims down all kinds of contribution including ambient. The reason the spotlight itself disappears is that the Lambert material is normally evaluated per vertex for illumination. The ground plane has just four corners all of which are outside the spotlight's area of influence. So, just the ambient light is applied. The shadow in the scene isn't baked on. It's in fact evaluated per pixel. But that's a separate part of the illumination process in this case. A spotlight is not a specular highlight, it effects diffuse and shiny objects, alike. Finally, you can think of the area outside of a spotlight as being in shadow. But that doesn't explain why the Lambert material effects it. If you use the Lambert material and were to test like the ground plane into a grid of squares, you'd start to see the effect of the spotlight as you can see in these images. Going from left to right, the ground plane is [INAUDIBLE] more and more, catching the spotlights effect better and better. However, evaluating the spotlights effect per pixel instead of per vertex, means many fewer triangles needed to be generated and processed. There are all sorts of different ways to view the world. Here are three, a fish-eye lens, a perspective view and an orthographic view. These are all different types of projections. A projection maps locations in the world to locations on the screen. A fish-eye projection is possible to simulate in computer graphics. Hey, just about anything can be simulated in computer graphics, but it's rarely used in practice. These other two projections are more popular for a number of reasons. First they act similarly to a how a camera captures a picture. So we're used to them. Straight lines stay straight unlike a fisheye lense. All in all the math is simpler. A perspective view we'll usually associate with seeing the world. Distant objects get smaller. An orthographic view is much more common in design software. In an orthographic view objects do not get smaller as they get more distant. Parallel lines in the real world stay parallel in this kind of view as opposed to perspective where they do not. This orthographic projection isn't normally how we directly perceive the world, but it's a form we commonly use nonetheless. For example, assembly instructions usually show an orthographic view. Architectural plans showing a front, side or top view of a building use this sort of projection. You can take measurements off of these types of views. Having the distortion caused by perspective projection would most likely hinder our understanding of what we're seeing. An orthographic camera can be used to examine objects from angles other than directly along an axis, such as these views. A demo using an orthographic camera follows. I find that I'm so used to seeing with a perspective view that I perceive an optical illusion here. If I zoom out using the slider and look at the whole scene, my visual system tells me that the front edge of the grid is definitely shorter than the back edge. Try it yourself, and let me know in the forums what you think. To define an orthographic camera in 3GS you do two operations. One is to define a volume in space. The other is to move and orient that volume of space as you wish. This first piece of code creates the orthographic camera and defines the rectangular box of space it's viewing. This box is defined by the left to right width limits. The top-to-bottom height limits and front-to-back limits, all in world-space coordinates. These correspond to the x, y, and z axes. The camera can be thought of being positioned on the plus z end of the box defined. Looking down the minus z axis, with upting the y plus direction, as usual. For convenience, I define a view size, which is how much vertical space I'd like to fit in the view. The aspect ratio is just that, what's called the aspect ratio of the window. It describes how wide the view is compared to how high it is. For example, if your view was 1000 pixel wide and 500 high the aspect ratio would be equal to 2. In other words you'd want your box to be twice as wide as it is tall. If you don't, you'll get a stretch view of the world, which is probably not what you wanted. What all this code does is defines a box in space. In this case it happens to be centered around the origin. By the way, I've seen some demos just use the camera's width and height directly as the camera size. Don't do this. You're setting the camera size in your scene in world coordinates which is most definitely not the same as the width and height as your canvas in pixels. If we stopped here and put our drinking bird in the scene, we'd get a side view of the bird. What we'd now like to do is position and orient this camera in space to make a more interesting view. This code does that. The first [UNKNOWN] moves the box by this amount. It's a translation. The same as for a normal object. The camera controls code does two things. First, it sets up what sort of mouse controller you want for the camera, if any. I came up with this custom orbit and pan controls class, extending the orbit controls class I found in 3JS. This control keeps the camera oriented, so that the plus y axis is always up. For a more unconstrained, but trickier to use control, the trackball controls class could be specified instead. In either case, the next line sets the target for the camera. The target is the location being looked at. This defines a new z axis direction for the Camera. The Y axis kept pointing up as best as possible. We'll talk about that later, when we look at the matrices forming this camera. Once we've positioned this box in space it defines what part of the scene we want to render. Whatever happens to be inside this box is what we'll see on the screen. The z buffer is applied as usual so that objects that are closer to the camera cover up what's behind them. This is just an overview of the basic idea. The camera defines a chunk of space defining it's limits and this chunk then gets positioned and oriented in space to make an image. I'll go over this process in detail in the following lessons Let's look a bit more in depth at how this box in space actually gets positioned and defined internally. First, we define our camera object as looking down the world's negative z axis and oriented so that the top of the camera is facing along the plus y axis. The camera starts at the origin, just like other objects. The first step is to move the camera to where we want to make an image from. We also tilt and orient the camera as desired. This is similar to how we have been orienting objects all along. We define some sort of rotation in translation to get your camera into place. We ignore scale, we're just trying to set up the camera in space. Giving the camera a position is simple enough. To compute the rotation needed to orient our camera, we use what's called a look at system. This is pretty much what it sounds like. You define a target location for what direction you want the camera to look. This defines the negative z axis for the camera's frame of reference. In other words, how it's pointing in the world. So here I have a real live camera, it even works. And with my z axis here, that tells me which way it's pointing, but unfortunately it doesn't tell me which way is up. It still has this one degree of freedom. So what we do, is we also define an up-vector of some sort. This vector gives us some guidance as to which way to orient the top of the camera. Typically in 3JS, we'll pass an up-vector such as zero, one, zero, the plus y axis as the up-vector. Given our view vector and this up-vector, we'd like to form a matrix that defines a whole frame of reference for our camera. We know where we want this z axis to look along. But we want to know where the camera's y and x axis end up. By specifying a general up direction, we do this. For example, say we have a camera pointing in some direction. Looking along its negative z axis. If we give an up direction, we can find the direction pointing to the right of the camera. Which is the x axis. By taking the cross product of this z axis and our up vector. Now we have two axis locked down. Camera z and camera x. We can find the true up direction, the camera's y axis, by then taking the cross product of the x and z axis. Remember that the cross product of two vectors will give us the vector that's perpendicular to both of them. This may all sound a bit odd and like a lot of work but is extremely common. It's probably the place where I use the cross product the most. The whole point here is that given a view direction and an up vector as a hint, we can get a frame of reference for the camera. We'll often use the worlds plus y axis as our up vector hint, but it all depends. Say we're a hawk soaring over some terrain. We're looking straight down, which would be looking down the y axis of our world. If we use the world's y axis as our hint, it doesn't make much sense. We define the negative z axis the camera's looking along, and the y axis for the camera's up direction. As vectors in exactly opposite directions. This is not very useful. In this case our up vector is more like the direction pointing out of the top of our hawk's head, the direction it's flying. Now that the up vector is not parallel to the look at direction, we can form a good frame of reference for the hawk. There's in fact a look-at method on the camera itself, but most of the time, we use the camera controls to let us move the camera around with the mouse. If you run the exercise code, you'll see a grid and not much else. The drinking bird has left the view. Why did he leave? Where did he go? We've had word that he's been sighted at these coordinates. Your job is to properly set the target to see what the bird's doing. You don't need to modify the camera's position or size itself. The solution is to set the camera control's target position to the proper coordinates. Once this is done, the bird's position is revealed. His quest for water is at an end. We've just shown how to position and orient a camera in world space. If we apply this matrix C to some camera object, the camera then gets placed in the world. However, we want exactly the opposite of this. We don't want to position the camera with respect to the world, we want to position the world with respect to the camera. Think about what's happening. We have some objects in a scene, they've perhaps had a number of modeling transforms applied to them. The objects are now all positioned in world space. What we want to do now is transform the world to the camera's frame of reference. Here's a camera and a model. Say we slide the camera five units along the world's x axis. From the camera's point of view, if we subtracted 5 from all the world coordinates, everything would be nicely oriented for the camera. The long and short here is that we use the inverse of the matrix that would position the camera in the world. Inverting basically says, no. It's all about me, the camera. Reorient everything with respect to me, me, me. This inverse matrix is called the view matrix. For this reason, this view matrix is sometimes written as the inverse of the camera matrix. The matrix that would move the camera to its place in world space. Another way to think of this view transform is simply as the very last modelling matrix in the chain. The M matrix represents all the transforms needed to get the object into world space. Modelling transforms. Now one last transform, the view matrix V takes the objects in world space and moves them into camera's frame of reference. If you deal with WebGL or OpenGL directly, you'll see these two matrices put together as a single matrix called the model view matrix. This matrix is applied to an object, immediately taking it from its own model space to view space. With the view transform in place, we now have all our coordinates in the cameras frame of reference. What we now want to do is define a box in this space to contain everything we're going to render. It's sort of like defining the size of a stage. As noted before, the orthographic camera's definition is to give a left, right, top, bottom, near, far set of values to define this box. These six numbers define the corners of a rectangular box in the view space of your camera. The first time I explained it, we made a box and then oriented it in space. What's happening as far as the matrices go is the reverse. We first orient the world to the camera's view of things. Then define our box with respect to the camera. In practical terms we usually make the left and right the opposites, along with the top and bottom. The near value is often zero, though can be any number. The idea is that the view matrix moved everything in the world to be placed relative to the lens of our camera. Creating the orthographic camera itself, makes a projection matrix. The box defined by the projection matrix is then in front of the camera and symmetric around its view axis. This box called the view volume in fact performs a projection. We're not just defining a box, we're saying take everything in this box and project it onto the plus z side of the box and make an image. You may have been wondering why are we looking down the negative z angle axis. This is done because it makes for a right handed coordinate system for the camera. The y direction is up. X is to the right. Giving a Cartesian coordinate system for the image formed. To keep things right handed, the z axis must be pointing toward the viewer. Meaning we have to look down the negative z axis. That said, three j s thinks about the orthographic camera is going in a positive direction. So the far value is specified as a distance along the negative z axis. Our chain of matrixes adds this projection to the front. In other words, the projection is applied next. When applied, a coordinate will get a new value in this projection's frame of reference. This frame uses what are called Normalized Device Coordinates or NDC for short. These NDC coordinates are in the range from -1 to 1 for X, Y, and Z. We'll talk more about this transform in a minute, but the point is, is that whatever is inside the box will have coordinates transformed to this new range. Here's the orthographic projection in all its glory. It really doesn't look like much. It turns out that the orthographic projection matrix is pretty simple and uses nothing you don't already know. So, that's the question to you. Does this matrix perform scaling? Does it perform rotation? Translation and projection? Check all that apply. My hint to you is to see the matrix zones lesson. By the way I want non-trivial answers here. For example, a pure rotation matrix has a translation of 0, 0, 0, that doesn't count. The answer is that scaling and translation transforms are being done. There are two ways to solve this problem. One is to look at the matrix and see if there is some sort of scaling and translation happening. The other is to realize what the transform is doing. It's defining a regular box in space by making it a different size along each axis, that's a scale, and by moving its origin elsewhere, that's a translation. Notice that if the left is equal to the negative of the right, which is the common case, the x translation value will be zero. Similarly, if the bottom limit is equal to the negative of the top, the y translation value is also zero. This means the box is centered along the negative z view axis, and so doesn't need to be translated in these directions. The perspective camera is where things get interesting. This camera is more like real life, with objects in the distance being smaller. That said it uses the same sort of pipeline as before. Internally you define a view matrix exactly the same as for the orthographic camera. However, then the projection matrix is formed differently. In a perspective view of the world, objects get smaller in the image, as they get further away from the viewer. Another way to say it is that if something is farther away, it needs to be larger in world space if it wants to appear the same size on the screen. This gives us the idea of similar triangles. Y2 divided by Z2 will be equal to Y1 divided by Z1, and so on. So perspective projection is going to involve division, but when we multiply a vector by a matrix, it's just some dot products, a series of multiplies and additions. This is where the fourth coordinate for our points comes into play. Up to now this fourth value has always been equal to 1. We run a point through any set of modeling transforms and the fourth coordinate value of 1 is left untouched. This all changes with perspective projection. Before explaining how projection works, let's talk about how the perspective camera is set in 3Js. It's similar to the Orthographic Camera, in fact, the creation call has few parameters. We know what the last three are. The aspect ratio is the image width divided by height, followed by the near and far planes. Remember the view frustum? It's back at last. The near and far distances are measured from the tip of the pyramid, where the camera is placed, down a central axis. Back when I was a young man, we called near and far by the names hither and yon, which I think is more poetic. You'll still occasionally see the terms hither and yon used in products, so keep an eye out. The first argument for the perspective camera is the field of view. This is the angle between the top and bottom planes of the view pyramid. Notice that in 3JS, this number is specified in degrees, unlike many other angles, which use radians. The field of view along with the aspect ratio fully described the locations of the four sides of the pyramid. For the orthographic camera, we define the location of every side of our volume box. Here the view is assumed to be symmetric around the central axis. In other words, instead of, a top an a bottom value, we need just a single angle describing both, the field of view. Same thing with the right an left, the aspect ratio along with the angle is enough. This is 3.js's way of doing things, an 99.9% of the time it's what you want. That said, it's entirely possible to specify each side of the pyramid. And webgl itself has a frustum that does just this. If you later change values on the camera itself, such as the field of view near or far planes, in 3js you need to call camera, update projection matrix in order to have these changes take effect. For most demos these camera parameters are usually set once on creation and rarely touched, so 3js doesn't spend any time checking them each frame. If you change these values while running your program, calling update projection matrix has three js evaluate these parameters and form a new projection matrix. Here's your assignment. Add a slider that controls the field of view parameter for the perspective camera. Use the dat.GUI library to set up the slider. Give the slider the name Field of View. This Field of View slider should have a range of 1 to 179 degrees, and start at 40 degrees. When you're done, the program should look like this. As you change the view, things should look considerably different. Once you have things working, I recommend you try moving around the scene. Compare changing the field of view with what happens when you use the mouse wheel or middle mouse button to move back and forth. There are two parts to the answer. We first need to write some piece of code like this to set up the slider itself. This code has run at initialization. In the render method we use the effect controllers field of view variable to set the camera's field of view, which is also in degrees. Once the camera parameters are set we call update projection matrix to make the camera changes take effect. We'll talk much more about field of view and exactly what it means, though I think you have a pretty good idea after doing this exercise. Here's the perspective matrix formed from the 3JS parameters, the numbers in the upper left three by three are scale factors, similar to how the orthographic projection works, though the z scale's a little different. There's also a translation factor for z. However, the big differences here are that there's a negative one in the last row. And the lower right corner now has a zero. I'm not going to derive this projection formula here. Most good 3D graphics texts run through this process. Also be aware that for some formulations, the near and far values are negative since you're traveling down the negative z axis. The one headache of keeping things in a right handed system is the whole plus is the axis points at the viewer problem. I'm using positive near and far values in this matrix, because that's how 3JS specifies things and thank heavens for that. In fact these two values must be positive numbers for the perspective transform. You can see things would get weird if the near value for example was somewhere behind the camera. For orthographic projections we can use whatever numbers we want for near and far. Even negative values since we're really just selecting a box in space. For perspective we're doing something more elaborate with transforming space so these values must be positive. The interesting thing is how this perspective matrix works with coordinates. Let's take a nice simple case. Field of view 90 degrees, aspect ratio one. near 1 and far 11. This gives us this matrix. Let's use this matrix on some test points and see what we get. So your task is to multiply this perspective matrix by three points and give the results. I want all four coordinates. Don't do anything beyond multiplying the points times the matrix, and don't forget that points have a fourth coordinate equal to one. Here are the answers. Instead of values of 1 in the last location, we have all sorts of interesting numbers. The four values produced are X, Y, Z and W. These are called homogeneous coordinates and they're used for projection. What we do next with these coordinate values is divide each value by the W of the coordinate. This is called the prospective divide or homogeneous divide. So for our three test points we had a value such as 0, 0, negative 1, 1. Dividing by 1 is simple enough. That gives us 0, 0, negative 1. We don't need to bother writing out the W value in the results, since W divided by W will always equal 1. For our next point, W is 11. Dividing all these coordinates by 11 gives 0, 1, 1. Our last point is a little more interesting. Dividing through by W gives us 0, 0.67, 0.83. Here are plots of the original points and view space and their corresponding new locations. Notice that the negative Z axis is pointing to the right for the frustum, and the resulting axis is plus Z to the right. Look at what's happened with these points and where they're transformed. They started inside or on the edge of our frustum. After the projection matrix is applied and division by W is performed the resulting points are in normalized device coordinates, anything in the range negative 1 to 1 for X, Y and Z is in the visible view volume. Let's take another example to show what happens to three objects that are the same size in world space but at different distances. When we transform to normalized device coordinates the relative area of coverage of the near plane stays the same. That is, the close object was half as high as the screen in our frustum view and transforms to half the height in NDC space. The second object is farther away and shows up as smaller. The third object on the back of the frustum is indeed much smaller than the others in normalized device coordinates. You might have noticed an interesting thing has happened to the depth of the second object. It started in the middle but it's moved backwards. We'll talk more about that in a bit as it's important. I left out a step that happens after projection and before division by W. Clipping. Say we have 2 points: 0, 4, negative 6 after perspective transformation that turns into 0, 4, 5, 6 and 0, 6, negative 4, which turns into this value. They form a line segment. These two points in this line segment are shown on this zoomed in part of our frustum. The second point is outside of the frustum. We want to have all coordinates inside our final view volume in NDC space so that we can render them. Clipping is done for line segments and triangle edges that poke out through the frustum. An edge can be clipped by any numbers of faces of the frustum. What happens here is that all the coordinates between the two points get linearly interpolated. For our example the point on the frustum faces halfway between our two points. The interpolated point is then 0, 5, 3.85. We then divide this point by W as usual to get a point in normalized device coordinates. Here's a rough overview of how clipping can work. You start with a triangle that's poking out through the frustum. Say the left face of the frustum first cuts the triangle. This creates two triangles. Later, the top face of the frustum chops off the other part of the triangle. This create three triangles total that will then be rasterized separately by the GPU. You as a user don't really have to know or care much about this clipping process. It happens automatically. It's worth knowing about mostly if you need to do similar testing operations on the CPU side. In computer graphics we often make a big deal about how we store 3D points in vectors as homogeneous coordinates, with the fourth element W. In reality, for almost every operation, the W value is either 0, meaning a vector, or 1, meaning a point. It's only after projection and during clipping that the W value is anything but 1. Once clipping is done and we're using normalized device coordinates, we're done with homogeneous coordinates. However, these homogeneous coordinates are important in that they're what the vertex shader produces. When the coordinate is transformed by the projection matrix, but before the division is performed, the coordinates are called clip coordinates. The vertex shader can produce other intermediate results, such as computing location once the model and view matrices are applied. It is required that the vertex shader produce a position on the screen for the vertex. This position is a homogeneous coordinate. The rasterizer then takes this position and performs clipping. Let's talk a bit about perspective camera control. The parameter with the most obvious effect is the field of view. As mentioned before, this value is the angle from the bottom of the view frustum to the top. It also adjusts the side to side angle, factoring in the aspect ratio. The field of view parameter acts something like a zoom lens. As you set this angle smaller and smaller, whatever is in the middle of the screen grows larger and larger. For these images, what I've done is have the field of view match the camera move more or less. In other words, as I zoom in on the model, I also move farther away so that the model takes about the same amount of room on the screen. As we zoom out the perspective distortion gets less and less. At the limit, that is, as we attempt to go infinitely far away and have the field of view approaching an angle of zero degrees the view becomes an orthographic projection. If you're practically infinitely far away, everything you're looking at is at essentially the same distance, as far as relative size goes. Which is how the orthographic projection treats this scene. This effect of having the field of view and the camera movement offset each other, has become a go-to effect in cinematography. If you want to express something that's weird or dreamlike happening in the scene you zoom in while moving backwards at the same time, or vice versa. Give this demo a try yourself and see how it looks. We'd say the view of the bird is pretty distorted. Really the problem is that you're not sitting close enough to the screen. It's sort of like being overweight. I could instead say I'm not dense enough. If I was denser, then I'd look thinner. Think of what the field of view represents. When you look at your monitor or tablet or mobile device, you're a given distance away from it. The screen itself is a certain height. This forms a real world field of view angle. A window on your screen is your window into a virtual world. Say this window in the real world has a 20 degree field of view for your real world eye position. If your virtual world also has the same field of view, you won't perceive any distortion. Objects toward the edge of the screen might look distorted if you were to back away from the screen. But at your current eye position, the field of view is perfect. The image is warped a bit in projection. But your eye is seeing the screen at a tilt that compensates for it. The formula for height for a given field of view and distance from the screen is height equals 2 times tangent of the field of view divided by 2, don't forget to use radians, times the distance. As the field of view increases, the tangent value increases, and so the height of the monitor increases. Increases. What tends to happen in video games is that the designer cranks up the virtual world's field of view so that the players can see more of what's happening around them. This can certainly lead to a more playable game, but this is also where the distortion comes from, from the fact that the virtual field of view doesn't match the real world field of view. If you moved your point of view so that you were at the apex of the virtual world's frustom right here, the virtual world would appear undistorted. Of course it might actually all be blurry at this distance, or you might get a massive headache, but that's the theory. We don't really need any formula for this one. This is 45 degrees and this is 45 degrees, so these angles must be 90 degrees. What that means is that these two legs of the triangle must be the same. So half of the height, distance equals 10. However, if you use the formula, that's fine too. You'll get the same answer. There are many ways to move a camera through a scene. We can walk or fly. We can look one direction while moving another and so on. Here we're moving a bit sideways and a bit forward while looking forward. I usually think of the camera as being in one of two modes, viewer-centric or model-centric. When things are viewer-centric the viewer is moving around through the world. When model-centric, a particular object in a world is being studied and the viewers goal is to see this object from different angles. In this mode we keep the camera focused on a location as we move. The place where we look is called the Target in 3JS. And in other systems. Note that this Target location is not at all necessary in making the various matrices need for viewing the scene. The target is much more related to user intent. Because the target is such a useful concept, many, but not all, camera control systems support the idea of a target in some way. To set the target we use the camera control's target parameter. We've set this parameter before. It provides the point that our camera is looking at, so setting the view matrix. By setting the target in the camera controls, you not only set the camera's view transform, you keep the camera pointed at this location when you orbit. If the object is moving, you can update the target each frame, and the camera will stay trained on it. Let's define some terms for various camera moves. Here's a top down drawing of a viewer and an object. When you adjust the field of view, that's zoom. When you move directly towards or away from an object you're looking at, that's dollying, it's not zooming. I have a zoom on my camera does not mean, if I want to make something larger, I run toward it and I yell zoom. It's so easy in computer graphics to either zoom in or dolly in, that they're sometimes not that easy to tell apart. However, they're certainly different in their effect on the camera's field of view and location. The one giveaway that you're dollying and not zooming is that if new faces become visible or disappear. Zooming changes the size of the image scene, but does not change visibility. All that said, I'll sometimes say zoom in, when the controls are actually dolly controls, just because most people don't know what dolly in means. The camera controls for most of the programs in this course have the mouse wheel and middle mouse button set to dolly the view in and out. The other common camera moves are panning and orbiting. Panning means moving left or right as you keep the camera pointed forward. For most programs we're using, the right mouse button pans. Orbitting means circling around the target position, similar to how a satellite orbits the earth. For those of you into first person shooters, it's circle strafing a target. The target location is often placed at the center of whatever object is to be examined. In most of our programs, left mouse orbits around a target location. Incidentally all these mouse moves are just what we happen to use. There are no iron clad standards among programs about which mouse button does what. Say you're controlling a camera by using a position and target system. For each of these camera moves, what must change? The target or the position? Check all or none that apply. Select an attribute only if it's changing a fair bit, say more that a foot for a given attribute. For example, if I said to move your camera so that it was pointing straight up. The target would have changed considerably, while the position would have changed just a small bit. So if you orbit to the right does the target or position change? Both or neither? Similarly for turn camera to the left, zoom in, and pan to the right. When you orbit the camera around something, the target's fixed and only the position changes. When you turn the camera to the left, you're fixed in position and the target changes. When you're zooming in you're not doing any kind of change because you're just changing the field of view. When you pan you're moving the camera perpendicular to the way it's facing, so both the target and position change. In fact, controlling where the target location is can be a somewhat frustrating part of using various graphics modelers. If you pan and then orbit, you're now orbiting around some different target location, which may not have been what was intended. Note that I didn't ask about the effect of dulling in the target in position. Dolly certainly changes the position, but it's up to the program whether to change the target in this case. If the control also moves the target, it's likely you're walking forward and don't particularly care where the target is located. If the target doesn't change, it's like you're walking towards and object and perhaps slow down as you approach it. Here you can see the effect of moving the near and far clipping planes through the scene. Seeing this kind of clipping is usually a bug, not a feature. An algorithm such as ray tracing doesn't have this sort of problem, as the mechanism there is to shoot rays from the eye. These two values, near and far, are necessary for rasterization to work sensibly, due to its use of the projection matrix. Well, at the minimum, you need to set the near distance. It's possible to form a projection matrix that gives a frustum with no far limit, the pyramid. The key thing about these two values is you want to set them to be as close together as you can, without causing any clipping to occur. The near plane is particularly important to move as far as possible away from the camera. The near and far values determine how the z buffer value is computed. Internally the z buffer value typically gets stored as an integer value with some number of bits. For example, 24 bits is common with 8 bits for what's called the stencil buffer. This is a separate buffer I'm not going to talk about in this course but that can be used for on-screen clipping and other effects. The z buffer has lots of bits, but not an infinite number of them. For example, if you're rendering a sheet of paper on top of your desk you can easily get z fighting even if you've modeled everything correctly and the sheet is slightly above the desk. At some pixels, the z value of the paper and the desktop will have the same value and the desktop can then bleed on through. The z depth range of values is spread between the near and far distances. It's clear that having these two distances close together directly benefits precision. However, with the perspective transform in particular, you want to move the near plane as far from the I as possible. Here's an example. Say we have our near plane at a distance of one unit away from the camera, and the far plane ten units away. The NDC z depth does not vary linearly, but instead forms a hyperbolic curve. For example, say we have an object that's seven units away. The NDC z value's about 9.0 when the near distance is one unit. In other words, the z depths of more distant objects are relatively higher. These objects that are farther away have to share a small range of z depth values. And so are more likely to exhibit z fighting. The reason the z depth values vary in this non linear way has to do with interpolation. We want straight lines to stay straight when using perspective projection. I won't prove it to you here. But think of train tracks disappearing into the distance. Near the camera, the railroad ties are visually far apart. As you move toward the horizon, the tracks get closer and closer together. The distance between the tracks is the same, of course. And the track stays straight. But the distance between them on the image changes. The w value for homogeneous coordinates is interpolated linearly. But when used for division, gives us this differing rate of change. To get back to setting the near and far planes. Say we're able to safely move the near plane to a distance of five units, and not cause clipping. We're effectively taking this piece of our original graph, and stretching it to our new range. First, we get the simple benefit of having a smaller range between the near and far. We also get a more linear graph. The more you increase the near plane relative to the far plane, the slower the NDC z depth actually goes to 1. The long and short is that moving the near plane away from the camera has a large benefit, much larger than moving the far plane in by a similar distance. Of course this all begs the question, how do we know where to set these planes. The far distance is usually relatively easy to compute. Either we know in advance or perform some rough computation to determine the distance to the farthest object in the scene. The near clipping plane is trickier. You usually have to have some rules such as not letting your camera get too close to the walls. Or some rule of thumb such as the, the near plane will be 1 1,000th the distance of that of the far plane. Some more elaborate systems will do a prepass, setting the near plane very close to the camera. It'll do a quick render of nearby objects, to determine a good distance for setting of the scene, and then render the whole scene. There's no single perfect solution. There's one other feature of cameras that I'm going to mention in passing here. Depth of field. This is the idea of simulating a real camera, where you can focus the lens to be at a particular distance. In depth of field algorithms, you control the focal distance and also how blurry objects will appear when not in focus. This type of algorithm is often done as a post-process. In other words, data is collected by rendering the scene, then image processing of some sort is used on this data to produce the final depth of field image. The difficult part is getting objects in the foreground to be blurry and also properly blend with the objects in focus behind them. 3JS has a demo showing a depth of field effect. It may show up a bit dark here. So see the additional course materials for the link and give it a try. It avoids the problem of blurry foreground objects by putting the focus nearby. So that only distant objects are fuzzy. There are other less expensive techniques you can do to get a depth of field effect. Here's a simple tilt shift post process that gives foreground and background blur. No scene information is used. A variable blur is just added to the top and bottom of the final image. We've had a series of transforms apply to the object. The object's model transform followed by the view and projection transforms generated by the camera. The perspective divide converts from clip coordinates to normalized device coordinates. There's one last transform and it's a simple one, moving from normalized device coordinates to window coordinates. In other words how do you move from a space of negative 1 to 1 in X, Y and Z to an image with a depth buffer. The answer is simple enough. Add 1, divide by 2, then multiply by the window's resolution. Doing this operation changes from negative 1 to 1 to a range of 0 to 1. I should mention at this point that the other popular API, DirectX, has normalized device coordinates for the Z value that range from 0 to 1 instead of negative 1 to 1. It doesn't really matter what range is used, it's just important to know that this range can vary. The X and Y ranges of going from negative 1 to 1 is standard throughout any system I've ever seen. In 3JS, you select some part of the screen using the set viewport method on the renderer. You give the lower left hand corner, and set the width and height. The settings here say to put the lower left hand corner of the view port a quarter of the way to the right of the origin, and at the bottom of the screen. The viewport itself should be half the width and height of the window. It's possible to have multiple viewports. Each viewport defines a piece of the image and you render separately to each one. This is very handy in modeling where you can have a few different views of the scene, possibly rendered in different ways. The conversion I gave from NDC to window coordinates assumes that the lower left hand corner of the image is the origin at 0, 0. It's worth mentioning there can be a flip in the y-axis within some systems such as the Document Object Model. Some systems that display the image generated consider that the upper left hand corner is 0, 0. If you see a flip in the Y direction during manipulation of the resulting image, this is likely the mismatch. While we're talking about 0, 0, please note that the lower left hand corner of the lower left pixel is at 0.0, 0.0. In other words, in floating point. The center of the pixel is not 0.0, 0.0. It's 0.5, 0.5. Almost all of the time, this is how you want to consider the center of the pixel. DirectX 9 got it wrong, making the center 0.0, 0.0. They fixed this in DirectX 10. I've seen textbooks talk about 0.0, 0.0 as the center of the pixel. Don't believe them. Using 0.0, 0.0 as the center of the pixel has the odd effect of making the lower left hand corner negative 0.5, negative 0.5. It makes simple conversion between floats and integers trickier much of the time. With the proper center, you just drop the fraction. I've occasionally seen circumstances where offsetting half a pixel can make things work out more efficiently. But much of the time, you don't want to do this. Now that we're at the pixel level, let's talk a bit about Anti-aliasing. By default, when we rasterize a scene, we find what's at the center of each pixel. This can give some fairly bad results, as edges of triangles will either be considered to fully cover a pixel, or not cover it at all. The binary nature of this process causes what's called Aliasing, giving a ragged look to edges. Informally, this problem is called the Jaggies or Stairstepping. When you see it in animation, it's called the Crawlies. What we'd like to do is get a nice smooth result, where each edge pixel is shaded proportionately to how much of it is covered by each object overlapping it. If the pixel is mostly covered by a triangle, use more of the triangle's color, less, use less. The hard part is in figuring out this coverage, it's expensive to compute, store and blend triangle areas for each pixel. Though maybe this will happen someday, about the same time we get personal jet packs. In the meantime there have been a huge number of Anti-aliasing schemes proposed for interactive 3D graphics. On one end of the spectrum is Supersampling. Where you simply create a higher resolution image, and then use all these extra samples to make the final image. For example, for 1000 by 1000 pixel image, you might render it at a resolution of 4000 by 4000. Now each pixel has 16 pixels associate with it in the high res image. Blend these together and you get a better result. This scheme is considered a bad idea, for a number of reasons. One is that it's expensive both in memory and processing cost. Another is that sampling in a little 4 by 4 grid is not much help in fixing the jaggies for nearly horizontal or nearly vertical lines. A scheme commonly supported by the GPU is called Multi-sampling Anti-aliasing or MSAA. The idea here is to compute a shade for the whole fragment once, and to compute the geometric coverage separately. In addition, a different sampling pattern than a grid is used, doing so helps narrowly horizontal and vertical lines considerably. This sampling pattern does vary depending on the hardware company making the GPU. The main costs here are processing and storage, but these costs are considerably less and give considerably more bang for the buck than brute force super-sampling. This is generally the form of Anti-aliasing used by default by WebGL. I say generally because there's a toggle for Anti-aliasing on or off, and it's not specified what form of Anti-aliasing is used. To turn Anti-aliasing on in 3GS consists of setting a simple Boolean parameter called antialias. I should note that turning this on doesn't necessarily do anything. It depends upon whether the GPU supports Anti-aliasing, most should, and whether the browser decides to allow it. Sometimes there are bugs that make it safer to keep Anti-aliasing off. Sadly most of the video lessons we've made showing 3GS demos do not have Anti-aliasing on, just because of such a bug. Welcome to the bleeding edge of technology, no one said there would be cake. Another class of algorithms for Anti-aliasing perform filtering on the image, this is a relatively new class of techniques. The first of these called Morphological Anti-aliasing, or MLAA, was developed in 2009. The idea is to use the existing image and possibly other data to detect sharp edges. When these are found, try to smooth just these edges by using nearby pixels. Amazingly, such techniques work pretty well, though can fail on thin lines, text, and other special cases. One of the most popular fitting techniques is called FXAA, which just needs the image itself to operate. I've used it myself in products and it's even included in 3GS. I think part of the popularity is the author Timothy Lates put a serious amount of effort into both making this shader run on just about every GPU known to human kind and thoroughly documenting the code itself. Just to spell it out in case you didn't get the hint, please do document your code, especially if you plan on having anyone else see it, which of course, anyone can do if you're using WebGL or 3GS. So, I'll say it again, document your code. You now have knowledge of all the basic elements for creating a scene, objects, lights, and cameras. You have a considerably easier time of it than these people, as far as rendering the scene goes. The computer does all that for you, letting you focus on how to craft the objects themselves. In the next unit, we'll show how to significantly increase the visual richness of models in your virtual world, through the use of texture mapping. Here's the perspective projection matrix. I hate it when objects get clipped by the near plane. What happens if I set the near distance to zero? The homogeneous coordinates cannot be turned into NDCs. All the resulting NDC Z values are zero. The X and Y values become mirrored, i.e, negated. The resulting NDC Z values are almost all one. So let's see what happens if we substitute zero for near in the equation. Here's the substitution. Let's simplify. This element goes to negative one and this element goes to zero. If you look at this matrix and examine its effect on the various coordinates, you'll notice he following: first, the last collumn was all zeros, so the W value of the coordinate will have no effect on the result. This leaves only the incoming Z value of the coordinate affecting the output Z and W value. Whatever the Z value is coming in, the resulting Z and W will be that original Z value multiplied by negative 1. In other words, the output Z and W values will always be identical. When you divide by W, the result will always be 1, except for the case where W is 0. The distance of an object from the camera will almost always be 1, which is not a useful result if you're trying to use the Z buffer. Here are a bunch of different coordinate spaces. You're sending a model down the pipeline, select the coordinate type the model starts in, and which types it may go through. In practice we might skip one of these coordinate steps. What I'm looking for here is for every blank to have a letter from this list. The object starts in model coordinates, choice F and is transformed to world coordinates, choice I. The object then gets transformed to view space a, so that the world is oriented with respect to the camera's frame of reference. In practice we often go directly from object space to camera space with a single transform, the model view matrix. The projection matrix is applied to get the clip coordinates, choice h. And the coordinates are divided by w to get the model into normalized device coordinates, choice e. Finally the data is converted into Window Coordinates to get it into an image to put it on the screen, choice C. In this exercise, I've set-up two viewports on the screen. A perspective view and a top view orthographic camera. I should mention there's one extra thing to set on the renderer in 3JS, when using multiple viewports, renderer autoclear equals false. This setting tells 3JS to not clear the screen before rendering the viewport. The top view works by defining the volume and space. The up vector for the view is set, somewhat arbitrarily, to be the plus x axis. During rendering the camera position itself is set to be up one unit, looking down at the target. The idea is to look at the target and then move back a bit to establish the look at direction. Note that the look at and up directions should never be parallel. Your task is to add two more viewports. Add a front viewport to the upper left, side the the lower right. The front viewport should look up along the positive x axis, the side viewport down along the negative z axis. Both the front and side viewports should track the target position similar to how the top viewport works. When you're done, the program should look like this. There are two parts to the solution. The front camera and side camera are created in a similar fashion to the top camera. The up direction needs to be set to y up instead of the top camera's x up setting. Really, the default is y up, but it doesn't hurt to set it. During rendering, the front and side cameras are repositioned almost identically to how the top camera is moved In this exercise, you'll implement a rear view camera. It's not quite a rear view mirror. Because left and right will not be reversed. Right now, the rear view camera is just focused on the center of the model. The position and target never change. Your job is to make the rear view camera actually look backwards. Directly away from whatever the full screen camera is looking at. For its position, use the same position as the forward camera. You'll probably need to use the camera's look at method. As well as some vector three methods. At least I did. I've set up the two view ports. The one tricky bit in the code is that scissoring is implemented. A scissor box defines a part of the screen that you want to clear. This is needed so that the rear view camera view port properly clears its area on the screen, since it overlaps the larger view port. See the code to see what I'm talking about. Here's the code added to the render method. The position for the rear camera is copied from the regular camera. The rear camera's target is set by computing the vector from the front camera's target to the position, and then adding the position itself to the vector. Here's a perspective projection matrix. Say you multiply a point with a coordinate, 3,7,0,1 by this matrix. I'll save you the effort. You get 3, 7, negative 2.2, 0. W is 0. If you try to divide negative 2 by 2 by 0 without safety glasses on, the world ends. In fact for any point X, Y, 0, 1, we'll get a W of 0. What do all of these points have in common? The points are exactly on one of the faces of the frustum itself. The points are in the anti-frustum, a mirrored frustum behind the camera. The points are on a plane parallel to the near plane that goes through the origin. Or the points are behind the camera. A set of points described by Z equals 0 is a plane going along the X and Y directions, through the origin. This is where the camera is located in projection space. So the third answer is the correct one. The points described by the last three answers are all located outside the frustum. These points should be culled, and any edges formed from them clipped to the view volume. This culling and clipping is done before division by W. Points on or inside the frustum are guaranteed to have valid W values. This one's quicker to ask than to answer for sure. Which matrix is which? There's a view matrix, a perspective matrix, and an NDC to Window matrix. This matrix has values in the first three positions in the last row, so it's a projection matrix of some sort. Of our choices, only the perspective matrix is a projection. Rotations affect the off-diagonal elements of the upper 3 by 3 of the matrix. The view matrix is the only one of the 3 choices with rotations in it. This leaves the third matrix to be the NDC to window matrix. This conversion is a scale and a translate, which is certainly what this matrix contains. Modify the existing code so that you display just the numeral 1 from the grid. You should pick out a piece of the text here that is a quarter of the width and height of this square. I'm not going to tell you much about how to code it, but here's a tip, if you want to look at demo code, you can always download it from GitHub or can examine it right in your browser if you dig a little bit in the browser's debugger. For example, here's where I found the oil rangle demo in a previous unit using Chrome's debugger. To get back to the exercise, when you're done, the view over the square should look like this. One straight forward way to solve this it to change the uvs to select the numeral. Here's the code that I modified in the square geometry method to make this happen. Just for the heck of it, I tried this texture on the teapot. It looks pretty cool. You'll sometimes run into textures being flipped top to bottom, with the y axis reversed. In 3js, you can correct this by setting the flip y parameter on your texture. The texture coordinates are a bit trickier when the texture's flipped. Say our image is 512 by 512 pixels in size, with the upper left hand corner of this texture being 0, 0. This is different than how U,V gets defined which use the lower left hand corner as 0, 0. Also recall that each of the U,V coordinates point at the outer edge of the image, not the texel center. The question to you is what Texel corresponds to U,V coordinates 0.2, 0.7? What I want here are two integers, not floating point numbers. Each Texel has a location in an array. I want this location. The answer for which column is relatively straightforward. U goes from 0 to 1 and there are 512 columns starting at 0. I multiply the U of 0.02 by 512 and get 102.4. I drop the fraction to get the texel column. As you'll recall from an earlier exercise, we drop the fraction. For the row, if I were to make the lower left corner to be texel 0,0, then we would simply multiply again and be done. Row 358 would be the answer. However, counting from the upper left corner is what we want. Think about the relationship. The lower corner's row 0 is the upper corner's 511, and vice versa. To convert from one to the other, we subtract the row from 511. For example, row number 1 at the bottom is indeed row number 510 when counting from the top. Subtracting 358 from 511 gives us row 153. The drinking bird could use a more pleasant outdoor environment. Your job is to add a grass texture to the ground plane beneath the bird. Just apply a simple color texture. Since the plane is large, please repeat the texture 10 times in each direction. See the exercises code for more details. When you're done the view should look like this. One hint, don't forget about setting the wrap mode. I didn't do this at first, and it was extremely confusing as to why the texture seemed to disappear. The solution has a few steps. First, the texture's loaded. Then, it's wrap mode is set to repeat. And the repeat factor itself is set to 10 for each axis. The materials map is then set to the texture to display the grass. Feel free to remove the grid lines, of course. If you do, you'll see that the bird doesn't feel very attached to the surface, since there's no shadow. The drinking bird most definitely needs a tail. And at long last, we're going to give him a feather. I've set up the tail polygon. This was a bit tricky, in fact. To avoid adding an object 3D, I added some code to change the order of the oiler angles are evaluated. Your task is to add the semi-transparent feather texture to the tail. This is how it should look when you're done. A few bits of code are needed. First, the tail texture is loaded. The material itself has its map texture set. And then, the transparent flag must be set to true, in order to use the texture's alpha. Just about anything can be changed with texture mapping. Beyond color and alpha, here's an example of changing the shininess for different locations on the body. The face is fairly shiny. The back of the head and shoulders less so. Here are the color and specular textures for the face. In 3.js, the specular map uses the red channel of the texture as the strength of the specular contribution. A level of 255 means the specular component is scaled to one. In other words, full strength dropping off as this value falls. Your task is to apply a texture as a specular map. See the exercises code for the path to the texture. Look at the online documentation for how to set the specular map for mesh fong material. On the left is the teapot without the specular map. Once you're done, you'll see a teapot like the one on the right. As you change the view, you'll see that just the specular highlights are textured. The two lines of code needed are one to create the texture, the other to set the materials specular map to this texture. You might try changing the specular map to map which sets the diffuse map instead. The effect is considerably different. For color textures, we saw that MIP mapping improved the quality of the images produced. Noisy patterns were blurred out. Normal maps can suffer the same sorts of problems with noisy sparkles twinkling on and off as we happen to catch a normal that reflects light directly to our eye. Say we use MIP mapping on the normal map here to avoid this problem. These bumps describe two ridges. We average neighboring shading normals to get the next level up. The topmost MIP map will be a single normal that's the average of all the normals at the bottom. So, if the object is far enough away, and this topmost level is used, it will not look bumpy at all. What happens when the surface is far enough away that MIP level one is used? The ridges disappear, the ridges become twice as high, the ridges become half as high, or the ridges reverse their directions? The answer is that MIP level 1 will also have no real bumps left, so the ridges will disappear. The two normals in each case will cancel out and the surface will look flat. This is the problem with normal maps, that they disappear when filtering is done. There are approaches that have been developed to help improve this situation, but MIP mapping itself is often not done on normal maps because of this problem. You start with the shiny teapot on the left, surrounded by a sky box. Assign this sky box cube map in the code to be environment map for the teapot itself. See the documentation about the teapots material for help. When you assign the map, you'll get the result on the right. If you do only one exercise during the whole course, make it this one. It's easy and result is stunning. Since the cube map is already loaded, all that's needed is to set the nth map parameter to this texture cube. When the word texture is used in the real world it is usually used to mean the feel of cloth or perhaps the roughness of a material. In computer graphics the word texture refers to any pattern or image applied to a surface to change its color, shininess or just about any other part of its appearance. In one sense adding a texture to a surface is another form of modeling. You're specifying the color, roughness or reflectivity of the object based on its surface location. The most popular way to add a texture to a surface is to use images. Any 2D image can be applied to a surface. In these lessons, we'll delve into how these images are attached to objects. And the various ways to control their display. Another important use for textures is to act as surrounding environments. While it can be expensive to attempt to have objects reflect other objects in a scene. It turns out to be extremely fast to reflect textures representing a whole environment. There are some limiations, but surfaces can be made to look much more realistic and visually rich by this simple addition. Even an approximation of refraction can be done with this type of technique. In this unit I'll provide you with the basics of how various forms of texturing works. I'll also be taking you on a tour of some of the major effects achievable with textures. I won't be explaining all of these in detail. The good news is, the code for almost all the demos you'll see are part of 3JS. This gives you the ability to easily prod and poke at these various effects, and see what makes them tick. I strongly encourage you to do so. I did this myself in creating some of these lessons, and it was a lot of fun. At its simplest texturing works as follows, you take some location on a surface. Given that location use some function to change the surface attributes at that location. For example you could change the color, shininess, transparency, normal, or even the surface height or shape. That's all there is to it. What the lessons ahead will cover is what sorts of functions used, and some of the popular ways to modify the surface. Let's take a concrete example. Well maybe not concrete, let's make it wood. In fact, let's make it the most popular object in video gaming, the crate. I looked it up. One site claims there are 740-odd video games with crates in them, and counting. You have some object, such as a box. You'd like to make it look like a crate. The most common way to apply a texture to a surface is to use an image and glue it on, sort of like wallpaper. What you'd like to get out is this, the crate texture applied to the box. In 3JS this is amazingly simple. Here's the code snippet that does it. The first line loads the texture, the second line applies this texture to a material. Done. Maybe I should just wrap-up this unit with that. For the simple case, everything just worked. 3JS has a reasonable default for applying a square texture to square box sides. That said, I should give you fair warning about loading textures. There are security concerns enforced with Web GL. The short version is that you can't normally load some texture from another site into your program. Your program and the texture need to be in the same local space in order to run. See the additional course materials for more information. The question is, given a box and an image texture, how do the two get attached? Given a location on the box surface, how do we find the corresponding location on the image? This is potentially a tricky problem. Say we just try to use the location in the world as our input. This won't work well, because our crate could be transformed with rotations and scalings. It might undergo an explosion that would change its shape. Any of these events would modify the world position on its surface, making it hard to get to the same pixel location on the image from frame to frame. This location problem is most commonly solved by adding two values to every vertex. These are called UV coordinates. Just like any other attribute attached to a vertex, these values are interpolated across each triangle during rasterization. Every pixel fragment generated for the surface will have these values available. Within the fragment shader these two values are used to look up the corresponding pixel location in the texture. A pixel on a texture is often called a Texel to differentiate it from a pixel on a screen. For example on this face of the box we want to find a texel color to use for this pixel. At this pixel the UV coordinate turns out to be U equal to 0.2, V equal to 0.7. Using these two values like coordinates on a map we can look at the texel on the image and use this to modify the color of the surface. Many of 3JSs geometry object come along with their own UVs built-in. Some of these are displayed here. For planar objects, the texture's put flat against the face. For round objects, the general idea is that the u-value goes from zero to one, around the equator. And V goes from zero to one, around the axis. To be honest, the mapping on the bottom of the cone shown here is pretty. But doesn't make a lot of sense to me. Maybe you'll be the one to fix it. Note that 3JS truly is a work in progress and anyone can submit bug fixes and improvement. I should also point out the tetrahedron. It's mapping is a bit distorted. In fact, if you look at the code, no UV mapping is given at all and 3JS creates one using some rules. Sometimes there simply is no natural UV mapping for the object being made. It's then pretty much up to the programmer to decide how to proceed. What is happening with each of these objects is that UVs are generated by the geometry class used to create the triangles. The form for making a triangle with UVs code. Here, three vertices are defined and put inside the geometry object. Three UVs are created. I put them in an array so I can reuse any that I want. A face is defined as usual. The three numbers given are indices to the three vertex locations in the vertices array. The way the three UVs are attached to this face is a little non-standard. The first element of the face vertex UVs is accessed. And the three uv's we want to use for this face are directly placed there. If you want to add more faces with UVs Note that you'll access the face vertex UV's class the same way each time. You'll access a ray element zero and pass in the three UVs desired. In this exercise, I'm going to start you off with a textured triangle. Your job is to make a textured square by expanding the square geometry class implementation. The answer involves adding another vertex, adding another UV, and finally adding a face and associating UVs with it. Here is my code solution. Objects with UVs and textures can be imported into 3JS. I won't be describing this process, as it doesn't have all that much to do with the principles of computer graphics. See the additional course materials for more information. What's important to know is how the UVs of an object affect how the texture is mapped to the surface. The basic principle is that by changing the UVs, we select a different part of the texture. Here are the textures that are attached to a single model. The way in which a model is associated with its texture is called texture mapping. The spheres and other objects we saw before had fairly natural projections of the texture onto their surfaces. For a more complex object, such as this humanoid, an artist uses a modeling program to assign the parts of the texture to the model. When a triangle mesh has a texture applied to it the texture is used by the whole mesh. Because of this, a single texture is used to hold all the different images for the various parts of the mesh. This kind of texture is called a texture atlas or a mosaic. I haven't talked about how to convert from U,V values to texels, that's a formula you can figure out. We have a texture of size 16 by 16 applied to a square, the U axis goes along the columns, the V axis goes up the rows. The lower left corner texel has a location of 00. The upper right is 15 15. With the X axis coordinate listed first. The lower left corner of this 00 texel has UV coordinates of 0.0, 0.0. The upper right has 1.0, 1.0. So the question to you is, what Texel is at location U,V equals to 0.72, 0.40. By the way, I want integer answers, not floating point. Converting is done by multiplying u and v by the number of texels in each direction, then dropping the fraction, not rounding. It's a similar process to how we converted from normalized device coordinates to window coordinates. For example, u times 16 is 11.52, and we drop the fraction to get 11. V times 16 gives us 6.4. Again drop the fraction, and we get the answer 6. Here's one way to prove it to yourself that you drop the fraction instead of rounding. Try computing the texel location using U V's for a 1 by 1 texture. A texture with 1 texel. You know in advance the answer you always want for the texture coordinate is the integer 0. A texture coordinate of 0.72 multiplied by 1 will have its fraction dropped, and you'll get 0. As will every other texel coordinate. If you round instead, you would get the wrong answer of 1. I like testing with simple cases like this. If the formula doesn't work for a single pixel conversion, we can't expect it will work for larger textures. It's interesting to note that the upper right hand corner of the texel, which has UV of 1.0, 1.0. Would when multiplied by the texture resolution of 16 by 16, then have texel coordinates of 16.0, 16.0, a texel that doesn't exist. In practical terms, this doesn't matter as the value 15.99999 repeating is the same value as 16.0. So, there is a valid texel and a valid texel location to sample. Here's our square with a few slider controls. I can scale the polygon to be larger in either dimension. The texture stretches to fit. What's more interesting is what happens when I change the u or v values. Here I'm changing the u value of the two points on the right edge. The value starts at one. If I increase the value, this has the effect of putting more copies of the texture along that axis. Let's switch to the mosaic texture. If I decrease the value of v, less of a texture is mapped to the square. I can do the same thing with u. This is how mosaicing works. Each triangle is given some fractional u and v values that select the piece of the texture needed. Give this demo a try and get a sense of how the values affect the rectangle and texture. Everything has been nice and rectangular so far. We've modified both the polygon and texture coordinates, but in such a way that these both always formed some rectangle. Now it's time to see what happens when this correspondence is broken. In this new demo, we can move the locations and change the positions and u v's of the two upper corners. Let's start by modifying the position of the upper right corner. You can see how this distorts the texture. Now let's change the u v coordinates. This also distorts the texture, but in an inverted way. For example, as we make v larger, the texture distorts in a manner similar to what we saw before. An increase of v instead of a decrease in the y coordinate causes this distortion. Try the demo yourself, and see what various changes do. When you're done, I'll ask you a question about what you saw. In the demo, when the texture is warped there's a crease down the middle of our square from upper right to lower left. What is the root cause of this discontinuity? The original texture was stretched or distorted before the corner was moved. The Three.JS system is broken. The square is made of a fine mesh of little triangles. The square is made of two triangles. The original texture is just fine, it's a square. Even if it were distorted in some funny way, why the upper left of the texture would be different than the lower right isn't clear. The second answer is incorrect. Three JS may have some bugs in it but this is not one of them. The third answer is interesting. In fact if the square was made out of a fine mesh of triangles, depending on how the triangles were created when the square was worked, could make the distortion look more natural. The fourth answer is indeed the reason. If you tried the show polygon option in the demo, you'd see the two triangles. Since the square is made of two triangles, moving the corner of the upper left triangle has no effect on the triangle in the lower right. I highly recommend reading the article that is listed in the additional course materials. It discusses this and a number of other common rendering problems. There are many different modes of raps such as gangsta rap, nerdcore hiphop, and crunk. I mean there are three main ways within WebGL of repeating a texture. These are called wrap modes. Here's a demo showing them in action. The repeat mode is the one you're most likely to use. It simply repeats a texture one after another. If you want to have things look continuous, such as water, then the texture itself needs to be what is called seamless, where its edges match up. If your texture is not seamless, one cheap way to tile it across the plane is to set the wrap mode to be mirrored repeat. This concrete texture is not seamless. I just grabbed a piece of an image on Wikimedia Commons. With mirrored repeat on, the texture flips on each repetition. This is a bit easier to see with the r texture. This wrap mode is not a great solution. It's often clear that the texture is being flipped in this way. Still, it's better than nothing. The third wrap mode is called clamp to edge. The drawback or feature of this mode is that the pixels on the edge are used to fill in the area where the texture does not appear. This is the default mode for wrapping, as it has some advantages for filtering along the edges when the texture is not going to be repeated. This code shows how to set each of the three wrap modes in 3JS. The s and t axes essentially mean the u and v axes. You can in fact mix and match these modes, repeating on one axis and clamping on the other. Now give the demo a try yourself. One way to change how many times a texture repeats is to modify the UV coordinates of the square. This is a bit intrusive, you have to modify the geometric mesh itself. 3JS has an alternate solution. It allows you to set two parameters on the texture called repeat and offset. These define a transform for the texture coordinates themselves. Repeat is essentially a scaling operation saying how many times a texture should repeat across the surface. Offset is a translation adding to the uv coordinate after scaling. Varying the offset shifts the texture. One use of this parameter is that you can animate a texture. For example, if you have a river, you can make the water flow by slowly changing one texture coordinate over time. Here you can see the effect of changing the repetition. If you repeat a texture too much, it becomes fairly obvious that it's being repeated. There are advance solutions such as using long tiles to break up this repetition. See the additional course materials for more on this topic. Say you have a very simple texture pattern, a checkerboard consisting of just four texels. What happens if you put this texture on a square? There are now many pixels all sampling the same texel. The obvious answer is that you want this texture to continue to look like a checkerboard, with a sharp edge between the squares. In other words, you want something like this, nice and crisp. However, if you viewed this texture from a different angle, it doesn't look so great. This is a zoomed-in view, just to drive home the point. You get this stair-stepping effect instead of nice smooth edges. This is called the jaggies. The problem is that the texture will return either black or white at every pixel. You'd like 50 shades of gray or so, but there's only these two levels. Sampling the texture in this way is called Nearest Neighbor. Whatever texel center is closest to the pixel center, is the color the pixel gets. The other option is bilinear interpolation, or linear for short. What this mode does, is takes the four closest texel centers and interpolates among them. The closer the pixel is to the texel center, the more of that color you get. If you turn on bilinear interpolation this is what you get. That's definitely too much of a good thing. You'd like blurring along the edges of the squares, but not across the whole texture. The problem here is that one texel covers a huge number of pixels. Let's say we aim to change this ratio to be about one texel per pixel. In other words, say our original two by two checker pattern covers 100 by 100 pixels. To bring the ratio of texels to pixels to be closer, let's make the two by two checker pattern be represented by an image texture of 64 by 64 texels, so that each checker square itself covers 32 by 32 texels. In other words, we'll just make the same pattern, but with a larger texture. Each texel in this larger texture will now cover around one pixel. This turns out to look pretty good. The stair steps are gone. You'll try the demo in a minute and see for yourself. In Three.js, setting the filtering to be nearest or linear is straightforward, there's not a lot to it. Using bilinear interpolation causes more taps to occur, that is more texture samples to be retrieved, but it's usually what you want. Generally, you should use Linear, which is the default. At the same time, you want to avoid situations where a texel covers a lot of pixels. Here's our original two by two texture. When we have a situation where a texel covers more than a pixel, this is called magnification. When magnification is occurring, the texture's mag filter is used. Another way to say it is if a number of texels divided by the number of pixels is less than 1, magnification is happening. You're magnifying the texture. Say we zoom out on our two by two texture and it all falls within one pixel. If a texel covers less than one pixel, that is, one pixel contains more than one texel, this situation is called minification. In this situation the mag filter is not used. Weâ€™ll deal with minification and how to set the corresponding Min Filter in the next lesson. At this point I want you to study a demo as Iâ€™ll be asking you about it. Try out various resolution textures and see how they work with nearest neighbor and linear filtering. Try the higher resolution texture patterns and try tilting the view to see what happens, then play with the zoom. Say you tilt the view a little and set the linear filter. As you increase the resolution of the texture, the checker edges get less and less blurry. At some point, the edges become sharp. If you now use the mousewheel to move in towards the texture, at a certain distance the edges suddenly become smooth again. This may not show up on the video, and the texture size it happens for you may vary. But you should see the edges suddenly get sharp and then smooth. Go back to the demo and try it yourself. The question to you is, why do the edges jump from smooth to sharp? What's causing the edges to be sharp again? Minification is occurring, linear interpolation is no longer being used, on average, each pixel covers more than one texel, the magfilter no longer applies. More than one answer can be correct so, check all that apply. As we increase the texture resolution, eventually each pixel covers more than one texel. When a pixel begins to include more than one texel, minification is occurring. Under these conditions, the MagFilter no longer applies, so its linear filter is not being used. So in fact, all four answers are correct. In general, we like the ratio between texels and pixels to be around one to one. If the texels cover too many pixels, we'll get blurring as we saw in the previous lesson. This isn't so bad for real textures, so erring a bit on the side of smaller textures is not the worst crime that you can commit. If a pixel covers too many texels, we get minification which leads to different problems. For a high resolution checker texture, it simply causes the edges to become sharp. However, the problem is much worse than this under normal viewing conditions. Changing the texture resolution does not help. When a pixel covers more than one texel, minification occurs. This is what's happening in the distance. There's a separate filter on the texture for this situation. The reason it's separate is that this problem has a different solution. Currently, this minification filter is set to be nearest neighbor. As we saw for magnification, this is usually a bad choice. Let's turn on bilinear interpolation for the minification filter, too. This helps a little bit towards the middle of the image, but not much. The problem is that each pixel in the distance covers a few texels. As we get closer and closer to the horizon, the number of texels per pixel increases. When the fragment shader looks up what's in the texture, it gets whatever texel happens to be at the center of the pixel. At the horizon, this selection is almost random, so we get noise as we vary between white and black texels. Even with bilinear interpolation on, it's still just noise. With magnification we found that too many pixels covering one texel caused blurring. Getting this ratio closer to one to one made things just right. Here we have too many texels in one pixel. One solution is to go in the opposite direction than we went for magnification. Instead of making our checkerboard have a larger and larger resolution, we'd like it if the distant pixels used a lower resolution texture. This would bring the ratio of texels to pixels about back to one to one. I can show this by lowering the resolution of the texture. I'll say it again, the video may not show much difference as these videos are heavily compressed so that they're quicker to download. Your best bet is to try this demo out for yourself. It follows this lesson. Anyway, changing the resolution helps a bit towards the middle of the image, since these pixels have more and more texels as we move to the horizon. Blurriness is much better than noisiness, especially when moving the view around. Things still look terrible on the horizon because every pixel in that area covers so many texels that even our two by two texel image fails. If we go to a one by one checkerboard pattern, that might help a bit. What in the world is a one by one checker board? Well, its just a single checker square, a single texel. Since I can't put both black and white in this texel, I make this texel grey. Choosing this texture is like the nuclear option, the horizon looks better than noise but now everything is grey. To sum up, at some point near the horizon, there are just too many texels to pixels. So to avoid noise we go to grey. Try the demo yourself at this point and see how changing the texel to pixel ratio affects the image. With magnification we've found that if a texel covered too many pixels, we could just make our original checkered texter larger and larger so that about one texel covered one pixel. With minification we'd like each pixel to cover no more than one texel. The GPU has built into it, special functionality that implements an algorithm called mip mapping. With mip mapping the GPU computes approximately what the texel to pixel ratio is. If this ratio is less than one Magnification happens and the mag filter takes effect. If this ratio is greater than one, minification is happening. This is where mid mapping comes in. What the GPU does, is determines the ratio of texels to pixels. If this ratio gets much larger than one to one A lower resolution version of the texture is used instead. As this ratio gets higher and higher, a lower and lower resolution of the texture gets used. These textures can be thought of as forming a pyramid. For example, say a pixel needs the texture color. Instead of taking whatever texel happens to cover the center of the pixel down at the bottom level. The GPU determines the texel to pixel ratio for this pixel to see how high up the pyramid to go. The texture at this level of the pyramid is then sampled. Since the ratio of texels to pixels is about one to one for this lower resolution texture, the rendering looks much better without the noise we saw before. In fact, for even better quality rendering, we can set the GPU to linearly interpolate between levels. This is called trilinear filtering. Say our ratio is in between two pyramid levels. We then sample the texture above and below from where our sample is in the pyramid, and interpolate between these two samples. My example pyramid was not drawn to scale. Within the GPU, the pyramid is created and stored in powers of 2. That is, if you start with a texture that's 64 by 64, the pyramid is formed with this texture, a 32 by 32 texture, a 16 by 16 texture, 8 by 8. Four by four, two by two and one by one at the end. This sort of textures is called the mipmap pyramid or the mipmap chain. Each smaller texture is derived from the larger one. One technique is to simply average the four texels of the larger texture to make the smaller which can be done in advance or by the GPU itself. There are some subtleties, such as gamma correction, that can change how these levels are formed. That's for a later lesson. This is an important point, so pay attention. You want to make your textures to be powers of two in size, both in rows and columns. You will not be able to use mipmapping if you don't. GPUs generally expect powers of two, so don't disappoint them, unless you know what you're doing and have a very good reason for doing so. Give the new version of the demo a try. You can now pick mipmapping as a filtering option for minification. The word mip in the name mipmap is an abbreviation for the Latin phrase multum in parvo, meaning "much in a small space." This is an excellent description, as all the extra textures generated in the mipmap pyramid don't take up much space, but prove themselves quite useful. So the question to you is this. Given a 32 by 32 by 1 channel texture, Memory taken up is 1024 bytes. How much memory is used in total by a mid map version of this texture? 32 squared plus 16 squared plus 8 squared plus 4 squared plus 2 squared plus 1 squared is 1,365 bytes. This is about 33% additional space needed for the mipmap. This approximately 33% figure is true for any mipmap and is never more than this percentage. Mipmapping looks much better, but goes to grey up the horizon. Modern GPUs can do even better than this by using anisotropic sampling. Anisotropic means having a property that has a different value when measured in different directions. Sampling means retrieving some piece of information meant to represent the whole. For example, a fragment shader will take a sample from a textures mipmap pyramid to represent the texture's color at that pixel. Where the word anisotropic comes into play is when we get to these pixels on the horizon. Think about looking along one column of the checkerboard. At the horizon a pixel may cover a large number of texels vertically, but few horizontally. That is, the number of texels differs in different directions. What anisotropic sampling does is take extra samples along the axis where texels are more frequent and blends these together. These extra samples generally give a better result, not blurring out as quickly as the mipmap does. Here's a summary of the various three.js settings for magnification, minification, and anisotropy. The minimal settings are listed first. The higher quality ones, further down. Quality does cost more GPU processing time, so you have to decide what settings you can justify. The norm nowadays is to have linear magnification and the best quality mipmap filtering on. I should note that anisotropic sampling does not exist on older GPUs. Though it's an extension in Web GL. Each vendor has its own secret sauce as to how they implement this algorithm. So rendering quality can vary. The maximum anisotropic sampling rate can also vary from GPU to GPU. Calling the getMaxAnisotropy function gives the maximum rate possible on the GPU running the program. This is usually a power of 2 and usually no higher than 16. You can set the value higher if you want. Three.js will use the highest level available on the card. More samples cost more processing power so, again, you need to decide what rate is reasonable. There are yet still better sampling methods possible, anisotropic filtering is not the ultimate answer. I went into perhaps excessive detail on this aspect of the GPU, but for a good reason. This whole process of sampling and filtering comes up again and again throughout all of computer graphics. If you take too many samples you're wasting time and possibly causing other problems. Too few and you can get artifacts. Where the samples are taken and how the samples are combined or filtered, can also make a huge difference. Many processes can be though of in these terms. Did I tessellate the surface enough to capture its essence? Should my computation happen in the vertex shader or the fragment shader? Should my shadow buffer have a higher resolution, or should I use a different algorithm? When I think of computer graphics and rendering as a whole, I think of it primarily in terms of sampling and filtering. Give the demo a try. It will show you the valid levels for your GPU. If you only see a value 1, it means your GPU does not support anisotropic filtering. Just as we can change the color of an object using a texture, we can also change its transparency. Here's a simple scene with a leaf added to it by putting a leaf texture on a square. Here's the same scene with the square selected, so you can see the underlying geometry. The two key things I needed to do was mark the square as transparent, and to make a texture with an alpha channel. Recall that a transparent object needs to be given a transparent material. This ensures that it'll be rendered after all opaque objects, and that blending is turned on so that any transparent pixels will blend properly with the objects behind. In this case most of the pixels are either opaque or fully transparent, but the idea is the same. By turning on blending, the leaf's RGB values are properly blended using its alpha values. Actually making the texture with an alpha channel, I leave to you. Packages such as GIMP, Paint.net, or Photoshop can create an alpha channel for an image. You'll typically want to use the PNG format to store the results, since PNG supports alpha. Here's an example of an object with its alpha texture. White areas are where the plant is, black is transparent, and gray alphas mean the pixels are partially covered by some leaf. When using transparency for materials we set the color's Alpha value to specify an object's opacity. You might have a red material and the Alpha is used to make it semitransparent. When used for texturing, we often think of the Alpha as the amount of coverage. For example, to create a cut-out leaf, you could specify for each textile whether the leaf is visible or not. An alpha of one or zero. However, you can get a better result by specifying an alpha between one and zero for textiles on the edge of a leaf. Each edge textiles Alpha represents how much the lease covers that particular textual. For example, this textile was covered by 6 10 of a leaf, so its alpha would be 0.6. What's interesting here is that a single alpha value is used for both opacity and coverage. It can even mean both at the same time. You could have a cut-out figure whose textile has a 0.6 coverage and a 0.7 opacity to get a 0.42 alpha. Whatever the interpretation, we use the same over blending equation when putting one object the top another. Using alpha for coverage is a simplification. For example, when we have just alpha value, we don't know the true area covered by an object. Combining a few coverages together is something of a best guess when we use blending. This is usually a fine assumption under normal circumstances, where only a few pixels have multiple overlapping edges. One idea you'll run across with transparent textures is that of premultiplied alpha. PMG files never used premultiplied alpha. Other formats vary. Texture data can be stored however you want in memory, either unmultiplied or premultiplied. So what's it mean? Here's the blending equation normally used for transparency. The source, that is the transparent object, is blended with the destination, the object behind it using the source's alpha value. Premultiplied alpha means just that. The term alpha source times color source is going to be the same every time this texture data is accessed. We can premultiply the original color by the alpha, and store this new color in memory. This new equation saves a multiplication every single time this text cell is accessed. Say we access this texture 60 times a second and a million text cells are accessed. That saves 60 million multiplies a second, which could start to add up. Here's an RGBA texel value that is not pre-multiplied. Let's called it unmultiplied. If we pre-multiply the RGB value by the alpha value, we get this result. If we undo this multiplication, we get the unmultiplied RGBA values back. All unmultiplied RGBA values can be converted to premultiplied RGBA values and then back again to unmultiplied if needed. Precision could be lost in converting back and forth like this, but otherwise it's always valid when starting with an unmultiplied value. However, you can start with a premultiplied RGBA value that cannot be properly converted and stored as an unmultiplied RGBA, and so could not have come from unmultiplied values. The question to you is, which premultiplied texels below come from valid unmultiplied numbers. Valid means that all four values of the unmulitplied RGBA are between 0 and 1 inclusive. The order of values below is RGBA as usual. My advice is to think about what the premultiplied RGBA value is representing, if you have any concerns about whether it can be converted back to an unmultiplied value. This first premultiplied texel would turn into this value. The 2.5 is clearly out of range of a value that can be stored in a texel, in the range of 0 to 1. So, this texel cannot be properly represented when unmultiplied. The second texel would convert to values between 0 and 1. This is a valid unmultiplied RGBA. To convert this third texel to unmultiplied form, the alpha value would have to be used to divide the RGB values. Dividing a non 0 number by 0 is illegal. So, this premultiplied RGBA value cannot be converted and stored in unmultiplied form. It's not a value that could've come from any and multiplied RBGA value because the alpha, which doesn't change in conversion, would multiply any RBG value and so set it to 0. The last RGBA looks like it might be invalid to convert, since you would need to divide by 0 to convert from premultiplied to unmultiplied form. However, this RGBA is entirely valid and is the same as in premultiplied form. It represents an entirely transparent pixel. If you started with an unmultiplied RGB of any value and an alpha of 0, the premultiplied form would be the same. Note that the premultiplied form would loose the RGB that the unmultiplied form had in this case. One cool thing you can do with a square that has a transparency map is to create a bunch of these and have them move around. When a large number of objects like this move around we call them particles. This demo has 100,000 particles. One particular useful way to display particles is as if they're 2D objects flat on the screen. The objects are actually in a 3D world, however. As the camera moves, these objects are reoriented so that they always face forward. In this example, we see squares moving about with each square always facing the viewer. You can then apply texture with an alpha channel to get an interesting look. By using transparency mapping, you can put whatever you want on a particle. Here are a bunch of dots moving around. An advantage of particles is that they can directly represent 3D scan to data. For example, here's some movement data captured using a connect as displayed with particles. Some researchers and developers are actively looking at how to use particles as basic building blocks instead of triangles. Here's an example from Markus Shuetz on his potree.org site that shows a realistic view made entirely of particles. This data was captured by a laser scan system by Faro Technologies. One advantage of particle systems is that data can be captured by a scanner and then directly viewed. No processing is needed to reconstruct any triangles or meshes. This program is running in the browser with more particles flowing in and filling in the gaps the longer you view it. That's one advantage of particles, you can stream them in as you wish, bit by bit. With triangles, you tend to send in either some simplified mesh or the whole object, causing objects to pop into existence. The particles themselves can be made to look more realistic. You can use any texture you want when you're doing transparency mapping. By using a sphere texture instead of a circle, these particles are more convincing as 3D objects. Weather is a common use for particle systems for making snow flakes or rain drops. For phenomena like rain, particles can have a lifetime where they're created, fall through the sky, and when they hit the ground, they disappear. Particles usually face the viewer. This idea of having objects always face forward is called billboarding. It allows us to make 2D textures seem more like 3D objects, as we've seen. This demo shows a popular use for transparent textures on billboards, simulating clouds. This demo was by the main author of Three.js in fact. Larger billboards of cloud shapes are combined and overlapped to give the illusion of puffiness. I have to conclude with one of the cliches of computer graphics, the lens flare. It's so easy to add this effect, that it's been in many games, though there probably have been more games with crates. The sparkling patterns that simulate interaction of light with the cameras parts are all transparent textures whose position are computed on the fly. Within 3JS, you can create a set of particles by using the particle system object. The code here shows one way to make some particles. For the material, we load a disc texture that is transparent around the edges. Using this texture and particle basic material says to make a billboard type of object that points towards the camera. In other words, we want to see this sprite texture always facing towards us. Setting size attenuation to false means we that want each particle to have a constant size on the screen. Setting the size to 35 means that we want the particle to be 35 pixels wide. The next bit is to make the particle system object itself by using a set of points in the particle material. By setting sort particles to true, this means we want the particles in the system to be sorted with respect to the camera's view. This is important, as otherwise blending will not occur properly. When you have objects with transparency, you need to draw them in a back to front order so that they combine correctly. The geometry object you pass into a particle system is a list of vertices. In this example, 8,000 particles are being created and added to the scene. I randomly generate these vertices within a box going from negative 1,000 to 1,000 and x, y, and z. And check that each is inside a sphere of radius 1,000. If not I ask for another random point. There are more efficiently is to generate to random points in a sphere, but this is quick to remember in code. Try the demo out, if you dolly out far enough, you'll see the sphere formed. I want you to change the particle demo a little. Instead of 8,000 particles placed at random, I want you to create 9,261 particles in a highly structured grid. Rewrite the point generation part of the code to place a particle at every point in a 2,000 by 2,000 by 2,000 unit grid centered around the origin, spaced 100 units apart. In other words, you'll want to a point at x, y, z equals negative 1,000, negative 1,000, negative 1,000, x, y, z negative 900, 1,000, 1,000 and so on throughout the grid in all directions up to point x, y, z 1,000, 1,000, 1,000. There are 21 points you'll place along any given axis. This should add up to 9,261 points, which is 21 times 21 times 21. When you're done you should see this on the screen. I've turned the background color to gray so that it's easier to see the particles themselves. My solution is to use three loops, one for each axis. Directly setting the X, Y, and Z values to each of the grid intervals. I then create a vertex with each point and add it to the list. I encourage you to modify various parameters in this particle system and see the effects. If you set size attenuation to false, the particles will have a size of 35 units in the world itself, instead of 35 pixels. If you turn off sort particles you'll see rendering errors as the particles are drawn in the wrong order from various view directions. Try removing the spray texture entirely to see how that looks. There are two other texture mapping methods that can add considerable detail to a model. The first is displacement mapping. Here a texture, called a height field, is used to change the height of the surface itself at each vertex. In other words, each vertex has a u v value. This value is used to retrieve the height value from the texture. The height value is then used to displace the vertex. In other words, move it upward or downward along it's normal. However, simply displacing the vertices does not change the shading normals of the surrounding surface. You would see the bumpiness and the silhouette of the object, but otherwise the shading would look fairly similar to the original. To change the shading normals themselves, another texture is applied to the surface. This texture is called a normal map, and is considerably different than other textures we've encountered up to this point. Here's the displacement map for the model, and here's what the normal map looks like. The grey level of this height field shows the amount to displace the vertex, with brighter pixels pushing farther above the original surface. The normal map is, in fact, derived from this displacement map, and there are tools to create this type of texture. Each normal in a normal map is found from the differences in the nearby heights in the displacement map. You can always tell a normal map by its distinctive light blue. The normal map stores an xyz direction vector at each texel. When we view this xyz value as an RGB color, we get this type of look. Specifically the red channel is x, with positive to the right, the green channel is y with positive up, and blue is the z axis. This xyz space is relative to the surface of the model and is called tangent space. If the normal is not to be changed at all, the normal stored is 0, 0, 1, straight at the z axis. A value of 127 in this texture means 0. A value of 255 means 1. The light blue then comes from an x and y value of 0, which gives a red and green channel of 127. The z value is 1, giving a blue channel of 255. So an unchanged normal has an RGB of 127, 127, 255, a light blue. The basic idea is that you're using this normal value to change the existing shading normal. The normal in the texture map takes the place of the shading normal, and relative to the shading normal's original direction. Here's the interesting part. Once you have the normal map, you don't have to apply the displacement map. They're separate processes. The displacement map moves the vertices. The normal map modifies the shading normals. Here we have, from left to right, the unmodified head, the head with just normal mapping, and the head with both. While the model with both mappings is the most convincing, using just the normal map gives you much of what you want but with many less triangles. In fact, artists often first create highly detailed geometric models. A program can then bake the normals into a normal map. The mesh can then be simplified, saving both processing time and memory while retaining much of the detail as the original. Having a surface that is smooth but has differing shading normals is not something we usually see in the real world. In reality, bumps are what cause the surface to change its orientation; however, it's fine in computer graphics to have one without the other. We've already been using the idea that the shading normal can be different than the geometric surface normal. Here we are extending that idea. Varying the shading normal per texel instead of per vertex. A question for you. Where does the displacement of the surface happen. That is, which part of the pipeline deforms and shifts the surface itself? Is it the vertex shader, during rasterization, the fragment shader, or the Z-buffer? There's only one part of the pipeline that can do this sort of operation. The answer is is that this displacement happens at the vertex shader. This shader has control of what happens to each incoming vertex. The vertex shader reads the displacement map texture at the vertices' location and uses it to displace the vertex. The other parts of the pipeline take these displaced triangles and process fragments created by them. Each fragment has a pixel location and cannot change to another pixel location. The fragment shader can apply the normal map, but this operation does not change the location of the vertex. Light mapping is a way of lighting objects to make them look highly realistic, or for that matter highly stylized. For static objects in particular, those that aren't moving, the idea is to pre-compute shadows, reflected light, and any other lighting effects desired. For example, you can see in this scene that the smaller cube has a subtle realistic shadow effect on the larger one. This type of effect is difficult to compute at interactive rates. This is how the process works. First, here's a scene without any lighting applied. Every surface is assigned an additional texture called a light map. This extra texture will be used to capture the light's effect. Some offline process is done to the illumination in the scene. As much time as desired, can then be spent determining how much light reaches each pixel on each surface. The results are stored in the light map. The light map is a mosaic texture, holding many different surfaces' illumination calculations. For example, the lower left corner of this light map is for the ground plane. You can see the two shadows of the cubes laying on it. When the objects are displayed, the fragment shader takes the object's color texture and multiplies it by the corresponding light map texture. This gives the surface an illuminated look. Here's the final result. Note that it's the same or darker than the original image without illumination. This is why light maps are sometimes half jokingly called dark maps, since they lessen the overall light. In theory you could bake the lighting into each color texture itself instead of using a separate light map texture. While possible, this can quickly get expensive. One major advantage of light maps is that they can have a much lower resolution than the texture on the surface itself, and can be stored in a single color channel instead of three. Rather than storing six high resolution textures per cube in this scene, each with the lighting baked in, just one texture and six small light maps are stored for each. Haven't said much about the process by which light is applied to the surfaces. One class of methods is to use some form of global illumination algorithm, such as path tracing. Another technique is called ambient occlusion, which is more concerned with the geometry in the scene. Instead of percolating light through the world, ambient occlusion techniques look for crevices, valleys, and other areas where light is less likely to penetrate. Here's a simple example of ambient occlusion. This particular form stores at each vertex how that vertex is in a corner or crevice. These values are then interpolated across each cube face and blended in. One last thing I should mention. There is a class of algorithms for interactive rendering called screen space ambient occlusion or SSAO. While just an approximation, this effect can look quite convincing and is widely used. At this point, I'm going to set you free. The lessons for the rest of this unit will be showing a number of demos related to more advanced effects. Most of them have code samples you can look at and see what makes them tick. Rather than me describing the five things you need to do to enable textures and hook them up, just look at the code and documentation. 3JS makes many of these effects surprisingly easy to use right out of the box. Be assured that there will still be some exercises in the problem set to get you applying some basic textures to surfaces. However, the effects on the lessons ahead are such that I'd feel a bit guilty making you connect some bits of code together just to see the syntax. This is where the roller coaster really starts to go wild and the best way for you to learn is to rip into the code yourself and see how it works. Instead of applying a texture to a surface we can apply a texture to represent the world itself. In the distance are clouds in all directions. The way this illusion is done is with a sky box. The idea is to put the viewer in the center of a box and put the environment on the walls, floor, and ceiling. Doing this reminds me a bit of The Truman Show, where Truman's world is highly limited and he runs his ship into, well, I don't want to spoil anything if you haven't seen it. The key is that you have to be careful to not let the viewer get too close to the sky so that he doesn't figure out it's just an illusion. If I turn on the wire frame for the cube, the trick is revealed. You can see the walls of the box. Without the wire frame, the illusion is seamless. If the viewer moves around, the center of the box moves with them. In this way, there's no distortion as the view of the walls never changes. This type of texture is called a cube map. It's made of six images that form the cube faces. The viewer is always placed at the center of the cube. For example, here's the frustum for the top of the cube, the plus Y face. The cube shape is not required. It's possible to have other geometries, such as a hemispherical dome around the scene. Whatever the shape, our eyes can't see the Z buffer, so can't tell the sky is only, say, 100 meters away. Sky boxes are typically used for star fields or, well, the sky, for times when you're not near the ground. Alternately, you can simply make the surrounding sky box a part of the scene such as shown here. Real geometry has the advantage that it can receive shadows from the object. The main rule is that you want to make sure that nothing pokes through the walls of the sky box itself. Oh, and one more thing, you definitely want to set the far distance of your camera so that it encompasses the sky box itself, otherwise it will be clipped out. There are many variations of using textures to modify the surface's parameters in the lighting equation. Another way to vary the surface while calculating illumination is to apply textures as light sources. We've seen how a skybox can be made to surround a scene. An extremely clever technique is to have surfaces reflect the skybox. This technique is called reflection mapping, or sometimes environment mapping. The basic idea is to make one object surrounding the world something that other objects can reflect. This is called the environment map. The illumination process is a bit like ray tracing, where a ray is reflected off a shiny surface and picks up color of any reflected objects. This difference is that there is only one simple object to reflect, typically a cube map. Unlike a skybox, the environment map is only used when a shiny surface needs a reflected color. You can certainly also put a skybox in a scene using the same cube map. This is commonly done in order to produce a convincing effect. However, the environment map is a separate object that is part of the shiny object's material description. That said, skyboxes and reflection maps work using the same idea. You can think of a skybox as a physical object which is what we usually do, or you can think of it as a texture function. What is the color of the skybox in the direction the eye is looking? An environment map is exactly the same only for reflected rays. What is the color of the environment map in the reflected direction? How it works is this. We render a fragment on a reflective surface such as a sphere. We compute the usual elements as desired such as diffuse ambient and so on. For the environment map, we also need the direction to the eye and the shading normal at the fragment. These two vectors are used to compute the reflection direction which is then used to find the texel on our environment map. This texel color can then be blended in to the final color of the fragment. The color from the reflection map makes the surface appear mirror like. The very simplest cube map to reflect is one that's at infinity. Remember how a directional light came from a certain direction but was considered infinitely far away? Directional lights are inexpensive to use because their direction vector never changes for any surface. Similarly if you define an environment map as being at infinity, you're saying that no matter where I am in my virtual world, when I look a certain direction I want to always get the same color back from my environment map. To sum up, the reflection direction is computed for a reflective surface, and this direction is then used to access the environment map. This algorithm has limitations somewhat the reverse of the skybox. The reflected objects are always seen as being infinitely far away. For example, say you have a reflective sphere in a room represented by an environment map in a skybox. If you move the sphere from the center of the room to near a wall, the reflection of the wall will not look closer. That said, reflection mapping is an inexpensive way to add convincing visual detail to a model. Our eyes are usually not very good at reversing the process of reflection. We see something reflected and our brains interpret the object we're viewing as being shiny. We don't waste brainpower trying to figure out if the reflections are perfectly correct, since in the real world they always are. Your task is going to be to derive the reflection equation. So you're given the V vector towards the eye, and the mirror surface is normal N. These 2 vectors are normalized and the dot product between them is computed and stored as D. You are now given 2 other vectors, dV which is the V vector multiplied by this dot product and dN, the normal multiplied by this dark product. You can see how this 2 vectors are produced here. For example DV is shown as the projection of N on to the V vector. And DN is shown as the projection of V onto the N vector. Here's our normalized reflection vector R, this is our goal. The question to you is given these 4 vectors, how do we compute this normalized reflection direction vector? You can use any and all of the four vectors in your answer which should consist of 4 integers. These integers can be positive, negative or 0. For example, if you think the single copy of DN vector negated is the same as the R vector, you would put 0, negative 1, 0, 0. Here's a hint. This answer is not correct. What values form the right answer? Looking at the figure, I can see I'm going to need to go to the right somehow to get to the R vector. The V vector looks like a good place to start negated. I put a negative 1 for this V vector. That got me going to the right. Now I need to go upwards. I could add the vector N twice, but this would take me too high. After adding N just once, I can see that I'm above the mirror surface. So another N would take me beyond my goal of R. The vector dN looks like a winner as this is essentially just the Y component of the V vector. Adding it to the negated V vector is the same as simply moving to the right. Adding dN twice gets us to the R vector, so two dN vectors are needed. The final formula then is R equals 2 times dN minus V or to expand out R equals 2 times the dot product of V and N, times N minus V. Just as you can create a reflection vector to perform reflection mapping, you can also create a refraction vector, and use that instead to access the environment map. This can be combined with the color of the object to get different variation of a material that looks a fair bit like glass. Snell's Law's what's normally used for computing the refraction direction. It's named after Willebrord Snellius, a Dutch astronomer and mathematician from the 17th century. However, it turns out that Ibn Sahl, a Persian mathematician, discovered this back in 984 A.D. This is an old formula and is as follows. In this diagram, we have a ray of light traveling through, say, the air, and hitting a transparent object, say, made of glass. In this equation, N1 and N2 are the index refraction of the air and glass respectively. The index of refraction is a physical value for a material that is essentially how much slower light travels through it compared to light in a vacuum. As examples, here are some indexes of refraction. Air is just about 1, it's very close to vacuum. Diamond is very different than glass and so that's one thing that differentiates them. When the index of the refraction of the second material is greater than the first the effect is that the sine of the angle must be smaller to compensate. This means the angle from the normal becomes smaller. In other words, light bends towards the normal when going from air to glass. The effect is reversed when going in the other direction. Going from glass to air causes the light to bend at a greater angle. This is what refraction mapping does. It changes the direction of the light passing through the transparent object. However, real objects have at least two refraction events when light passes through them, both when entering and exiting the surface. Refraction mapping does just the one refraction. Refraction mapping is not quite as convincing as reflection mapping, in my opinion. Part of the reason is that the distortions produced are not as complex as the real thing. More importantly, all that you can ever see through these transparent objects is the environment map. Nothing else. Notice that with the spheres demo, lovely as it is, that you can't see any spheres through any other spheres. It's possible to make this happen, but it is a lot of extra computation. With reflection mapping, we get sharp reflections. This is good as far as it goes, but many reflective surfaces are glossy. They're shiny but they're not mirror like, reflections are a bit blurred. One way to simulate this effect is to blur the reflection map. This blurring is usually done in advance and the texture stored. Some cleverness is needed to properly filter from face to face of the cube map, but utilities exist that perform this function for you. Think about the Blinn-Phong specular reflection model for a moment. Blinn-Phong and other reflection models work by taking the lights direction in eye direction. They use these to decide at a point on a surface how much that point is likely to be a perfect mirror, whatever portion is a mirror reflects this light. In reality, much of the world most of the time is sending photons to any given point in space from many different directions. Look around and everything you can see is sending light your way. It doesn't matter if the light is from an emitter or a surface, except that emitters are usually brighter. What an environment map does is treats the whole surrounding world as a source of light. While blurring an environment map is not entirely physically correct, it is extremely convincing. There is less of a problem of a reflection not matching up with what's in the environment, since you can't see a clear reflection on a burnished surface. Another advantage is that these blurry cube maps can be considerably smaller in size and still look good. For the two cube maps I showed, the original had faces that were 512 by 512 texels in size, the blurred version only 128 by 128. A lower resolution is all that's needed since the color changes more slowly. If you want a wider range of materials, you can blur the original reflection map by different amounts and get different gloss textures. You can also mix and match between sharp and blurred reflections on the same surface, giving an impression of a multi-coat material. Combining these ideas with different reflectivities, illumination models, and surface colors can give a wide range of effects. We've used cube maps for skyboxes and environment maps for sharp and glossy reflections and for refraction. It turns out we can even use this same mechanism for diffuse lighting. For these previous techniques we needed the direction to the eye and the shading normal in order to compute a reflection or a refraction ray. For the diffuse component, we just need the shading normal. Remember, the diffuse component doesn't depend on the eye's direction. We used the dot product between the normal and the light source vectors when we computed the effect of a single light on the surface, using Lambert's Law. Now think big. For a particular point on a surface, consider every texel in our environment map as a source of light, emitted or reflected. Take the dot product of the direction vector to each texel from the surface and compute the dot product with the surface normal. Add these all up. You now have the contribution of the entire environment map on a single location. You might think doing this is a bit expensive to do for every pixel we want to illuminate, and you'd be right. Doing this properly is a good task for path tracing. However, there are a few things working in our favor. First, once you've computed the sum of all the contributions of all the texels for one point on one surface, you've actually computed it for all points with the same normal direction. Remember that the environment map is infinitely far away, so what's true for one surface normal is true for them all. The other element working in our favor is that a cube map can be used to store the results of our computations. After we compute the sum of all the contributions for a given normal, we then simply store this value directly at that location, in the cube map texture. This texture is called the diffuse environment map, or irradiance map. Now when we want to sum total lighting for the surrounding environment for a given normal, we can just look it up on the cube map. One other advantage that we have is that the diffuse map can be quite low resolution while this original sharp environment map had cube faces that were 512 by 512 texels and our gloss map was 128 squared. The diffuse map is only usable at only 32 by 32 texels. Nonetheless, it can be expensive to compute the diffuse map, so what we do is the same as for the glossy map. We compute the texture in advance with some image processing software. This gives us the texture we need to light the scene. During rendering, we just use the shading normal to find the overall contribution of the environment's lighting to the surface fragment. Here's the car painted with diffuse paint. Something you probably never see in the real world. With just a directional light on representing the sun. Notice how the area in shadow is entirely dark. Filling this with a solid ambient light wouldn't look much better. The final result, showing the surrounding environment in shadows gives a highly realistic effect. This demo runs at interactive rates. Looking at demos like this gets me excited to add another ten lessons on various topics that have to do with texturing. For example, I'm feeling a bit guilty that I haven't talked about high-dynamic-range textures, HDR for short. The quick version about HDR textures is that they're used to capture a wide range of lighting levels, basically, more bits per channel. This wide range is particularly useful for environment maps as the light sources in the scene can in reality be magnitudes brighter than other objects. Think of the sun. These textures allow a variety of materials to use them in a more physically believable fashion, and also, avoid banding artifacts due to precision limits. For example, some objects such as plastics mostly pick up just bright lights as reflections and little else. HDR textures allow us to use the same environment map for these materials as well as highly reflective metal surfaces. Here's a very nice demo showing an important technique for cube maps. Up to this point the techniques described have used cube maps created in advance. In fact the GPU is plenty powerful enough to make cube maps on the fly. Look at this demo carefully. What's great about it is that the shiny knot figure and cube are reflected in the sphere, changing position every frame. The way this is done is to make a cube map from the viewpoint of the spheres position. The spheres center in each face of the cube map forms of a view uuu. You render the scene six times, one for each cube face, and each rendering is then used for the new cube face. The sphere itself is not rendered into the cube map. After the cube map is rendered, it's immediately applied to all of the objects in the scene. The cube map includes the knot and block in it, so you can see these reflected in the sphere. The exact reflection is not precisely right, but close enough that we accept it as correct. For efficiency, this same map is used on the knot and block objects, but the reflections are so distorted that we don't notice that the environment map is incorrect. That it includes two objects and not the third. Notice how this demo is recursive in nature. The objects reflected are also reflective, each object when rendered is using the previous frame's environment map on it, so, they all look reflective. I recommend toying with this demo and changing objects and materials to see how it changes what you see. This technique of generating cube maps on the fly is popular in racing games for example. The car is removed from the scene and the rest of the environment is rendered to the cube map which is then applied to the car. [BLANK_AUDIO] We've seen how adding just simple color textures can vastly improve the look of a model. GPUs are very much optimized to access textures having considerable numbers of transistors dedicated to just this task. This in turn has led to a huge number of uses for textures. The idea of applying two or more textures has any number of other uses. For example, a surface can be made visually rich by giving it textures that add scratches, decals, planetary cloud cover or any number of other phenomena. Multiple textures can be combined by addition, multiplication or other operations and all affect the same attribute. Mirror reflection can be done by rendering from a different angle and then using the reflected image as a texture used at each pixel. This is a whole huge sub category where a scene is rendered, and then its resulting image is post processed in various ways. We'll talk a bit more about these when we get into shader programming. Because three.js is a web component, it can interact in interesting ways with other data. For example, you can take a video and use it as a texture. This texture can then be placed on multiple moving screens as shown in this demo. Now that you know the basics of texture mapping, you're equipped to go ahead and dig into other people's demos and see what makes them tick. Better yet, you can not try out some of your own effects. Texturing is a powerful tool and I look forward to seeing what you do with it. For the following tasks, which type of shader is normally used, vertex or fragment? Note that you can check both or neither. The tasks are blurring an image, changing an object's shape, evaluating an illumination model, and performing Gouraud interpolation. Blurring is an image processing effect. So most of the work is done by the fragment shader. The vertex shader is the only one that can move vertex positions. So is the one that can change an object's shape. Evaluating an illumination model can be done on the vertex or pixel level. So both shaders can do this work. Gouraud interpolation itself is done in the rasterization stage. So neither shader does anything along these lines. Here's the flashlight effect. It's pretty simple to code. We'll look at that in a second. However, the effect is not very convincing, as the flashlight cannot currently move around. This is the only addition I made to the fragment shader. The vViewPosition is the location of surface in view space. Actually, this is the position negated, but for our purposes, it doesn't matter too much. Since view space is looking down the negative z axis, and x and y are zero at the center of the screen, the flashlight always shines at the middle. We can return early if the surface is outside the given flashlight radius, so that only the ambient contribution lights the object. I've added the uniform uFlashOffset, a two element vector that gives the center point for the flashlight. There are sliders hooked up to it, but the vector is not used in the fragment shader itself. That's your task. If the X, Y view position in this location are close enough together, the flashlight should illuminate the surface. Your job is to change this code. When you're done, your slider should move the flashlight around. Look in the original course materials for GLSL documentation links. You can solve this problem with a one line change by using the proper GLSL built-in function. Instead of checking the length of vViewPosition from the center of the view camera screen. Take the distance from vViewPosition to the uFlashOffset position. The distance function is built in and works for any type of vector, two, three or four elements. In this exercise, we start with a square tessellated into a grid of smaller squares. This grid allows us to deform the surface. Deformation happens in the vertex shader. What I want you to do is modify the incoming vertex position. Since this value is passed in, you can't just change it. You'll need to copy it over to a temporary variable and modify that. You also need to use this temporary variable in the rest of the shader instead of the position. How I want you to modify the position is by using this formula. The first half of the function finds a point on the sphere. The second half translates the whole object down by a constant amount so that it stays in view. The uSphereRadius2 is a value passed in by the main program. It's the square of the radius set with the slider. XPOS, YPOS and ZPOS are the position coordinates. To access an individual value of a vector, use x, y or z. When you're done, you should see something on the screen that looks like part of a sphere. It's a bit bland and we'll talk more about this later. Definitely save your work since we'll build on it. Here's one solution. First, the incoming position as copied over so I can modify it's values. I then set the Z position with the formula given. I do this over two lines and explain what each step does. The rest of the code remains the same other than changing every use of the position vector with new position. In the previous exercise, the vertex shader was modified to change the surface locations so that a part of a sphere was formed. However, the illumination looks pretty bad. It doesn't look very curved. As we rotate around, there are lighting computations going on. The shading changes but somethings not right. Why is the illumination incorrect? Is it that the vertex shader revserved the triangles loops? The fragment shader is now wrong and must be modified. The vertex shader did not modify the shading normals or the geometric normals have been warped by the vertex shader. The problem is that the vertex shader changed the positions but did not modify the corresponding shading normals. All the shading normals continue to point straight up. In theory, the fragment shader could also be modified, but it doesn't have to be. It's not wrong. It could use the position to compute the normal. It's much more straightforward, however, to modify the shading normal in the vertex shader. This, in fact, is the next exercise. In addition to computing a new position to form a piece of the sphere. I want you to also set the normal. For a sphere centered at the origin this is straight forward. Notice that the normal and the vector to the point both go in the same direction. Take the solution from the previous vertex shading problem and now change the shading normal to point in the correct direction. You'll have to use a temporary normal just as you had to use a temporary position before. I should note you don't have to normalize this shading normal as the fragment shader will do that later. Here's the whole vertex shader. There are two changes to the previous solution. A temporary normal is copied from the new position before this position is translated away from the origin. This new normal is then used to compute vNormal which is the model view shading normal. One thing about Blinn-Phong specular highlighting is that the drop off is always gradual around the fringe. Even if you turn the highlight power up to 1000, you'll still see some drop off fringe. You also can't control the width of the highlight. Your task is to threshold the basic Blinn-Phong specular term. What this means is that after computing the Specular contribution, test it against the variable uDropoff. If Specular is less than uDropoff, set it to zero, else set Specular to 1. When you're done, the solution should look like this. Note that there's a slider for drop off that allows you to change how wide the highlight is. For the solution, I added a line of thresholding code, using a turnary if statement. One problem you might notice with this algorithm is that the highlights are actually too sharp now. If you look closely at the result, it's quite aliased since values are either on or off. This sharpness is also true for our tune shader. But it's even more obvious here because the contrast between light and dark is so great. There are ways of modifying the function so that it is smoother at a transition point. Though because it's view dependent, it's hard to get this exactly right. Alternately, anti-aliasing schemes such as FXAA, can clean up problems like this. I leave it to you to experiment. One cheap technique to give a material a slightly different look is to use what's called wrap lighting. Remember with toon rendering, how we shifted where the diffuse term went from light to dark? We can do the same sort of shift with the diffuse term, but not simply threshold the value. You're task is to change the diffuse term to be this wrap lighting term. See the additional course material for more information about how this works. When you're done the ramp slider will suddenly alter the look of the surface. This equation is from an article on skin rendering by Simon Green. This lighting change is not a huge effect for teapots but can give a more realistic look to some materials. The standard diffuse term is changed to this line of code. The hardest thing about this exercise to me was remembering to put the dot zero after the one in the equation. It won't compile without this. There are a few canned materials that 3JS provides. You can have a diffuse surface, a glossy surface, and a reflective surface. You can make objects transparent. You can even apply textures. In this unit we'll start by digging a bit deeper into the graphics pipeline. One important element of the modern GPU is that significant parts of its pipeline can be programmed to do whatever we want. This is particularly compelling for displaying materials. You can use a wide variety of equations to simulate how a surface looks. With this ability to fully control the illumination model, you can create a wider and wider range of substances as you learn more techniques. You can create objects that look refractive like glass, or semitransparent and hollow. You can make objects that have unusual optical properties. Once you get into illumination models and shader programming the world takes on a slightly different look. I know myself when I look at some unusual mineral or beautiful wood grain I think, hm, how many shader instructions could I do that in? Here's our basic programmable pipeline, with a vertex shader and a fragment shader. Once upon a time both these stages were not programmable. They were handled by what was called the Fixed Function Pipeline. Transistors in the GPU were specifically dedicated to the Transform and Rasterization processes. The programmer basically set a bunch of switches and values that controlled the flow of data. However, there's no real programmability involved. The Nintendo Wii was about the last to use this type of architecture. In 2002, GPUs started to appear that included vertex and fragment shaders. A shader is truly a programmable piece of the pipeline. In other words, you actually send little, or sometimes large, shader programs to each of these units. These programs are written in a C-like language, if you know what C or C++ looks like. Up to this point, we've been having Three.js do the programming for us. When we create a material, Three.js actually creates two small programs, one each for the vertex and fragment shaders. When the object with that material is to be displayed, these shaders are loaded with their programs. The triangles are then sent down the pipeline, and the programs are executed. This decision to make parts of the pipeline programmable is in fact one of the challenges of computer chip design. The designers have to decide how many transistors are dedicated to memory cache and registers, how many to instruction processing, and how many to dedicated algorithm logic. CPUs spend their transistors on memory caches and instruction processing. GPUs use to spend theirs almost entirely on algorithm logic. Which is also called fixed-function processing. What's great about the fixed-function logic is that it can be extremely fast while using less transistors and less power. The drawback of fixed-function logic is that you're locked in. The trend over the past decade or so has been to add to the GPU more and more instruction processing, in other words, more general programmability. In our pipeline the vertex and fragment shaders are programmable. Only those tests that are commonly performed and have large bang for the buck have survived as fixed function hardware in the pipeline. For example, triangle set up and rasterization as well as z depth testing are used all the time and do not normally need programmability. The vertex shader performs the transform of the vertex position to the screen. Its inputs are the vertex from the triangle along with whatever data the programmer wants to provide. For example, matrices and the colorful material could be passed in. The output of the vertex shader is a vertex with a transformed position. And possibly other information, such as the normal. The transform triangle is then rasterized. Triangle set up sends the data at each pixel inside a triangle to another programmable unit, the fragment shader. If you use Microsoft's DirectX API, this is called the pixel shader. Here's the layout for the fragment shader. This shader is handed various information from the triangle being processed. Somewhere to the vertex shader, the programmer can feed in any other data needed to process the triangles data at each pixel. The fragment shader's program is run, and the output not surprisingly, called a fragment Is typically a color and a z-depth, and possibly an alpha if you're using transparency. By the way, the reason we call it a fragment is that it represents the piece of the triangle covered by the pixel. At this point, this fragment color plus z-depth, is compared to the stored depth in the z-buffer. If the fragment is closer than the z-depth previously stored. The triangle is visible at this pixel and its color values are saved. The z-buffer test is again a fixed function bit of hardware. Notice how the fragment shader is pretty similar to the vertex shader in the way it functions. In fact, modern GPUs use what is called a unified shader in the hardware itself. These shader processors are assigned on the fly by the GPU, to execute vertex shader or fragment shader programs, depending on where the bottleneck is found. Say you're given this pipeline. You're told that the fragment shader is turning out to be a huge bottleneck in this extremely primitive GPU design made by Acme GPU's and Bubblegum, Incorporated. Their GPU has a single vertex shader and a single fragment shader processor. This is pretty pitiful and, in fact, is what differentiates different GPU's in the same class. Some will be more powerful simply because they have more shaders on the chip. Modern GPU's process hundreds, if not thousands, of vertices and pixels simultaneously with these cores, which is why they are called massively parallel. Acme knows that they'll need to add more cores to be competitive, so they're making a much larger chip. They know that it'll be able to have a few hundred separate vertex shaders and separate fragment shaders, plus room for other features. You've been hired to design their next chip, which will have many more transistors available. Your team brainstorms a few architectural ideas and you're asked to evaluate these. Your task is to decide what's worth considering. Reverse the order performing the fragment shader before the vertex shader, test the z-depth before running the fragment shader and abort if test fails, rework the shader cores so they can be used as vertex shaders or fragment shaders, and the last idea is divide each triangle into four smaller triangles and render these. Personally, I think half of these ideas are worth exploring further. The same brainstorming sessions there are no bad ideas. Fair enough, but at some point you have to call out the ones that some counterproductive or incoherent. The first idea here is in that category since it makes no sense at all. The input to a fragment shader is the point inside a triangle, typically produced by triangle set up. The application doesn't feed service points the GPU. It provides triangles. Even if it fed such points the GPU, these points would not normally be properly placed on the screen. That's something the vertex shader does. Transforms the triangle to screen coordinates, so forget idea number one. The second idea is a good one, if we know early on that the fragment is visible, then we don't have to use the fragment shader at all. GPUs take advantage of this and so perform the Z-depth test early. So, saving not executing that fragment shader's program. This type of speed up is called Early-Z, after the fragment is computed, its then put into the Z buffer and color image as usual. However, there's a little subtle catch with this idea, if the fragment shader, itself, actually changes the Z-depth value passed in. Then you can not safely check this Z-depth before running the shader program. That said, about 99.44 of all fragment shaders never touch the Z-depth. In fact, in web GL it's not currently possible. Though this has been proposed as an extension. This third idea of making each core be usable as either a vertex a fragment shader is worthwhile. This in fact ia how modern GP is architected. Vertex and fragment shaders are close enough together and functionality that a single core can be used as either. This is called a unified shader. An advantage for the fragment shader bottleneck problem is that such course to be allocated on-the-fly. If fragments of piling up, cores are assigned to them. If vertices are queued up, cores are moved to this area. The first solution doesn't change anything as far as the fragment shader goes. The same number of pixels will be covered and generate fragments so that the fragment shader still has to work as hard. Where's the end, adding more triangles means more strain on other parts of the pipeline, such as the vertex shader. So, this idea causes more overall [UNKNOWN], not less. The Blinn-Phone reflection model looks pretty good. It's been used for well over 30 years, because it's fairly flexible and quick to compute. On fixed function graphics hardware, where you could not program the shaders, this reflection model was the one locked into transistors and could not be changed. Back then you could simulate other reflection models by verious tricks and setting a large number of switches on the chip, but for the most part you were stuck with what was burnt into the transistors. Once vertex and fragment shaders became available, all this changed. You can now program whatever functions you wanted. When you select a material and lighting, what the 3JS library does under the hood is create efficient vertex and fragment shader programs for you. However, you can create and modify your own materials by using 3JS's shading material. Vertex and fragment shaders have similar structures. You first define the inputs to the shader. There are 2 kinds of declaration in 3JS, uniform and varying. In web GL itself, there are also attributes to a vertex shader, which are the values such as the vertex position normal and UV coordiinate data. These attributes are built in 3JS, so do not have to be declared. In fact, they have those exact names, position, normal, UV. Uniform data is whatever is constant for the triangle being shaded. For example, values such as the position of the light sources and the color and shininess of the material are uniforms. A value is labeled as varying if it is computed or set by the vertex shader as an output, and passed to the fragment shader as an input. The vertex shader outputs these values and then the rasterizer interpolates these across the triangle's surface. In other words, these values can and will vary per fragment. You're in total control of what gets interpolated. The more data per vertex that you want interpolated, the more time and resources it takes. Nothing comes for free. The fragment shader also takes uniform data as inputs. The fragment shader then, as a minimum, outputs a fragment color with the name GL frag color. I say as a minimum, because in fact, in many graphics APIs, the fragment shader can output to a number of images at one time. This is called MRT, for multiple render targets. Targets is another name for output images. Unfortunately, WebGL does not support this, yet. To summarize, here's 3JS's view of the world. A vertex shader has inputs that are uniforms and attributes. And always outputs a GL position, and can output varying values. The fragment shader takes these varying values along with whatever uniforms it wants, and outputs a GL.Frag Color. I've run through the basics of what the vertex and fragment shaders use for inputs and outputs. Say you have a triangle with a shading normal stored at each vertex. Which of the following are true? The vertex shader can output the average position of the three vertices. The fragment shader can use the interpolated shading normal. The vertex shader can output a color for the fragment shader to use. The fragment shader can use the position of the third vertex in the triangle. Check the box if this statement is true. The vertex shader only knows about a single vertex at a time. It normally doesn't know about the whole triangle. So this statement is false. The vertex shader is given just one vertex at a time, so has only its position. The second statement is true. The vertex shader can output the shading normal, which is then interpolated across the triangle's surface. The fragment shader will probably want to normalize the shading normal before using it. The third statement is also true. Even though the vertex itself does not have a color stored in it, the vertex shader can compute and output anything it wants. Even though we call the outputs things like position, normal, color and so on, as far as the GPU is concerned, it's all just data to interpolate. The fourth statement is false for the same reason the first is false. Each vertex is processed separately by the vertex shader, so data from a specific vertex cannot be passed to the fragment shader in this way. You could work around this limitation by doing expensive things like storing the data for all three verticies at every vertex and have the vertex shader sort it out. A better solution is to use the geometry shader if available. Currently it's not supported by WebGL. How many times have I said this? But I really do expect this will change in the future. When available, the geometry shader is optional and is applied after the vertex shader. It allows you to access the data for the triangle as a whole. You can in fact use the geometry shader to create new points, lines and triangles. The heart of a shader is the actual shader program it runs. This program is passed to the GPU as a string of characters. I'll repeat that, since it's a bit unusual. Instead of sending compiled code or binary data, or some other processed information to the driver, you actually send it a string of characters. WebGL shaders are written in GLSLES, the OpenGL Shading Language for Embedded Systems. No one calls it GLSLES. Everyone simply says GLSL. If you use DirectX, you'd use HLSL -- thy are High Level Shading Language. In either case, the character string is what gets sent to the graphics driver. The graphics driver compiles these into assembly instructions that are sent to the GPU. Here's how to create a shader string in Javascript. The first line is how Javascript says it's going to define an array. What we're doing here is defining a string for each line of our program. That's one way to get shader programs into 3GS, but it's usually not the most convenient. There are all these quote marks and commas, which are visually confusing and a pain to add and maintain. That said, one advantage of this approach is that you can glue together strings on the fly to make shaders. This is in fact exactly what 3GS does. When you ask for three lights on a shiny material, it creates a custom shader for exactly those elements. If you have a specific fixed shader in mind, an easier way is to make your program a separate text resource. I won't work through the mechanics here. This isn't really a web programming class. But see the code in this course's get hub repository for how we do it. Basically, you create a separate text file or script element that has your program's text in it. The GLSL language is sort of like the C language, which is kind of like JavaScript anyway but simpler. Because vectors are so important in graphics, the language also has vector support built in. This vertex shader computes the diffuse Lambertian lighting component at each vertex. At the top you can see the material color defined as a uniform as something the vertex shader uses as an input. Similarly, the lights direction is passed in as a uniform. Vec3 means a three-element vector. RGB for the color, XYZ for the lights direction. Next is a varying color, vColor, which is an output color. We'll use this in the fragment shader. The vertex shader is required to always output GL position. Which is a predefined variable. This vector holds the clipping coordinates of the vertex location on the screen. This next line normalizes the direction to the light. This operation is done here more for convenience of the designer so that the light direction does not have to be passed in normalized. You could get rid of this line if you require that the light direction must be normalized before being passed into this shader. The diffuse reflection model is computed on this next line. We take the dot product of the vertex normal and the light. We use the maximum function here to make sure the result is not negative. A negative number happens when the surface is pointing away from the light. We don't allow the light's contribution to be negative. That would be like light sucking energy from the surface. The final line of the program itself computes the output color, called vColor, by multiplying the color by the diffuse contribution. The shader language is a lot like C or Javascript, but has some nice, built in features. Here, we're multiplying a vector, the material color, times a single value. The language knows to multiply each value of the color vector by the diffuse value. The position GL_Position, and the color vColor, were generated by the vertex shader. These are passed on to the rasterizer, which interpolates these across the triangle and creates fragments. Each fragment is then passed through the fragment shader. You'll be happy to hear this shader is very simple. All this shader does is takes the interpolated color, vColor, and copies it over to gl_FragColor, the built-in output variable for what color appears on the screen. The fourth coordinate is set to the alpha value, which says how solid the surface is. This is normally 1. Notice how the language elegantly understands vectors. VColor is a vector with three coordinates. We construct the gl_FragColor, a four-element vector, by using vColor and appending a number. GLSL knows what this means. In general, the language is very aware of common vector and scalar operations. Here's another example of vector operations. Say we have an input specular color and a specular amount. We've computed a value for the specular variable, how much it should affect the fragment's color. In this line of code we multiply this variable, a single floating point number, by a vector with three elements. We add this to the fragment color by specifying which components we want changed. Here the R, G, and B components are modified. The component names for the 4 elements are XYZW for points and vectors, RGBA for colors, and STPQ for texture coordinates. S and T are sort of like U and V. I won't go into all the features of the language, see the additional course materials for full references. One thing worth mentioning is that there all sorts of built-in functions. Some you'll probably be familiar with, such as absolute value, sign, power, and square root. Others are more specific to graphic, such as normalize, dot, cross product, and reflect. For debugging shader code, the browser's debug console can often give useful errors. For example, in this line of code, I put a representation for a floating point number that's perfectly valid in C, but is not part of the GLSL language, so it was flagged here. Sometimes the errors are a bit cryptic. In this case the 0 should be 0.0. GLSL is very picky about having a floating point number have a decimal point. In either case, you'll get a blank screen or not much warning otherwise. I sometimes found myself adding a line or two at a time and seeing if everything stays on the screen. If not, I comment out the lines until my scene reappears. I call this binary search debugging. One common way of giving a cartoon like appearance to a surface is to give it two or more shades of the same color. For example, in this drawing the sheep's wool has just two tones, light and dark. You're going to do the same sort of thing with the diffuse shader we just described. The idea is to modify the diffuse component. In the diffuse shader code we computed this value by taking the dot product of the normal and the light. Instead of using the diffuse component for the shade, we really want to identify two zones. Light and dark. If the normal is pointing some fair bit towards the light, consider the area lit. Otherwise, it's dark. Take this sphere as an example. Normally we clamp the diffuse term at zero and not let it go negative. Above zero and the light has greater and greater contribution as the dot product increases. This is realistic, but we can do whatever we want in the shader. One idea to get a two tone effect is define a critical angle, essentially a dot product value. Below this value will make the surface uniformly dark, otherwise it's fully lit. For this exercise you'll start with a fragment sheeter that performs diffuse sliding per pixel. I've hooked up a uniform called uBorder to the user interface. Change the program to instead compute the diffuse component as follows. If the dot project is greater than uBorder, then diffuse is 1.0. Else, diffuse is 0.5. Make sure to use floating point number and don't leave off the point 0 in 1.0. When you get this new code in place, it should look like this. Once you have the solution, try the border slider, and see the effect. If nothing for negative values of border, you might want to think about the effect of the max function. Here's one way to code the answer. We still compute the diffuse component, but no longer take a maximum value, since we're going to compare it to uBorder. The comparison here then changes the diffuse value so it gives a cartoony effect. Here's a more compact solution, using a conditional operator instead of a full-blown if, then, else statement. This critical angle rule is entirely arbitrary of course. You could even try more elaborate rules. For example, this code gives three levels of illumination. It gives an image with a different feel, a bit more depth and a little less cartoony. This type of effect was almost impossible with the old fixed function pipeline. With shaders it's easy to experiment with different illumination models. This kind of reflection model is called Toon Shading, or Cel Shading, or sometimes Posterization. Toon is short for cartoon. Cel is short for celuloid, the material for the transparent sheets that are used when animation is drawn by hand. There are all sorts of variations for this type of rendering. For example, you can change colors based on the shading level and get more elaborate effects. In this rendering, a separate image processing shader is used to detect the edges of the object, and outline them. This type of illumination model is one example of what's called Non Photorealistic Rendering, or NPR for short. Photorealistic means "looks like a photo." If you think about it, there is a nearly infinite number of ways to draw things that do not look realistic. The running joke is that we've been doing non-photorealistic rendering on computers for decades, since realism is hard to achieve. The way I see it is that non-photorealistic rendering has an intent behind it. Just as we assign different fonts to text to convey a different impression, NPR is a choice we can make about 3D rendering. Scott McCloud and his book, Understanding Comics, talks about amplification through simplification. The idea is that by displaying only the relevant pieces of information, you strengthen your message. For example, an automobile owner manual usually has simplified drawings instead of photos. Non-photorealistic techniques often try to convey information in a more sketchy or artistic way. For example, a sketchy look can let the viewer know that the result they're looking at is in just an early stage of design. The cel shader we developed can fail in various ways. Which of these causes the most problems for the cel shader? Texture mapping, reflection mapping, shadows or transparency? Transparency is the main point of failure. The other methods changed the illumination on a single surface. The final value is then mapped in some fashion to a color. With transparency, you're first mapping the opaque material to a color then you're blending this color with whatever transparent filter is covering a pixel, then mapping that result to some new color. It's mixing results from shaded pixels with solid colors which is unlikely to work well. We'll see in a later lesson that image processing is often a better way to perform cel shading, as this maps from some final shaded color to a solid value just once. I find myself usually modifying the fragment shader in a program, since that's where all the per pixel processing is happening. Let's back up a bit and show how various parameters were produced by the vertex shader. The position and normal of the vertex are passed in with these names, position and normal. A few built in matrices are used for transformation, namely projectionMatrix, modelViewMatrix and normalMatrix. In Three.js, these are always available to the shader if desired. In WebGL itself you need to do a little more work. There's currently not a modelViewProjectionMatrix of these two matrices multiplied together. Maybe you'll be the person to add it to Three.js, as it's commonly used and would be more efficient to use here. What we get out of this shader are a few vectors. First the gl.Position is set, which is the location in clip coordinates. This vector must always be set by the vertex shader, at a minimum. One of the features of the vertex shader is that you can change the shape of an object. You can't really change it in the fragment shader. The normal in modelView space is computed here, using the normal transform matrix. Finally, a vector from the location in modelView space toward the viewer is computed. First the position in modelView space is computed, and then negating this vector gives the direction toward the viewer from the surface, instead of from the viewer to object. Remember that the camera's at the origin in View space. We don't really need the temporary mvPosition vector. We could have combined these last two lines of code. This example is here to show how to compute it. To sum up, the vertex shader took as inputs the model space position and normal. It transformed them to create a point in clip coordinates for the rasterizer. It also transformed the normal and position. The resulting transform vertices are then interpolated across the triangle during rasterization, and sent to the fragment shader for each fragment produced. We've shown how an image texture can be applied to a model. However, these sorts of mappings are only skin deep. If we carve away any part of the object, the fact that a 2D texture is getting used is revealed. What we'd like is something that's more like a real material. The solution is to create a 3D texture that fills space. One way to define this texture is to actually create something like a stack of images. Unfortunately 3D textures are not yet available in WebGL. My guess is wait a year. Another method is to make a function that takes in a point on a surface and gives back a color. If you think about it, these are equivalent in intent. Using images also takes in a 3D point and gives back a color. Each approach has its strengths and weaknesses. Functions have an unlimited resolution compared to textures, but you also have to be careful to filter their results. While I'm at it, I should mention you can shade points on surfaces however you want with a fragment shader program. Here's an example of animated procedural textures in use. These examples are 2D functions. They don't fill space, but show how elaborate effects can be computed on the fly in the fragment shader. Let's take the teapot shader and vary its color with location. Here's what the function should do. You want to multiply diffuse by one half plus one half times the sine of u scale times position x, y, and z. For this position value, use v model position, which is provided. The function here makes sine waves through space in three directions. Normally a sine wave goes from negative one to one. We want to scale it so that it's between zero and one. So these one-halves bring that into range. The result will look like this when you're done. Try out the slider to change the overall scale of the function. Here's one solution, it directly multiplies the diffuse component by the function. I highly recommend trying out any other functions you want right here. The main thing is to aim for functions that give back numbers in the range zero to one. The numbers can be outside this range, smoke won't come out of your computer. All that happens is that the value gets clamped to zero or one. In this exercise, you'll start with a different procedural texture. It works fine, but I want you to know about an important debugging technique. Since you can't put break points inside a shader, it's hard to know what's going on. The answer is to send various shader computations to the screen itself. In other words, instead of putting your final result into GL_FRAGColor, put these other intermediate values. In fact, you have three whole color channels to use and view on the screen. Your task is to send the following program values, diffuse, ugain alpha, and attenuation to each of these channels. All three of these values is in the proper range of zero to one, so need no adjustment. Here's what things will look like when you're done. Under material control in the user interface is a slider called gain alpha. So, I think it will change the green channel to match the value selected. One bit to notice in the code is that there's a sub routine called compute bias. It's fine to use sub routines inside shaders, and they have a C language type of interface. One solution is to replace the last line of the fragment shader with this line of code. The order of the values is red, green and blue. An alternate solution is to set each of these individually, either solution is fine, you could even use x, y, z, w for the coordinate names, the shading language interpreter doesn't care. I wanted to show this colorful multivariable way of debugging. And it's sometimes handy for seeing correlations between parameters. In practice though, it's usually clear to copy just a single value to all three color channels. One physical phenomenon that is sometimes factored into the illumination model is described by the Fresnel equations. These equations were discovered by the French engineer and physicist Augustin-Jean Fresnel. He received little recognition of his work in optics while he was alive. Much of it was published after his death in 1827. Happily, what evidently gave him the most pleasure was the discovery and experimental confirmation of his scientific theory. The Fresnel equations themselves have to do with the amount of reflection and refraction off a surface. The basic relationship is this, the more on edge you look on a surface, the reflective it is. If the surface is refractive, in other words transparent, the amount of light transmitted will drop off considerably as you approach the shallow angle. My favorite example is a lake or the ocean. The next time you're in shallow water, notice that you can look straight down and see the bottom. As you look out towards the horizon, the angle of your eye to the surface's normal approaches 90 degrees and reflection will dominate. For materials that aren't transparent, this effect still takes place. The Fresnel effect is particularly noticeable for what are called dielectrics, which is a fancy way of saying insulators. In other words, material such as glass, plastic, and clay. For example, glass is about 25 times as reflective at a shallow angle than directly head-on. An experiment you can do right now to prove this to yourself. Take a sheet of normal paper, nothing very shiny or rough, and put it on a flat surface such as a book. Hold the paper so that you're hold it nearly edge on and look towards a lit area, such as a window. Here I'm using the tablet. At nearly 90 degrees to the surface, normally you will see the paper become quite reflective. In fact, the Fresnel effect should also lessen the diffuse contribution. As the object becomes more reflective in real life, there are less photons left to go off in other directions. There is one implementation detail I want to mention. You should use the normal that represents the mirror orientation. For example, for a mirror reflective surface, this is simply the shading normal. For most other materials, it's not. With the Blinn-Phong illumination equation, the idea is that the material is made of microfacets. Only a few facets are angled just right to reflect light towards the eye. These facets have normal vectors pointing exactly halfway between the light and eye vectors. Recall that the special direction is called the half-angle vector. Happily, this is a vector we compute anyway for specular highlighting, but it's important to use this direction and not the shading normal. Try the demo to get a sense of the effect. Notice how a nearly edge on angle causes the reflectivity to increase. The Blinn-Phong reflection model has been around for more than 30 years. It used to be hard-coded into older GPUs from the early 2000s. It's easy to evaluate and somewhat intuitive to control. One technique worth mentioning with this reflection model is that you can get the specular highlight a different color than the diffuse components. For example, if the specular component is given a white color, the object looks more like a shiny plastic. If both the specular and diffused components are multiplied by the same color the material looks more metallic. However this classic reflection model is not energy balanced, you'll notice as I change the shininess, the material looks smoother, but overall the amount of light reflected becomes greater. If you think about it, this makes a lot of sense. Here's the Blinn-Phong equation again. You take N dot H, and take the maximum between that and 0, and raise that to the power of shininess. The graph of the angle between N and H, versus the specular intensity, is like this. Clearly the area under the graph for cosine squared is smaller than for cosine, so the amount of energy coming from the surface will be less as you increase the shininess. Cosine cubed has even less overall energy. As the shininess goes up, the area under the curve goes down. Two changes give a better result, one that's both more plausible and easier to control. One idea is to attenuate the specular term by the lumbersian fall off. In other words, just like diffuse, make the specular term drop off as the angle of the light to the surface becomes less straight. N dot L. The other idea is to make these narrower curves be higher, giving them roughly the same volume at 3D. This idea is captured in the last term. As shininess increases this last term also increases, when combined with the Lambertian term this new equation gives a reasonably-balance result. Here is the original Blinnâ€“Phong equation. You can see with the shininess of three it's overall much brighter than a shininess of 100. By energy-balanced I mean that changing the shininess does not noticeable change the amount of energy reflected from the surface, You can see the effect by running the demo that follows. By the way, this demo puts all its shaders inside the JavaScript program itself, if you want to look at an example of how that's done. Using the Lambertian N dot L term also eliminates a serious problem with specular falloff. You may have noticed it yourself with the basic Blinn-Phong model. Here's a view of the model with the low shininess and the light coming up from behind it. The diffuse term drops off smoothly, but the specular suddenly drops to zero, giving a pretty bad result. By using the Lambertian dropoff, the specular term now fades properly. Further restrictions can be built into a reflection model, such as maintaining energy conservation. This means that there's not more energy reflected from the surface than shines on it. For example, the diffuse term should drop off when the specular term is stronger. This sort of reflection model is called physically-based since it has some relationship to how light truly works. Within film production and game creation there's a trend towards physically-based systems. One advantage is that materials are more understandable by artists. For example, with our energy balanced Blinn-Phong material we can adjust the shininess without worrying as much that we'll also have to adjust the overall intensity. One element in a physically-based system is getting the materials right, but that's just to start. For example, the time of day might change from daytime to twilight, so that the overall illumination in the scene is considerably less. Instead of making everything dark, some rendering systems try to adapt. This is similar to what a photographer does by adjusting the exposure. One way is to compute a high precision image and automatically change the brightness and contrast to make the displayed image look good. This sort of analysis and display is called tone mapping and is fairly commonly used in games and other applications now. I want to give you a taste of how materials can be represented in a more general way. You'll also learn a great term to impress your friends and confound your enemies. Think about a surface and how it reflects light. The two variables we use are the light's incoming direction and the amount of light reflecting towards the eye. So at it's simplest, a material can be represented by this function,given a light and eye direction, give back an intensity. This function is called the BRDF which stands for Bidirectional Reflectance Distribution Function. Let's look at that phrase. First, it's a function. The inputs are the light and the eye. The function depends on two directions so it's bidirectional. These directions are normally given with respect to the surface itself, that is, each vector is often given as two numbers, the altitude angle and the azimuth. The altitude is the angle away from the normal, and the azimuth is the angle of the vector when projected onto the plain. The phrase reflectance distribution means how the light is spread. One simple example is a perfect mirror. The reflectance distribution in this case is that when the eyes direction is exactly equal to the lights reflection direction, all light is reflected towards the eye. Every other eye direction gets no light. Another basic distribution is diffuse reflection. For some given incoming light direction, the direction to the eye doesn't matter. That's the definition of diffuse reflection. Since this value is constant, diffuse is then represented by the surface of a hemisphere. Specular highlights are represented by lobes. This distribution represents a glossy surface, where light is reflected in a general direction. The lights direction determines where most of the lights energy is reflected. If the load gets wider, the specular reflections spreads out. Written this way, our BRDF needs four numbers, two for the light and two for the eye. But if you think about it, most materials really only need three. These two altitude angles and this azimuth between them. For example if you put a sheet of paper on a table top and rotate it. Both the light azimuth and eye azimuth angles change with respect to the paper, but the angle between the two remains the same. Most materials are fine with three numbers. Here are 2 materials, that as you rotate them, it doesn't really change how they reflect the light. Here's a material where it does. As you rotate it, the light reflects in different ways. The second kind of material is called anisotropic. Compared to isotropic, which is how most materials are. Good examples of anisotropic materials are. Brushed aluminum and velvet. A simple wave making a material anisotropic is to give the object a pair of normals instead of a single normal. In this exercise you start off with our energy balance to blend on reflection model for a plane. What I want you to do is change the single normal to be two normals as if the material has grooves in it. If you look closely at the fragment shader, you'll see I cheat a bit. I take the fragment's location and use its x and z position to make these new normals. I can do this because I know the original normal is mostly pointing up the y axis. Your job is to take these two normals and use them instead of the original normal. Basically you need to apply the reflection model twice, once to each normal. To keep things about the same intensity, make both of these contributions be each just half as strong. I'd like to point out that GLSL has the idea of a loop built into it. This should prove handy for this exercise. Here's the result which, even though it's a hack, looks pretty nice I don't mind saying. Remember, the major rule of graphics is it just has to look right. One thing to keep in mind, the groove parameter, when set to zero, should give a result exactly the same as the original reflection model. If it doesn't you'll need to figure out what to fix. The brute force answer would be to copy and paste the reflection equation and substitute in the normals, making sure to divide each contribution by two. I'm hoping you used a loop. The overall coding change with a loop is to take the two jiggled normals and use them in the shading equation. Each iteration is multiplied by a half so that the total contribution is the same. I simply made up the anisotropic function out of my head. However, there's considerable research about how to capture BRDFs from materials, and how to make functions to compactly represent them. BRDFs are just the start. There's also the BSDF, the Beet Sugar Development Foundation. We're more interested in the Bi-Directional Scattering Distribution Function. This type of function captures both how light reflects from and transmits through material. There's also the BSSRDFs which stands for Bidirectional Surface Scattering Reflectance Distribution Function. Say that one three times fast. This function is important for materials like marble and milk. For these materials in particular, the light enters one location on the surface, bounces around inside the material, and comes out somewhere nearby. One other extremely important material that has this sort of scattering is skin. Getting skin to look good for interactive rendering can be quite involved. But the results are more convincing than using some simple reflection model. See the additional course materials for more information. That said, the key factor here is scale. The effect of subsurface scattering lessens as the viewer's distance from the object increases. Close up, a photon might exit at a location that's a fair number of pixels away from where it entered the surface. From farther away, they may be no change in pixel location. In fact the diffuse component for all non-metallic materials comes from subsurface scattering. It's just that in many cases this scattering is over an imperceptably small distance. Metals themselves are essentially all specular. Let me say that again, because all this time we've been living a lie. Metallic objects have no lambertian diffuse term. Well, not a lie, I just like being dramatic. Really, diffuse is simply an approximation of which we should be aware. Using it's fine, even high-quality applications do so. It's quick to compute and looks plausible. In reality, metals can indeed be given a roughened surface to give them a glossier, diffuse look. So, a diffuse term is fine. However, on a an atomic level, metallic objects have a free floating soup of electrons on the surface which absorbs and reemits incoming photons. If your surface represents a shiny metal, you probably don't want a diffuse term. Insulators have a diffuse term because the photons undergo subsurface scattering. Most of the time the entry and exit points are so close together it doesn't matter. But the direction of exit certainly does. Materials such as that in an unglazed clay pot, concrete, and even the moon itself, are rough enough that the lambertian reflection model doesn't capture them fully. This again turns out to be a matter of scale, having to do with the relationship of surface roughness with subsurface scattering. Admittedly, trying to capture all of these effects leads to a lot of work and possibly inefficient shaders. These subsurface scattering renderings are from 3D Studio Max and rendered offline, not at interactive rates. The main thing is to realize we don't have to stick with illumination models from the 1970's because of inertia or ignorance. Using reflection models based on how the real world works has a number of advantages. First and foremost, if everything is properly modeled, your virtual world acts like the real world. Change lighting conditions, and you don't have to tweak material settings to look good. For design software, this assurance can mean that you can trust what you see on the screen to have some relationship to what you manufacture. Physically based rendering is also a great help to virtual world content creators, such as game and film makers. It's a time saver to have predictable illumination models, as the artist does not have to learn obscure sliders that have no real world counterparts. It's vastly reassuring, knowing that materials won't show some glitch from a certain angle, and knowing that lighting can be changed without destroying the sense of realism. Rather than limit creativity, a well-designed system makes for a more productive and unrestrictive environment. Say you want to have some pixel have half the intensity of some given pixel. Logically, you'd set the pixel's channel values to be half that of the original's. This is certainly a reasonable idea, and in fact, I recommend it once you have your system set up properly. The only problem is that displays do not actually respond to channel settings in this way. A monitor has a power law response to an input signal. If you send a signal of say, 0.5, you get a result down here. You send a signal of 1, double it, you get way more than double the result. This non-linear response of monitors sounds like something that should just get fixed, but it's in fact a feature. It turns out that our eyes have a similar response curve. A monitor has a limited number of levels of color it can display. To get good coverage of levels that the eye can detect, it's a fine thing to space these levels out in this non-linear way. However, we usually think we're calculating colors in a linear space. If I compute a color that's half as intense, I expect to see that result. Many applications ignore this mismatch between how to calculate color and how to display it. And it's less noticeable than you might think. If everything you do is in this space, then you often don't feel anything's wrong. It's what you're used to seeing. In fact, I'll warn you that many of the shader program examples in this unit blithely ignore this problem, since the focus is on keeping the program simple. However, there are a few important places where computing in the monitor space can cause problems. First, lights don't add up correctly. If you have 2 spotlights overlapping, this area of overlap will not be twice the intensity as the individual spotlit areas. If you want to be more physically based and be able to trust that your virtual world has some connection to the real world, you need to add the lights together sensibly. The hues of colors can shift. As you vary the intensity, the channels shift in a nonlinear fashion and so shift the perceived hue. Anti-aliasing won't look as good. Say you have a white polygon on a black background. If a pixel is half covered by a polygon, it should be half the intensity as one fully covered by the polygon. If no correction's made for the monitor's nonlinear response, the half covered pixel will be dimmer. This error will cause even highly sampled pixels to be wrong, and give what's called braiding or roping along the edges. I say this a lot, but you really owe it to yourself to see the additional course materials for a comparison. Mipmapped textures will appear dimmer in the distance. The simplest case is the mipmap for a two by two checkerboard pattern. The one by one average mipmap is a gray level of 0.5. But as I've said maybe five times now, this gray will appear darker on the screen. This can cause textures with high contrast to look dimmer, as these lower resolution mipmaps are accessed. The solution to these problems is called gamma correction. We have two separate wishes. We want to compute our renderings in a linear space, and we want to compensate for the power curve of monitors when we're displaying. At its simplest, the equation's just this. You take the computed channel value, you raise it to the 1 over 2.2 power, and that gives you the value you need to display. This value of 2.2 is called the gamma correction value. Older Mac displays used 1.8 or so, but now 2.2 is the norm for all monitors. For example, say we have a value of 1.0, 0.6, 0.2 in our linear compute space. We raise 1 to this power and we get 1 back. 0.6 raised to the power gets 0.793. Finally 0.2 gives us 0.481. Notice how each channel changes by a different factor. 1.0 didn't change at all. 0.2 more then doubles. This is why color shifting can occur if you don't gamma correct. Gamma correction is something that's applied to the whole image just before display. If you try to do it yourself you have to be aware of precision problems with your input data. For example, if you have only 8 bits per channel coming in, you won't be able to get 8 bits of good data coming out. The good news is that Three.js has gamma correction built in. All you have to do for it is ask. One last detail is that when you look at a color swatch or a texture on the screen, you're seeing the monitor's power curve version of the data. If you want to use that data properly when computing lighting effects, you actually have to raise each channel's value to a power of 2.2 before using it for anything, either making mipmaps or making images themselves. Three.js uses an on-the-fly approximation, squaring the texture's color when it is sampled. To sum up, whatever we see on the screen is in this nonlinear gamma space. To use it in lighting computations, we want to make the data linear. So we raise each channel's value to the 2.2 power. We perform our lighting calculations, then at the end we go back to the monitor space by raising to 1 over 2.2. Here are the corresponding Three.js calls. Some GPUs and APIs have built-in capabilities for dealing with gamma properly. Unfortunately, these aren't currently accessible through WebGL or Three.js. Internally in Three.js, there's some approximation of gamma. So a value might be squared, instead of raised to the 2.2 power, to bring it into linear computation space. To see the effect of gamma correction, take a look at the demo that follows. I haven't talked about texture access in fragment shaders. The short answer is, you can access textures in fragment shaders. The theory of sampling and filtering is the same as you've already learned in previous lessons. Most of the rest is just the syntax of setting up the texture access. The key bit is that in the shader you use the texture 2D function. This gives back the color from the texture which you then use as you wish. Search the Three.js code base for this keyword and you'll find more than 40 shaders that use texture access. The main file for material shading is WebGLShaders.js, and it's worth your time to give it a look if you want to become better with shaders. The other function to look for is textureCube for cube maps, which takes a 3D direction as an input. Many of the rest of the shaders perform image processing, where the pixels of the image are used to form another image. Let's take a concrete example. You want to convert a color image to grayscale. The formula is luminance equals this much red, this much green, and this much blue. This formula is for linearized colors, and luminance is intensity, the grayscale. There's also another formula for luma, for when the inputs are not gamma corrected. It's surprising to me that in both formulas how important green is and how little blue matters. Whichever formula you use, you might think you could simply add it as a final step in a fragment shader. However, transparency again is a problem. You want to apply this formula as a post-process. That is, you want to convert to grayscale after the whole scene is rendered. But by then it's too late. The image has already been sent to the screen. Well, in fact, you can send the image off-screen and have it become a texture. This is called, simply, an offscreen texture, pixel buffer, or render target. This texture is not typically not a powers of two texture, it's not normally 512 by 512, or anything that you'd use to make a mipmap chain. Our goal is to perform image processing on this so-called texture. We want to read a pixel and convert it to gray. You'd think there'd be some mode that could just take one texel and spit it out to a new pixel, but the GPU is optimized for drawing triangles and accessing textures. The way we use this texture is by drawing a single rectangle with UV coordinates that exactly fill the screen. You'll hear this called a screen-filling quad. The UV values are then used in the fragment shader to precisely grab the single texel that corresponds to our final output pixel on the screen, one for one. This sounds inefficient, and in a certain sense it is. But this process is fast enough that often a considerable number of post-processing passes can be done in each frame. In Three.js, the Effect Composer class lets you create and chain different passes together with just a few lines of code. For our grayscale post-process, the vertex shader is along these lines. This is almost as simple as a vertex shader can get. It copies over the UV value and projects the screen-filling quadrilateral to clip coordinates. The whole point of rendering the rectangle is to force the fragment shader to be evaluated at every pixel. The fragment shader code is also quite simple. The texture is accessed by the screen location, essentially. So each textel will be associated with its corresponding output pixel. We use the color of this textel to get the grey scale color, in this case using the luma equation. This color is then saved to the fragment's output color and we're done. But we don't have to stop there. Multiple post-process passes can be done, and it's often necessary or even more efficient. For example, a blur can be done in two passes, a horizontal blur and then a vertical blur. Doing so results in fewer texture look-ups than a single pass. Multiple passes can be quite memory efficient as the input texture from one pass can become the output texture for the next, and vice versa. This process is called ping ponging, as the flow of data goes back and forth. Here the horizontal blur uses the left image as input, the right for output. The vertical blur uses the right as input, left as output. Say we now want to convert to gray scale, we'd add another pass and reverse the direction again. Here's an example of grayscale in action. The critical idea here is that we can do all sorts of things in this fragment shader. We can sample nearby texels and blend them together to blur the image, detect edges, or 100 other operations. We can chain together each post-process to feed the next. This demo by Felix Turner shows some of the many effects you can achieve. Three.js, in fact, has lots of post-processing operations in its library, and it's easy to add your own. I highly recommend looking at the additional course materials for links to Felix's tutorial and other worthwhile resources. In theory you could take an output image and use gamma correction on it as a post process. I mentioned that there's a problem with trying to gamma correct if you have just eight bits per channel. Let's put a number on it, your job is to figure out that number. Say we walk through all possible channel levels, zero through 255 and gamma correct each value. Here's an example for an input of one. There are few steps to convert a channel. First, if you convert to floating point number by dividing by 255. Then use the gamma correction value to raise it to a power. Convert back the range zero through 255, by multiply by 255. Then round the fraction to get the channel number. If the input is zero this converts to zero on output. No surprises there. With one we jumped up to a level of 21 on output. Level two jumps to 28. At these lower levels we have to boost the output considerably to get the equivalent gamma corrected value for display. Out of the first 29 output levels, zero through 28, we've used only three. Already there are 26 levels out of 255 we'll never use. On the low end, some levels are never used. On the high end, you can have the opposite. That a number of different input levels map to a single output level. The question to you is, how many unique output levels are there? I'm expecting an integer value and it's clearly less than 256. Use whatever means you like to figure out this answer, any programming language you like. This is an actual, real life programming assignment. I personally wrote a little JavaScript program. I used this bit of code to show me in the debug console, the answer it computed. The answer is 184, you lose more than a quarter of all output levels when you gamma correct an 8 bit image. Doing so can lead to what's called banding. Especially with the lower values where you can see the different levels due to the gaps. Here's one solution; it uses the observation that each channel computed will be either a new one, in which case you want to add it to a count. Or it's a previous one that you've already seen. That bit of code was from Patrick Cozzi, and frankly, it was better than mine. Patrick, Mauricio, and later on Branislav did massive amounts of work on this course's material. Thanks so much, guys. This course is at least twice as good as it would have been without their help. You've now learned about the most powerful thing about GPUs, their programmability. GPUs are astounding things, and I guess that could be almost considered inevitable. A quarter of our brain is used to process what comes from our eyes. We're a very visual species. If we were blood hounds, we might be using NPUs, nasal processing units. Me? I was amazed when 3D accelerators caught on in the last half of the 1990s. Who would pay a few hundred bucks just to accelerate a few applications? Luckily, I was wrong. I've decided to ask a few other people what they've found surprising over the past years. Related to interactive rendering, what the biggest surprise to you over the past ten years? Is there any new capability, platform or social phenomenon that has stood out in your mind? &gt;&gt; I think that the way the games industry is really driving the graphics hardware vendors is very interesting to me. I've gone through several cycles in my career where the hardware goes from, let's have a separate card for graphics, to let's put it back on the CPU to, let's go back out to, to the graphics card. And it sort of has this, this cycle. And, now we're seeing the point where, graphics cards have more power than the CPU, in many cases with, with all the cores in them, and the program-ability. And the way that the vendors are working together with the games developers and with the other developers, we try to figure out what is going to keep programmable. And, as you know, if you can turn a given algorithm into a fixed function and just, hard code into transistors, you get much better performance like texturing.It's one of those things where. Originally was programmable, but then they put it into a fixed function to get the performance out of it. So working together with the industry I really like the way that's going. I didn't see that as much 20 years ago. with the card vendors. They were sort of on their own, but now they have that collaboration going on. &gt;&gt; I mean, I think, what we see with, you know, mobile and handheld devices is really a big disruption and, you know, it has been amazing what's happening here like how much the amount of rendering power you have like in your pocket every day. And often I think like one way of, you know, just seeing how amazing it is like if you sit down like in an airplane, where they have another of those like touch screen devices, but it's often like five or ten years old because you know the upgrade cycles for airlines is so long, it looks like stone age technology compared to what you have in your pocket. So you know, that, that's obviously something that is happening now, and it's, it's pretty amazing. &gt;&gt; With internet, I mean when I started doing rendering it was very much on a desktop, you hit the Render button, and you came back the next day. And that was like a big thing for me and a big problem. so now to see live interactive web rendering is kind of blowing my mind. And I'm just, I know that soon we'll be able to model and completely render lighted objects without having to you know, hit the Render button and, and dread the results, yeah.