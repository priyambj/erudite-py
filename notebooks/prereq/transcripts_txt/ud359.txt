Hi, and welcome to Introduction to Data Science. My name's Dave, and I'll be the instructor for this course. I've worked as a data scientist in Silicon Valley, most recently at a small company called Yub and before that at a company called TrialPay. I'm formally trained as a physicist, and I originally became interested in data scientist because I love the idea of improving the quality of people's lives or. Building really cool products by using data and mathematics. In this lesson, we'll discuss data science at a high level. Together we'll find out what data science is and discuss what skills are required to be a data scientist. We'll also hear from a bunch of other data scientists about interesting projects they worked on. And discuss how data science is being used to solve a bunch of different problems. This lesson in particular is going to be a little bit different than the others. We're not going to build as much. I think it's important to understand data science at high level before we dive into the details. Alright, well I'm really excited about this course, so why don't we get started. People have many different conceptions of what data scientists do. Some might say that a data scientist is just a data analyst who lives in California. While others might say that a data scientist is a person who's better at statistics than any software engineer, and better at software engineering than any statistician. As you can see, definitions vary wildly from place to place, and from person to person. So before we get started, let me ask you a question. What do you think data scientists do in their day-to-day work? Type your thoughts in the text box below. Don't worry, there are no right or wrong answers and this quiz will not be graded. Let me tell you my perspective. From personal experience, data scientists today are people who have a blend of many different skills. This Venn diagram shows a definition of a data scientist that I like a lot. A data scientist is someone who knows mathematics and statistics, which allows them to identify interesting insights in a sea of data. They also have the programming skills to code up statistical models and get data from a variety of different data sources. Furthermore, a data scientist is someone who knows how to ask the right questions and translate those questions into a sound analysis. After doing the analysis, they have the communication skills to report their findings in a way that people can easily understand. In other words, data scientists have the ability to perform complicated analysis on huge data sets. Once they've done this, they also have the ability to write and make informative graphs to communicate their findings to others. Here are some things that a data scientist may do in his or her daily work. They might wrangle data. That is, collect data from the real world, process it, and create a data set that can be analyzed. Once they have a data set, they may analyze trends in the existing data or try to make data driven predictions about the future using the data at hand. Based on this models or predictions, they cannot only build data driven products but also communicate their findings to those other data scientists and the general public. File visualizations, reports or blog posts. But hey, this is my point of view. Why don't we talk to some other data scientists and hear their thoughts. My name is Pi-Chaun Chang. Mm-hm. My background, so I've been doing Computer Science ever since college. Mm-hm. and, you know, I did a PhD CS PhD at Stanford. Mm-hm. And I worked at Google for four years and now I'm in a start-up called AltSchool. The term data science is actually pretty new to me as well, even though now I think about it, I have been doing data science since the day I was at in Taiwan doing a Masters in, in speech recognition. The way we would understand speech is to collect a lot of data and understand how to model things like a phoneme in speech. And how to understand you know, people's language processing requires a lot of data collection as well. And at Google, which is a company that collects a lot of data, I also did personalization there which requires a lot of data going in, you know, understanding a person's behavior. So, that to me is data science. Using data to build a useful model or to understand a particular pattern that is useful, then later on, for other software applications. As we discussed earlier in this lesson, a data scientist needs to have substantive expertise. What does that mean? Well typically it means that a data scientist knows which questions to ask, can interpret the data well and understands the structure of the data. You can imagine that a data scientist needs to know about the problem that solving. For example, if you are solving an on-line advertising problem, you want to make sure you understand what types of people are coming to your website. How they are interacting with the website and what different data means that can help you ask the right questions like, Are people falling off and not completing our ads at a certain point in the flow, or do people complete more ads at a certain time of the day? You would also, then, be very familiar with how the data is stored and structured. And that could help you work more efficiently and more effectively. This is why it's important for a data scientist to have substantive expertise. It's important to note that data scientists usually work in teams. So it's normal for data scientists to be stronger in some areas and weaker in others. So even if you, as a data scientist, don't have a tons of substantive expertise, if you have great hacking skills or know a lot of statistics you can still be a valuable member of a data science team. As we discussed earlier in this lesson, a data scientist needs to have substantive expertise. What does that mean? Well typically it means that a data scientist knows which questions to ask, can interpret the data well and understands the structure of the data. You can imagine that a data scientist needs to know about the problem that solving. For example, if you are solving an on-line advertising problem, you want to make sure you understand what types of people are coming to your website. How they are interacting with the website and what different data means that can help you ask the right questions like, Are people falling off and not completing our ads at a certain point in the flow, or do people complete more ads at a certain time of the day? You would also, then, be very familiar with how the data is stored and structured. And that could help you work more efficiently and more effectively. This is why it's important for a data scientist to have substantive expertise. It's important to note that data scientists usually work in teams. So it's normal for data scientists to be stronger in some areas and weaker in others. So even if you, as a data scientist, don't have a tons of substantive expertise, if you have great hacking skills or know a lot of statistics you can still be a valuable member of a data science team. Now that you have a better Idea of what data science is, and what data scientist do, let's talk about how data science can be applied across a wide spectrum of industries. You might have signed up for this class under the notion that if you become a data scientist, you'll end up working for a Silicon Valley startup. Well it's true that most tech companies do employ data scientists. Data science can also be used to solve problems in many different fields. What are some examples of the types of problems being solved using data science? Well for one, Netflix uses collaborative filtering algorithms to recommend movies to users based on things they've previously watched. Also, elements of many popular social media websites are powered by data science. Things like recommending new connections on Linkedin, constructing your Facebook news feed or suggesting new people to follow on Twitter. Many other online services or apps such as dating websites like OKCupid or right sharing services like Uber using the vast amount of user data available to them, not only need to customize and improve the user experience but also to publish interesting anthropological findings regarding people's behavior in the offline world on their blogs. Okay, so I know what you're thinking. So far, all of these seem like problems data scientists are expected to solve, but data scientists work in many domains. Data science concepts are integral in processing and modeling data in the field bioinformatics where scientists are working on projects like annotating genomes and analyzing data sequences. This past summer, data science for social good fellows in Chicago worked on a project attempting to solve Chicago's bus crowding issues using data. In addition, physicists use data science concepts when building a 100 terabyte database of astronomical data collected by the Sloan digital sky server. This one's cool. Analyzing electronic medical records allowed the city of Camden, New Jersey to save enormous amounts of money by targeting their efforts towards specific buildings accounting for a majority of emergency admission. Finally, NBA teams like the Toronto Raptors are installing a new technology, sports view cameras, on the basketball courts. They collect huge amounts of data on players movement and playing styles. This helps teams to better analyze game trends and improve coaching decisions. You've probably noticed by now that data science is making an impact in areas far and wide. Data science isn't simply a trendy new way to think about tech problems. It's a tool that can be used to solve problems in a variety of fields. Data scientists are working at Silicon Valley start-ups to enrich our online experiences, but they're also doing important work in our cities, in our laboratories and in our sports stadiums. As you can imagine, since data science is being deployed in a wide range of fields, data scientists use many different tools, depending on the task at hand. Two of the most common tools used by data scientists are Numpy and Pandas. Numpy is popular because it adds support for things like multi-dimensional arrays and matrices. It also adds a lot of mathematical functions that base python lacks. Pandas on the other hand, allows us to structure and manipulate our data in ways that are particularly well suited for data analysis. If you happen to be familiar with the scripting language R, Pandas takes a lot of the best elements from R and implements them in Python. Both Numpy and Pandas are popular tools in the data science world, so being familiar with them will help you read other people's code in the future. Why don't we quickly discuss a couple of the more important features in both Numpy and Pandas? First, why don't we talk about NumPy. NumPy has built-in functions for a lot of basic tasks you'll perform while doing statistical analysis. So examples might include calculating the mean, median, or standard deviation of an array. For example, let's say I had an array of all of the integers ranging from one to five, called numbers. Technically, NumPy arrays are different from Python lists, but performing these operations on a Python list like this will cast it as a NumPy array behind the scenes. So let's not worry about it too much. If I wanted to find the average of this array of numbers, I could simply type numpy.mean(numbers). We see that it returns that the mean is 3. As we know is the case. Similarly, if I want to find the median, I can call numpy.median(numbers). Finally, if I wish to know the standard deviation, I can just call numpy.std(numbers). These are just a few of the functions that make it easier to analyze data when using NumPy. Now that we know a little bit about manipulating data, why don't we talk about how we'll store and reference it using Pandas. Data in Pandas is often contained in a structure called a data frame. A data frame is a two dimensional labeled data structure with columns which can be different types if necessary. For example, types like string, int, float, or Boolean. You can think of a data frame as being similar to an Excel spreadsheet. We'll talk about making data frames in a second. For now, here's what an example data frame might look like, using data describing passengers on the Titanic and whether or not they survived the Titanic's tragic collision with an iceberg. You'll be using this very data set for project number one. Note that there are numerous different columns. Name, age, fare, and survived. And that these columns all have different data types. Age is all integers. Survived is all Boolean, et cetera. There are also some not a number entries. This is what happens when we don't specify a value. How would we go about making this data frame. First, I'll create a Python dictionary called d where each key is the name of one of my columns and the corresponding value is a Python series where I first pass in an array with the values for the actual data frame and then an array of indexes where I want those values to go. And notice that in the case of fare where there is a not a number value, I only provide three actual values, but then I provide the three corresponding indices. Once I've created this dictionary, I can pass it as an argument to the DataFrame function to create my actual data frame. Here I'll call that data frame df. You'll see that the data frame we've printed here matches the one that we had on the tablet earlier in this lesson. Throughout this rest of this lesson, you'll learn to interact with pandas and numpy using this medal count data from the 2014 Sochi Winter Olympics. We have the 26 countries who medal at least once, and how many gold, silver, bronze, and total medals they earned during the Sochi Olympics. Create a pandas dataframe called Olympics_medal_counts_df containing the data from the table of 2014 Sochi winter Olympics medal counts. The columns for this dataframe should be called country name, gold, silver and bronze. There is no need to specify row indices for this dataframe. In this case, the rows will automatically be assigned numbered indexes. Your code should go here All right, why don't we walk through the solution. First we define these Python lists called countries, gold, silver, and bronze, which give us ordered lists of the country names, gold medal counts, silver medal counts, and bronze medal counts as they appear in the Sochi medal count table. Then we create this Python dictionary, Olympic medal counts. Where the keys are our column names and the values are panda series containing these lists. Finally, we create our data frame Olympic_medal_counts_df by passing olympic_medal_counts into the DataFrame function. Now that we know how to create a dataframe, why don't we talk about how we can access the data. We can operate on specific columns by calling on them as if they were a key in the dictionary. For example, if we wanted just the name column of this dataframe, I could simply type df name. I could also grab more than one column by passing in a list of column names as opposed to just one column name. For example, say I wanted the name and age columns. I could say df name, age. I can also call on specific rows by calling the dataframe objects load method and passing the row index as an argument. For example, if I only wanted the row corresponding to passenger Braund, whose index is a, I could simply say df.loc a. We can also use true false statements regarding columns of the dataframe to subset the dataframe. For example let's say I wanted rows of this dataframe only where the passenger age was greater than or equal to 30. I could simply say, df where df age greater than or equal to 30. You can see here that I've only picked out rows b and d, which were the rows where our passenger is in fact older than 30. This ability to subset our dataframe based on true false statements in the index is not limited to the entire row. I can also perform this operation on particular columns. For example let's say I only wanted this survived information for these two rows, b and d. I can simply say, df survived df age greater than or equal to 30. Let's pick apart what this statement is doing since it's a little bit complicated. First, df survived is going to pick out only the survived column of our dataframe. This section here says, I basically only want the indices where df age is great than or equal to 30. Then I say, of this array of values, give me only the values where the indices are equal to the indices where this statement is true. Also allows you operate on your data frame in a vectorized and item by item way. What does it mean to operate on data frame in a vectorized way? Let's say we have the following data frame. This data frame has 2 columns, 1 and 2. And 4 rows, a, b, c, and d. All of the values are integers. We can call data frame that apply and provide us the argument sum arbitrary function. In this case, numpy.mean to perform that function on the vector that is every single column. So when we call df.apply numpy.mean. What we get back is the mean of every single column in df. This is itself a new data frame. There are also some operations that simply cannot be vectorized in this way. That is, take an numpy array as their input and then return another array or value. We can also, in this case, call map on particular columns. Or apply map on entire data frames. These methods will accept functions that take in a single value, and return a single value. For example let's say that we said df one.map lambda x x greater than or equal to 1. What this does is goes through every single value in the 1 column, and evaluates whether or not that value is greater than or equal to 1. If we were to call df.applymap lambda x x greater than or equal to 1. The same function is evaluated over every single value in the data frame. As opposed to just the 1 column. Why don't you try doing some basic analysis on our Sochi Olympic medal count data? Using the data frame you created in the previous exercise, compute the average number of bronze medals earned by countries who earned at least one gold medal. Save this to a variable named avg_bronze_at_least_one_gold. Your code should go here All right, why don't we walk through the solution? Here we've created the olympic_medal_counts_df data frame as we did in the previous exercise. Now we're going to define this variable called bronze_at_least_one_gold. What we're doing here is we're picking out the bronze column of the olympics_medal_counts_df data frame. Then we're picking out only the subset of that column. With the indices matching the indices where olympic_metal_counts_df's gold column is greater than or equal to one. Then, we're using numpy.mean to take the average of those values in assigning them to this variable, avg_bronze_at_least_one_gold. Why don't you try operating on our Sochi medal count data in a vectorized way? Using pandas, numpy, and the dataframe's apply method, create a new dataframe called avg_medal_count that indicates the average number of gold, silver and bronze medals that were earned amongst countries who earned at least one medal at the 2014 Sochi olympics. Your code should go here. All right, why don't we walk through the solution? Here we define our olympic_medal_counts_df data frame as we have in the past couple of exercises. Then we simply say avg_medal_count is equal to olympic_medal_counts_df and we pick out only the gold, silver and bronze columns and then we use the data frames apply method and provide us the argument numpy.mean I'd like to discuss one more neat trick that Numpy will allow us to do very easily. Numpy has a numpy.dot function that will allow us to easily take the dot product between two vectors. Let's walk through what it means to take the dot products first. Let's say I have an array called a, whose values are 1, 2, 3, 4, and 5. And then I had an array called b, whose values were 2, 3, 4, 5, and 6. These two vectors are indexed in the following way. Let's say that the 0th entry of A is 1 and the 0th entry of B is two. And the first entry of A is 2, and the first entry of b is 3, and so on. The way that we take the dot product is we say we're going to multiply together the 0th entry of the two arrays. And then we're going to add that to the sum of the first entry of the two arrays. And then we're going to add that to the product of the second entry of the arrays, and so on. So in this case, we end up with 1 times 2 plus 2 times 3 plus 3 times 4 plus 4 times 5 plus 5 times 6, which is 70. We could easily do this in Numpy using numpy.dot. Just watch. Here I define a and b just as we did a moment ago. If we just say numpy.dot (a,b), I get 70. I can also use the Numpy.dot function to multiply arrays into matrices. If you're not familiar with how to multiply an array into a matrix, you should follow the online resources found in the instructor notes. But for now, just know that if we have this array 1, 2 in this matrix, 2, 3, 4, 5, 6, 7, we would essentially multiply them together and get an array where the first entry is 1 times 2 plus 2 times 3. The second entry is 1 times 4 plus 2 times 5. And the third entry is 1 times 6 plus 2 times 7. If we perform this multiplication down here, we would get a two-entry array where the top entry is 2 times 8 plus 4 times 9 plus 6 times 10, and the bottom entry is 3 times 8 plus 5 times 9 plus 7 times 10. Since these multiplications don't commute, Numpy.dot is smart enough to throw an error if you try to ask it to do something that doesn't make sense. Let's say I wanted to perform this multiplication right here and I called this guy a and this guy b. I could simply call numpy.dot(a,b) and what I get back is 8, 14, 20, just as we would get if we performed this arithmetic here. All right, why don't we walk through the solution. Here, we're going to start with our olympic_medal_counts_df DataFrame, as we have in the previous exercises. First, we're going to create a new, smaller dataframe called metal counts, which is the olympic_medal_count_df DataFrame, but only the gold, silver, and bronze columns. Then we're going to use the numpy.dot function to matrix multiply this metal_counts_matrix with the 4, 2, 1 array which represents the number of points that each country would score for a gold, silver, or bronze medal. So what we're going to get here is an array, where the value is 4 times the number of gold medals plus 2 times the number of silver medals plus 1 times the number of bronze medals. Down here, I'm going to define this Python dictionary called olympic_points, where again I provide us the keys, the column names, and as the values panda series, where I provide as the argument first the list of country names and here, this array points, which again has the total number of points that each country earned. Finally, I'm just going to say olympic_points_df is DataFrame of olympic_points. This is just the tip of the iceberg when it comes to pandas and numpy's functionality. If you're interested to read more about what these libraries can do, I encourage you to check out the full documentation which are found in the urls in the instructor comments below Through the class project, you'll investigate New York City subway ridership as a data scientist might. First, you'll pull some publicly available data on subway ridership, and also on New York weather conditions, using the New York MTA website and the Weather Underground API. Then, you'll answer some questions about subway ridership using statistics and machine learning. Does the weather influence how many people ride the subway? Is the subway busier at certain times than others? Can we predict subway ridership? Finally, you'll develop some charts and graphics that communicate your findings, and synthesize everything into a cohesive write-up that your friends or family might find useful and informative. This may sound daunting, but we'll be going through this step by step, and learning how to use the necessary tools as we go along. So for me, the reason why I even come into this field of data science is my passion for something specific, which is natural language. Right, so I, and it, I observe the people who work around me who knows about data science. I think one thing that's very important is they either have, like, a passion for the particular data they're looking at. Like, you know, natural language, or like speech recognition kind of data. Or some people are just very interested in patterns in data. Like when they see some kind of some, data they would just like, try to calculate you know, like the mean of the data, the variance of data. They, they, it comes natural to them because they want to find patterns to the data. So I think for anyone who wants to become a data scientist, it's good to think about, what kind of data you are interested in doing, and start with that. And then later on when you have this skill of analyzing data, you can apply it to any other kind of thing. To recap today's lesson, data science is a discipline that incorporates methods and expertise from a variety of different fields, to glean insights from large data sets, and then use those insights to build data driven products, or to create action items. There's no cookie cutter way to become a data scientist, but a lot of data scientists have a very strong background in mathematics and statistics, and also have the programming skills to handle these data sets and also to perform analysis. Currently, data science is most closely associated with the tech field, but more and more data science is being used all over the world in a variety of different fields. In order to solve new and old problems, more efficiently and effectively. Now I know what you're thinking. This sounds awesome, data science sounds really cool, I want to work on a project get me some data. Unfortunately, data seldom comes in formats that we want. It might be in a format we've never seen, it might come from a weird source. There might be missing or erroneous values. Because of this, any data scientist worth their salt is skilled at acquiring data. And then massaging it into a workable format. That's what we're going to be covering in the next lesson. Welcome to the second lesson of Introduction to Data Science. In the first lesson, we discussed data science at a high level. So we talked about the skills that a data scientist usually has, and also some of the problems that you can solve using data science. One of the most important skills that a data scientist can have, is the ability to extract and clean data. This is usually referred to as data wrangling or data munching. Believe it or not, some data scientists say that they can spend up to 70% of their time data wrangling. This may sound crazy to you, but it's not hard to imagine this happening. Let's say you have some really cool analysis project that you want to do. Say, look at a variety of factors, and figure out why the life expectancy in City A is higher than City B. Well okay, let's say that all this data lives on a website. So, you write some scripts to go and pull this data from the website. Great, but then you need somewhere to store the data, so you're going to need a database. So okay, we, we use a database, we store all the data there. And then you look at the data and you realize, oh wait, there are a bunch of missing values, or a bunch of this data looks weird. So you need to develop some, some method to take all this data and clean it up. This is data munching. And all of this is necessary if we want to answer our original really cool question. Why is the life expectancy in city A, higher than the life expectancy in city B? In this lesson, we'll discuss data munching basics using 3 data sets. One containing information on a bunch of baseball players, called the Lawmen baseball database. Another containing information on enrollments in an Indian identification program called Aadhar, and also data from the last FM API. Using these 3 data sets, we're going to discuss the formats that data can come in, how you can acquire data from these various data sources, and also how to inspects the data and see if it might have any missing or erroneous values. Alright, well let's get started. Hello, my name is Nick Gustafson, and I'm a data scientist at Udacity. My background is I did my undergrad in neurobiology and I went on in graduate school to continue studying neurobiology, and neuroscience in particular, and kind of, computational neuroscience and looking at algorithmic And all mechanisms of learning and decision making. And while towards that, hm, tail end of my graduate work I, eventually kind of realized I did not want to stay in academia, however I still have this kind of strong passion for analyzing data, modeling data, understanding You know, patterns within it, and kind of, you know, applying scientific methods, and data science just, just felt like a natural progression from there. You may ask yourself, what is data munching? Let's work through an example and demonstrate the concept to you. Imagine we're trying to figure out if right handed or left handed baseball players have a higher batting average and we're given this table of data to answer the question. Let's look at the table. Look, this value is incorrect. You can't have a batting average greater than 1,000. Also, it looks like we don't have any left or right-handedness for Ichiro Suzuki. Data wrangling is the art of dealing with and or converting missing or ill-formatted data into a format that more easily lends itself to analysis. But before we can do that analysis, we first have to get the data that we want to analyze. Three of the most common sources from which can get data are from files, from a database, or from websites through web APIs. We'll cover all three of these in this lesson. Before diving into the nitty-gritty details of data munching, I want to ask if you've ever had to analyze really messy or unorganized data? Even if you're not a data scientist, you've probably dealt with unwieldy data at some point in your life. Maybe you had trouble reading the receipts from a bake sale you organized, or you tried to keep track of your personal budget in an Excel spreadsheet. What methods or tools did you use to make your life easier? Please share your experience with the class by typing your answer in the forum. I've added a link to the discussion thread for you in the instructor comments. By now you've realized that data is often messy and unorganized. Because of that, a large amount of a data scientist's time is spent extracting and cleaning the data that they will use to perform analyses or make visualizations. Let's get our feet wet and first discuss the various ways that we might load our data. If we want to programmatically process and analyze our data, it's very important to understand the structure of the data itself. The did it, that also will, depend on the project Say that one more time. On average, over 50% of the time it's just kind of. Well, maybe be spent like, combing through the data and like just, figuring out maybe idiosyncrasies in it. And You know. You quickly apply an algorithm, you look at it. You don't get the results you expect, so you dig in a little closer and like see that there are these weird little kind of edge cases that are, are ending up more prominent in your data than you previously expected. So, you have to go back and scrape and comb through it again, and it's just this iterative back-and-forth process. Often acquiring data to do analysis doesn't require any particularly fancy methodology. It's just a matter of finding the right file somewhere on the internet and downloading it. A lot of the data that exists out there, particularly on government websites, is stored in text files. For example, let's say we wanted to get a database of all major league baseball statistics. I kind find this data at http://www.SeanLahman.com/baseballarchives/statistics. If I visit this website, I can see that the data I want is available in a variety of formats. A comma delimited or CSV version, but also a Microsoft Access version, and a SQL version. Three of the most common data formats are CSV, XML, and JSON. I want to quickly discuss what one record in the Lahman Baseball Database would look like in each of these different formats. It just so happens that the first row in this data set corresponds to a pretty famous baseball player. Hank Aaron. Let's first discuss what this data looks like in the format we originally downloaded it in, CSV. In the CSV format we usually have a series of rows. With each row corresponding to an entry. There is a header row at the top of our file. Where our comma separated values do not correspond to an entry but instead tell us what each entry means for the rest of the document. In the case of this baseball document, our first row tells us that every row will have some identifiers, the Lahman ID. The player ID, the manager ID, et cetera. Then we'll see birth year, birth month, birth day and then a bunch of other statistics. If we go and look at the entry for Hank Aaron. We see all of these values, 1 for Lahman ID, aaronha01 for player ID, et cetera, separated by commas. Note that if a particular player doesn't have a value for one field, for example Hank Aaron does not have a manager ID, we simply see two commas in a row. In the case of an XML document, we end up with something that pretty closely resembles HTML. We can have a document element, which is opened. It can then have a series of tables, which are also opened, as we see here. The table has a number of children, which correspond to the values that we just discussed in the CSV 1 for Lahman ID, aaronha01 for player Id, etc. Note that when we're missing a value in XML, things are treated a bit differently. For example, since Hank Aaron was not a manager, we just open the manager ID field and then put a slash at the end. Finally, for a JSON document, we have a number of JSON objects indicated by these curly braces. A JSON object looks a lot like a Python dictionary. We have keys. Which correspond to what we would see in the header row in a CSV file followed by values. Note that in the case of a missing value, we simply open and close the quotation marks. Oftentimes, the benefit of XML or JSON is that they support nested structures in a way that CSV documents cannot. I don't want to go into this in great detail, but you can imagine that the value corresponding to a particular JSON key, say braves, could itself be another JSON object. Here's what that might look like. One final note, is to remember that when we talk about different formats that data can come in, it's not a matter of the file extension being .CSV or being .JSON. A file format really has to do with how the data is organized inside of a file. So we could easily have data that is formatted in JSON or CSV, but that comes in a file whose extension is .txt. Since the online MTA subway data comes in CSV format, in the weather underground API responds to requests with a JSON object. We'll discuss how to load and process files in these two formats. While we won't explicitly discuss loading and processing XML files, it isn't that different than the material will cover with regards to JSON and CSV. If you'd like to learn more about dealing with XML data in particular. I recommend you take the data wrangling class with Mongo DB taught here at Udacity. For now, let's discuss how to work with CSV files. Let's revisit the CSV baseball data that we downloaded earlier. CSV is a very popular way to store data, likely because it's easy to read and comprehend for both humans and computers. It also probably doesn't hurt that Microsoft Excel can export data as a CSV. As you can see, every element of each row is separated by a comma. If we wanted to load this data into Pandas, we can do this in one line. It's super easy. We simply import pandas, and then write baseball_data equals pandas.read_csv Master.csv. This will load the comma separated data into what Pandas calls a data frame, where we can retrieve each column like this, print baseball_data name first. We can also create new columns on the data frame by manipulating the columns in a vectorized way. For example, if I wanted a new column that was the sum of each player's height and weight, called baseball_data height plus weight, I could write baseball_data height plus weight equals baseball_data height, plus baseball_data weight. Now, say that I wanted to write my data to a new CSV file that included my new height plus weight column. Again, this is a simple one-line process using Pandas. I can simply call the data frame objects to CSV method, and provide as an argument the file name of the file I wish to write to. In this case, baseball data with weight height dot CSV. There are a number of optional keyword arguments I could provide here beyond the file name. For example, if I wanted to use a separator besides a comma, if I wanted to specify how to handle missing values, et cetera. You should check out the panda's documentation if you'd like to learn more. Alright, we've shown you how to load some data into pandas from a CSV file. Now, let's see if you can load a csv into python and manipulate the data in a basic way. What we've provided here is the skeleton of a function called add full name, which takes in two arguments, path_to_csv and path_to_new_csv. Assume you'll be reading in a CSV file with the same columns as the Lahman baseball data set. Most importantly, there are columns called, nameFirst and nameLast. Write a function that reads the CSV located at path_to_csv into a pandas dataframe, and then adds a new column called nameFull, with that player's full name. For example, in the row corresponding to Hank Aaron, nameFull would be Hank Aaron. After that, write the data in the Pandas dataFrame to a new CSV, located at path_to_new_csv. Your code should go here. Alright, let's walk through the solution to this assessment. First, we import the pandas module, then we read in the csv located at path_to_csv, using the pandas read_csv function. Then we're going to create a new column in the dataframe called nameFull, which is the concatenation of the nameFirst column, a space, and the nameLast column. Finally, we're going to use pandas to_csv function, to write the new data frame to a new csv file located at path_to_new_csv. We've talked about loading data from flat files by way of CSV, but a lot of the time, the data that data scientists work with are stored in relational databases. A relational database is similar to a collection of spreadsheets. In each spreadsheet, there are columns and rows. A column specifies a value and its type such as player ID or team ID or record. Each row contains values for each column such as 1 or 1 or 245-150. Or 1 and Yankees, things like that. In the database world, we call each set of rows and columns a table rather than a spreadsheet. And the tables are usually related to each other in some way. It's very important to be somewhat familiar with relational databases. Since they're used in so many places and also since we'll be storing some of the data required for our class project in a small database. Let's take some time and talk briefly about how one can store and retrieve data from relational databases. In this course, we won't discuss the design of databases. Topics like normalization, indices, keys, things like that. Some of these topics will be covered in Data Wrangling with Mongo DB. To illustrate concepts with relational databases, we're going to use some data on the Aadhaar program, that is acquired from India's online Open Data Portal. Aadhaar is a relatively, new initiative in India. It is a 12 digit, unique number that, enables identification for every resident of India. According, to the Indian government. Once instituted, the Aadhaar will provide a universal identity infrastructure, which can be used by any identity-based application. The dataset, we'll be using describes the number of people, who enrolled in or were rejected by the Aadhaar program. Cut by various characteristics, such as district, subdistrict, gender, and age across India. As data scientists, the Aadhaar program is interesting to us, because it is a huge dataset, representing one of the most significant populations on Earth. We can study the Aadhaar enrollment dataset to learn a bit more about India's population. And also, what type of people are applying for and successfully enrolling in the Aadhaar program. Before we begin our discussion of relational databases, why do you think a database might be useful? Check all options that apply. It is straightforward to extract aggregated data with complex filters. A database scales well. It ensures all data is consistently formatted. Data is redundant. Relational databases are a hot, new technology. Alright. The correct answer is that relational databases are useful because it is straightforward to extract data with complex queries. A database scales well. And relational databases ensure data is consistently formatted. What do each of these mean? Well, first off it's easy to extract data from the database with a complex one line query. We can easily say choose all records for people where their age is greater than 50, and their weight is less than 50, and the city is equal to Mumbai. We can do this with flat files as well, but its a lot more work. Database is also scale well. Its not uncommon to have databases with hundreds of thousands or millions of entries. Since all information is ideally stored in one location, It's easy to update, delete, and add new data to the database in a scalable way. Think of the [UNKNOWN] data for example. India has a population of 1.2 billion people. That's a really big data set. It's important to have a solution that scales well. Finally, relational databases have a concept of something called a schema. Which basically says that each column in a table Is always of the same type you can't have some people's age be a string while the age of others is an int. Relational databases are built to have as little redundancy as possible that way if we want to update a value we only have to do it in one place and we can ensure that our data remains consistent throughout the entire database. Also, relational databases are well established and have been used for some time. Even if they were hot and new, that's not a great reason to use a technology. And now that we know why relational databases might be useful, let's discuss how to use them in more detail. How does the Aadhaar data fit into the definition of relational database that we just covered? In the case of the Aadhaar data, our database would have just one table called Aadhaar Data. It has numerous columns for example, registrar, enrollment agency, state, district, and a number of others. We can imagine a related table might have information on all the Indian districts such as their populations and their size. Relational databases always have a schema. What is a schema? In layman's terms a schema is a blueprint that tells the database how we plan to store our data. More specifically, a schema basically says that for a given table, every single row or entry will have the exact same number of columns that correspond to the same values and that each column's value will be formatted in the same way. So, for example, in this table, I can say that there are four columns: district name, capital city, size, and population. District name and capital city might always be strings, where size might be a float, and population might be an int. This list of columns and their requirements on how their values are formatted define the schema for this table. Now let's add some entries to the table. Let's say the first [UNKNOWN] I wish to enter is corresponds to the district Gujarat which has 60,439,692 people, has an area of 196,024 square kilometers and has a capital city of Gandhinagar. All of these entries fit the schema, so inserting this row into the table is fine. Now, let's say I had another I wish to add, the Maharashtra district, which has a capital city of Mumbai, and I provide no other values. This would also be allowed, but for all columns for which I don't specify values, the value would either be set to null or default values that are stored in the table definition. Now, say that we want to try a new row to our table. Madhya Pradesh, 28, 50.0, 60. Would this new row be allowed in our table, given the schema? Check Yes or No. Inserting the row, Madhya Pradesh, 28, 50.0, 60 into our aadhaar table would be allowed, despite the fact that it's sort of gibberish. Madhya Pradesh's size will be stored as 50.0 square kilometers and it's population would be stored as 60. The interesting thing here is that the capital would be stored as 28, a string which doesn't really make much sense. But, in any case, the data would still be stored. You should always be careful to define the data types for your columns in a sensible way. For example, storing numbers as strings is problematic. Since it disallows us from doing numerical computations on the column. For example, taking the average value or finding the minimum value. Alright, now let's say we have a bunch of our data in a relational database. Your next question is probably, how do we get it out? And what can we do with it? Well, let's try to learn about this using our aadhaar data. Now, data is usually retrieved from a relational database using one of a family of languages that I'll call SQL like languages. Well, there are some small variations between the languages. They have the same general concepts and syntax. Now, how could we query the Aadhaar enrollment data set with SQL like syntax, if data was in a database. Stored in a table called Aadhaar data. Well, the most simple thing that we might want to do is simply to select all of the data, that is, all rows and columns. If we wanted to do that, we could type something like select star from aadhaar data. Note that the SQL syntax is pretty human readable. We're just saying we want to select star or everything from the aadhaar data table. If we were to run this command as is, we would produce tens of thousands of rows which is probably not super constructive right now. So what we can do is, limit the number of rows by just saying limit 20 at the end of our SQL command. If we ran this query, let's see what we get back. Oh, great, the first 20 rows of the data. These are rows describing enrollments for the registrar Allahabad Bank, with various enrollment agencies, districts, age, genders, et cetera. Now, say we only wanted some of these columns. Say, district and subdistrict. We could just ask for those specific columns instead of star. For example, we might write, select district, subdistrict from aadhaar data, limit 20. Alright, why don't you try writing a quick sequel query of your own on the India aadhaar data set. What we're going to do is read in an aadhaar data CSV to create a pandas data frame which can be queried using sequel like syntax. We'll rename any columns that have spaces in them with underscores and set all characters to lower case. So if the columns names more closely resemble columns names one might find in a table. Why don't you write query that will select out the first 50 values for registrar and enrollment agency in the aadhaar_data table. Your queries should go here. So that's how we pull all of the data out of SQL database. Alright, why don't we walk through a query that will generate the correct results? Note that we select the two columns that we want from the table, registrar and enrollment_agency, from aadhaar_data. In order to get only the first 50 rows, we add a LIMIT 50 to the end of our query. So the correct query is SELECT registrar, enrollment agency FROM aadhaar_data, LIMIT 50. Let's discuss a couple of more complex things that we want to do. We may not always want to pull thousands of rows, or even the first 20 rows in order. Maybe we want a specific subset of our data. For example, maybe we're interested in all data corresponding to the state Gujarat. In order to do that, we'd simply write, select star from aadhaar_data, where state equals Gujarat. Note that to limit our results, the where clause needs to go directly after the table name. SQL syntax really does mirror the way we might say things out loud very closely. Now, let's see what happens if we run this command. And there you see, we have all the data where the state is equal to Gujarat. The last thing that I want to quickly discuss ,are some functions that usually exist in query languages like Sequel. Such as ,group bys and aggregate functions. So let's say that I wanted to create some transformed version of my data. For example, what are the total number of enrollments per district? . I could write something like this. Select district,. Sum (aadhaar - generated) from aadhaar - data, group by district. Let's talk about this a little bit more. What's happening exactly? . Sum is what we call an aggregate function. An aggregate function takes some set of values, usually numbers and performs a mathematical operation on them. We've used sum ,but other aggregate functions include count. Min, mean, max. You get the idea. Operations that one could perform on a collection of numbers. But wait. Every single row is only one number. So how do we get to collections of numbers? What we basically say here is take each distinct district. And then for all of the different values of aadhaar_generated corresponding to a row ,for that district. Sum them up. So, we start with our aadhaar data table. Take each district ,and sum up the count aadhaar generated. Now in order for our results to make sense, we are only going to want one row in our output for each district. So we throw in this group by clause on the end, which essentially says. Let's only have one row per district in our results. There can be numerous clauses in a group buy. So we could also say this, select district, subdistrict, sum aadhaar generated, from aadhaar data, group by district, subdistrict. Note that whatever columns we select, that we don't aggregate, we need to group by. In this case, district and sub-district. We could also put a where clause here, so in order to sum up aadhaar generated for people over 60 in each district, I can just add a where clause in after the table name, as we discussed earlier. If we were to run this query. Giving us select district, sub-district, sum aadhaar generated, from aadhaar data, where age greater than 60, group by district, sub-district. If we were to run this query, we would have a row for every combination of district and subdistrict. And we would also have for each row, account of how many aadhaar were generated ,for people over the age of 60. So now that we've discussed how to write slightly more complicated sequel queries, why don't you give it a try? We're going to read in our aadhaar_data csv to a pandas dataframe. Afterwards, as we did before, we'll rename the columns by replacing any spaces with underscores, and by setting all characters to lowercase. So that the column names more closely resemble column names you might find in a sequel table. Can you write a sequel query that will select from the odd hard data table? How many men, and how many women, over the age of 50, have had aadharr generated for them in each district? Your query should go here. The correct answer is SELECT gender, district, sum aadhaar_generated FROM aadhaar_data WHERE age > 50 GROUP BY gender, district. Let's talk through this query. First, we wanted to know how many men and women in each district had aadhaar generated. So we select gender, district and sum aadhaar_generated from the aadhaar_data table. Since we're using an aggregate function, we need to include a group by, with our non-aggregated fields, in this case, district and gender. Finally, we want to limit this to men and women over the age of 50. So, we include the where clause, WHERE age > 50, after the table name. Previously, we've shown you how to access data that sits in a file or a database. But what do you do when you want to access data that sits in some website, like Twitter data? We could just go to twitter.com and search for Udacity, but then we'd have to manually write down all the data. We could try to rate a web crawler to go through the pages HTML, but as you can imagine, that could get kind of complicated. Luckily, many tech companies like Twitter allow users and developers to access data directly in a simple, machine readable format. They do so through something called an Application Programming Interface, or API. There are several different kinds of APIs, but one of the most common types, and the one used by Twitter, is a representational state transfer or REST API. I know I sound like a broken record here, but if you'd like to learn more about the nitty gritty behind REST APIs, I'd recommend you take the data wrangling class with MongoDB. Let's use last.fm's API as an example, and let's see how we can interact with it to get data and find out the top albums in the US, China, and Spain. As you can see on the left hand side here, there are a few different last.fm API methods that we can talk to and retrieve information from. For example, let's check out the album.getInfo method. As you can see this page tells us the type of data that the API method will return like artist name, album name and a bunch of other information such as language or a music brains ID from the album. But the question is now how do we actually get to this data from last fm's API. It's actually really simple. We can simply copy and paste a link like this into the web browser. And look at all the information that last.fm returned to us. Okay, now I want to point out a few interesting things about the URL. See all the parts after the question mark? That's how we define API parameters. Like the method. The API key, the artist name and the name of the album that you want data for. So for example, if we wanted information on a different Rihanna album, like the album Unapologetic, we could simply change the album parameter here from Loud to Unapologetic. Let's see if that works. Look, we now have the data from Rihanna's album, Unapologetic. Pretty simple, huh? I want to highlight another thing. Let's look at this data. It looks kind of, strange, right? This data's actually in JSON format. The same JSON that we discussed earlier in this class. As I insinuated earlier, we can kind of, think of JSON data as a Python dictionary. Let's make this data easier to read and I'll show you what I mean. See how each piece of data in this JSON object corresponds to a key, very much how like every value in a Python dictionary corresponds to a key. So our first key here is album. And the value is actually another JSON object. Here we have key name, value unapologetic, key artist, value of Rihanna, and so on. Although JSON is not equivalent to Python dictionaries, you can think of them as similar abstractions. You may think it's pretty inefficient that we have to type a URL into the browser every time we want to grab some data from last.fm's API. There has to be a more efficient way to do this and you're right. We can write a simple Python program that uses the JSON and request libraries to do exactly what we did manually a few moments ago. If we run this program right here, let's think of what it returns. Look, the data that this Python program returned is exactly the same based on data that you saw in the browser a second ago. This program was less than 10 lines. First we specify a URL, then we simply say request that get, that URL and call.text, in order to get the text. We assign that value to data. We print type of data which we saw was Unicode, and we print data itself, which was the JSON object. But if we go back and look at the JSON object itself, we see that it's in this funky string format that makes it very hard for us to parse out the interesting information. We could write a Regex to parse out what we want, but that can get pretty painful really quickly. The JSON library will make interacting with JSON data as easy as 1, 2, 3. Let me show you how. We've modified our script ever so slightly. After initially assigning data, we reassign it. We say data equals json.loads data. What json.loads does is it interprets a string and assumes that it's a JSON object. By doing so, we convert the JSON data into a Python dictionary. We'll see that when we print type data. Once we convert the data into a nicely structured dictionary, we can get the interesting bits out as if we're simply accessing a dictionary. For example, typing data artist. Let's see what this script produces now. As we can see, what we return here is that the type of the object is a dictionary and then we have a Python dictionary. Now, let's put everything together. Let's use last.fm's geo.gettopartists API call to write a short program to find the top-performing artist in Spain. Here's a hint, on the page here it shows that we have to pass in the country parameter. Also, you may want to use the JSON library so it will be easier for you to read the JSON data. If you need to register for your own API key, you can do so here. Your code should go into this space. Alright, let's work through the solution. First we import the json and request libraries. Then we provide the URL that we're going to be making our API call to. Then we make our API call using the request library, and load the results into a dictionary. Finally, we print out the name of the number one top artist. In the data dictionary, we will look at the top artist's key. Then we look at the artist key there. We look at the first entry. And we look at the name parameter. As you can see, the URL is the biggest difference between this code and the sample code in the previous video. Mainly I've just changed the parameters and the URL variable. To make sure that I'm getting the top artists, I am calling the geo.gettopartists method. I'm also passing the country parameters in by setting the parameter country equal to Spain. And once I have retrieved the data, I'm loading it into a json file in this line. Data equals json.loads(data). This lets me find the top performing artist quickly and easily by accessing it almost like a python dictionary. You could also load the data as json and try to just parse it with the regex, but I personally think that would be really painful. Now that we have acquired our data, whether the informal flat file sequel like relational database, or an API, you might think we are ready to dive in and do some analysis. Alas, I am afraid, we are not quite there yet. There can often be problems with our data. It might not be in a format that allows us to easily perform the desired analysis, or maybe there's a bunch of bad or missing values. In this next section, let's discuss how to deal with these issues so that we can get to the exciting part of any project, the analysis. Let's talk briefly about sanity checking our data. What does it mean to sanity check data? Well, put briefly, if we're sanity checking data we want to quickly determine, does the data make sense? Is there a problem? Does the data look like I expect it to? There's a ton of work you can do to sanity check your data. We can draw plots to visualize the data. You can run some basic analyses and tons of other things. I don't want to focus too much on this in this course. If you'd like to dive deep into the subject, I'd recommend taking you down to these exploratory data and analysis course. However, to do just the bare amount of sanity checking, Pandas DataFrames do have a useful method called the Scribe. Let's illustrate how Panda's describe function works, using our Laman baseball data set. Say that our baseball data was loaded into a Panda's data frame called baseball, using Panda's read.csb function. Like so. If we call baseball that describe, what do we get back? You can see that baseball that describe returns a data frame in it of itself. For every numerical column, we see the count, mean, standard deviation, mean, 25%, 50%, 75% and maximum values. We can do some quick checking to make sure there are data generally make sense. Here. LahmanID has actually been read in as a number, which is a bit misleading. We won't be doing any arithmetic on it. But we see that the minimum birth month is one, and the maximum birth month is 12, as we would expect. We see that the minimum birth date is one, and the maximum birth date is 31. That makes sense. And we see that the mean birth year is actually 1928, which to me is surprising. I'd think that it would be a little bit later. Investigating values like this, we can tell pretty quickly what our data looks like, and whether there might be any significant outliers in our data. In other words, are the min or max way larger than the values corresponding with the 25th or 75th percentile. Although we won't discuss exploratory analysis of data in depth, there's one thing that you might notice when looking at a summary of your data. It may have a bunch of missing values. This is evidenced here by the differences in count for our various columns. Since this is a particularly common problem. Let's discuss why values may be missing. And different methods we can use to mitigate the effect of those missing values on our analysis. In just a moment we're going to discuss what to do when there are missing values in your data. But before we get into the technical details you might ask yourself why that data's missing in the first place. Why do you think that values may be missing from your data? Share your thoughts in the box below. Don't worry, your response won't be graded. There are a number of different reasons why values may be missing from your data. But here are a couple that I think are particularly common or important. So, the first is that occasional system errors may prevent data from being recorded. Another reason that data may be missing, is that some sub set of subjects or event types are systematically missing certain data attributes. Or maybe missing entirely. Let's dive deeper into both of these reasons. One explanation for missing data is that something may be failing occasionally when collecting data. For example, if you were conducting a traditional door-to-door survey, maybe the person handling the documents loses a few or tends to miss some streets at random because they don't know the geography of the town well. One explanation for missing data is that something maybe failing occasionally when collecting your data. For example, if you were conducting a traditional door to door survey, maybe the person handling the documents looses a few, or tends to miss some streets at random because they don't know the geography of your town very well. This is a case of a researcher or collection technique causing missing data. The offline example of someone conducting a traditional door-to-door survey can easily be translated to the digital world. Imagine your data collection software fails one out of every 1,000 times, or you have a couple of service outages. You could have similar data loss. If this were to happen in our baseball player data set, we might see some missing values here and there, just because we made some error in collecting them at random. Another more harmful reason values may be missing is referred to as non response. In the case of non response, a certain subset of people chooses not to answer particular questions. Or don't respond at all. This can lead to biases in your data and consequently, false conclusions. In the case of baseball data, say we were interested in the batting average of players. Maybe, since they're, I don't know, self-conscious about their batting average. All outfielders just don't provide a value. This would make our results inaccurate and lead us to conclude that the average batting average was 312 even though that is based only one data point here, Derek Jeter's. The important point here is this, if the missing values from your data are distributed at random. Your data may still be representative of the population. However, if values are missing systematically, it could invalidate your findings. It's important to design your experiment or data collection method to minimize these effects. Beyond this, it's very important to check your data to see if such effects are present. So let's say that there are missing values in your data and they're distributed at random. What do we do? There are two approaches that I want to discuss here, partial deletion and impution. Partial deletion is exactly what it sounds like, limiting our data set for analysis to the data that we have available to us. Let's discuss partial deletion further. For the following discussion let's use the Sean Lawman database of baseball players. There are a number of players for whom many values are missing. Now let's say that we wanted to ask a couple of different questions. For example what are the average life span and average height of baseball players? There are many players that we can't include in the analysis, either because their birth date or death date is missing, or because they're still alive. In this case, we may want to perform partial deletion. One method we could use is called Listwise Deletion. In the case where we perform Listwise Deletion, we'd exclude a particular data point from all analyses even if some useful values were present. So if using Listwise Deletion, Barry Bonds, who has a listed height, but not death date, would not be included in either analysis, even if we were calculating the average height of players. On the other hand, in the case where we might perform Pairwise Deletion, we would exclude a particular case from the analysis for tasks which are not possible with the data at hand. So we wouldn't include Barry Bonds in our sample when calculating life span since there's no death date, but we would still use his height when calculating the average height of all baseball players. In scenarios where we don't have very much data, or where removing our missing values would compromise the representativeness of our sample, it might not make sense to throw away a bunch of our entries just because they're missing values. This could severely impact the statistical power of whatever analysis we were trying to perform. In this case, it likely makes sense to make an intelligent guess at the missing values in our data. The process of approximating these missing values is referred to as imputation. There are many different ways to impute missing values. And different techniques are constantly being developed. I want to quickly discuss some relatively simple ways to impute missing values in our data. Let's note that imputation is a really hard problem. Each of the methods we'll discuss introduce a certain biases or inaccuracies into your data set. We're discussing some of the most simple ways to impute data, but much more sophisticated and robust methods are out there. Let's first discuss what would seem to be the easiest way to impute a missing value in our data set. Just take the mean of our other data points and fill in the missing values. So, for example, let's say that Ichiro Suzuki and Babe Ruth are missing values for weight in our baseball data set. Well, okay, no problem. We can just take the mean of all other players weights and assign that value to Ichiro and Babe Ruth. In this case, we would assign Ichiro and Babe Ruth both a weight of 191.67. Wow, that seems really easy, right? There's gotta be a catch. Well, let's first discuss what's good about this method. We don't change the mean of the height across our sample, That's good. But let's say we were hoping to study the relationship between weight and birth year. Or height and weight. Just plugging the mean height into a bunch of our data points lessens the correlation between our imputed variable and any other variable. Another method that we could use to impute missing values in a data set is to perform linear regression to estimate the missing values. We'll cover linear regression in more depth in the next lesson. But the general idea is that we would create an equation that predicts missing values in the data using information we do have, and then use that equation to fill in our missing values. Okay so, what are the drawbacks of using this linear regression type technique? Well, one negative side effect of imputing missing values in this way is that we would over emphasize existing trends in the data. For example, if, if there is a relationship between date of birth and height in MLB players, all of our imputed values will amplify this trend. Additionally, this model will produce exact values for the missing entries, which would suggest a greater certainty in the missing values than we actually have. In any case, let's say we did want to fill in the missing values for weight in our baseball player data. We could train a linear model using the existing data that we have, and then use that model to fill in these missing values. Let's say we did want to fill in the missing values for weight in our baseball data. We could train a linear model using our existing data. That is, entries that have position, left or right handed batter, average, birthdate, deathdate, height and weight. And then use that model that we've created to fill in these missing values. Alright, why don't you try to impute some values on your own. Pandas dataframes have a method called fillna. Which take in a value and allow you to pass in a static value to replace any NAs that exist in a dataframe or series. You can call this function like so. Data frame column, fillna value. Using the numpy.mean function, which calculates the mean of an numpy array, why don't you impute any missing values in our Lahman baseball data sets weight column, by setting the missing values equal to the average weight. Your code should go here. All right, let's walk through the solution to this guy. It's a pretty straightforward one line solution. We simply say baseball weight equals baseball weight.fillna numpy.mean baseball weight. What we're doing here is saying let's compute the mean of all the existing non NA values in baseball weight. Then let's take all the NA's in baseball weight. And replace them with this mean value. Let's take the baseball weight column, and replace it with this new column, which doesn't have any NA values in it. These methods are literally just the tip of the iceberg when it comes to data amputation. I wanted to make sure that you were exposed to the concept, but there are much more robust and sophisticated methods to fill in missing values in your data. Filling in the missing values on a data set by computing the mean of the field in question, or fitting a linear model, are somewhat effective and simplistic methods, but they can both have negative side effects and amplify or attenuate preexisting trends in your data. Now that you're equipped with some basic skills necessary to acquire and clean up data, we'll use these tools in order to begin working on our class project, analyzing the dynamics and workings of the New York City subway system. First, you'll use your knowledge of how to acquire data via an API, to obtain weather underground data on New York City, for a month in 2011, and write it to a file. We'll then use our SQL skills to run some basic queries on the data and get a sense from what our day looks like. Once you're comfortable with the weather data, you'll grab a bunch of data related to the MTA subway itself from the MTA website. In addition to gaining some basic familiarity with the MTA data, you'll do some processing and cleaning of that data. Such that it'll be very easy to do analysis on the data in the very near future. The coolest one I've worked on so far. The one that I there are several that we've done kind of, on the team. But the one that sort of seemed most flushed out and seemed kind of have the best effect within the company thus far is the we made a coaching dashboard for the course managers, to kind of assign each student a risk index, which measures You know, roughly, like it looks at various features related to this, you know, behavior and usage of the site and tries to you know, figure out how well they're doing. And also on top of that, a sort of urgency index which then tracks sort of changes in risk and allows one to in theory kind of group students into trends. To recap, over the course of this lesson we've discussed a bunch of the basic skills required to do data wrangling or data munching. So we've discussed some of the formats that data might come in. Such as CSV, [UNKNOWN], and xml. We've also talked about how to get this data from a variety of sources. Such as SQL like databases, flat files, and API's. We've also talked about how to look at the data, see if things look all right, and if they don't, some of the methods to mitigate these problems, such as data imputation. Over the course of doing these basic acquisition and cleaning tasks, you'll usually discover some basic things about your data. But you might also want to do some more sophisticated analysis to learn some more subtle things about your data set. That's what we'll learn in the next lesson, and then we'll apply those insights to the New York City subway data. Welcome to the third lesson in Introduction to Data Science. During this lesson, we're going to be focusing on some basic methods in statistics and machine learning that you can use to analyze data. During the last lesson, we mainly focused on methods to acquire and clean data from a variety of sources, such as relational databases, APIs, and flat files. However we also discuss some basic methods that you can use to verify the integrity of your data. And while you're doing that you might learn some very basic things. For example, you might know who the tallest shortstop to ever play in major league baseball was, or, how many people over the age of 65 enrolled in the [INAUDIBLE] program in a particular district. You might find, though, that you want to answer some more subtle questions. For example, we might want to know, are left handed batters any better than right handed batters? Or, hey, is there any correlation between age and the [UNKNOWN] rejection rate? We'll use statistics and machine learning to answer these types of questions. We'll also find that these methods are really useful for answering questions about ridership on the New York City subway. For example, we might be able to discover whether time of day, or the weather outside, influence how many people are riding the subway. During this lesson, we're going to assume some basic familiarity with stats. Either you haven't seen the stuff or if you feel lost, it might be a good idea to enroll in Stats 95. I've included a link in the instructor comments. Alright, let's jump in. When performing an analysis, we can usually achieve statistical rigor by way of a significance test. These test to find whether or not a sample of data can disprove some conventional wisdom or assumption, with a predefined level of confidence. Let's first discuss at a high level why statistical rigors important. Let's say that you worked in an office of 1000 people, and you wanted to find out what the most popular favorite color in your office was. It would be a lot of work to ask all 1,000 people. So you decide to just poll 10 people to get a pretty good idea of the office's favorite color. When you ask these 10 people, each one says "blue." With that statistic, you might say," Well, since these 10 people out of 1,000 said blue is their favorite color, blue must be everyone in the office's favorite color." Of course both you and I have some intuitive idea that this probably isn't true. We just happened to come across ten people who all liked blue. Statistics formalizes this kind of logic. So we can assess the feasibility of a result that we get with a smaller sample when trying to say something about a larger population. You can also imagine a case where at first glance, our results might suggest that there's no significant difference but it turns out that there is. For example, say a website was running an AB test between two different versions of our landing page. They're trying to see which version of the page converts more visits to clicks. Say they run a test for weeks and weeks, placing 500,000 visitors into each test group. One strategy had a conversion rate of 50%, another had a conversion rate of 50.5%. If we run a significance test on this data, we can say with a 95% confidence level that there is a statistically significant, albeit small difference between the two strategies. So my name's Kurt Smith, I'm a data scientist on the Analytics team here at Twitter. Been here about two and a half years right now and I took, you know, a somewhat long and windy path to getting here. I actually started out in college studying chemical engineering and then ended up doing a PhD in computational fluid dynamics and molecular modeling And spent quite a while working in academia doing computational research I went from there I moved to San Francisco several years back, and I initially started working in the chemicals industry doing a lot of molecular modeling. And from there the first big leap I would, I may that I would say brought me more to the data science direction was, moving to a startup in the healthcare space, that was doing a lot of data analysis on a risk modeling around chronic disease. And that really was the opportunity, or gave me the opportunity to take a lot of my mathematical background, and apply it to data driven problems. After working in that area for a few years, I started to look around in other fields with interesting data challenges. And that brought me to moving to social media and Twitter. Okay, why is statistics useful in data science? There's a lot of reasons, obviously. The, the, the first reason, I would say, is, the most basic one and this goes back to why statistics was, you know, developed a hundred or so years ago. And it really is to make sure that you're making reasonable inferences from data. So you hear a lot of people talk about big data and the value of big data and how we can you know, have data on just about everything nowadays. And it's easy to get caught up in that and it's important to remember, that anytime, you're looking at any sort of data whether it's observational or experimental. You want to make sure that you're making valid inferences, right? You want to do things like check for statistical significance, understand confidence intervals. Make sure that you're not pointing you're working with in, in certain conc, wrong direction because you've over interpreted some data so I think that's the first, probably most important reasons why statistics is important to data science there's a lot more just working with data, day in and day out there's a lot of what I would call. Sort of, you know very fine-grain technical skill, to just make sure, you're doing the right thing, and you're extracting the most information you can out of data. Alright, so just to recap, why are statistical significance test useful? Check all that apply. They provide a formalized framework for comparing, and evaluating data. They can make a bad result look good. They enable us to evaluate whether perceived effects in our dataset, reflect differences across the entire population. Alright. So, why are statistical significance tests useful? So, they do provide a formalized framework for comparing and evaluating data. And they do enable us to evaluate whether perceived effects in our data set reflect differences across the whole population. They do not make a bad result look good. Significant sets are useful because they provide a formalized framework for comparing and evaluating data. Different tests have different assumptions and rules that they incorporate, and using a particular test ensures that everyone is on the same page in so far as what we're assuming about our data. Significance tests also enable us to evaluate whether perceived effects in our data set reflect differences across the whole population. As was the case with our company, where ten out of ten people polled preferred the color blue. Sometimes an affect that we seen in a small sample does not reflect what might be true across the entire population. A statistical significance test let's us formally determine whether or not this might be the case. Unfortunately, a bad result is not going to look any better or worse as a result of using a statistical significance test. If our data's bad, or there's really no difference between our two samples. We're not going to be able to undo that with a test. It is possible though that different tests might give us different results. The really important thing and we'll go into this a bit more is that you need to use the right test in the right situations. Why don't we talk a little bit about how we might actually run a statistical significance test. Let's say that we wanted to compare the batting averages of left handed and right handed hitters in major league baseball. So we're curious in answering the question, is there any difference between the batting averages of lefties and righties? We could just look at the data and try to answer this question without a significance test. But then we really couldn't trust our answer very much. It's a much better idea to answer this question using a statistical test. Many statistical tests that you might use to analyze data make an assumption about the probability distribution that your data will follow. There are many different probability distributions out there, but one of the most common and the one that I want to discuss is the normal distribution, which is also sometimes referred to as the Gaussian distribution or a Bell curve. If you've taken an introduction to statistics course, the normal distribution should be familiar. Which of the images below depicts a normal distribution? Check the box below the correct one. Normal distributions generally look like this, although their width and center can change based on a distribution's parameters There are two parameters associated with a normal distribution. The mean, mu, and the standard deviation, sigma. These two parameters plug in to the following probability density function, which describes a Gaussian distribution. The expected value of a variable described by a Gaussian distribution is the mean, mu, and the variance is the standard deviation, sigma squared. Normal distributions are also symmetric about their mean. If you've taken an introduction to stats course, the normal distribution should be a familiar tool. Here are the equations for a few different normal distributions. Can you determine from their probability distributions, the mean, standard deviation, and variance? Fill in your answers in these boxes below. So, mean would go in these. Standard deviation would go in here, and variance in these guys. Recall that the equation for a normal distribution is 1 over root 2 pi sigma squared, e to the minus x minus mu quantity squared, over 2 sigma squared, where mu is the mean and sigma is the standard deviation. Sigma squared is the variance. If we know the form of this equation. It's not hard to look at all of these equations and figure out the values for the mean. Standard deviation and variance. They are five, 12, and 144. Zero, five, and 25. Eight, one, and one. And zero, one, and one. Now that we've familiarize ourselves with the normal distribution, let's discuss one of the most common parametric test that we might use to compare two sets of data. Such as our samples of left handed and right handed [INAUDIBLE], that would be the t-test. The t-test like many statistical tests aims at accepting or rejecting a null hypothesis. A null hypothesis is generally a statement that we're trying to disprove by running our test. For example, that two samples came from the same population. This might mean that left-handed and right-handed batters show no real difference in their batting average, or that a certain sample is drawn from a particular probability distribution. For example, if we had a sample of 20 heights and weights of Arbitrary baseball players, we might want to test how likely it is that those 20 people are drawn from the known MLB player population. A hypothesis test such as the t-test is usually specified in terms of a test statistic. The test statistic reduces your data set to one number that helps to accept or reject the null hypothesis. When performing a T-test, we compute a test statistic called T. Depending on the value of the test statistic T, we can determine whether or not or null hypothesis is true. In the case of the one sample T-test, our null hypothesis would be that the population mean, mu, is equal to our sample mean, mu not. In the T sample case, which we'll be more concerned about for the purposes of our example, The null hypothesis would be about our population means, m0 and m1 are equal. Let's talk more about the two sample t-test, since we'll want to compare two different samples in our class project. There are a few different versions of the t-test that one might employ ,and they depend on really on what assumptions we make about the data. So we might want to ask questions such as ,do our samples have the same size ?,and do they have the same variance? . Let's discuss a variant of the t-test called Welch's t-test in more depth. Since it's the most general. It doesn't assume equal sample size ,or equal variance. In Welch's t-test ,we compute a t-statistic using following equation. T equals mu1 minus mu2, divided by the square root of sigma1 squared over n1. Plus sigma 2 squared over n2. Where mu I ,is the sample mean for the Ith sample. Sigma squared I is the sample variance for the Ith sample. And NI is the sample size for the Ith sample. We'll also want to estimate the number of degrees of freedom, nu, using the following equation. Nu is approximately equal to. Quantity sigma1 squared ,over n1 ,plus sigma2 squared over n2 ,squared over sigma1 of the 4th over n1 squared nu1 ,plus sigma2 to the 4th ,over n2 squared nu2. Where mu I is equal to mi minus one ,and this is the degrees of freedom associated with the Ith variance estimate. If you're unfamiliar with degrees of freedom again it might be a good idea to brush up on your stats concepts with the audacity's intro to stats course. A link is provided in the instructor comments. All right so once we have these two values, we can estimate the P value. Conceptually, the P-value is the probability of obtaining the test statistic at least as extreme as the one that was actually observed ,assuming that the null hypothesis was true. The P value is not the probability of the null hypothesis is true given the data. So again, just as a thought experiment. Say we were testing whether left handed or right handed baseball players. Were better batters by looking at their average batting average. If the P value is .05, this would mean that ,even if there is no difference between left handed and right handed batters, since that's our null hypothesis. So, even if this was true, we would see a value of t ,equal or greater to the one that we saw 5% of the time. When performing a statistical test like this, we usually set some critical value of P. Let's call it P critical. If P falls below P critical, then we would reject the null hypothesis. In the two sample case, this is equivalent to stating that the mean for our two samples is not equal. Calculating this P value for a given set of data can be kind of of tedious. Thankfully, we seldom have to perform this calculation explicitly. All right. Why don't we try computing these two quantities, t and nu, given some example batting average data. So, let's say that I had two samples. One with 150 data points, a mean batting average of 0.299, and a variance of 0.05. The other has 165 data points, and mean of 0.307, and a variance of 0.08. What are the values of t and nu given this data? Fill in your answers in these boxes below, and round them to three decimal places. We can just plug the information provided into our equations for t and new. If we look at the equations for t, t is 0.307 minus 0.299 over the square root of 0.05 over 150 plus 0.08 over 165. Which is going to be 0.280. Meanwhile new is going to be the quantity 0.05 over 150 plus 0.08 over 165 squared. Over 0.05 squared over 150 squared times 149 plus 0.08 squared over 165 squared times 164, which is going to be 307.199. The t statistic here, again is sort of some idea of, you know, how extreme is our result and how likely is it to disprove our null hypothesis? Meanwhile, the nu here is kind of some idea of how many independent variables went in to calculating this t-value? All right, so we've established that we can do this t-test and compute these t-values and nu values and, and p-values in the abstract mathematical sense, but you might be wondering how do I do this in python? Is there a simple way to do this? Well, the answer is yes, this can be done in the following way. First you simply import scipy.stats. And then you call scipy.stats.ttest_ind and then supply as arguments two lists, which are your two sets of data, and then an optional argument, equal_var=false. This indicates whether or not we think the variance of our two samples is equal. This equal_var=false argument makes this particular call of the t-test equal to Welch's t-test. This function will return a tuple. The first value is the t-value for your data. And the second value is the corresponding p-value for a two-tailed test. The values returned by scipy.stats.ttest_ind assumes that you are performing a two-sided t-test where we're only testing for whether the means of our two samples are different. Say we wanted to do a one-sided t-test, that is, we're interested in testing whether one mean in particular is greater than or less than the other. How might we do this given the output that this function produces? Write your answer in the text box below. Here's a hint. We can still use the t value and p value that this function returns. Don't worry. This won't be graded. Alright, so with the symmetric distributions like our normal distribution, the one sided p-value is simply going to be half of our two sided p-value. Remember that the p-value is the probability that given the null hypothesis is true, we would observe a test statistic at least as extreme as what we saw. So instead of having both sides of this distribution, we're really only going to have one side of the distribution. So we have a one-sided t-test where we're checking whether the mean of sample one is greater than the mean of sample two. We still want p over two to be less than our p critical, maybe that's 0.05, but we want our t value to be greater than zero. Whereas for a less than one-sided t-test, where we're testing whether the mean of sample one is less than the mean of sample two, we want p over 2 to be less than p critical, but we want our t value to be less than 0. Okay, why don't we work through an example. Let's say we had a CSV file located at this directory, and that it contained a number of columns. Among them are a player's name. We call that column, name. There's also a column called handedness, L for left-handed R for right-handed, and a column for the player's career batting average, called avg. Write a function that will read that data into a Pandas data frame, and then run Welch's t-test on the two cohorts defined by handedness. With a significance level of 95%, if there is no difference between the two cohorts, return a tuple consisting of true, and then the tuple returned by scipy.stats.ttest. If there is a difference, return a tuple consisting of false, and then the tuple returned by scipy.stats.ttest. So an example of what your output might look like is false and then the tuple 0.405, 0.006. Your code should go here. Since we're using Welch's t-test, we're assuming our data is sampled at random and that both samples follow a normal distribution. We don't make any assumptions about the variance of both samples. They may be the same, but they also may be different. Alright, why don't we walk through our solution function compare_averages. Remember that we're performing a t-test on two sets of baseball data, left-handed and right-handed hitters. We want to perform a Welch's t-test and determine with 95% confidence whether or not the average batting average of the two cohorts is different. In other words the left handed hitters have a statistically significant different average batting average than right handed hitters. First, we read our data into a panda's data frame. So baseball date equals pandas.read_csv and then the location of our csv file. Then we're going to split the data into two data frames, one for left handed hitters and one for right handed hitters. We do this by saying baseball_data_left equals baseball_data, and then index on baseball_data handedness equals L. We can do the same thing for right-handed hitters. Then we'll use scipy.stats.ttest_ind to perform a Welches t-test on these two data frames. So results equals scipi.stats.ttest_ind and then we pass in baseball data left average, baseball data right average, and we say equal_var equals false. Now we're going to produce an output in the desired format. So if result one is less than or equal to .05, that is if our p value is less than or equal to .05, will return false and then the output of scipy.stats.ttest_ind. Else will return true, and then the result of scipy.stats.ttest_ind. So this is a basic implementation of how we can do Welch's t-test using Python. When performing a t-test, we assume that our data is normal. In the wild, you'll often encounter probability distributions. They're distinctly not normal. They might look like this, or like this, or completely different. As you'd imagine, there are still statistical tests that we can utilize when our data is not normal. Why don't we briefly discuss what you might do in situations like this. First off, we should have some machinery in place for determining whether or not our data is Gaussian in the first place. A crude, inaccurate way of determining whether or not our data is normal is simply to plot a histogram of our data and ask, does this look like a bell curve? In both of these cases, the answer would definitely be no. But, we can do a little bit better than that. There are some statistical tests that we can use to measure the likelihood that a sample is drawn from a normally distributed population. One such test is the shapiro-wilk test. I don't want to go into great depth with regards to the theory behind this test, but I do want to let you know that it's implemented in scipy. You can call it really easily like this. W and P are going to be equal to scipy.stats.shapiro data, where our data here is just an array, or list containing all of our data points. This function's going to return these two values. The first, W is the Shapiro-Wilk Test statistic. The second value in this two-pole is going to be our P value, which should be interpreted in the same way that we would interpret the p-value for our t-test. That is, given the null hypothesis that this data is drawn from a normal distribution, what is the likelihood that we would observe a value of W that was at least as extreme as the one that we see? All right. So let's say that we look at our data and it's clearly non-normal. Or we use a statistical test like the Shapiro–Wilk test and find that our data's non-normal that way. Is there anything that we can do? Well, first off there's some math that says that we have enough data. That we have, you know, a large enough sample size we can actually use tests that assume normality. For example, the t test. Even when our data is not normal. But there also exists nonparametric tests that we can use to compare two samples. A non-parametric test is a statistical test that does not assume our data is drawn from any particular underlying probability distribution. One such test is the Mann-Whitney U Test which is also sometimes referred to as the Mann-Whitney Wilcoxan Test. This is a test of the null hypothesis that two populations are the same. Again I don't want to go to in depth into the theory of how this test works, but I did want to tell you that things like this exist and show you that this test can also easily be implemented in Python using Psi Pi. We'd simply say U and P are equal to scipy.stats.mannwhitneyu. And provides as arguments are two samples which we'll call here X and Y. This function will return U, the Mann-Whitney test statistic. As well as P, which is the one sided P value for this test. The P value here again acts as it did for the T test. Note that the Mann-Whitney U test simply tests whether or not these samples came from the same population. But not necessarily which one has a higher mean or a higher median or anything like that. Because of this it's usually useful to report Mann-Whitney U test results along with some other information. Like, the two sample means, or the sample medians, or something like that. Just to quickly recap, what is the definition of a non-parametric test? A non-parametric test is a statistical test that assumes the data is drawn from a non-parametric probability distribution? Assumes the data is drawn from a non-Gaussian probability distribution? Does not assume the data is drawn from any particular underlying probability distribution? Or is the same as Welch's t-test? Check the correct answer. The correct answer is that a non-parametric test does not assume the data is not drawn from any particular probability distribution. There's not really any such thing as a non-parametric probability distribution. This terminology non parametric really describes test types or statistical methods. Assuming a non-Gaussian probability distribution would still assume some probability distribution. And finally, Welch's t-test is definitely not a non-parametric test, since it assumes a certain underlying probability distribution so there's no way that the non-parametric test is the same as Welch's t-test. These have just been some of the methods that we can use when performing statistical tests on data. As you can imagine, there are a number of additional ways to handle data from different probability distributions or data that looks like it came from no probability distribution whatsoever. Data scientists can perform many statistical procedures. But it's vital to understand the underlying structure of the data set and consequently, which statistical tests are appropriate given the data that we have. There are many different types of statistical tests and even many different schools of thought within statistics regarding the correct way to analyze data. This has really just been an opportunity to get your feet wet with statistical analysis. It's just the tip of the iceberg. Now we know how to analyze existing data. However, is there any way we can make predictions about data we'll collect in the future, using data we've collected in the past? Write some ideas on ways to do this, in the text box below. Try to think about methods we used to predict Titanic survivors in Assignment number one. In addition to statistics, many data scientists are well versed in machine learning. You might be wondering what exactly is machine learning. Well, machine learning is a branch of artificial intelligence that's focused on constructing systems that learn from large amounts of data to make predictions. Machine learning could also be useful to predict which movies you might like on Netflix. Or how many home runs a batter may hit over the course of his career. These are all potential applications of machine learning. Okay, why is machine learning useful? So that's, that, that's a great question and, and it, you know, I, I think it touches on this, this sort of classic question, or this long running discussion, on what is the difference between statistics and machine learning. there's, there, there's a lot of great papers. There's one classic paper, I, I forget the name exactly, but it's something along the lines of the two cultures, and talking about why these two different approaches developed. So I think, if you look at machine learning, it, it grew out more of a computer science direction, and, and it grew out of a lot of areas where people had very practical hands on questions, right? How can we build the best recommendation system? How can we make the best predictions or classifiers for, for a given problem? And machine learning has developed a lot of really, really good techniques for doing that, for doing things that work very well in practice. And so I, I think that's there, there are a lot of applications where that's really what you're getting at particularly when you have the opportunity to build a product that will you know, take some action based on, based on some algorithms. Those are areas where machine learning is really at the cutting edge of coming up with the most effective ways of making decisions in real time based on data. I see. You might me wondering what the difference is between statistics and machine learning. In short, the answer is not much. More and more, the 2 fields are converging and they share many of the same methods. However there are couple of significant philosophical differences between the 2 subjects. Generally, statistics is focused on analysing existing data, in drawing valid conclusions, whereas machine learning is focused on making predictions. What this translates to is the following: In statistics, we care a lot about how our data was collected, and drawing conclusions about that existing data using probability models. For example, we might try to answer the question are left handed hitters statistically better than right handed ones? In the case of machine learning, we're a little bit more focused on making accurate predictions and less committed to using a probability model if there's a more accurate method that doesn't use them at all. As long as our machine learning method consistently makes accurate predictions for example, how many home runs will a player hit, we aren't too worried about what assumptions the model makes. There are many different types of machine learning problems, but two common ones are supervised learning and unsupervised learning. Machine learning typically involves generating some type of model regarding the problem we're trying to solve. We'll feed data into this model and then try to make predictions. In supervised learning, there are labeled inputs that we train our model on. Training our model simply means teaching the model what the answer looks like. So, for example, if we were detecting spam using supervised learning, we might have 100 emails where we know whether or not they're spam. We can use these emails, where we know whether or not the email is spam, and train a model that can predict whether or not a future email will be spam based on a bunch of characteristics, maybe its contents, or whether or not it has an attachment, or things like that. Another example of supervised learning would be estimating the cost of a new house, given that we have a number of examples where we know about a bunch of features like the square footage, or the number of rooms, or the location. And we also know how much that house sold for. We could train a model, and then predict how much a future house will sell for, given we know all the same parameters. This is an example of regression. When performing unsupervised learning, we don't have any such training examples. Instead we have a bunch of unlabeled data points, and we're trying to understand the structure of the data, often by clustering similar data points together. For example, if we were to feed an unsupervised learning algorithm a bunch of photos, it might split the photos into groups, say photos of people, photos of horses, photos of buildings, without being told a priori what those groups should be. It might not know that the groups are people, or horses, or buildings, but it can tell that these distinct groups exist. Sure, sure. That's a great question. So I would say, you know, just as a, as a caveat, that really depends a lot of the type of problems any given data scientists are working on, the type of questions you're, you're looking at. For me personally, a lot of the questions that, that I look at and a lot of the things that interest me, tend to be fairly high level and somewhat loosely defined and I mentioned, looking a lot more at the social aspects of how people use social networks. And a lot of topics like that, sometimes the most useful thing is really to just get a broad and qualitative understanding of, of the data you're looking at. So, two of the most valuable techniques I found for that are clustering. There are various approaches, k-means clustering, hierarchical clustering. And then other dimensionality reduction techniques or things like principle component analysis, PCA. One, one thing, just speaking very specifically that I found very useful is the combination of k-means clustering and PCA. So if, if you generate clusters and then use PCA to take the most, meaningful vectors to plot those clusters on, you can often do a really nice job of reducing dimensionality of a data set and understanding, the significant differences between clusters in a visual way. Let's talk a little bit more about a specific example of supervised learning. We might want to ask the question, can we write an equation that takes as input a bunch of information. For example, height, weight, birth year or position about baseball players and predicts their lifetime number of home runs. To do this, we want to write an algorithm that does the following. Takes in the data points for which we have all these input attributes, and the player's lifetime number of home runs. Then builds the most accurate equation to predict lifetime number of home runs, using these input variables. We can then use this equation to predict the lifetime number of home runs for players for whom we did not initially have number of home runs. But we do have all the other data. So, their height and weight and birth year and position maybe. One way that we might be able to solve this problem is using linear regression and one basic implementation on machine learning is to perform linear regression by using gradient descent. What is this? How does this work? Lets learn about gradient descent by working through an example that utilizes our baseball data set. When performing linear regression, we have a number of data points. Let's say that we have 1, 2, 3 and so on up through M data points. Each data point has an output variable, Y, and a number of input variables, X1 through X N. So in our baseball example Y is the lifetime number of home runs. And our X1 and XN are things like height and weight. Our one through M samples might be different baseball players. So maybe data point one is Derek Jeter, data point two is Barry Bonds, and data point M is Babe Ruth. Generally speaking, we are trying to predict the values of the output variable for each data point, by multiplying the input variables by some set of coefficients that we're going to call theta 1 through theta N. Each theta, which we'll from here on out call the parameters or the weights of the model, tell us how important an input variable is when predicting a value for the output variable. So if theta 1 is very small, X1 must not be very important in general when predicting Y. Whereas if theta N is very large, then XN is generally a big contributor to the value of Y. This model is built in such a way that we can multiply each X by the corresponding theta, and sum them up to get Y. So that our final equation will look something like the equation down here. Theta 1 plus X1 plus theta 2 times X2, all the way up to theta N plus XN equals Y. And we'd want to be able to predict Y for each of our M data points. In this illustration, the dark blue points represent our reserve data points, whereas the green line shows the predictive value of Y for every value of X given the model that we may have created. The best equation is the one that's going to minimize the difference across all data points between our predicted Y, and our observed Y. What we need to do is find the thetas that produce the best predictions. That is, making these differences as small as possible. If we wanted to create a value that describes the total areas of our model, we'd probably sum up the areas. That is, sum over all of our data points from I equals 1, to M. The predicted Y minus the actual Y. However, since these errors can be both negative and positive, if we simply sum them up, we could have a total error term that's very close to 0, even if our model is very wrong. In order to correct this, rather than simply adding up the error terms, we're going to add the square of the error terms. This guarantees that the magnitude of each individual error term, Y predicted minus Y actual is positive. Why don't we make sure the distinction between input of variables and output of variables is clear. If we have age, weight, height and batting average, and are trying to predict batting average, what are the input variables? Check all that apply. In this example, age, weight and height are the input variables. We are hoping to use these to predict our output variable which is batting average. So, we've determined that we want to figure out which set of parameters provide the best predictions for our output variable, but how can we do that? We'll use an algorithm called gradient descent. First, we need to define some cost function, which we'll call J of big theta. I'm going to use big theta here, to represent our entire set of thetas, and I'll use this notation throughout the rest of this lesson. The cost function is meant to provide a measure of how well our current set of thetas does, at modeling the observed data. So, we want to minimize the cost function's value. As we discussed just a moment ago, when we're doing linear regression, our cost function J of theta, can simply measure the sum of the squares of the differences between our predicted and observed values. I am going to formalize this a little bit, and say this, J of big theta, is equal to one half, times the sum from i equals one to m, of y predicted of x i. Minus Y observed i squared. Where Y predicted x i, equals the sum of n from zero to big N, of theta n x n i. So, there's a lot going on here, and I have color coded this. So, why don't we walk through it. First, we're just saying that J of big theta is equal to one half, times the sum over all of our data points, of the predicted Y, given our Xs, our input variables, minus the observed Y squared. So, this is just our error squared term. Summed over all the data points. No different than the equation that we have up here. Down here, we're just defining the way that we calculate the predicted value of Y. Given our input variables. And the way that we do that, is that we say we sum from N equals zero, to big N, of theta N times X N. So, we're just saying that we sum up the X N each input variable, times its weight, theta. This is no different that the sum that we had, on the last slide. Note that we include an N equals zero term here, which corresponds to a constant term in our model, which doesn't correspond to any of the input variables. For those of you familiar with linear algebra, if we wanted to, we could also express this as theta transposed times x. If you don't know linear algebra, don't worry about this, this is just another way of expressing the same thing here, that might be easier for some students to comprehend. So how do we find the correct values of theta to minimize our cost function J of big theta? We'll use a search algorithm that takes some initial guess for theta and iteratively changes theta, so that J of theta keeps on getting smaller until it converges on some minimum value. The algorithm we're going to discuss is called gradient descent. Here's a one dimensional depiction of what gradient descent might look like. We have some starting point where J of theta is large, and we continue to try new values of theta. And J of theta keeps on getting smaller and smaller, until we arrive here, the final value. This theta value gives us our smallest value of J(theta). And so we know that we've minimized J(theta). This is what gradient descent might look like in one dimension. Let's see what it looks like in two dimensions. Here again we see that we start at some high value of J(theta). And we continue to iteratively update theta 1 and theta 2 until we arrive here, the global minimum of J(big theta) Okay, so in the general mathematical sense that's how we would optimize the parameters on our model theta by using batch gradient descent to minimize our cost function, j of theta. Below is an implementation of linear regression and gradient descent to try and determine how many lifetime home runs a player in a baseball data set will hit. Given their height and weight. I'd encourage you to really look through all of this code and make sure you understand how it works. But for now, I'd like you to write some code here that updates the values of theta a number of times equal to num_iterations. You should initialize an array called the cost_history, and every time you've computed the cost for a given set of thetas You should append it to cost history. The function should return both the final values of theta in your theta array, and cost history array. Your code should go here. Feel free to use other helper functions in this code, such as compute cost. So why don't we walk through our solution here. First we initialize this variable m, which we say is equal to length of values. So we're basically saying here hey, this is how many data points we have. We also initialize this variable called cost history, which is going to track how our cost function evolves over our iterations. We say for i in range num iterations, which basically says hey we're going to update theta in the costs. And number of times equal to the number of iterations. Let's calculate the predicted values. We'll do that by taking the dot product of the features and theta. Then we're going to update our values of theta. This looks pretty complicated, but you'll note that it's the same as the equation that we just discussed, which we'll pull up again here. So, we subtract our observed values from our predicted values, take the dot product of this vector with the features, and then essentially multiply by this alpha over m feature and subtract that from theta. This is effectively gradient descent in action, so we're taking a very small step in the direction of the steepest gradient. Then we're going to use another function called compute_cost which we see up here. So you compute the cost function given our features, our values and our theta. Then we'll append the newest cost to the cost history list. And once we've done this a number of times equal to the number of iterations, we'll not only return our final set of thetas, but also the entire cost history. You'll see that if we run this function, we get this array, which are theta values. About 45 minus 9 and 13, and these values kind of indicate how significant each factor is in determining our predicted value. So this 45 corresponds to a constant term, the minus 9 corresponds to height and the 13 and change corresponds to the weight. And then we see here how the cost function has evolved over time. Ideally, if we had a perfect model, this cost function would approach zero. You'll see that our cost function doesn't, you know, get close to zero. It, it does decrease a good amount as we train this model, but it doesn't do super well. And so this might lead you logically to a natural question. Which is, hey you know not all models are created equal. Maybe our model is actually bad. Is there some way we can evaluate our models? Why don't we discuss this a little bit more? Alright, so say that we've determined the coefficients for a linear model for our data using gradient descent, or even some other method. Just because we're able to come up with some model doesn't mean that it's a good one. The data could be distinctly non-linear. Or, maybe the attributes that we've trained our model on have little to no bearing on our output variable. We need some way to evaluate the effectiveness of our model. One way we can measure this is a quantity called the coefficient of determination, also referred to as R squared. If we have a bunch of values, yi through yn. A bunch of predictions, fi through fn and an average value for our data, y bar. We can define the coefficient of determination as R squared is equal to 1 minus the sum over n of yi minus fi squared, over the sum over n of yi minus y bar squared. The closer R squared is to one, the better our model. And the closer it is to zero, the poorer our model. The coefficient of determination gives us a pretty effective way to evaluate how good our model is. Let's make sure that you're comfortable with the coefficient of determination and computing it. Write a function that, given two input numpy arrays, data, and predictions, that returns the coefficient of determination, r squared, for the model that produced the predictions. Numpy has a couple of functions, numpy.mean and numpy.sum, that you might find useful here, but you don't have to use them. Your code goes here. All right, let's walk through this solution. So first we're going to compute the denominator, so the data minus the mean of the data squared, and then we'll take the sum of that. Then we're going to compute the numerator, which means that we'll compute the predictions minus the data squared, and sum that up. And then we'll compute R squared. So we'll just subtract this fraction, this guy over this guy, from one. Then we'll return R squared. What we've covered so far in this class is a very simplistic and a very partial coverage of linear regression with gradient decent. If you were ever to try and use linear regression to solve a real world problem, there are a lot of additional considerations that you would want to think about in order to seriously implement linear regression with or without gradient descent. First of all, gradient descent is only one implementation of linear regression. There are a bunch of other ones, and in some sense, they may be better. Ordinary Least Squares for example, is always guaranteed to find the optimal solution when performing linear regression, whereas gradient descent is not. Additionally, we haven't really talked about parameter estimation and putting confidence intervals on those parameters. In the models that we've built, we've given exact values for all of our thetas. But as you can imagine, there's some confidence that we have in those values. You can imagine doing more thorough statistical analysis in saying what are the confidence intervals on these parameters? And we could answer questions like what is the likelihood that I would get this value for this parameter if this parameter actually had no effect on our output variable? We also might worry about issues like over or under fitting. This isn't so much a problem with linear regression, but with more complicated models, you might expect our model to over-fit. Say we had some data like these red points that was approximately linear. If we had a model with many different parameters, we might be able to fit the data points exactly whereas the actual underlying model is this black line. One way to deal with this is usually to split our data into a training set and a test set. Then we can train a model on the training data, and then test it on the test data, just as the name would imply. This is called cross validation. We might also see under-fitting, where we have data that's clearly nonlinear, but we only try and fit a linear model to it. This is also a problem that you might encounter. Another issue worth mentioning again is that our cost function may have numerous local minima. That means that when using gradient descent, our algorithm can become trapped in a local minimum that isn't actually the global minimum of the cost function. This isn't a problem of other implementations of linear regression, but when using gradient descent, this means that maybe we perform gradient descent numerous times, randomizing our initial values of theta with different random values every time. You may also already know this, but when using a random number generator to generate random values, it's important to seed your random values and to store that seed. That way, you have a result that's replicable and other researchers can look at your findings and also produce them on their own machines when they run the same script. This has been a whirlwind tour of a bunch of additional considerations that we didn't really touch in this lesson. These are all really important in machine learning, statistics, using linear regression in the real world. I really wanted to focus here on just implementing gradient descent and the basics of how it works. But if you really want to use any of these things to solve a real life problem, you'd want to think about local minima, you know other implementations of linear aggression, the confidence intervals on your parameters and if you're using a statistical modeling package in R or Python or Stata, they'll usually give you these confidence intervals. And use a reliable implementation of linear aggression. So what, what are the most important dos and don'ts to keep in mind in data science? I think I would mention two. One is a bit more, qualitative and one is a, a bit more, quantitative. So, on the qualitative side I, I would say the first thing is any problem you're looking at, it's always very valuable, I find, to start by thinking about. What sort of things do we know? What sort of expectations do we have? And what sort of qualitative things can we get from, an exploratory analysis of the data? So, as I mentioned, using things like k-means clustering and PCA are a good way to start to do some sort of dimentionality reduction. Some way of getting the data down to a point where you can. Look and get some qualitative insights. Understand the general structure of it. And quite often in, in certain topics that we work on, you have some intuition of, of what you'd expect things to look like. You can start to see patterns, emerging data that, you know, that makes sense. That, that. Either confirm or, or possibly go against other theories or ingrained beliefs that people have. so, so I think getting data down to that point is very important. The more quantitative suggestion I would have, has to do with trying to understand causal connections. So, this is a very common thing that comes up. In building predictive models you may have a number of features. That you put into a model to predict some outcome. And it's very, very important when doing that. So, often the question comes out not just being able to predict an outcome. But understanding which features are actually causing it. It's important to use a lot of caution around that. To never just sort of, dump a bunch of data into a model with lots of features, and then just naively look at the thins that have the strongest weights in your model and say, oh. Here's what's driving it. So, it's very important when you're asking those kinds of causal questions to proceed very carefully and, and rigorously. Okay. Do I have tips or advice for data scientists? To become a data scientist. Yeah, for aspiring data scientists. That's a great question and I think going back to the issue of what is data science. Like I said, I think there's really I would, I would divide it into 3 somewhat different parts. And, and so one of the first pieces of advice that I would give to people looking to get in the field is to think about what you really like, what, what part of this kind of work do you really enjoy. I think for some people it really is the process of building things, of writing code, of building a product that let's say you know, surfaces recommendations or finds, you know similarities between users of a product. So you know, if you really like that, that sort of aspect of building things there's a lot you can do in that direction particularly around improving your coding skills, things like that. If you really enjoy the analysis part, the statistical and mathematical side of things, there's a lot more you can do there in terms of coming up to speed with new machine learning techniques, learning statistics more in depth. And then finally, on the communications and strategies side there obviously a lot that you can do to improve your communication still, skills. Understand how to abstract from the details, how to abstract out the high level issues that are important to a company. So the first piece of advice I would really give anyone looking to move into this field, is to understand which of those 3 areas you enjoy the most or what the right mix is for you. And make sure you're focusing on the skills that, that will help you get ahead in the particular direction that's of the best fit for you. So, that's gradient descent. As we mentioned, gradient descent is just one machine learning technique and one of the most basic. There are a number of sophisticated machine learning algorithms for predicting and classifying our data points. And if this is interesting to you, I'd really recommend you to go out there and read more about machine learning, and really dive into some of those algorithms. Let's talk about Assignment 3. In this assignment, you'll use methods we learned during this lesson. So the t-test and linear regression using gradient descent, to make some conclusions and predictions about our data. First you'll use the t-test in order to compare various sub sets of our subway data to try and make some conclusions about subway ridership. For example, do people ride the subway more if it's raining? Or maybe they ride more if it's the weekend. Then we'll use linear regression to try and build a model that predicts how many total riders the New York City subway will have. On a particular day and time, given a variety of factors. For example, the time of day, whether or not it's a weekday, how the weather is, et cetera. In this lesson, we are discussing basics, statistics and machine learning methods that you can use to analyze your data. Not only can we draw conclusions that are statistically sound about existing data, we can also make predictions about the data that we might collect in the future. During this lesson, we have chosen to really zero in on a couple of methods. Specifically Welsh's T-test and linear regression with gradient descent, but as you can imagine, this is just the tip of the iceberg. There are tons of statistics and machine learning techniques out there. If you're interested, I would really encourage you to go out there and read more about those stats and machine learning. Now that you're familiar with these statistics and machine learning techniques, you'll use them to make predictions about subway ridership in New York and also to draw some conclusions about the existing subway data that we have. Once you have made these conclusions and predictions you might find that you'd really love to communicate them to other people. Be they your friends, or your teachers, or your family. One of the most effective ways to do that is through data visualization. That's what we'll cover in the next lesson. So far in this course, we've discussed how to acquire, munge, and analyze data. These are all really important skill sets for a data scientist to have. However, an equally important skill is the ability to communicate your findings. Sure, this can be done through a presentation or through a blog post. But, one of the most effective ways to communicate your findings is through a data visualization. During this lesson we'll be focusing on the basics of data visualization. We'll talk about how to use a variety of visual cues to encode information and also how to make concise and effective visualizations that really communicate what you're trying to say. We'll also go over how to make some of these different types of visualizations in Python. Once you have these skills, you'll be able to take the findings that we've accumulated about the New York City subway system. And really make effective visualizations that communicate these findings either to your friends, or your family. Or, anyone else. All right, why don't we begin? To get started, let's answer the question, what is information visualization? Information visualization is the effective communication of complex quantitative ideas. This is usually done through geometry and color. What do we mean by an effective communication? Well, an effective communication is the expression of ideas with clarity, precision, and efficiency. An effective visualization will let you see things that might go unnoticed if you didn't visualize the data. Any data set contains tons of information but you might missed out on trends, behavior patterns, and dependencies if you don't visualize it. Additionally, an effective visualization can highlight certain trends in the data or tell a story to people who might be looking at the visualization. So why don't we take a look at an all time classic visualization of information, Charles Minard's graphical depiction of the terrible fate of Napoleon's army when they marched on Russia. Alright, so, here's a little bit of background on this visualization. Napoleon invaded Russia in 1812, and suffered a number of devastating losses. So Charles Minard created this chart to describe the sheer amount of loss that Napoleon caused in a simple, yet powerful graphic. So if we start at the left of the graphic, we are at the Polish Russian border near the Neman River. And this flow line shows the size of Napoleon's grand army 422,000 as they invaded Russia of June of 1812. The width of this band indicates the size of the army at each place of the map. In September when the army finally reached Moscow, it had dwindled to 100,000 men. The path of Napoleon's retreat from Moscow is depicted by this lower, darker band which is linked to a temperature scale which is hanging down here. And also these dates that we see at the bottom of the chart. It was incredibly cold, and many froze on the march out of Russia. This chart shows that the crossing of the Berezina River was a disaster. And the army struggled back to Poland with only 10,000 men remaining. Menard made this plot to really highlight the huge amount of loss that was caused by Napoleon's march on Russia. Look at how many men died marching to and from Moscow. As we mentioned earlier, information visualization is the effective communication of complex quantitative ideas. What information do you think is depicted in this visualization? Share your thoughts in the text box below. This quiz is free form so there's no right or wrong answer. I'll share my analysis with you in the following video. This visualization packs a lot of information into a small area. So, here are some of the details that are communicated. We know the size of Napoleon's army. We know the location of the army at various points in time. The color coding on the march into and out of Moscow tells us the direction of the army's movement. And we're also shown the temperature on various dates during Napoleon's retreat. Alright, so we've identified what information this graphic is communicating. But, you could probably imagine a really bad graphic that had all the same information. You might look at it and have no idea what's going on. Or, it might not tell the story as effectively. What makes this graphic so effective? Can we quantify effective information visualization down to a few important ingredients? I'd like to hear your thoughts again. In your perspective, what are the ingredients to good information visualization? This is a free-form question, so there's no right or wrong answer. Type your answer in this text box. I'll share my thoughts with you in the next video. Generally speaking, there are four important ingredients to any information visualization. Visual cues, coordinate systems, scale and data types, and context. Let's use this line chart, which shows total home runs by year in major league baseball to showcase each of these ingredients. So, the first ingredient was visual cues, or visual encoding. This is encoding data with shapes, colors, and sizes. For example, if we look at this chart, we can show that each point shows the number of home runs in a given year. And we can also see that the lines connecting the points give us some sense of the rate of change of number of home runs hit from year to year. There were also coordinate systems. So we need to place our data point somewhere on the chart. A coordinate system gives us a structured space that dictates where the shapes and color should go. This gives meaning to and x, y coordinate on this chart. For example, in this particular graphic, the x-axis represents time, and the y-axis represents total number of home runs hit. Whereas the coordinate system tells us what various dimensions of our visualization may correspond to, the scale or data type will tell you where exactly your data needs to go. There are three types of scales that we could use: numeric, categorical and time series. In this example, we're using numerical data on the y axis, the total number of home runs, and we have time on the x axis, the year. Note that both of them are a linear scale, so numbers are evenly spaced out. There are other type of scales, which we'll discuss later. And the final important ingredient of an information visualization is context. So, if your audience is unfamiliar with the data, it's your job to clarify what values in your graphic represent and explain to people how they should read your chart. So, in this particular visualization, I've provided some context by giving the chart a title. Number of NLB home runs by year. Labeling the axes, year and total home runs, and I could do a little bit more by annotating the chart potentially, so there are a variety of things that might inform the total number of home runs hit in a particular year, maybe the beginning of the live ball era, which we might put somewhere over here. The crackdown on steroid use, which happens somewhere around the year 2000, et cetera. These annotations might help people interpret this data and understand the story of, why the number of home runs hit by year is changing over time. It's important to emphasize that data is the driving force behind all four of these ingredients. My name is Don Dini. I'm a principle data scientist here at the AT&T big data group. My background is in computer science in artificial intelligence specifically multi agent systems. Before that, I did physics but then I got, I got struck over the head with the magic of computer science. And I moved into there, and into our visual intelligence. And then I taught for a while at USC, and then moving up here to the Bay area and worked for a couple of start ups. Doing a lot of work on real time system for collecting data and doing real time analysis on them. And and now I am here. So I think my experience has been that human beings are hard wired to receive things in story form. So if you can craft a narrative behind what you are doing, it makes it easy to, it makes it it makes it more compelling. And you know, it doesn't have to be you know, like a coming of age story or anything like that. It could simply be like like there was this problem, there exists this problem. Here's what people previously did, here's why it's not any good. We did this, here's what we found and like that's the end. So there's this sort of like there's this narrative arrow that goes through it. and, so that's one and number 2, I guess the other tip that I would have is think about your audience. Who is your audience? Are your audience is your audience technically oriented, or are they not technically oriented? Is your goal basically, is your, are you trying, is it a bunch of people that you're trying to recruit to come and join you, in which case maybe you want to show them like cool demos. Or are they you know, just just like a, a room full of your hard-nosed science colleagues who are just going to brutally scrutinize whatever you present to them in which case, you may want to do something else. My name is Rishiraj Pravahan and I'm a principal data scientist at the AT&T big data center in Palo Alto. My background actually is in high energy experimental high energy physics and I was, before here, I was at CERN for about 7 years. That's where I did my PhD and post-doc for a few years. So that's sort of where I came from. And I became a data scientist almost as a natural sort of step towards learning about data and dealing with big data, as you know, the LHC produce a large amount of data. And we have to handle it and analyze it so along the way I picked up the skills to become a data scientist, and so. In terms of communicating your findings and when you are a data scientist, you try, you tend to think in terms of code and in terms of ideas that are very mathematical etc. Or statistical you do want to have the rigor, the mathematical and statistical rigor, but you also want to make it such that somebody who is not familiar with that, may understand, what you're talking about. So, so make it into a story. Make it into something that is easily understandable and communicable in English, for example. Or, whichever language you prefer in your group. As we mentioned in the previous video, visual cues are one of the main ingredients to an effective visualization. In the next couple of videos, we'll cover some of the main visual cues that you can use to represent data in your charts. The first one that we're going to discuss is position. When using position as a visual cue, you compute value by looking at the position of a point compared to the position of others in the same space or coordinate system. Here, this would translate to this point's x and y coordinate. Position is useful in coding because it takes up less space than other visual cues. It's really space efficient. Each data point is simply represented as a dot. Also, points often have the same size which allows you to easily identify trends, clusters, and outliers by plotting all of your data at once. But if you have too many data points in your plot, it can be a challenge to identify what each point represents. For example, imagine that this chart were covered with dozens of green dots just like this one. It might be hard for us to identify individual dots and really understand what they mean, or what day do they correspond to. Another commonly used visual cue is length, which is often encountered in the form of bar charts. The length of a bar indicates the intensity or value of your data point. For example, the longer the bar, the greater the absolute value. One more encoding that's very common is angle. Angle ranges from 0 to 360 degrees on a circle. Angles are commonly used to represent parts of a whole. A pie chart is a really common example of this. The sum of the wedges in this pie complete a circle and give us 360 degrees. One negative aspect to using angles as a visual encoding is that human eyes can have a difficult time differentiating angles. For example, 25 degrees and 30 degrees may look very similar. Because of this, angle is an encoding that you may want to avoid if you're trying to show very small differences. [BLANK_AUDIO] Let's discuss a few more visual cues, the first of which is going to be direction. Direction is similar to angle, but the direction encoding relies on a single vector's orientation in a coordinate system. By looking at the slope of a given line, we can easily see increases, decreases, or fluctuations within our data. However, direction is tricky for many of the same reasons that angle is tricky. We can easily differentiate a horizontal or vertical line, but when we're dealing with lines with angles it's harder to see the difference between two that are both negative or positive. Another encoding that can be used is shape. Shapes and symbols are commonly used to differentiate categories of objects. For example, say that we had a scatter plot. We could use triangles and squares to show trends for two different types of data, say two different baseball teams, or two different districts in our Aadhaar data, something like that. We can also use the area or volume of our shapes to encode information. So bigger objects typically represent greater values. Like length, when we encode information within an area or a volume, we are basically representing our data with size. So, say we were using circles. The greater the value of the data point, the bigger our circle is going to be. We can also encode data into our visualization using color. We can typically include values with color in 2 different ways, hue and saturation. They can be used individually or in combination. Color hue is what we typically refer to as color like red or green or blue. Different colors used together usually in the key categorical data where each color represents different group. So if we're using our baseball data maybe green is short stops, red is outfielders and orange is second basemen, something like that. Saturation is the intensity of color for a given hue. So if you select a color as blue, high saturation will be very blue and the color would look faded as you decrease your saturation. We usually use saturation to encode intensity or quantity of a value. So maybe if we were encoding the number of home runs with saturation, more home runs would be darker and fewer home runs would lighter. Or, if we were encoding Aadhaar enrollments, more enrollments would be darker and fewer enrollments would be lighter. We could also use hue and saturation in combination. So you can see here that no hue and no saturation is set to 0 and in this particular encoding, the more saturated our purple-like color is, the more negative we are. And the more saturated our green-like color is, the more positive we are. This might accentuate the differences in value and tell us when the value is positive or negative and how intense the absolute value of that effect is. When using color to encode information, a pretty good general rule of thumb is that you shouldn't use more than a dozen colors to encode categories effectively. If you were to use more than 12 colors, it might be hard to quickly differentiate between categories, and your visualization is going to become a little bit harder to parse. Now that we've discussed a few visual encodings, can you rank the visual encodings listed below, with one being the most accurate and four being the least accurate? Saturation, angle, position, and area. Just type your rankings in these boxes. All right, the most accurate visual encoding of these four is position. After that is angle. Area is the next most accurate. And finally, saturation is the least accurate of these four visual encodings. In 1985, two scientists from AT&T labs published a paper on graphical perception and methods. The study determined how accurately people read the visual cues that are presented here. This resulted in a ranked list of most accurate to least accurate visual cues. So you can see that position is more accurate. Where as hue is less accurate. What does accuracy mean here? Well, in this case, it just means how easily are people able to perceive the values in your data set given the visual encoding that you've chosen. You might wonder why saturation and hue are considered inaccurate. Well, this is a great example. One should be a little bit cautious when using color, hues and saturation as visual cues. Like all aspects of visual perception, we don't perceive color in an absolute manner. For example, these are all different shades of gray. When trying to compare and contrast them, it's difficult to know which shades of gray correspond to exactly which values, or just how much darker one shade of gray is than another. For this reason, it's really hard for viewers of your visualization to really know what a different shade of gray might mean. How much bigger is one data point than another? Because of this, you should be careful when using color, hues and saturation, to encode information in your visualization. This ranking up here is really a oversimplification of how visualization works. So you should really use it as a guide, rather than a definitive rule buck. Efficiency and exactness are not always the goal of our visualization. Sometimes color, saturation or color, hue or volume or area can be really effective to communicate what we are trying to tell the viewer. However it's good to know generally how well people will be able to read different visual cues. Now, we know a bunch about how you may encode information into your visualization and how to make an effective visualization, but we still haven't yet discussed how you can make graphics like this, short of drawing them with pen and paper. There are a number of packages for plotting in Python. One of the most popular is Matplotlib. For this course, however, I'd like to go over plotting using a Python library called ggplot, which very closely recreates the syntax used in R's ggplot2 library. If Matplotlib is so widely used, why should we use ggplot? Well, I'd like to use this package for a few reasons. First, what it produces is a bit more aesthetically pleasing than Matplotlib. Second, it's an implementation of a pretty neat concept called the grammar of graphics, which basically claims that there's a grammar involved in composing graphical components of statistical graphics. The gg in ggplot actually comes from grammar of graphics. It also plays nicely with the pandas DataFrames we've been using in this course. To quickly summarize the ideas behind the grammar of graphics, plots convey information through their aesthetics such as x-position or y-position. The elements in a given plot are geometric shapes, such as points, lines, or bars. Some of these shapes can have aesthetics of their own, such as their size or their color. You can think of creating plots in ggplot through the grammar of graphics as adding layers to our plot. The first step in creating a graphic is always to create our plot, which is essentially going to be our canvas. This can be done by calling ggplot data aes(xvar, yvar). Data here is going to be a pandas DataFrame, and xvar and yvar are going to be columns in that data frame. So what we're doing here is saying let's make a ggplot. The data source is going to be our data frame, and the quantities that we're interested in plotting are xvar and yvar. This might be district and number of Aadhaar enrollments or position and number of players, something like that. So what we've done here is we've made our ggplot. We've said that the data source that it will use is pandas DataFrame, and that the variables that we'll look at are xvar and yvar. This might be district and number of Aadhaar enrolled if were using our Aadhaar data or team and total number of players if we were using our baseball data, something like that. Okay, so, so far that we've said that we'll have a plot which is mapping xvar to the x-axis, yvar to the y-axis, but we haven't said yet what type of geometric object is going to represent this data. So if we add plus geom point to this statement, we'll create a scatter plot. If we also add plus geom line to the graphic, we'll connect all these points to each other with lines. Now, say that we wanted these points to have a particular color. We can pass color equals coral into geom point, and also pass color equals coral into geom line. And after we do that, both the points and the lines will have the color coral. This is the second step of making a plot in ggplot, that is choosing which type geometric objects will represent the data. The final step here is going to be adding some labels so that our plot will have some context, like a title or an x-label or a y-label. This can be done much in the same way that we added the points and lines to our plot. We can add a ggtitle to title our plot. An xlab, which will be x-label, to provide an x-label. And a ylab to do the same with the y-axis. Now all I have to do is precede this entire command by Print. And I'll produce a plot in Python. Why don't you try implementing these ideas to create a graphic of your own? Lets assume that we have a csv file called hr_year.csv, which contains two columns, yearID and HR. YearID is the year and HR is the total number of home runs. With these two columns, the csv file contains the number of home runs hit in Major League Baseball every year. Can you write a function line plot that creates a pandas data frame from this csv file and then creates a chart with points connected by lines, both colored red, showing the number of home runs by year? You want to do this using the GG plot library with the syntax that we just discussed. Your code should go here. All right. Let's take a look at the code here. So first we create a pandas data frame called hr_year, that reads in the hr_year.csv file. Then we just print ggplot where we say that our data source is going to be the hr_year data frame. And the variables that we want to plot are year ID and home runs. Then we add geom points, with the color red. Then we add geom lines to the color red and then let's add some labels so our plot is able to interpret. So we'll title it total home runs by year. With the x label year and the y label home runs. We just print this and we'll generate our chart. Now let's see what this produces. Here's the plot that this generates. We see that we have a bunch of points that are red, where we indicate the number of home runs hit every year. They're connected by red lines, which gives us some sense of the rate of change. We have years on the X axis. Home runs on the Y axis. We have the labels that we put in. And also the title that we told our plot to show. So we've talked about visual cues to encode and represent your data. Another thing we consider is the different types of data that are available. Most data can be categorized into 3 basic types. In fact, we've seen all 3 types of this data during our walk through of the baseball data without explicitly referring to them as such. The first of these types is numeric data. Numeric data as you might expect, is any data where our data points are exact numbers. These data have meaning as a measurement such as a baseball player's height or weight or as a count, such as number of hits or home runs for a player or how many players there are on a team. Statisticians also might call numerical data, quantitative data. Numerical data can be characterized into discrete or continuous data. Discrete data has distinct values whereas continuous data can assume any value within a range. For example, a player's number of home runs would be a discrete data set. You can only have discrete whole number values like 10, 25, or 34. A player cannot for example, hit 14.375 home runs. A player either hits a home run or he doesn't. On the other hand, continuous data are numbers that can fall anywhere within a range. Like a player's batting average which falls between 0 and 1000. So a player could have a batting average of 0.250, they could also have a batting average of 0.357 or 0.511. And so again just to hammer this home, examples of numeric data might be a baseball player's height or weight or their number of home runs or a number of hits or a number of doubles. The take away here is that this is data that are numbers and they are not ordered in time. They're just numbers that we've collected. Categorical data represents characteristics, such as a player's position, team, hometown or even, handedness. Categorical data can take on numerical values. For example, maybe we'd use 1 for first baseman and 2 for second baseman. But these numbers don't have a mathematical meaning. That is, we can't add them together or take the average. There's also something called ordinal data, which in some sense is a mix of numerical and categorical data. In ordinal data, the data still falls into categories, but those categories have some order or ranking. For example, a scout might rank a baseball player's power on a scale from very low to very high. Another example is that you might give a movie anywhere between 1 star and 5 stars and these might be ordinal categories. For plotting purposes, ordinal data is treated much in the same way as categorical data. But the groups are usually ordered from lowest to highest, so that we can preserve this ordering. The final type of data we'll talk about is time series data. Time series data is simply a collection of observations obtained through repeated measurements over time. In plain English, a time series is simply a sequence of numbers collected at regular intervals over some period of time. For example, we might measure the average number of home runs per players for many different years. Time series data is not so different from numerical data. The real difference here is that rather then having a bunch of numerical values which don't have any time ordering, time series data does have some implied ordering. There's a first data point collected and a last data point collected. Now, let's talk about scales for categorical data. A categorical scale provides visual separation for different groups. And it often works in tandem with a numerical scale, as we'll see here when we plot home runs on the y-axis. A bar plot, for example, can use a categorical scale on the x-axis, and a numerical scale on the y-axis. So let's say that we were measuring home runs by position. The positions we've used here are unordered, but you can imagine that if we have ordinal data, we'd want these categories represented in the order that the ordinal data suggests. That will make it easier to compare and contrast your bars, or whatever encoding that you've chosen to represent your data. Finally, let's talk about time series data. The thing is, we can measure time with various different granularities. So we could use months or days or years on this x-axis, which will let us visualize our data in a discreet way. Depending on what you're trying to communicate with your data You might use a different time scale here. So, if you're looking at fluctuations that happen on a very short time scale, days are more appropriate, but if we're looking at long term trends, we may want to look at data on a month to month or year to year basis. Alright, so we've talked about how proper scales can help present data more effectively, but why don't we look at an example where scales are not used well? When used incorrectly, scales can misguide or confuse readers. This visualization from New South Wales shows that the NSW health system is recruiting more nurses. But, I can tell you it's not a good visualization. Can you tell why? Enter your answer in the text box below. The problem here is that the scale used here is inconsistent. Four stick people are originally used to represent 43,000 nurses. However, 28 stick people are then used to represent an increase of just 3,000 nurses. And then, just after that, about eight stick people are used to represent an increase of only about 1,000 nurses. As you can see, this growth doesn't follow a linear or logarithmic scale. It's just kind of random, and it's very confusing to the reader. You'd look at this and think that the number of nurses exploded, but it really only increased by 3,000. We can only assume that some kind of mistake was made here, that this doesn't efficiently communicate what's actually happening. Now that we've discussed it from different visual cues and different data types, as well as the scales to use for those data types, why don't we circle back to making graphs with ggplot and try to do something a bit more advanced. This time, we'll want to write a function lineplot_compare, that will rad in a CSV file called hr_by_team_year_sf_la.csv. This file contains 3 columns - yearID, HR, and teamID, which represent the total number of home runs hit each year by either the San Francisco Giants or the LA Dodgers. Why don't you produce a visualization comparing the total home runs hit by year for the 2 teams. Note that to differentiate between multiple categories on the same plot in ggplot, we can pass color in with the other arguments to aes rather than in our geometry functions. For example we could say ggplot data aes xvar, yvar, and then color=catetory_var This should help you to make this chart. Your code goes here. [BLANK_AUDIO] So first we're going to make a pandas data frame. Again, I'm going to call it hr year. That will read in hr by team year sf ls.csv. Then we'll create our ggplot. So we'll say print ggplot hr year, and note that we say aes yearID, HR, and then we set color equal to teamID. Then we're going to add geometric points and geometric lines, but not pass on a color. Otherwise this is very similar to our basic plot, which didn't have different lines and points for different teams. Let's see what this creates when we actually run it. As you can see we have here a chart which compares the total number of home runs hit per year, for the L.A. Dodgers, called here LAN for LA National League. And the San Francisco Giants, called here SFN for San Francisco National League. We have two different sets of points and lines which are color coded according to team, and we're able to compare how the total number of home runs hit have varied by year for the two teams. All right. Now that we've learned the basics of visual cues, different data types, and different skills that we might use to plot those data types. Why don't we focus in more on time series data? Since that's the format that our New York City subway data and weather data will come in. We'll discuss different things we can include in the plot for time series data Starting with something simple like a scatterplot, and adding more complex features like maybe a Louis curve. With each iteration, we'll show how we can combine visual cues, data types, and appropriate scales in our charts to achieve visual excellence. Again, why don't we use baseball data as an example. Let's plot the winning percentage of my least favorite team as a Yankees fan, the Boston Red Sox, from 1960 until 2010. Let's start with a simple scatter plot. Let's start with a simple scatter plot. Each dot in this chart represents the winning percentage of the Boston Red Sox in the year between 1960 and 2010. In this chart, the main visual cue that we're using is position. The year for our particular winning percentage is dictated by where we are in the x coordinate. And we see what the winning percentage is by looking at a data point's position on the y-axis. Secondly, we can note that this plot provides pretty good context. We have an X labeled Year, indicating what the x-axis represents. A Y labelled Winning Percentage, telling us what the y-axis represents. And we have a pretty descriptive title. Boston Red Sox Winning Percentage. We might include by year to make it even better. This will help people who are unfamiliar with this data really look at this chart and understand what's happening. But as you can see, a scatter plot here really puts focus on the individual values. It's a little bit hard to discern trends. It could also be hard for a reader to mentally fill in the gaps between the points and really understand what's happening from year to year. It's for this reason that we might want to use a line chart. So, to mitigate the shortcomings of a scatter plot, we can plot the same data as a line chart, and connect the dots with lines, as we see in this image. Putting lines here really emphasizes trends. So, as we can see, there was a pretty bad series of Red Sox teams in the 90s. And that the Red Sox teams have been doing pretty well in the 2000s. It's a little bit harder to see this when we just have the points without the lines. That being said, having lines that connect each point really puts a focus on year to year variability. We can see, for example that from 1964 to 1965, there's a huge drop. But it's a little bit harder to have a good sense of the more global trends. Are the teams getting better or worse on average? For this reason, we might introduce a lowest curve instead of these lines. If we waned to emphasize long term trends in our data, rather than year to year variability we might plot a loess curve over our data points. Loess is a form of weighted regression. We won't go into it too much in this course, but you can see here that it captures the overall trends in our Red Sox data rather than the year to year effects. So we see that the themes were getting better up until the 80s, got a bit worse in the 90s, and have continued to improve in the 2000s. Adding this line makes it easier to take a quick look at our chart and come away with an understanding of the big picture. Where as if we just had the points or have the lines connecting each point, it requires a little more heavy lifting by the viewer. All right. So it's great that we can visualize the winning percentage of the Red Sox from 1960 to 2010. But what if we wanted to do a slightly more complex analysis to try to understand what is driving that winning percentage. We'd have to incorporate more variables. But how? One way is that we can use the visual cues that we're familiar with to encode even more information into the chart. For example, let's go back to our scatter chart. We can use the size of the points to represent the number of home runs hit by the Red Sox players that year. Dots that are larger meant that more home runs were hit that year whereas smaller dots mean that fewer home runs were hit. Hopefully, what we would see is that bigger dots occur when the winning percentage is higher. I've filled the plot in with some dummy data just so that you have an idea of what this might look like. We can even double up on visual cues. For example maybe we also use the saturation of the red color to indicate how many home runs are hit. So our larger dots will be a more intense shade of red, whereas our smaller dots will be a less intense maybe almost pink shade of red. A redundant visual cue can help reinforce what might be a challenge to see if we only used one visual cue. So by using both size and color here, hopefully we really hammer home to the viewer that there were more or fewer home runs in a given year. So as you can see, applying the right types of encodings and additional details to our charts whether it be lines or, you know, additional colors or sizing on our points we can really transform our visualization from just an uninspiring dot chart to a better chart that's easier to read. Where the underlying relationships between various aspects of our data Can be more easily comprehended and visualized by our viewers. My tips for aspiring data scientists would be to sort of learn the tools for data science very well, very, very well. And of course through several curriculum, for example, Udacity from university courses, etc. Moreover, to use them in the correct way. You know, one of the analogies I like using is Galileo, before Galileo, the telescope existed, but he used it to look at the stars and it changed the course of history. So it's important to know your tools, how to use them, and use them to do something innovative and new and different. And that's my advice. The success or failure of the models that you create is in a very large part determined by essentially what are you paying attention to about your data. So, this is the problem called feature selection. And I think that not enough time is spent in doing this. It's very, very important so it's, in a lot of data science shops, it's very easy to just sort of take, records that come in, and then put them into a logistic regression, and you get some output, and then like, you can call it a day. and, you know, maybe often times that could be enough to take you, like, you know, 50% of the way there. But really the difference between, like, a mediocre and a top-notch shop, is like you have sat and thought about what are the things to pay attention to? Like should I synthesize these features into a new feature? Is there something new about this phenomenon that I should be capturing but haven't yet, haven't yet captured? So there's a lot of sort of there's a lot of process that goes into, just the feature selection process which is, is hugely important. More advice, which just occurred to me, is there's an additional, there's a world of tools out there that are not merely limited. so, so everything you would learn in the machine learning class is fantastic. It forms a great foundation. Beyond there, there's a wealth of mathematical tools, and the more of them that you know, the more that it will help you. So things, for example, like differential equations, or stochastic processes such as Markov chains. Knowledge of these things allow you to break down a problem that you know nothing about. You can sort of break it down a little bit, a little bit further into pieces that give you just that extra bit of predictive ability or or insight into a problem. And the more tools that you have at your disposal the more it will help you. All right. So why don't we recap what we've discussed in this lesson. First, we discussed the components to an effective information visualization. They were visual cues, the coordinate system, the scale and data types, and the context. After that, we dived a little bit deeper into the various different ways that we can encode data visually. These included its size, position, color, and many more. We also discussed various scales and data types such as numerical, categorical, and time-series. We took a deeper dive into time-series, since that's what most of the data for our assignment will look like. And finally, we went over the basics of creating graphics using ggplot, a library for Python. For this week's assignment, you'll use what we've learned in this lesson. That is the basic principles of data visualization and the basics of how to create a graphic in ggplot. You'll create a few basic visualizations that describe things about the New York City subway. So, you'll want to communicate to people our statistical findings from assignment 3, but also maybe just some general information regarding the subway. Ridership at various hours of the day, how many people are going into and out of different stations, etc. In this lesson, we've discussed some of the basic skills necessary to make an effective information visualization. We've talked about some of the best ways to visually encode data, and also some of the most effective methods for charting different types of data. We've also gone over the basics of making visualizations in Python. If you find the individualization really interesting, I'd strongly encourage you to look at some blogs or websites that aggregate some of the most interesting data visualizations. It's not hard to find them. Just go to Google, type in data visualization, and you'll come across some really interesting stuff. You'll use this new found data visualization know how to make some really interesting graphics that communicate our findings so far about subway ridership in New York City. I'll be really interested to see what you guys come up with. Welcome to lesson five. This may be shocking to hear, but thus far, all of the data that we've used in this course has been relatively manageable. What do I mean by that? Well, we can fit it onto one machine pretty easily, we can load it into memory and we can run all of our analysis in a serial fashion. But imagine that we are working with some very large data set. I'm not talking hundreds of gigabytes here, I mean terabytes, or maybe even petabytes. At this point, it would make sense to introduce something like the MapReduce programming model, which allows us to take our data, distribute it across many different machines, and run our computations in parallel. If you spent any amount of time around data science circles, you've probably heard tons of crazy terms like hadoop or hive. Or a MapReduce or distributed file systems. More generically, you've probably heard a lot about big data. During this lesson, we'll discuss how to use the MapReduce programming models to solve some very simple problems. We'll also discuss other big data tools like Hive at a high level. That way, next time when you find yourself at a data science cocktail party, you'll be able to keep up with the conversation. Alright, well why don't we get started then. Before we get started, let's make sure you're clear on when the MapReduce programming model is appropriate and when it isn't. People use the term big data a lot. But often data isn't actually that big, in the grand scheme of things. Some people might tell you that big data is anything too large to deal with easily in Excel. That's definitely not the case. It's generally safe to say that your data is quote unquote big if it's too large to fit onto one disc. A reasonable size at which to set the lower bound for big data at this point in time is probably about five terabytes. For example, say Google was trying to catalog and index all of the books in the world to find out which words have appeared most often. It'd be impossible to load the text from all the books in the world into a single disc. It's simply too much data. This is when we should look to the MapReduce programming model. Beyond this first constraint, because of the specifics of the MapReduce programming model, MapReduce only works for tasks where you hope to employ many workers simultaneously who do not have knowledge of each other's actions. Why is that? MapReduce splits a large job up into several smaller chunks that each fit onto one machine and occurs simultaneously. These machines do not communicate with each other while performing their computations. Certain tasks, that would be very easy to do using a SQL-like database or simple Python script, can become very complex when attempting to do them using MapReduce. Because of this, the smartest thing to do is to only use MapReduce when your data is truly big. You might be wondering, what are some of the ways that the map produce programming model, can be used to solve interesting problems? Well, in which of the situations below do you think map produce may have been used? Discover new oil reserves? Power an e-commerce website? Identify malware and cyber attack patterns for online security? Help doctors answer questions about patients' health? It turns out that all four of these problems can and have been tackled using mapreduce. Chevron utilizes the mapreduce programming model to sort and process data from ships that search the ocean for seismic activity indicative of oil reserves. Tons of online commerce sites, eBay for one, use mapreduce to manage their huge amounts of data on sellers, buyers, and transactions. Some companies use mapreduce to process data to identify malware and cyber attack patterns. A company called IPTrust for example used mapreduce to assign trustability scores to IP addresses. Finally products developed by a company called Apixio leverage mapreduce to analyze large amounts of data and help healthcare professionals answer questions about their patients health. As you can see mapreduce can be applied to a solve a wide range of problems. I'm sure you're anxious to use mapreduce to solve our problems so let's discuss how the mapreduce programming model actually works. Okay. So, say I truly have more than five terabytes of data. Seems like MapReduce is the best tool for the job. How exactly does MapReduce work? MapReduce is actually just a parallel programming model for processing large data sets across a cluster of computers. The most important thing to understand about MapReduce at a high level is that computation is done via two functions. The mapper and the reducer. So, in the most general sense, we start out with a collection of documents or records. If we were indexing all of the books in the history of the world. As we discussed before maybe each book is a separate document. We send these documents in a distributed way to many mappers. Which each perform the same mapping on their respective documents and produce a series of intermediate key value pairs. We then shuffle these intermediate results and send all key value pairs of the same key to the same reducer for processing. We do this so that each reducer can produce one final key value pair for each key. If we didn't send all values corresponding to a given key to the same reducer, we would end up with many final, key value pairs for each key. Which is not desirable. Here is one way to explain the Mapreduce programming model. Say that I wanted to count the number of occurrences of each word that appears at least once in a document. Let's use the text of Alice in Wonderland. Here's a bit of text that says Alice was begining to get very tired of sitting by her sister on the bank And of having nothing to do. If I wanted to solve this problem without Mapreduce, I might create a Python dictionary consisting of all the words and their counts. I could go through the document and say, for each word in the document, if there is a key for that word, add one. Otherwise, set the initial for that key equal to one. And instead of applying it to this short sentence fragment from the book, we'd apply it to the entire book. Before we solve this problem with Mapreduce, why don't you try to write a Python script along the lines of what we just discussed, that will get the job done. Given many lines of a text, create a dictionary with a key for each word, and a value corresponding to the count of the word in that text. Note that we want the words to be stripped of any capitalization and punctuation. We just want the basic words. Here's some code to get you started. First, we import system string. And then we initialize an empty dictionary, which will hold our words and values. We cycle through the lines of the input, and for each line we create an array, data. Which is essentially all of the words in that line, split by white space. So if we started with this line. Hello, how are you? It would become, hello, how, are, and you, in an array of length four. Your code should go here. After we split the line by white space, and before we print out the dictionary. All right, so let's walk through this solution line by line. We first initialize an empty array, word counts. We then cycle through all of the lines in the document. For each line in the document we create an array, data. Data is the line stripped of any surrounding white space and tokenized based on the white space. For each string in data, we create a new key. And we remove any punctuation from the word and make sure that the word is lower cased. Then, we check if the key exists as a key in word counts. If it does, we add 1 to the value for that key. If not, we initialize that key in the word counts dictionary and we set it equal to 1. When we've cycled through all of the lines And all of the keys in each line. We print out the dictionary word_counts. Which should have a count of each word, in our document. You can imagine that if a text that we wanted to analyze was really large, rather than a short book like Alice in Wonderland, maybe a document containing all of the books ever written. This script would take an extremely long time. It might even be impossible to run the scripts, due to the inability to fit all of that data onto one machine. In this case, it makes sense to employ MapReduce. How would I solve this problem using MapReduce? You can imagine that if the text we wanted to analyze was really large, rather than a short book like Alice in Wonderland. Maybe a document containing all of the books ever written, this script would take an extremely long time. It might even be impossible to run the scripts due to the inability to fit all of that data onto one machine. In this case, it makes sense to employ MapReduce. So how would I solve this problem using MapReduce? Let's break through the word count example by writing a mapper and reducer in Python. Recall that the mapper will take in a document. In this case, a collection of words that appear in Alice in Wonderland, and will return various intermediate key value pairs. In this case, each word in the value one. These key value pairs will then be shuffled to ensure that every key value pair with the same key ends up on the same reducer. And each reducer will perform some operation on all of the values corresponding to a particular key. In order to produce one final key value pair. In this particular case, we're counting up how many times each word occurred. So for Alice, Wonderland, Caterpillar, and Of, the final key value pair is Alice one, Wonderland one, Caterpillar one and Of, one. Whereas two appeared twice, so the final key value pair there is Was two. The code you see below is a Python implementation of a mapper for our word count problem. Imagine we have a huge document filled with many lines of words. We can send different chunks of the document to a number of different mappers to get the job done. What's this mapper code doing? Let's take a closer look. The mapper takes in a document. For each line of the document, it creates and array consisting of the words in that line of text. We then cycle through the words. We clean each word up so we remove any punctuation and make all the letters lower case. Then for each word we emit a key value pair. The word itself and the number one. Note that the key value pair is separated by a tab. So, if a particular word appears multiple times in the document, we will emit multiple identical intermediate key value pairs. The word and the number one. Over and over again. Let's say we ran this map around the following sentence. Hello, My name is Dave. Dave is my name. The mapper would split the words on the white space like so. The mapper would then emit the following key value pairs. Hello one, my one, name one, and so on. The next step in the map reduced design paradigm, is to shuffle the results produced by the mappers and send all of the key value pairs corresponding to certain keys to certain reducers. So, if we ran the job with just one reducer, that reducer would process all the keys. But, if we ran the job with two reducers, one reducer might process half the keys while the other reducer process the other half. So, let's go back to our word count example. Below is a Python implementation of a reducer for our word count problem. Let's walk through this reducer line by line and discuss what it's doing. First, we set initial values for word_count and old_key to 0 and None respectively. We take in our data which you'll recall will be a bunch of key value pairs separated by a tab. If we have strange row with more or less than a key and a value, i.e., the length of the data array is not equal to, we'll just continue on to the next row. All of these lines down here are essentially summing up the count for every single key. We continue through all the key value pairs for a particular key until we notice that we're on to a new key. Once this happens, we print out the key and its final word count. We then set word count equal to 0 once again. You'll note that this implementation will not emit a key value pair for our final key. Because of that, we have to include this final bit of code down here. So if old key is not equal to none, let's print out one final key value paie. So, say that we ran this reducer on all of the intermediate key value pairs our mapper in the previous slide produced. We would go through all key value pairs with the key hello. There's only one, so we emit hello, 1. There are however, 2 key value pairs with the key my. So we would emit the final key value pair, my 2 for this key and so on. So now if we use the mapper and reducer we just wrote on a more significant example, say the text of Alice in Wonderland, we can check that it works and ensure that it produces the same type of result as the Python script that you wrote earlier in this lesson. We're going to simulate running this map reduced job by just running these scripts locally, using Python. Note this isn't a real map reduced deployment, we're only running this on one computer. If we run the script, we see that we have an output where we have all of the words that appear in Alice in Wonderland and the number of times that each of those words occurs. Note that all of the words are lowercase and are stripped of all punctuation. Alright, now we know how to use the Mapreduce programming model to count words. That's great, but counting words is pretty specific and it's not amazingly interesting. Can we use the Mapreduce programming model to do more interesting things? We sure can. Lets revisit the AADHAAR enrollment data from way back in lesson two. Say we want to know how many enrollments happened in each district. We can perform this calculation using the map produced programming model. Of course, we saw in lesson two, we could easily perform such a computation with a sequel query. Or a Python script. Using Mapreduce to solve this problem isn't necessary with data our size. But imagine if we had terabytes and terabytes of data. Say, line items for the AADHAAR enrollment of every single Indian citizen. We'd have no choice but to use Mapreduce. Lets look at the CSV file containing our Aadhaar enrollment data once again. Each row has a number of columns such as, registrar, enrollment agency, state, district, Aadhaar generated, enrollment rejected, and a bunch of other information. If we want to count the number of Aadhaar generated per district. The columns that we're most interested in are district and Aadhaar generated. Can you fill in the missing pieces of the mapper? So if you wanted to complete this job using the mapper programming model, we would need to write a mapper and reducer. Why don't you give it a try. Here is the skeleton of a mapper for this job. We go through every single line in the input. In this case, it's going to be our CSV file containing all of the rows and our Aadhar-generated data. You're going to have to go through each line, which will be a list of comma-separated values. The header row will be included. Took a nice each row using the commas and emit a key value pair containing the district and the number of Aadhhar generated separated by a tab. Make sure that each row has the correct number of tokens and make sure it's not the header row. In order to count the number of Aadhaar generated per district using map reduce, we'll also have to write a reducer. Here's the skeleton of a reducer function that you'll fill in. We initialize aadhaar_generated to 0, and set old_key to None. You'll cycle through the list of key value pairs emitted by your mapper, and print out each key once, along with the total number of Aadhaar generated, separated by a tab. You can assume that the list of key value pairs will be ordered by key. Make sure that each key value pair is formatted correctly before you process it. Here's a sample final key value pair. Gujarat\t5.0. Let's walk through the code you wrote to complete the mapper function. First, we create an array data, which splits each row of data on the comma. If the length of the data array is not equal to 12, which is what we expect given our aadhaar data. Or the first entry is equal to registrar, which would indicate that this is the header row. We continue on the the next row. Otherwise, we print the district and the number of aadhaar generated separated by a tab. Now, let's talk about the reducer function. Recall that our reducer function will consume the key value pairs emitted by our mapper. So we create and array data for every single line, which will essentially be of length of two containing the key and the value. Note that we split on the tab, which we input into the output of our mapper. If for some reason data is less than or greater than length two, we continue on. There's something wrong with this key value pair and we shouldn't process it. Next, we set this key and count equal to the key and number of aadhaar generated in data. Recall that the reducer receives the key value pair sorted by key. So if this is a new key, let's submit the final key value pair. The key and the total number of aadhaar generated, separated by a tab. Then let's set aadhaar generated equal to zero. Otherwise, let's add the number of aadhaar generated in this particular key value pair to the total number of aadhaar generated for this key and let's continue onto the next value. We include this last if clause for the last key in our data. Because there's no next key after the last key. If we didn't have this, we would not admit a key value pair for the final key in our intermediate data. So here after we've done all this other processing up here, we just say for the last key, hey, let's make sure we emit the key value pair. Alright, so over the course of this lesson we've discussed how to perform some basic computations using the Mapreduce programming model. For example, counting words in a collection of text, or aggregating adhar generated in a particular district. These jobs are easy to write in the Mapreduce programming model, and even so, solving these problems with Mapreduce requires a lot more code than they would if we didn't use Mapreduce. For example, compare our original word count solution using Python dictionaries to our Mapreduce solution. As you can see, the Mapreduce implementation is quite a deal more involved. You can imagine that if we wanted to do something more complicated, say compute the similarity between the number of documents, or implement some machine learning algorithm, things could get pretty hairy, pretty fast. During this lesson, we focused on the map reduced programming model. A very common open source implementation of the map reduce programming model is HADOOP. HADOOP couples the map reduce programming model with a distributed file system. In order to more easily allow programmers to complete complicated tasks using the processing power of HADOOP, there are many infrastructures out there that either built on top of HADOOP, Or allow data access via Hadoop. Two of the most common are Hive and Pig. Hive was initially developed by FaceBook, and one of its biggest selling points is that it allows you to run map-produced jobs through a sequel-like querying language, called the Hive Query Language. Pig was originally developed at Yahoo! And excels in some areas Hive does not. Pig jobs are written in a procedural language called pig Latin. This wins you a bunch of things. Among them are the ability to be more explicit about the execution of your data processing. Which is not possible in a declarative language like sequel syntax. And also the ability to split your data pipeline. Hive and Pig are two of the most common Hadoop-based products, but there are a bunch of them out there. For example Mahout for machine learning, Giraph for graph analysis, and Cassandra, a hybrid of a key value and a column oriented database. Yeah, so I was physics major in college and then I was physics major in grad school. And I continued on to get my PhD in physics. But what I found during my PhD is that I, I really love doing data analysis, and that my real passion was for sort of trying to understand the world through data. And I wanted to find a job where I could have a big impact and really make a difference. So all of our data is on Hoodu, and, we use Pig to reduce it. But there are many similar tools like Hive that are used at other companies, but it's really important to be able to take all the data that, that's far too large to fit on any computer, and be able to aggregate it down or reduce it down. To a point where it's small enough that you can look at, look at it. Like, on a single machine. Or make a sort of, reduced plot of it. So being able to handle the data and pull the data from different sources is very useful. So, what's really great about Pig is that it abstracts away from the user the actual implementation of how the calculation is done. So, you can write in a more abstract language what kinds of operations should be performed to your data. You can say, for example, join this data set to this data set, or filter out this data set, or reduce this data. But you're not telling it how to do the analysis. And then Pig can decide the fastest way to do it for you. So instead of having to write very detailed jobs describing exactly what should be done, you can just specify in an abstract sense the calculation that needs to be done, and you can let the computer do the hard work of figuring out the faster way to do the calculation. So what I really love about being a data scientist, is I love that I can use data to help these software companies out. And the reason I love that is because at the software companies you're able to [UNKNOWN] help build systems that scaled a hundreds of millions of users, that can have an enormous impact on the world. And I really love that you can bring quantitative reasoning and quantitative thinking to help out all of the users. I love the impact. Why don't we apply Mapreduce to our class projects? This week, we're going to synthesize all of our findings up to this point, the statistical analysis, the data driven predictions, and visualizations into a blog post that your friends or family would be able to read if they wanted to learn some interesting facts about the New York City subway system. It's often helpful to begin a blog post with some interesting, but basic facts about the subject you'll be discussing. You might recall earlier in this course, I provided some statistics about the New York City subway, like daily ridership, number of stops, ect. I'd like you to collect some statistics, about our New York City subway data. More specifically, how many subway riders were there, over the course of May 2011? And how many people passed through each station on an average day using the Mapreduce programming model? Well, that's it. You've completed Udacity's Introduction to Data Science course. I really hope that you've enjoyed it, and you've learned a lot. Having taken this course, you should now be familiar with a lot of the tools and techniques that data scientists are using on a daily basis. You've also completed an entire analysis project investigating ridership on the New York City subway system. And you can share that with your friends or your family. If you find yourself still interested to learn more about data science, I'd highly recommend you take some of the data science courses being offered by Udacity. They cover all of the topics that we've discussed in this course at a much higher level of depth. If nothing else, I hope that you've learned that data science isn't just something being used in Silicon Valley. More and more, data science is being used to solve problems of all different types in industries all over the world. Well, so long, and thanks for taking the course.