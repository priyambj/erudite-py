Hi. My name is Dwayne Lessner. I'm a Technical Marketing Engineer at Nutanix. And here with me- I'm Karl Krueger. I'm a Course Developer here at Udacity. Welcome to our course on Intro to DevOps. We'll take you through what makes up the principles of DevOps, explain how to apply them where you work, and showcase some tools that can help you along your journey. We'll start out by talking about what DevOps is, where it came from, and what problems it's trying to solve. We'll dive into what makes dev and ops people tick. How to understand the other side, gain some empathy. And how they are motivated to get to the end goal of creating better work flows to save you time, and to keep you from pulling your hair out. And we'll talk about some tools for those work flows. Deployment, automation, and monitoring tools that can really change your working environment for the better. Throughout we'll sprinkle in some examples. We'll also take a look on how we can measure the benefit of dev ops in use. So with that let's get into it. In a general sense, the purpose of a company or other organization is to create value. That value can be physical or digital, intellectual or social, but most companies or organizations are not made with an idea of hey, I want to invest a lot of money and waste a lot of employees' time to make something that is completely useless and worthless. To create value, first you need an idea for a product or service but having an idea isn't nearly enough. Customers don't pay for just ideas. They want to see something that does what they need, a functioning product, a working service. An idea that you don't actually ship is nothing but vapor water. There are often a lot of people involved in taking a product or even just one feature from idea to delivery and these people have to communicate. They have to pass the product to be from one person or team to another for design implementation, testing and delivery. And, in the software business, organizations doing these things have often fallen into a particular pattern or anti-pattern. It's a pattern that can really get in the way. This classic organizational structure has strict borders between development, quality assurance, operations and other groups involved in delivering a product to the user. One team finishes their part then passes the product in it's current shape to the next department for the next step and no matter how efficient the process within single departments, if there are high walls in between them problems and inefficiencies creep in. All too often, there's not a lot of communication between departments and the attitude might be, throw it over the wall it's not our problem anymore, we met our deadline. And if the customer at the end throws the product back saying, this is not what I wanted, then a nasty cycle of blame can start. This kind of organization can have a lot of different problems. It tends to be slow and ineffective at delivery. It can't learn from failures because people are blaming one another. It's hard to make changes especially large and sweeping changes. Operational problems tend to stack up and become worse and everybody involved is pretty unhappy. These were challenges that a lot of organizations faced, especially as teams and the services they ran got larger, it got harder for them to work together. Developers and operations teams, both technical functions in the company, but with different interests and cultures, seemed to be particularly badly affected. In 2009, a grassroots movement of practitioners started that became what is now known as DevOps. This movement is experience-based, decentralized and open to all, but it has gained a lot of traction in both the open-source world and in big-enterprise companies and as the problems its solves are becoming more and more apparent in more and more companies, so is the adoption of DevOps growing. But what exactly is DevOps and how does it help? The landscape of software development has drastically changed over the last 10, 20 years, and will continue to do so. I think the scariest phrase in the English language is we've always done it that way. With advances in hardware and software, the pace of innovation has only accelerated. Old methodologies simply can't keep pace. Instead of shipping one major version every couple of years, software companies are continuously updating services that are offered online. When was the last time you got software delivered via CD? People just expect new features and fixes to happen quickly without disruption to their daily lives. Common roadblocks like getting access to infrastructure have pretty much been eliminated. Virtualization has changed the game for operational teams. Now you can create servers in minutes and harness the full power of those physical resources. Development teams have more opportunity to create templates, clone, snapshot, roll back, and destroy whole environments. There's also online services like AWS or Digital Ocean, where you can go and test your code. Today, people don't have a lot of sympathy in our on demand society. If your site doesn't work or your app is buggy, people have multiple ways to let you know, which leads to tarnishing your brand's image. This puts extreme pressure on developers and operations but also product managers and business people. How do you manage everything at this new pace? How do you change old habits and organizational structures and processes to adapt? A development process with a long detail design phase, and a planning phase, and then another long implementation phase is slow, inefficient and error-prone. To top it off, organizational silos that prevent communication leads to stale information, misconceptions, and further slows down the process. What arose from the ashes of past mistakes? Agile development was born. Constant feedback with an iterative approach allows to find problems quickly for both developers, and operations teams. But Agile covers just the first part of the application life cycle, which is development. Another part of the puzzle comes from using Agile and Lean approaches, incorporating operations. With people still working in their Silos, bottlenecks remain. Breaking down the silos, allow collaboration through all stages of the development. And this brings us to the definition of Devops. While there's no one official definition for DevOps, there are several different descriptions that practitioners have come up with. One of them is that DevOps is the practice of operations and development engineers participating together in the entire service life cycle from design to development and to production support. Another definition coined by the folks at Chef Defines Devops as a cultural and professional movement, focused on how we build and operate high velocity organizations, born from the experiences of its practitioners. Now, we should be clear that development roles here don't just include people who's title is developer or software engineer. It's everybody who's involved in making the software before it hits production, like product managers and QA testers. And operations roles likewise include everyone who's involved in running and maintaining production, systems engineers, DBAs, network engineers, security folks, and so on. So, if DevOps is a practice, how exactly do you do it? Well the first practitioners who started the movement, also coined the acronym CAMS. It stands for Culture, Automation, Measurement and Sharing. We'll talk more about each of these components. But first lets check in on what DevOps is not. There are a lot of different views on DevOps going around. So I'd like to clear up a few things about how we're going to be talking about it in this context. First off, doing DevOps doesn't mean that ops is going away or that devs are taking over ops responsibilities. Dev and ops both have experiences and knowledge to contribute. Neither role is being eliminated. Another thing DevOps isn't, is just a bunch of tools. In this course, you'll learn about several tools for doing things like deployment and monitoring. But using these tools is not the same thing as doing DevOps. Here's an analogy. Google Calendar is a great tool for helping you get to meetings on time. But putting your meetings on a calendar doesn't make you show up on time. And it doesn't cause you to want to show up on time, either. If you're like me, you'll want to show up to meetings on time because you respect your coworkers and you want to get things done with them. And if you had a meeting conflict, or show up late because you forgot about a meeting, you'd feel like you failed to meet up to that. But using a calendar program is a tool to help you accomplish a goal you already had. So DevOps is kind of like showing up to meetings on time. It's not a matter of using particular tools, even though tools can help you do it. It's an organizational change that's supported by those tools. But, at the same time, it would be basically impossible to organize large meetings without some sort of calendar. And likewise, it's not practical to scale software deployment without some tools to do that. The tools that you'll learn about in this course provide support for making big changes in how you manage software systems. And, although, in this course you'll be working by yourself rather than in an actual large organization, I hope that you'll get a sense of how to apply these ideas and tools in your own organization and your own projects. How does DevOps fit into the world of software development, and relate to things like agile development and continuous integration that have been around for a while? Agile software development is based on iterative development, where requirements and solutions evolve via collaboration between self-organized and cross-functional teams. The term was coined in 2001 when the agile manifesto was formulated, so it's been around for a bit longer than DevOps. But in many organizations using agile development, not everybody has been invited to the party. There's a good chance that you already use some form of agile development in your organization. But it covers just a couple of steps in the whole process. There are other steps that need to be covered. Agile development generally covers the steps from planning to coding to building and testing. And then iterating on the design again. It also emphasizes collaboration between cross-functional teams. Testing here includes unit testing of code as well as integration testing and user acceptance testing. But agile doesn't necessarily speak to release process. And deployment and operation can look very similar in an agile organization to in a traditional software organization. And that's not necessarily the best solution. When we start to improve this, by automating build and unit test processes, this is covered by continuous integration. The automation process helps with the iterative development and makes sure that changes or new features do not break other functionality. Automating it all the way to the release is covered by continuous delivery. Continuous delivery pipeline also makes it easy to control user testing and release the software when all the tests pass. Now, to make sure it's possible to deploy all these releases into production to thousands or maybe millions of users, there has to be an automated scalable way to run operations. And to make sure that everything runs smoothly and to improve the whole process over time it's necessary to measure it all. But for that measurement to be actionable, there has to be good communication and a culture of sharing and collaborating between all the teams involved. Adding automation to this system is good, but it's only when you combine it with a culture of communication and collaboration between teams that you really have the DevOps approach to the software life cycle. Let's talk about the project you'll be working on in this course. You'll be using some common development, packaging, deployment, and monitoring tools to build a smooth work flow for building and launching changes to a web application. Although you'll be working by yourself, we'll talk about how you'll be able to apply the ideas you learned here to your own work place or future projects. You'll start with fixing a common problem, difference between development and production environment, by creating golden images that can be used by developers and on production alike. Then you'll step into the shoes of a developer and use the golden image from the previous step to launch a local development server for a web app. After that you'll build out a continuous integration service. A system that will watch for check-ins of new code and automatically run tests and package it up. Then you go back to the developer role and see how pushing your changes to your team repository automatically makes changes to the integration environment. And since you can't improve what you can't measure, you'll configure a simple monitoring system that will allow you to see how your system is performing. You'll also see how the perspective of what important things to monitor, depends on the role you're playing. This, of course, is not the only way to implement some of the DevOps principles in an organization or a project. But, it will give you a practical overview and let you walk in the shoes of different people for awhile. Will help you better understand some of challenges other teams faced, teach you how to communicate with them better and how to work together to solve the problems you're facing in your everyday work. Here's a story about a company with a problem. Noodle Search runs the world's best site for finding pasta. Management rewards the developers for shipping new features, like gluten free rice Noodle search. So the developers want the operations team to push out new releases as quickly as possible. But sometimes when releases go out, the production systems slow down. Then the users start complaining to tech support that they can't find their linguine in a timely fashion, and half their wonton queries are timing out. Then operations goes and installs additional servers and everything gets better, until the next release. What are the causes of this problem? Is it that the development environment doesn't match production? That new releases aren't being tested for performance? That operations should automate the release process? That the business isn't anticipating its scaling needs? Or that developers are being too harsh to the operations staff? Mark which ever of these that you think apply. In this case, we don't know whether the development environment matches production, but that doesn't seem to be the cause of this trouble. The technical problem is that new releases aren't being tested for performance, so the performance problems don't get discovered until the release is done. Automation doesn't seem to be the issue here. But because of the lack of testing, the business isn't correctly anticipating when a new release will need more server capacity. That's the organizational side of this problem. And this last one, maybe it's true, maybe it's not but we don't actually know. We mentioned previously CAMPS. The first letter letter stands for Culture. What does that mean? Huge inspiration for DevOps, is Agile. And one of Agile's key points is, people over process over tools. You can have the most efficient product, development, operations, and marketing teams in the world, but if they do not communicate, there's no feedback or collaboration between teams, it can lead to a huge waste of time and resources. Lean is also a major inspiration for DevOps culture. Lean is centered on streamlining operations, reducing waste, with the end goal of maximizing customer value. Improving separate points in the production pipeline is not enough, you have to improve the whole flow. Ever hear the phrase hurry up and wait? Reminds me of traveling on international flights. Having a faster silo doesn't really help in getting me to the plane quicker. I usually just end up waiting in the next line. This is also true for DevOps. If we focus on our customers as the main goal, it becomes an easy common ground for all concerned parties. A great culture is also important, fostering respect. You can also call it hug ops. When you can understand and you can empathize on both Dev and Ops teams, you can really focus your energies on positive outcomes. Usually known as going to work thinking how can I make this harder, but there are various reasons as to why it happens. The next principle we should talk about is automation. Whenever you write a piece of code to perform a task that otherwise you do by hand, that counts as automation. There are a lot of steps of both development and operations processes that can be automated. One big one is deployment, setting up a server to run an application. Once upon a time, sysadmins would spend a lot of time setting up physical servers individually. And when you needed to install software upgrades, you'd log into each machine and run the upgrade. Now smart operations people would automate as much of this as possible, like throw together a shell script that logs into each server and runs the upgrade. But that kind of work generally wasn't visible outside of ops. There's something that ops did to make their own job easier, but it wasn't recognized as a vital function of the organization. But today, on the other hand, you don't have one or two physical servers for an application. You might have dozens of server instances on a cloud platform, or you might have hundreds to support a large and popular application. At that scale, you can't reasonably manage them by hand at all. You need automation just to get the job done. Now on the development side of things, there are also a bunch of things that can be automated. This isn't really new with DevOps. The agile movement embraced unit testing, acceptance testing, and continuous integration. But DevOps ties these into the deployment process, eliminating the boundary between developer side automation and operation side automation. After all, if we're both working on the same service, why should we have two incompatible views of it? Well, we'll be talking more about automation in lessons two and three, and you'll get some hands-on experience with automating parts of the testing and deployment workflow. Next let's talk about measurement. Tthis is another corporate of DevOps. If you want to improve something you have to be able to measure it. For instance, suppose that a new software release is expected to make the application faster. How would you know if it actually does make it faster? It is not very rational to say I expect it to be better, therefore it is going to necessarily be better. Sometimes even an expert is wrong. So when we want to know how their service is doing or whether we've improved it, we look to take measurements of its performance. Now in traditional IT organizations, infrastructure monitoring has been an Ops responsibility. This includes things like noticing if a server has crashed or if an application is unavailable or if a disk is filling up. But that's a pretty limited view on what's worth measuring. The service is a lot more than just the servers. With participation from the development team, we can get much better monitoring. Developers can make sure that applications export useful data, and they can write services with an eye to make them integrate with systems for recording and analyzing that data. And we could also plug monitoring into the release process. For instance, if a new release causes the software to slow down, maybe we want to roll it back. Or even simpler, if we know the traffic to our service is lowest at a particular time of day, maybe that's the best time to do a release. But measurement actually goes beyond even dev and ops. We can use the same measurement tools to record business metrics, things like customer activity, transactions per second, even measurements of customer happiness. And we could also measure the effectiveness of our own tools. For instance, some dude in a purple t-shirt might tell you that using certain deployment tools will let you do releases a lot faster, but why should you trust that guy? You can measure it and find out for sure. On the operations side, we can talk about measuring errors and failures, and find out whether these new practices we are adopting are actually reducing them. Then we'll talk some more about measurement in lesson three, where you get some more in-depth look at monitoring and some hands-on experience as well. It's very important to align all the different stakeholders towards that same goals. First, everyone needs to have a shared view on what the current situation is. What are the goals? What are the current practices and flows? All of the stakeholders should participate and work together to formulate the end goals. Shared input generally creates shared responsibility and ownership. People are willing to work together if their thoughts and opinions are being heard. It's important to have a shared view on problems and identify and eliminate bottlenecks. The bottlenecks can be inside departments or between them. It's not really important as long as they're being addressed. And it's also important to have shared benefits amongst the team. You want to make sure that you have all the involved stakeholders get the benefit of a new product release. To achieve that you need to define and measure key performance indicators for the whole cross functional team. You cannot improve what you cannot measure. Here's another company with a problem. Swanky Chat is a start up that makes communication software for awesome people, but sometimes their awesome users start sending weird new messages that cause their servers to crash. Swanky Chat operations team calls these messages of oncoming doom, or MOODS for short. Ops tries to get the development team to fix the mood bugs, but the devs are usually busy writing awesome new features. So in order to keep the servers up, ops puts an application level firewall in place that filters the bad messages out. That works just fine, but so as a consequence, nobody worries about the MOOD bug anymore. But then the developers release a new feature, Perky Soundtracks for video chat, and it works great in their development environment. It's fast and it's awesome and the QA testers even love it. But when it launches in production, it turns out that the firewall rule for the MOOD messages also blocks the traffic for the soundtracks, so the new feature doesn't work at all and the launch is a big flop. Debugging it takes a long time, and the next three feature releases are all delayed. So what do you think are the causes of this problem? Is it that the development and production environments don't match? The new releases aren't tested for performance? That a workaround relieved the pain and hid the severity of a bug? That firewalls are unreliable and should never be used? Or that the operation staff should be fired and replaced with more developers to fix bugs? So in this case, the feature worked great in development and testing, but those environments didn't match production. The firewall rules are an important part of the production system. They're what's keeping it running, but they aren't represented in the development environment. So that's one problem. However, it wasn't really a performance problem. So this one's not a right answer. Rather it was a bug that never got addressed by developers because an operational fix that was meant to be temporary relieved the pain, made it not urgent anymore. Now, firewalls are actually a pretty good tool. So, number four here is not correct. And properly fixing this problem in both the short and the long term required both ops and dev skill sets. So this last one isn't right either. So to recap, the problem was that ops correctly deployed a workaround, but as a consequence the bug was never actually fixed. And the change that ops made to the production environment was never reflected in the development environment. So developers and testers didn't notice that their new feature wouldn't work in production. Congratulations, you've reached the end of the first lesson. And you've learned the principles that are the basis of Dev ops, culture, automation, measurement, and sharing. In the next lesson, we'll go deeper into the culture, automation and sharing topics. And not only in theory, but also in practice. And now you have an idea where we're coming from with this course. We'd like to know where you're coming from too. On the next page, there's a link to a survey about your background and experience. And what you think of the course. Please take a moment to fill it out. Then, we'll see you in lesson two. Hello again, welcome to lesson three. In this lesson, we're going to be talking about two things that can really improve your dev ops approach to the systems that you work on. First we'll talk about continuous integration and delivery, or CI and CD. These are systems for managing your software life cycle, taking your applications code all the way from the repository through testing and into production. And trying to automate all of the steps in between. The CI tool we'll be showing you is called Jenkins. And then we'll talk about monitoring, which is just about my favorite subject in systems engineering. We'll be showing you monitoring and data visualization using a tool called Graphite. Okay. Let's do it. In lesson two we talked a bit about developer and production environments. But it's more complicated than that. Between the developer's workstation and production, there are lots of nooks and crannies that our application code has to land. There are no hard and fast rules on how many environments you need to maintain, and will most likely be driven by use cases. In most cases you'll have at least six environments. The starting place for all code is the local environment, which is just the developers work station. Secondly you'll have the development environment. An example of this can be a server where the developer can deploy their own branch of the code and play around without messing up someone else. It's often referred to as the sandbox. After the development environment an integration environment is needed. It can be a CI build target or for developers to test side effects. This is where the app gets built and tested and changes are merged back into the main working branch. It is possible to have a separate test, QA environment for functional, performance testing, quality assurance and user acceptance testing. Or, if the team is small, it can be just one environment that gets deployed right after the CI process. Up next is moving your code into the staging area. The staging area is a mirror production. It includes databases that usually have a fairly recent copy of live data. By now a lot of problems or issues should have been caught before moving code into production. Production serves the end users and clients and hence can have scale and size that you might not be able to test within your other environments. It's also possible to have production canaries, a subset of production servers where the new version gets deployed, the ability to test performance with real workload but limiting the impact of any negative things that might come up. As the code gets written on a local developer work station, how does it transition into production and how are the environments different? Clearly there must be some difference between a development server and a production server. For example, databases may have old versus current data, and software patches may be out of sync. So, let's talk about Development Workflow. Although details will be different from organization to organization, the general ideas will stay the same. Developers just don't edit code files on one big file server. They use version control systems to track their changes and keep them in sync. There are a bunch of different version control systems out there, but some of the most important ones these days are Distributed VCS such as Git. When a developer writes their change, they commit it to their fork or branch of the project. Then, when their work is ready to be merged in, they issue a pull request to the main branch and request a code review from their colleague. Then, once the review is done, they can merge their change in. So there are various points that testing can happen. Developers can manually run tests on their own code before code review or as part of that process. Integration and full system tests rely on running the whole service, often on a testing server. After that, it's common practice to deploy the changes to a staging server for further tests. The difference between a test server and a staging server is that the latter resembles production as closely as possible. For instance, having real data. And then, if everything goes well, the build gets promoted to production. Depending on the application, it can be a good idea to do rollouts incrementally. So that, even if, after all this testing, the release turns out to break the service. At least it doesn't break all of it at once. Even if you never want to have to use it, having the ability to rollback is really important. As you can see, there were a fair number of steps in this process. If the handover from every step to another was manual, it would be slow and potentially error prone. Slow manual processes create bottlenecks and errors create extra unplanned work that uses up time to do planned work. So, by automating as much as we can, and making sure that people can fix the bottlenecks in their work process, DevOps practices can give us much more time to do the actual productive work. The first part of the process is continuous integration, an automated process of getting changes into existing codebases and building and running tests. The goal's not to create a backlog when you go to deploy. A boring deployment would be deemed a success. No surprises is a good thing. When embarking on changing and updating code, a developer takes a copy of the current code base to work from. As other developers make changes to the source code repository, the local copy gradually ceases to reflect the repository code. Not only can existing codebase change, but new code can be added as well as new libraries and other resources that create dependencies and potential conflicts. So, what's the best practices to prevent this from happening? You want to maintain a code repository. Automate the build. Test the build. Commit your changes often. Build each commit. If it breaks, fix it right away, while it's fresh in everyone's mind. And when possible, test from a clone of the production environment. While most of these practices are automatable by tools, committing code is still a manual process. Developers have to adopt this approach for it to work. If someone will commit their changes only once a month, it's going to create problems, a human problem. So continuous integration just means that whenever you check in new code, it gets integrated, and built. But seriously we're not going to do that by hand. Running the compiler and test suite is a job for a program. And no surprise there are a lot of pieces of software to do this. One of the most popular ones is Jenkins. It's an open source project written in Java. You have to host it yourself. There are also several hosted CI solutions. Like Travis CI and Circle CI and others. All of them work on the same principals. The CI system, watches a code repo or branch. What it is looking for is new commits written by developers. When a new commit occurs the CI systems grabs the new commit and spawns a build process. The build process runs and builds things such as compiled binaries or data files. Once the build is successfully complete, the CI system then spawns tests that can be run on the build artifacts and then it reports the results back to the developer. Now, since most of the hosted CI solutions require subscription, for the next part of this course, you will be setting up and using your own Jenkins server So, here we are in the Jenkins console. Even without logging in, we can look at various projects that it's been set up to build, like adventure. This project is watching a GitHub repo for another Udacity course, and every time a student checks in, in addition to that repo, this Jenkins instance recompiles it. You can see the history of it over here under build history. Now, if we go back to the main page, and login, then we'll be able to access the server wide settings. For instance, here are the security settings that let us say what users are allowed to do. And over here is the list of plugins. Jenkins is kind of a plugin based system. Everything you want to do, such as pull from GitHub or run JUnit tests or various other things is there on a plugin. Now that we're logged in, let's take another look at that adventure build target. Under the project configuration, you can see the name, there can be a description, here's what repository it's gotten from with a GitHub URL here, and down here is the build schedule. It says how often the build can be run if Jenkins notices changes. H/15 in this field means that it should be run every 15 minutes. And down here you can see the build script that's actually being run. Turns out that this is just a mockup. And it runs the disk usage, or du Unix command. If this were a real build it would probably run compilation and packaging here. And then if we look in an individual build at the console output we can see the output of that script. Having this kind of log from every compilation is really handy. Suppose that something had gone wrong ten builds ago, would you rather be diagnosing that from someone's email saying, something broke and I don't know what? Or from a log file, that says exactly what's going on? So it's one thing to say that our project build successfully, but how do we know that it works right? The best answer that software engineering has come up with for this is to use a lot of different kinds of tests. Now testing is a hug subject and we actually have a whole course on it, but let's take a quick look at some different kinds of tests. Unit tests are tests the developers write along side the application code. Each test checks the behavior of one specific unit of code, like a function or class. They're designed to be automated. Developers will run them repeatedly as part of the development process. Regression test on the other hand, get written as part of debugging and troubleshooting. Once you've fixed a bug, you want to make sure that it doesn't come back due to some future oversight or regression. So you add a test that checks specifically for that particular bug. Like unit tests, regression tests are automated and run frequently. A smoke test is something that gets run after the build, kind of a sanity check to make sure the system basically works before doing any more refined testing. For instance, if our project is a web server, we might just bring a server instance up and fetch the home page just to make sure it doesn't crash. The term smoke test comes from electrical engineering. You can imagine plugging an appliance or a machine in and seeing if it catches fire, but it also has an older meaning in plumbing as a test for newly installed pipes. Before putting water in a pipe, plumbers would fill it with pressurized smoke to see if the smoke leaked out anywhere. A system integration task is just a little bigger. It's a test of the system including not just our code, but also the back-ends and dependencies and data that our code makes use of. You might often run it on dedicated test machines. Before we treat infrastructure as code, we can spin up test instances as VMs dynamically when we need to. In doing that, we'll test our deployment scripts too. An automated acceptance test is something that you'd write to verify that a system or feature does what the user wants it to. Now it can't replace everything that a human QA tester might do, but there are testing frameworks that let automate quite a lot. Whole workflows, clicking buttons, filling out data fields, whatever a user might do in your app. And then depending on your service, you might have a manual QA step as well before launch goes to production. If that's the last step before launch, the QA tester's approval can actually tell the continuous deployment system that it's okay to go ahead with the roll out. So there we have six different kinds of testing, unit testing, regression testing, smoke testing, system integration testing, automated and manual acceptance testing. While we're talking about testing, let's talk about bugs too. Having a single, unified bug tracking system can go a long way to improving communication amongst teams. If it's accessible to everyone in the organization to report issues and collaborate on fixing them, then that's one less barrier to getting real work done. Continuous integration makes sure that the software builds and runs successfully and then you do the testing. There are still steps between this point and the software release. Automating the release pipeline down to the software release is called continuous delivery. But since some of QA is a manual process, you cannot run the next step completely automatically. You need a manual trigger in the pipeline to deploy to the next stage, which is usually a staging server. For example, testing against a real database. You can run more automated and manual tests on staging. Once successful you can manually trigger the deployment. It's important to note that a manual trigger does not equal a manual build or deployment. Build and deployment is automatic, it's just the trigger that starts the process is manual. There are integrations with communication software that allows these manual triggers to be launched in a more publicly visible and convenient way than logging into a particular software program. For example, there are chatbots for popular team communication platforms like Slack and HipChat that can be integrated with Jenkins, CircleCI, and other CI solutions. These chat-bots can send build test information to a public chat room. And react to commands written into the chat. That makes it easy and fast to coordinate within the team. If the CI build and test passes, using chat, you can poll all involved developers with something like, my change is good to go. And then issue that command into the chat-bot. This makes for a fast and transparent release process. We will not be setting up these chat services, but see instructor notes for more information. If you have a service that handles thousands of user transactions every second, over dozens or hundreds of servers, you're not going to sit there and watch the log files to see if it's working right. You, as a human being, literally cannot read thousands of messages per second in hundreds of log files. You can't react the difference between an acceptable and an unacceptable level of errors. Without monitoring systems to be your eyes and ears you literally do not have the ability to notice or care about something happening that fast. There are a few different ways that we can gather data about our services. Not all of them apply to all services, though. One is external probing, or sending test queries. Some outside system will send queries in to our system and see if it performs as expected and record the results. Another is application-level metrics that are exported by monitoring interfaces of our serving processes. This might include things like the number of queries they're handling per second or what the latency is. Those can be pushed into a monitoring system or pulled into it from the server. Another thing to monitor are stats that are exported by our runtime environment, for instance, the memory profile in a Java JVM. And then there's also information about the computers that we're actually running on such as the load average or the number of disc errors. You can actually very typically discover that a disc is about to fail by monitoring the number of errors it's having. Other sources of monitoring data can include logs analysis, query profiling such as Google App engines app stats, and external analytic services. There are also a number of data products that a monitoring system can provide. The one everyone knows about is alerting. Getting a page saying the system is too slow. But your monitoring system can also provide charts and graphs for performance analysis. Say things like this release is 10 percent faster than the last one. And also for capacity prediction. For instance, if it takes ten instances to support 1,000 transactions per second. Then if we scale to 2,000 transactions per second, we should expect to need ten more instances. Also for growth measurement. You can plot a graph out of your monitoring system and see that your service is doubling traffic every three months. You can combine that with a capacity prediction to get capacity needs forecasting. Tell you how many computers you're going to need next quarter. And last but not least, your monitoring system can also provide great metrics for debugging problems. For instance, suppose you're trying to diagnose why your servers are crashing. If you notice from your monitoring that each instance is using more and more memory proportional to the number of user queries, that might indicate to look for a memory leak. So here we are on the Jenkins console, and this is a build pipeline with a bunch of automatic steps in it. If this build, starts whether automatically or manually, it will run each of these steps in order. But let's say we want to add a manual step, for instance, this might a be a point at which a human QA tester will need to approve a particular build for release. To do that, we'll need to do a few things. First, we need to decide where that step will be inserted into this pipeline. We might decide, for instance, that we want it to after tasks are run but before their release. Second, we need to write the step itself. We'll set it up to just echo a message to the logs, just as a demonstration. So in the new step, in the build trigger section, well, we want this to be a manual step, so it isn't triggered by anything. Then we go to the step that we want to be before the manual step. And we set it to have a post build action, which will build other projects manual step, and we specify the downstream project as being our new step. Now, in the pipeline view, we can see the new step, but we want the release step to be after the manual QA step, so let's change that too. Instead of being triggered by the Test step, the release will now be triggered by the ManualQA. Save that. Go back to the Pipeline. And now we have them in the right order. And when we run it, well, let's see what happens. So you can see, the later steps turn blue and the first one turns yellow, then green, and the colors change on down the pipeline. And when it gets to the manual step, it'll wait for us to manually run it, and it's all green. So, now it's your turn. In your Jenkins instance, add a step to a pipeline, set the preceding step to have it as a post-build manual step, change the following step to have the manual step as a build trigger, and then run the pipeline. What do you think all those different colors actually mean? So when we ran the pipeline, we saw a number of different colors. We saw Blue, which meant a step had not yet been run. We saw Tellow for steps that were currently running. Green for ones that had succeeded. And if you saw one that was Red, that would mean a step that was failing Okay, great. So now you have Jenkins taking your service all the way from your code base to the borderlands of production. But once you're there, how do you know if your service is working right? One of the principles of DevOps is measurement. That includes measuring how effective our process is, but it also includes measuring how well our services are running. What is it we want our service to do? If it's a web server, well, we want it to answer user queries, we want it to be fast, we want it to return actual results and not errors. Measuring those things is what service monitoring's for. >From an operations standpoint, your service monitoring is a lot like the instrumentation in an airplane cockpit. You know how an airplane pilot can keep the plane on course even when there's no visibility? They need instruments even to tell if the plane is in level flight or if it's diving into a crash. Or another way of thinking about it is that service monitoring is like a scientific instrument. You can see things with a microscope or a Geiger counter that you can't see with your naked eye. If there's dangerous bacteria in your water supply, looking at a drop of water with the microscope let's you know what it is that's making people sick. If you have dozens of servers and you want to know whether they're doing the right thing, or if they're serving errors to maybe even just 1% of users, you're not going to find that out by just refreshing the home page or something. You need a monitoring system to tell you. One of the biggest misconceptions about monitoring is that it's only for alerting. That monitoring is there to page Ops when the service is broken. But alerting is really only one of many different ways that monitoring can help you and your organization. Monitoring doesn't just tell you when your service is misbehaving, it also tells you things that are business relevant, such as how your service's traffic is growing. That's important not only for capacity planning, hey, we need more servers, but also for business purposes. Like, you want to buy our website? We get 10,000 page views per second. Your service exists for some purpose, like making money or teaching people or letting the world read your brilliant postmodern hypertext novel. Your monitoring can tell you something about how well your service is meeting that purpose. For instance, when you're AB testing new features. Monitoring can also be a big help to developers. Let's say you want to make optimizations to the code to make your service faster. How are you going to know whether you've succeeded? The service monitoring provides you a source of truth on that. It's also good for finding certain kinds of bugs. If you see your service's memory usage creeping up over time, that's a pretty good sign that it has a specific kind of bug, a memory leak. And if something changes during a release, like say a big increase in CPU usage, that can be a good sign of a bug as well. Because good monitoring can be generally useful throughout the organization, it should be widely accessible. When business managers, sales people, or developers want to know the growth trends of the service, they should be able to go to the same stats that the services operations people use. But you know, monitoring is pretty vital for operations work, too. >From an ops standpoint, we can talk about measuring facts about our infrastructure resources, like individual servers and our network. Measuring resources like CPU utilization, RAM usage, disk space, network bandwidth, lets us know if our infrastructure's overloaded. Or better yet, it can give us warning well before it's overloaded so it can scale our service correctly. On servers, we might also want to record things like the hardware temperature and the disk I/O error rate. By looking at that, you can often predict a disk failure before it breaks your machine. How about on the application side? If we have ten web servers with a load balancer in front, we probably want to know how many requests each server is handling. And the monitoring system can sum those numbers up to get the total number of queries the whole app is taking. If you care about how fast or slow your service is, well, you have to measure its latency. And likewise, if you want those web servers to be serving 200s like a happy web server, instead of 500s like a broken one, you'd better monitor whether all those requests are succeeding or failing. And once you have all this information recorded, you can do things like calculate how much total CPU is needed to handle the average transaction. Or how service latency responds to increased RAM pressure. Or any number of other things. In a really awesome world, you could even plug that back into your continuous delivery automation. And before letting a release go out to production, load test that release and make sure that it's not a lot more resource intensive than the last version. So, there are a lot of open source systems for different aspects of monitoring. This is a really vast subject, so we can't really cover all of it. Couple of important points, one you won't be able to monitor every aspect of your system with only a single tool. That's okay. Pick a couple. It's important though that whatever monitoring systems you pick be generally accessible within your organization so that other people can learn from them as well. One of the most flexible approaches to monitoring involves having some kind of key value data store, with a simple API to dump in whatever values you want to monitor. This is the approach favored by Graphite, InfluxDB, etcetera. Then you have various front ends to visualize the data. Graphite offers it's own web front end for it's data, Graphite also works well for a more flexible display. And there are a lot of other systems that are mentioned in the instructor notes. Now for the app server you've been working within this class, we've provided a Graphite monitoring server with symmetric setup. You should check it out. Congratulations, you've reached the end of this course. But this has been just an overview. There are lots of organizational changes we can talk about, a lot of tools we could show you, a whole lot more ideas on how to improve your project's life cycle. Check out the links in the instructor notes for some resources, blogs and books that can help you take you to the next step. As you apply what you've learned in this course, please come back and share it with us and with your fellow students. Thanks for joining us and good luck. Welcome to lesson two. In this lesson we talk more about cultural differences between Dev and Ops working groups. We'll also focus on one specific area of difficulty, compatibility between development and production environments. As we've said before, you can't solve cultural tensions just by deploying some software tools. But in this specific area, we can make a big difference by making it much, much easier for people to build compatible environments. The tool we use to do this is called Packer, and what it does is to build standard deployment images that will work in both production and development systems. So let's talk about some differences in perspective. Developers write code. They are responsible for implementing feature requests, fixing bugs, and so on. Their incentives tend to be more feature oriented. They want to have big launches as accomplishments. Building new features might be dependent on exploring new technologies or approaches that have not been used before. Taking new risks. Operations people run services. They might take care of physical servers, or run services on hosted infrastructure. They do deployment upkeep and trouble shooting. Their incentives tend to be more stability oriented. Anything that requires new untested technologies or introduces new features is a potential problem for the stability and performance of the system. The work Ops does is also usually invisible to the company. Ops is noticed when something does not work, not when everything works fine. The new release that developers get praise for might cause problems in production, and it's Ops that gets paged if the app is unavailable to the user. While both of these perspectives are valid, sometimes they seem like opposite in conflicting interests. Dev can feel like ops is blocking their work by not providing resources fast enough or not being supportive of new cutting edge tech. And Ops can feel like Devs are being irresponsible and reckless and just implement new features without thinking about performance and security. And there are other players in the field, as well, like product management. Product managers may be even more feature oriented than devs. They may make external promises, and they might not ever talk with traditional operations staff. If the organization has a security team, they may be even more change-overse than ops, in the fear of introducing new, unknown vulnerabilities. Or, on the other hand, they may be more willing to risk service down-time, in order to patch nearly discovered vulnerabilities quickly. And then there's business management, responsible for budgeting and buying resources. Ops may have to requisition additional resources from them and reason why they need those resources, even if the underlying problem might be performance of the new release. So how can you get all of these involve parties to understand each other's different perspective and work toward shared goals. After all, the server that's offline doesn't have any features and that's bad for everybody. Here's another company with problem. Wind and Shade Software makes a wildly popular, distributed, weather-forecasting app. They're getting ready to build version 3, a major upgrade from last year's version 2. But the whole company remembers the problems that plagued the 2.0 release. In the first meeting after the version two launch all of the different groups involves spent hours blaming each other for the problems. The product team was unhappy that they didn't get to test the apps efficiently before it was almost time to launch. So it went out with bugs and bad performance problems. Version two is substantially slower than version one. The ops team was unhappy because when they finally received the app to be deployed in production they found out that it used significantly different software than version one. Not all of the dependencies were well documented. There were also OS specific issues because the developers were using Windows while the production servers were running Linux. And the performance problems meant that they had to stay up all weekend deploying new servers to handle the load. It turned out the devs had never tested it with more than five simultaneous users. And the dev team had been struggling with deadlines. In order to make their own process faster, they were maintaining their own dev server instances without involvement from ops. But their setup ended up being so complicated that whenever they hired a new developer it was nearly impossible to give that person a consistent setup. Especially when they tried following ops suggestion of using Linux, which they didn't know that well. So, in the run up to version 3 they'd really like to avoid another painful debacle like the version 2 launch was. But just pointing fingers and laying blame just isn't going to help. So the teams get together and try to find some practical solutions they can agree on to fix these problems. Which of the following solutions do you think might help Wind and Shade's launch trouble? Should they use agile development practices? Should they run production servers using the dev's desktop OS? Should they involve the ops team earlier in development? Should they implement a suite of performance tests? Should they buy some faster server hardware to make the app faster? Should they make smaller incremental launches instead of big ones. Or should they require devs to only use technologies that have been vetted at the start of the project. So agile development practices are generally a good idea, provided that you involve all the stakeholders and not just the development team. Running the production servers using the devs' desktop OS, probably not such a great idea. Desktop OS and production are completely different environments. Involving the ops team earlier in development sounds like a pretty good idea. We can make sure that everybody knows what the expected technologies are going to be. Not have any nasty surprises when we go to deploy. Implementing performance tests, that sounds like a great idea. If the devs are regularly running performance tests as part of their development process, then we won't have any bad surprises about performance later on. Buying faster server hardware to make the app faster, well that might work or it might not. It may just be that the performance problems are algorithmic, and adding more server hardware might not help for very long. Making small incremental launches instead of big ones, this seems like a pretty good idea to me. Although, in some business environments that might not be a possibility. Still, it's something very good to consider. And requiring devs to only use technology as vetted at the start of the project. This sounds like a recipe for being very inflexible, and probably would get in the way of the developers making a good product. So let's not do that last one. So one of the problems was that Dev and Ops were not talking with each other early in the development process. Ending up with incompatible environments is a pretty bad result from that. Suppose that Dev knew what Ops knows. That the production system has to all work together as an integrated unit that can be tested and scaled. It can't be patched together from pieces the last minute before launch, and suppose that Ops knew what Dev knows. That the system under development needs to change rapidly. The technologies involved just aren't going to be the same forever. With that kind of shared knowledge, Dev and Ops could work together to come up with a common deployment environment that would work well both in development and in production. That would deploy well onto servers at scale, while also developers make all of the changes that they need to build new features. This is just one example of how technical change can be made in response to the social and organizational challenges of DevOps. There are different ways to implement this particular change, but first, let's take a look at the situation in which this problem arises. One of the approaches to unify the production and development environments is called a golden image. The idea is to take the operating system, libraries, and our application itself, and wrap them up in a standard or golden master image of a virtual machine that can be used and reused without the need to configure each new machine individually. These golden images can be used both on desktop workstations as a virtual machine and on servers. You can also create golden images of containers. Another approach is to just install a base operating system and then use a configuration management system that performs automatic installation and configuration of software on every machine under its control, thus making sure that all the machines have the same configuration. To be sure of that, the installation should start from the same base OS image otherwise there could be OS level differences. The difference is that a golden image system requires more work upfront, and has to be regenerated whenever there are any changes. But once it's done, it is very fast to boot up a new server or VM with it. With the configuration management system on the other hand, booting up will be slower because every machines needs to install the same software on the first boot. However, if there is a configuration change, it's easy and fast to get these changes out to the machines without the need to regenerate the image. And, of course, you can have something in between. For instance, install the base software and libraries, perform the main configuration, create a golden image and then use it to bring up the machines. Then once they're up, use a configuration management system to perform smaller rolling updates. If both the image creation and configuration management processes are automated and in a version control system, it's easy to reproduce these steps. And that's what you'll do as the first step in this project. Build a golden image that has all the software necessary for dove and also works in the cloud for production server. You won't work with a configuration management system in this project because that topic would require a whole course of its own. Maybe later. For now, you'll have to download and install a couple of programs. Press next to see the instructions. The previous page gave you the installation instructions for Virtual Box, Packer, Vagrant, and Git. Once you've installed those on your local machine, mark the boxes here, and continue on. Now that you have the prerequisites installed, let's talk about the project you'll be working with. For this course we've provided a web app that has a build script to compile it, and some tests to run. In order to run this app, your server will need to have a particular version of node.js, and additional packages to be able to build and run the tests. Without some automation to make this faster, it would take an unreasonable amount of time to install it on each development system and production server you might have, this is why we've decided to create a golden image, we'll use Packer to do that. Packer can create golden images for different build targets, for example, VirtualBox and VMware, for use on your workstation, and also for cloud providers, like Google Compute Engine, Amazon AWS, DigitalOcean, and Microsoft Azure. So you can run the exact same operating system, libraries, and everything else no matter where you might be deploying your app. Once this image is built, you'll be able to use Vagrant to bring up a virtual machine on your own computer from that image, share a folder with it, connect to it, see the app running, and then change the app if you wish, then build and run tests on it. First, let's talk more about Packer, how does it work? There are a few terms related to Packer that you need to know. A template is a JSON file that defines one or more builds by configuring the various components of Packer. Packer will read the template and use that information to create multiple machine images in parallel. Builders are components of Packer that are able to create a machine image for a single platform. Builders read in some configuration and use that to run and generate a machine image. A builder is invoked as part of a build in order to create the actual resulting image. Example builders include VirtualBox, VMware, Amazon EC2 and more can be created and added to Packer in the form of plug-ins. Builders take a source image that is different for each specific builder. So, this Virtual Box builder will have an ISO as a source. Where as an Amazon EC2 builder will have an AMI as a source. Provisioners are components of Packer that install and configure software within a running machine, prior to that machine being turned into a static image. They perform the major work of making the image contain useful software. Examples of provisioners include shell scripts, and also Chef or Puppet scripts, that install software. Post-processors are Packer components that take the result of a builder or another post-processor and process that to create a new artifact. Examples of post-processors are compressed to compress artifacts, upload/upvote artifacts, in this case, vagrant builder vagrant box. Artifacts are the result of a single build and are usually a set of ID's or files to represent a machine image. Every builder produces a single artifact. And a build is a single task that eventually produces an image. For example, the same Packer script could run builds for VMware, AWS, and VirtualBox. So Packer reads a template file, picks the appropriate builder for each platform that's listed in the template, and runs a build to produce an image. Then once the machine is up and running on that platform using that image, it runs the appropriate provisioners to install whatever other software configuration you need it to. That's then saved out as a modified image. And after that's done, it optionally runs one or more post-processors on it, and in the end saves the resulting artifact. So, given what you've seen here, how would you switch from Ubuntu to CentOS in Packer? Would you run Packer on a machine or VM that's already running CentOS? Would you change the source in the builder configuration? Would you change the source in the provisioner configuration? Would you change the target in the post-processor configuration? Or is the answer that you can't, Packer only supports Ubuntu? So, the correct answer here is that to switch from Ubuntu to Centos, in a given Packer configuration, you change the source in the builder. Now it's time for you to use that installed software and build the images. We've provided all the files in a Github repository. You should go ahead and clone it to your local workstation. The files for this exercise are in the packer templates directory. If you look in applicationsurfer.json, you'll see that here we're using shell provisioners. That's just for simplicity, so you don't have to learn a new syntax. Even if you're not familiar with shell scripts, don't worry. Take a look at some of them. Notice that they're just a file, and in that file you'll just see one command after another. It's also possible in Packer to use other provisioners, like Chef or Puppet or Ansible or Salt. These can provide more advanced functionality. You can see the provisioners listed here in applicationserver.json. And each one points to one or more files in the scripts directory. They'll run in the order they are listed. An interesting note is the provisioners are used for all builders unless specified otherwise. As you can see, there's a couple of scripts that are run only for the virtual box iso builder. Okay enough talk. Go ahead and clone the repo changed to the packer templates directory and follow the instructions in the readme or in the instructionettes. When you run the packer build, it will take a while, after all it first has to download the OS image, then install nvm, then install all the software, then at the end compress and save the image. Once you do that a look through the application-server.json file and see if you can answer these questions. First, what directory does the post-processor save the final artifact in? There's a variable for it. Enter that variable, including the curly braces around it. And second, what's the file extension for the final artifact? Put that here. So the answer is that the post-processor saves the final artifact in a directory named by the variable .Provider inside curly braces. And the extension for the final artifact is box, because it's a virtual box. Box. Okay, so now you have an image. Now what? This is where Vagrant comes in. The tagland for Vagrant is development environments made easy. And, that's what we want it to have, right. Vagrant allows you to create, run and connect to development environments in a very easy way. We've provided a configuration file for Vagrant or a Vagrant file. For convenience it's in the virtual box directory, where packers saved its artifact. You just have to register with Vagrant the artifact that you just created. You can do that with the Vagrant box add command. You just specify name parameter. Which I'm just having be devops-appserver. Now that it's successfully added, since there's a vagrant file in this directory. You can do vagrant up, and it will bring up the dev environment. It'll also mount shared folders, so you can easily use your favorite editor or IDE for for development. By default, only the host's current directory share to the target/vagrant directory. But, we have provided some additional configuration. And once the machine is up, you can type vagrant ssh to log into it. Now you have the development environment ready, and can get the application source. That's the next part of the instructions in the Read Me. We've provided a sample application that also serves as a presentation on the topic of Devops. So, since this is meant to be a development environment, it would make sense to have a fork of the application you can push to. So, if you have a GitHub account, you can fork the repo in GitHub and then clone it down onto the vagrant home directory. If you'd rather just get it running, you can clone the master repo, but you won't be able to push to it. Which really isn't much a development environment. Then cd into the app directory and do the last couple of steps in the readme. run sudo npm install, then run the tests using grunt -v. Then after the test pass, just open up your regular browser and go to localhost:8080. And when you do, what's the first word you see on the app's home page? Fill it in here. So once the app has fully loaded, big surprise, the first word you'll see is Devops. Great. Now you've seen how Packer can make it easier to be sure you're running the exact same software from development to production without worrying about changes sneaking in. And no matter where your development and production environments go, you can expect a standard compatible environment. Whether you're running on servers in your closet, or a hosting provider's data center, or on a cloud provider, it'll work the same. Up ahead in lesson three, we'll take a look at automating some of the grungy work taking freshly written code from a repository, testing it and building it out for deployment. See you then.