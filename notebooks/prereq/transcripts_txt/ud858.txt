Hi there. Hi, and welcome to our course on building scaleable applications in Python. My name is Magnus, and I work at Google, with cloud technologies. And I'm Karl, I'm a course developer at Udacity. Together we will teach you how to build great applications that run on Google infrastructure. And that can scale. And scale. And scale. And scale. And scale. Because at Google people really do know how to build scalable systems. That's right and after this course you will also be able to write scalable applications that run in the Google data centers. I will be teaching you the theory behind building scalable applications. And I'll be showing you how to implement them in Python. So with that why don't we get right into our first lesson. So we're sitting here with Urs HÃ¶lzle, Senior Vice President of Infrastructure here at Google. Urs, could you briefly describe your role here at Google and why Google has all of these data centers? I am responsible for technical infrastructure at Google and that's the team that builds all the data centers, network servers, software systems that runs our internal cloud and we built that a long time ago for our own needs. And we have very large scale applications and so we needed a very capable cloud. And now we're making that cloud available to everyone through App Engine, Compute Engine, and our Cloud Platform overall. Urs, so what excites you about Google Cloud Platform and App Engine in particular, which is the focus of this course? What's great about the Google Cloud Platform is, is really, one, that we can make our great internal infrastructure accessible to every developer out there. And then, in the Cloud Platform, App Engine is really the, that, that one system that makes it really easier for you to start very small. And then scale to a very large user base. So that if you have a game that suddenly becomes popular, our back-end and our front-end is going to scale with you. And you don't really have to change your application very much. And that's rare to see in infrastructure. Usually you have to do the hard work up front. Now, with App Engine, you kind of don't really have to do the hard work at all because we can do it for you. We're out here by the Computer History Museum in Mountain View, California. The Google headquarters is just around the corner. But before we start to build our applications on the Google cloud platform, let's look at the history of building server side applications. Fifty years ago this is where most server-side applications were born. On a physical computer sitting under the desk in some office. Yeah, seriously. A physical computer, you know, one that you can touch and smell. And the generated heat from the fan outlet, so you could warm your feet, if the office was cold. Well, it didn't take long until the server-side applications grew bigger. And it was no longer possible to run them on a computer like this. So the pizza box computer was created because it looked like a pizza box. And these were great because you could stack them on top of each other and on top of each other and on top of each other. And voila! The computer rack had just been born. And the entire shebang was put in a dedicated room called the data center, because all the fans made too much noise and stuff. But it was no longer possible to warm your feet against the fan outlet of course. Okay, let's continue our story on the history of server side computing. At this time we enter the race for scalability. Because once the computer rack was created, we could start to stack servers on top of each other to save space. And this was of course great. But now, another problem surfaced. Because even though these servers were great from a stacking perspective, it was still the case that one application could only run on one server. And this became a big problem, because as the internet grew, these applications got more and more popular. And more and more popular. And more and more popular! And since there is a limit to the performance of one server, this architecture was not sustainable and had to be abandoned. The one application, one server strategy was a dead end street and a radically new solution was required Let's do a quiz now and talk about increasing capacity. The question is what are the options to increase capacity? Is it, to buy more powerful hardware, to optimize your software, to copy and run the software on more servers or to change the software to run on more servers? Check all that apply. Let's see the answers. Buy more powerful hardware. Yes. That could certainly help, but it will not provide unlimited scalability. What about optimizing the software? Yes. That could certainly also help. But that, will not provide unlimited scalability either. Copy and run software on more servers. Well, normally, software cannot just be copied, and run on more servers. To be able to run on more servers, you typically have to change the software, so this option is also an alternative. In fact, if you can change your software to run on more servers, you are starting to work with the scalable architecture. This is because the capacity will not be limited by the hardware you use, or how much you can optimize your software. When you need to scale, you can add more servers. As we concluded, a radical change was needed a brave new world. A completely new approach to building applications needed to be developed that could scale to meet the demands of the rapidly growing internet. Not a key step to building a scalable system is you need to decouple it from executing on a single computer. You design a scale out architecture as your application scales outside the scope of a single computer and this distributed architecture needs to be a socalled, share nothing architecture. Meaning, that it must not depend on any shared resources that can become the bottleneck, this way if your application needs a scale, you can just provide more computers to provide the additional capacity required. That means, you can run your application on. All the things. And to get even more flexibility the concept of virtual machines was born which enables you to decouple a physical machine from the application execution machine. This had great flexibility gain since the infrastructure turned virtual you would now run your scale out application on several virtual machines instead of physical computers. Problem solved, right? You can now scale indefinitely by adding these standard building blocks of performance that your application can utilize and you have decomposed your application from the specific execution machine, great. Now only one question remains, how do you design your application to be able to scale like this? Let's take a closer look if we have solved the problem. First of all, virtual machines are nowadays a standard offering by any cloud service provider. You rent any number of virtual machines. And you pay by the hour, or minute or so. This is called IaaS, or Infrastructure as a Service. This is, because a computer and therefore also a virtual machine, can be considered infrastructure today. So getting back to our original picture. Problem solved, right? We could use standard virtual machines from any cloud service provider. And achieve unlimited scalability by distributing the execution of our application, across these virtual machines. On a high level, it may appear so. But let's look at what you need to do to actually realize this architecture. And as it turns out, creating this architecture for your application is a very complicated and resource intensive problem. Let's look at some of the things you need to think about and take care of. VM Management. Starting and stopping the virtual machine. Operating system images. Software management. Installing, patching and upgrading. A powerful web server. Installing it and configuring it for the correct settings. A distributed database where you can read and write data, from all of your virtual machines. A distributed file system, where you can read and write data which is accessible from all your virtual machines. Application monitoring, with a consolidated view of any problems occurring in your distributed execution environment. Availability including data replication, hardware software resilience, geographic disaster recovery strategies. Back ups, recovery, automatic fail overs, et cetera. Security set up and management in your distributed execution environment. And load balancing. How to spread the load across the VMs, including application level session management. And all of the other things that we did not have space to list here. As you can see, this is an extremely complex problem to solve in a scalable way. So even though our solution looked good on a high level. We have now created an architecture that requires a monumental engineering effort to realize. At the same time, as you have limited amount of time and resources. Remember that we started the service side history, by looking at the server sitting under the office desk. And sadly enough, for many projects we have reached a present day. These are the challenges many projects are facing today, when trying to build scalable systems. Now, let's do a reality check with a question for you. Since you have limited time and resources, select three of these problems you would prioritize over the others. Well that was not easy, was it? No it wasn't. And that was kind of a trick question, because all of these things are important to consider when building a scalable system. So let's be clear about this. Plain virtual machines are great in many ways. But if you have to do all of these things yourself, we could consider using them a dead end street. So let's look at a better solution. Because this should not be a problem that every single developer of server side applications need to think about. Fortunately the Google cloud platform has a product that takes care of most of these things. It's called App Engine, and that's what we will be using in this course. Let's take a closer look. App Engine is a development and deployment platform that takes care of many things for your application. For example, it performs all the scaling for you automatically. So when your application runs on App Engine, and there is more traffic it needs to manage. Then App Engine will increase the number of instances to use to take care of the load. This is called scaling up. And at other times, when your application is not so busy. App Engine will reduce the number of instances to save resources and cost. This is called scaling down. And this scaling is done automatically. You don't need to write a single line of application code to get this benefit. And it doesn't stop with automatic scaling. The App Engine platform has functionality to deal with all of the things we've previously looked at. So there is no need for you to do any design or coding to deal with that. And this is no coincidence. Google has been the leader in designing scalable systems and all that experience was built into App Engine when it was designed. As is everything runs on Google infrastructure. There are Google engineers watching over these systems day and night, to make sure everything is running smoothly. So that you, can focus on your application. In Cloud language, App Engine is classified as a PaaS solution. And this makes sense, because App Engine is a platform that takes care of a lot of the complexity of building scalable applications Now, when you have been introduced to some of the benefits of app engine, let's do a little quiz. For which of the following could app engine be used and benefit from its strengths? A news site like New York Times, a device driver testing system, a personal portfolio site, or a video game? Check all that apply. OK, so let's go through the solution. A news site like New York Times, absolutely. It's an internet service, it has lots of users and the load on the service probably varies so the scaling of app engine could be used a perfect match. What about a device driver testing system? Well, it of course depends on what kind of system this is but just the fact that device driver is part of the title, make you think that it needs access to low level resources within in a computer. Remember, app engines are generic platform to build scalable applications and from that perspective this does not sound like a match. What about a personal portfolio site? Just because it's a personal site and probably does not need massive scale, does not mean that app engine can not be of great use. App engine simplifies development of server-side applications greatly, so this is a good match. What about a video game? Well, if we just mean the user interface part then app engine is not a good match, app engine is used to develop server side applications. Okay. So by now, you've heard a lot of positive things about App Engine and how it can help you build scalable applications. But let's do a reality check here. Surely, there must also be trade offs involved when using App Engine. Let's start off with programming languages. Since App Engine provides services to control the execution environment, it also has to set up programming languages it supports. This list has been extended over time, but App Engine currently supports Python, Java, PHP, and Go. And since App Engine is an execution container, it also means that there are specific versions of these languages that are supported, so this is something to think about. Then moving over to low-level controls. Since App Engine was designed to create scalable application as easy as possible, it's execution environment also encapsulate many low-level controls. For example, operating system type and version, virtual machine configuration, or network controls, et cetera are not visible, and cannot be configured in App Engine. And that goes for other things, as well. App Engine strikes a great balance between providing the convenience you need to develop scalable applications without sacrificing the flexibility that you want. But with this in mind, don't be surprised later on when you see App Engine is not like a regular virtual machine, because it's not. All right. That's it for the reality check. Let's now look at some App Engine success stories. We will now look at three App Engine success stories that are representative, because they use the strengths of App Engine. The first one is a real-time earthquake monitoring system in japan. So in this picture, lots of data is collected by NIED, which is an institute in Japan. They look at the data and then send image files to App Engine, every second. Clients can then view these images files to see the activity. Well, as you can imagine, when there is no earthquake activity, there are not many clients that are interested in watching this system. But on the other hand, when there is earthquake activity, the system needs to serve a lot of data to many clients. And this is, of course, a great application for App Engine. Which can scale the number of instances up and down, depending on the activity and the number of clients accessing the system. Another great success story of App Engine is Song Pop, an app you may have used, which is a game where you guess the song or an artist. Well, Song Pop grew from zero to 10,000 queries per second in six months. And as of early 2013, it had more than 60 million users. Song Pop uses app engine, again, because of its ability to rapidly scale up and down based on the user load. And one that you've certainly heard about is Rovio, the creator of Angry Birds. When Rovio developed a web based version of Angry Birds, they used App Engine because of its automatic scaling. But also for many of the other functions it provides. And these things are exactly what you will learn when you take this course. So stay tuned. Okay. Let's involve you in a little survey here and ask you about sites that are running App Engine. Please feel free to search on the web. Please list the site that is running on App Engine, and also list the site you think should run on App Engine. And get involved, go and discuss and share your views with other people on the forum. So I'm setting here with the founding CTO of Udacity, Mike. Mike, how did you come to the decision Udacity? Because Udacity is running their courses own App Engine, in fact that's what this course is all about. How did you come to the decision of, of using App Engine? The very first website we built was built for AI classes, a single course. And it was built on a bunch of VPS servers, and we had underestimated the interest that we were going to get. Which was awesome, but we ended up with two orders of magnitude more users than we expected. And I was a relatively novice web developer. And so we hit every single problem you can hit, in terms of scaling up the infrastructure to meet the demand. And so the entire 14 weeks of the course we were just playing catch up the whole time. Fixing problems, and adding more bandwidth, and adding more machines and memory. And coming out of that, we were looking at launching a bunch more courses and we were really uncertain about how much traffic we'd be getting. We've seen a lot of peaks. And so one of the engineers, we're a very small team, but one of the engineers had some experience using App Engine. And, it sounded like a really great solution to the problem of uncertain amount of load, and took all of that headache. Because we were spending about half of our engineering hours doing sys admin and dev ops work, just trying to keep stuff running. And when you have a small team and you want to focus on getting new features out. And you want to focus on usually working on what your good at, then it is wonderful to have a platform that cushions you, for the spike and the loads. And pushes you in the right direction. That's fascinating, that's exactly hat we're trying to teach in this course. What would be your key messages to the students of this course, what would you tell them now? So I think the advice I'd give to the students is, it's important to remember there are trade-offs with any framework or platform you use. And one of th things, after using App Engine for a couple years, we found is. It seemed like there was a lot more work, at times, just to get started, to get something simple up and running. But, what we found with time is that it pushes you in the right direction. It pushes you to the best design practices, for building a scalable applications. Maybe, if you're trying to build something which will serve 200 people, you could do it. Quite a bit faster if you are just running it on a single machine. But the second you get up to 2,000, or 20,000, or 200,000 users, it's made you make decisions, which will really put you in a better place. And that is, that's it, I think the most absolutely valuable thing that it provides. That's great. Thank you so much, Mike for taking this time. I'm sure that the student will find this valuable. Thanks, and glad I could help. And I really hope that after taking this course, students have a little bit of an easier time than we did at the beginning. Are you excited yet? We hope you are. We think you should be. Just look at all of these pictures of Google data centers. All the Google services run on the infrastructure that you see. And with the Google cloud platform and app engine, this infrastructure is also available to you. That means that the code you create in this course will execute alongside all of the other Google services. And can leverage from the same scale security and performance that Google has built out over the years. We can't wait to get started and have you on your way to write and deploy your own code into these data centers on your journey to writing scalable applications in the Google Cloud. Hi, we hope you're now all excited to build scalable applications using App Engine and the Google Cloud platform. In this lesson, we'll get down to the nuts and bolts of everything, and have you deploy your first code to App Engine. This will involve an number of different steps, but don't worry, we'll all do it in a structured and orderly fashion to make sure you're on your way to become a rock star App Engine developer. So let's look at what we're going to do in this lesson. First, we're going to look at the App Engine development cycle and create an App Engine project where you can upload your code. Then we're going to look at cloud endpoints that allows you to create a backend that can work with any frontend device technology. After that, we'll get you started with a development environment where you develop code for App Engine. And then, we'll introduce Conference Central, the application that we will build out throughout this course. At the end of this lesson, you'll have a working application running in the Google Cloud. Isn't that exciting? Let's go, shall we? The first question you'll probably ask yourself at this point. How do I access App Engine in the first place? Well that is a pretty important question to answer. And we're happy to tell you that it's super easy. So first of all, here you are. A star developer. And you should already have a Google account, that you probably use for many different Google services. Such as Gmail, YouTube, Google Plus, et cetera. Well, guess what? Your standard Google account also allows you to create app engine applications that run on the Google cloud platform. So there is no additional account required. What you do is simply log on to the developers console that we will go through later. And register the App Engine applications you want to create. And once you've done that, the time has come for you to create those great cloud applications. So you need to sit down, and hack, and hack. And hack, and hack until you have something awesome that you want to test. And then you can fire up an instance of App Engine, which runs on your local development machine. And that is of course, great. Since it allows you to use all your standard tools. Such as debuggers and providers to get those bugs out. And once you've done that, it's time to run your applications on computers in the Google data centers. So, with just a click or two, you can deploy all of that great code to App Engine. And of course, you can have many applications running there. And when you and your front-end developer friend have finished your application. You tell all your users about how cool it is, and that they can start using it. And when you use App Engine, it's very easy to support any kind of device and operating system. Computers, smartphones, tablets. You name it. And hopefully, the users will think your application is so cool, that they will tell their friends about how cool it is. And hopefully they will tell their friends. And your application goes all viral. And you get all of these great ratings and recognitions. And awards and stuff. But you don't need to worry about your site going down because it has become so popular, since you built it on the Google cloud. It will scale and scale, and scale on the Google infrastructure. So you get to feel like a million bucks. And become a rock start developer that everyone wants to be with. This is exactly what we would like to happen. And that's really what this is all about. App Engine and Google cloud platform, allows you to create applications that run in Google data centers. And can be used by any device. So you can sit with your laptop and create apps that scale to serve billions of users worldwide. It's really, really cool when you think about it. Well, hang in there. Because you're just about to create your first App Engine project. Okay. So we now come to the part where we will create a new project for App Engine. You create new projects using the developer console, which can be accessed at this URL. This is a console that allows you to configure many different Google services. But in this course we will focus on the App Engine specific parts. Go there and create a new project. We will talk about installing the development environment a bit later on, so don't worry about that right now. While creating the project, you will be presented with the following dialog box. Pay special attention to the project ID. The project ID must be unique and it has certain naming restrictions. Having a project ID you're happy with, though, can be quite handy, since you can access your application through a URL containing it. After you've created your project, this is the URL that you can use to access it. So in this example, the final URL will become http://apt-octagon-533.appspot.com. See, that's the project ID was this. So it's now time for you to go and create your new project. So now that you registered a product, it's time for a quiz. Which of these project ID's are valid and are they available? First alternative is ud-859, hi_there, hi7 or cheesecake0123456789, please select which are valid and which are available. All right, so welcome back from the quiz. So these are the answers, first we have ud-859. Well the dash character can be used in project id's but it's not available. Then we have hi_there, well underscore cannot be used in project id's. So it's not a valid project id and therefore not available either. What about hi7? Well a project ID must be at least six characters, so it's not valid and it's not available. Then we have cheesecake 0123456789, well this is a valid project ID but it's not available Okay, so now we have registered an app engine project and we are almost ready to do some coding. But just before we do that, let's talk about supporting different devices for a while. One big challenge when designing service-side applications today is that there are so many different devices from so many different vendors that you need to support. Different browsers, smartphone, tablets and computer. All having different API's that should be used. Initially, we will be using the browser for our course project. But later we will be able to support a mobile phone without rewriting a single line of code. Having to consider this when you design your server-side application is a lot of work. So how can we solve this problem? Well, the Google Cloud platform has solved this problem for you with a product called Cloud Endpoints. When you use Cloud Endpoints you can create one single API for your server application called a Cloud Endpoint API. Cloud Endpoints can then generate a client side API for many different devices from many different vendors. This is great of course because it allows you to focus on just server side application and Cloud Endpoints will make sure that it supports many different devices. You will learn a lot more about Cloud Endpoints since we will use it when we build out the course project. Enough said, we have now created an app engine project using the developer's console and looked at the benefits of Cloud Endpoints. It's time for you to get a development environment up and running. Hi, welcome. I'll be guiding you through the coding lessons in this class. The first thing you'll do is deploy a simple app engine application that uses endpoints. You'll learn how to define endpoint functions. You'll learn how to call an endpoint function from a webpage using the JavaScript client library. You'll also learn how to use the APIs Explorer, which is a cool tool for exploring publicly available Google APIs, and which also lets you test out your own application's API. The main coding project that you'll be building in this course is an application called Conference Central. It let's users schedule conferences and also register to attend conferences. You'll be focusing on implementing the backend functions. Just like on a cooking show where some of the cakes are already baked ahead of time, we've written some of the code for you, so that you can get straight to work implementing the more interesting bits. You'll start out by downloading a skeleton version of the conference central application. Then, you can keep working on it, from lesson to lesson. But, at any point, if you don't complete the exercises in a lesson, don't worry. We provide new starting code for each lesson. Okay, let's go write some endpoint functions. Now, it's time to setup your Development Environment, which is to say, The Tools You'll Need for this course. There are four things you'll need to use Google App Engine with Python. First, you'll need a text editor. Anything with syntax highlighting will work just fine. You can use whatever you're comfortable with, but I'll use sublime text when showing you code examples. Second, you'll need a working installation of Python 2.7, Python 3 will not work. To check which version of Python you have, just run Python dash dash version or Python dash big V from the terminal. You'll need git. Version control in general is a must have for any project, but we're hosting the project files on GitHub, so it's pretty much the right tool for the job. And you'll also need the Google App Engine Software Development Kit or SDK. You can download the SDK from the URL in the Instructor Notes. For Windows and Mac, it will come with a nice GUI tool called App Engine Launcher. If you're running Linux, you'll use command line tools devappserver.py and appconfig.py. There's a link in the Instruction Notes with more details on those. Another option is the Google Cloud command line tool, G Cloud. This includes tools for App Engine as well as other Google Cloud services. However, G Cloud is a bit harder to setup for basic purposes. So, I'll be using the basic SDK and App Engine Launcher for the examples you'll see in this course. Now go ahead and make sure your system is setup for development. Check off the steps you've completed and continue when you're done. Now let's take a look around the code repository for this course. We have provided some examples and scaffolding for the main app you'll be working on. Follow the link in the Instruction Notes to get the course code repository on GitHub. >From there, you can choose either to clone the repository or just download it as a zip file. You can see that there are folders for each of the lessons. We'll provide you with the starting code for all of the exercises. All the exercises build on the previous ones, so you can start with the first one and continue working on your version until the end of the class. Or you can grab the starting code for a particular exercise and continue from there. Now your development environment is set up and you have the starting code. I'll walk you through a very simple Endpoints API project. You can find the code in the Lesson_2 folder named 000_Hello_Endpoints. Add that whole folder to your code editor. The app.yaml file is where the configuration of the app is stored. There are important configuration parameters that go here, like your project ID and the version number of your app. Project ID needed when you deploy your app and you can have several versions of your app running at the same time. Down here there's configuration for URL handlers that determine what should be done with a request. You can configure static file paths, and for Endpoints, what to do with when an Endpoint request is performed. As you can see, there's a secure always line for the handlers. That means the connection between the client and the server will always be over HTTPS. Even if the client initially tried to use HTTP. And at the end, you'll see the library's configuration. For this simple app, you just need to include the Endpoints library. So, back up in the handlers, you'll see there's an entry for slash that points to static/index.html. But what's much more interesting is this handler for Endpoints that points to helloworld_api.APPLICATION. And there's this file called helloworld_api.py, and this is where the application code lives and where the endpoints are defined. Let's first go through the process of running and testing a project locally, then deploying to App Engine. The app's very simple, and it runs out of the box. Let's first run it and see what it does, and then take a look at the code that makes it happen. First, open up App Engine Launcher. Add a new project and point to the directory for the hello endpoints app. As you can see it gets added and it will set a port for the web interface and different one for the admin interface. The default port for app engine is 80/80. If you add several projects to the launcher, the port for the next app you add to the launcher will be different. So when testing locally, pay attention to that port number. Click the green Run button. Then click on the Logs button to see the deployment process, and if there are any problems. Look at the logs. Once the app is deployed, you'll see several lines. One will tell you the URL and the port that the app can be accessed at. When you go to the URL for that starting module default, you should see this page. Hm, it looks really basic. Since this course is about the back end part of the app, we haven't put any CSS or any other front end work into it. So click on the buttons and see what happens. You know, my name's not actually Zebra, so I'm going to change that. That's much better. Okay, great. The local server works. But if you want other people to be able to access your app, you have to deploy it onto App Engine. Let's do that. It's actually almost as easy. You just press the deploy button and it'll want your Google login. The password is swordfish, by the way. Let's take a look in the logs and see how the deployment process goes. Hm, appcfg.py has finished with exit code one. Exit code one is usually not a good deal. Let's see up here. Begin server output. This application does not exist. App ID equals you, your project ID. Oh, it's complaining that it can't deploy to that. That's right. You have to actually enter your own app ID into the configuration first. So remember when you created an App Engine app, get that project ID from the developer's console. Don't use the one that you see in this video, it won't work for you. Get your own. Then, in the code, open up app.yaml, and replace your project ID in the application line with the one you got from the console. And save that. And when you go back to Google App Engine Launcher you'll see that the name has already changed. Now you can deploy the app and watch in the logs as it gets sent to app engine compiled and deployed. Eventually, you'll see, completed updated of app, and the URL for your app will be your app id.appspot.com. If you go to your app id.appspot.com, you should see the very same thing. So you now have the app running both locally and on App Engine. So now it's time to explore how it works. Let's take a look back at the code. Let's take a look at helloworldapi.py, and here we go, endpoints.api, that sounds familiar. Look at this, endpoints.method. Hm, one of these things, endpoints.api and endpoints.method, oh this uses Python decorators. Maybe we should talk about those first. Up ahead in this course, you'll be using a Python language feature that you might not have seen much of before. This feature is called the decorator syntax. If you're already comfortable with decorators, feel free to skip ahead. A decorator is way of modifying a function or class definition by wrapping it with another function. A decorator can modify how the decorated code works or even hook it up to other pieces of code entirely. For instance, to register a class as being part of an API or to connect a function up with a framework. Here's what a decorator call looks like. You will always appear on the line before a def or class block. That is before a function or a class definition and it will always start with an at sign. And some decorators have arguments of their own while others don't. The decorator syntax is a shortcut for calling a function that wraps the function or class that's being decorated. Right after the function somefunc is defined, Python will call the function special to wrap that function up with extra behavior. Hey, how about you try it out? Up ahead, you'll see a piece of code that you can add a decorator to, then you can tab over and take a look at the source code of the decorator itself. So if you add the timer.timedFunction decorator to the sumUp function, it's the same as if we had reassigned sumUp to be this wrapped up version. And when the code called the sumUp function afterwards, it returned the sum of the numbers from 0 to 10 million. But it also printed out how long it took to add up. That timing feature was added by this decorator. The details of how decorators work, are rooted in functional programming. I've put a couple of links in the instructor notes, that go into much more depth. And just like other functions, decorators can appear in Python modules. You'll be seeing an example right up ahead in this course. Okay. Cool. You now know about decorators, so let's take a look at how the endpoints are defined. The endpoints API supports a lot of arguments, but here we're just using name and version, which is the required arguments. As you can see, the API name and the class name don't have to be the same. The methods inside this class will become the methods of our endpoints API, each gets decorated with @endpoints.method. And the arguments here tell what type the method takes in their request and what it returns and some other things too. Now, every API method has a request class and a response class, even though some of them are void or empty. The request and the response classes end up being different subclasses of ProtoRPC protocol buffer types, which I'll tell you more about in a moment. For the response class, you just need to define a class that's a subclass of messages.message. It can contain as many fields as you want. In this simple example, it just contains one field called greeting. We also have to define the request class. This request is pretty simple. It also just contains one field, name, which is a string. But later, we'll see requests that are a lot more complex. The resource container class supports options that messages.message doesn't, such as query string parameters, which we'll use later. So we'll use that now just to have a good example. And again, each API method is a method inside the API class. That function returns a message object. Here we have a hello object and we decorate that method with the @endpoints.method decorator. The decorator tells the API system the request class. Here we have VoidMessage and then here we have REQUEST_CONTAINER and the response class, which is Hello. And some other information, like the http_method. And down at the bottom of the file, down here on line 46, we have the code that actually starts the server that supports this API. About these message classes, what's up with them? And why do these fields have the number one on them? That looks kind of weird, but there's a good reason for it. It has to do with how endpoints works. Endpoints APIs use Google's protocol buffers or protobuf, which is a way of representing data structures efficiently across many different programming languages. Let's take a look at that in some more detail, up ahead. Looking backup at the top of helloworld_api.py, you'll see these imports from the protorpc library. Now, protorpc is a tool kit for building APIs and it uses Google's protocol buffers as a message format. Protocol buffers or proto buff to its friends is an efficient data format that can be used through lots and lots of programming languages, including Python and JavaScript. Now, unlike Python itself, protobuf is statically typed. Every message and every field has a declared class. That means that you'll see a lot of type information in APIs and messages that use protobuf. And a lot of the syntax that you'll see in endpoints APIs has to do with protobuf. For instance, in protobuf, every field in the message has a number. These messages has only one field, so that field is number one. But messages with several fields will have their numbered one, two, three and so on. The classes that fields can have are listed in the ProtoRPC field classes documentation, which is linked in the Instructor's Notes. Also, every field has a name. That's what we see here in the Hello class. The name of the field is greeting. It's type is messages.Stringfield and one is its number. Protobuf also has a concept of nested messages. A field inside one type can be, instead of a basic type like String or Boolean, it can be another message type. Since we're going to be seeing a lot of these protobuf things, let's do a quiz. Here are some Python variables. And over here is the outline for protobuf message class. Now take a look on the messages module documentation in the instructor notes for a list of the field classes, then fill in the appropriate class definition for each field in the message. So, course is a string field, and it has field number 1. This could also be a bytes field, since this string does not have any high Unicode characters in it. But it's a good idea to support Unicode though, since things like people's names aren't guaranteed to be in plain ASCII. Like student, for instance. So that's definitely going to be a StringField. And we'll give it the number 2. They started variable is true, so that's a Boolean. We'll call that messages.BooleanField with field number 3. Current_lesson is an int, so that goes in an IntegerField, number 4. And finally, fraction_completed is a float, so that goes in a FloatField. And that'll be a number 5. How do you know that there's anything happening in the back-end and that's it's actually using an Endpoints API? After all, we could do all of these things just directly from JavaScript with no back-end involved at all. Well, Google gives us a really cool tool that lets us test out an Endpoints API and access the Endpoints API behind our app. It's called API Explorer. You can access it by adding /_ah/apis/explorer to your URL. This even works if you're on localhost. And that will redirect you to this APIs Explorer page, which I will now make a little bit larger so you can see it. And APIs Explorer page is a very nice way to test if your endpoints work correctly. You can look in it at endpoint, and you can see the different API methods, helloworldendpoints.sayHello and sayHelloByName that that API exposes. And if you look at one of these, you'll see the fields, in this case, just name, that that API supports. Now, if I plug my name into that field, press the Execute button, down underneath, it'll show a request and a response. The request here is this GET HTTP request accessing localhost port 8080, sayHelloByName with my parameter. And then down here at the bottom is the response with a 200 OK and an object containing one field called greeting that says, Hello Karl. So again, these methods here correspond directly to methods that are exposed by our API. Now, it's important to understand that the API Explorer page and the home page are completely independent. There are two different paths that access the very same API in the back-end. You could delete that Hello frontend with the buttons entirely. And the API Explorer frontend would still be able to execute the endpoint functions. In fact, that same API can be accessed not only through those frontends, but also potentially through other frontends like a mobile app, or in fact, somebody else's code entirely as long as you granted them permission. So now let's go back to the hello world front-end. As you can see, the two methods are exposed by two different buttons, and the response is shown as a JavaScript alert. Let's take a look at how the front-end communicates with Endpoints to achieve that. >From the config file, app.yaml, you can see that when the root URL is accessed, the request is routed to a static_file, static/index.html. Let's take a look at that file inside the static directory. As you can see, it's a pretty short piece of HTML. It has two buttons and one input field. What's more interesting are these JavaScript includes, although it's a static file hello.js and it also loads this Google API's JavaScript client library. And once that's loaded, it calls an init function, which is defined in hello.js. Let's take a look at hello.js in this init function specifically. So here's the init function and it initializes the GAPI client library with the name of your endpoint, helloworldendpoints. And it is a callback, which in turn initializes the buttons. The API calls loadCallback, which calls enableButtons, which sets up the greetGenerically and greetByName methods attached to the button onclick. Which means that when the first button is clicked, it calls greetGenerically, and when the second button is clicked, it calls greetByName. And greetGenerically in turn uses the GAPI client library we saw earlier, in fact, the helloworldendpoints API to call sayHello. But on success, the request gets passed back to sayHelloCallback. GreetByName does much the same thing, also passing the response to sayHelloCallback. And on that callback, the page displays an alert with the value that was returned by the Endpoints API. So as you can see, it's really easy to access an Endpoints API using the GAPI JavaScript library. But you don't need to use it because you can access the endpoints for development and testing entirely through the API Explorer. As a matter of fact, for the rest of this course, you'll be working with just the back-end code and we'll provide the front-end code for you. So before we go on, let's make sure that your app runs and deploys as expected. If you've run your app locally and deployed it onto appspot, mark these check boxes and put the project ID of your app down here. Cool. You'll be using the very same work flow to deploy the back end apps for the rest of this course. Now let's make the Hello App a little more interesting by adding a new endpoint with some different inputs. Let's make it possible for your app to say good morning or good evening or even good breakfast time if you want. Add one more endpoints function to your application called greetByPeriod that takes a period input as well as a name input. The period input could be morning or afternoon or evening or, or breakfast time. And this function will return a hello object, whose message is, Good Breakfast Time Marcus Aurelius, if that's what you happened to pass to it. Note that you do not have to write the front end code for this. Just to find the endpoints method. But of course, you're welcome to add this functionality to the web front end as well, if you wish. So when you're done, check off these items on the checklist, and then fill in your project ID There are two things to do. Your end points method need to accept two arguments, period and name. Both of them are strings, so you have to create a new resource container with two fields. And then you can define the method itself. The request will be handled by the same request container you just wrote. The output will be the same hello class. And the path will be greet by period as we said. So you've seen that the APIs Explorer lets you try out the API that your application exposes through endpoints. Google uses the APIs Explorer in the same way to let you investigate the APIs that are publicly available for the services that it provides. For example, let's consider Google's translation service. You can go to tranlate.google.com to translate some text from one language to another. However, you could also write applications that do translations using the Translate API. And you can try out the Translate API using the APIs Explorer. It's easy to find the APIs Explorer for Google's public APIs. Just search for it. When you go to the APIs Explorer, you'll see a list of all of the APIs that Google provides for developers to use. Search or scroll down to the Translate API and click on that. You'll see the functions of the Translate API exposes through endpoints. These are functions that you, as a developer, can use in your own application. Now a task for you. Go to the APIs Explorer for the translation service to find out the meaning in English of the name Gundega, the name of the instructional designer for this course. Using the Translate API, find the language.translations.list function. Use a target language of en and then try to find out what Gundega means in English. Then from the returned message, find out what the detected source language was. So, the response from the translations API, tells us the answers to both of these questions. This message has two fields, translated test and detected source language. Detected source language is "lv", which is the language code for Latvian. And translated text tells us that 's name is Latvian for buttercup. Alright. Let's have something more concrete to work on. If you've organized events, you've probably used sites like [UNKNOWN] or eventbrite.com. Well, let's say you wanted to create an app to organize conferences. That type of application would need to support at least the following. A user can register, a user can create conferences. Users can attend conferences. And of course, it needs to be accessible both from the browser and the mobile device. Well, we're going to create such an application. And it's going to be called Conference Central. And you're tasked to build it. And since it needs to support multiple backends, you will use Cloud Endpoints to create this application. Let's get started with Conference Central, by having you download the skeleton code. Now, let's take a quick tour of the completed Conference Central application that you will build. But before we do that, let's do a quick demo of the finished application. As you can see, you're presented with a very nice landing page for Conference Central. And if we scroll down, you can see options to view conferences. Create conferences and update your profile. But, before that, let's sign into the application. Sign in, we'll use Google Plus sign in. Once we are signed into the application, we can start to explore the functionality. I can, for example, go to my profile where my personal information is stored. I can also show the registered conferences. So here you can see the name of the conference. The city where it will be in. The start date, organizer, how many people are registered to conference. And how many tickets are available to the conference. You can also query for different conferences. For example, the ones you've created. Or the ones that you will attend. You can also query by other types of properties. And you can view the details for a conference. You can also of course, create new conferences. Well that's an overview of Conference Central. The application that you will build and run in the Google Cloud. Now it's time for you to do some work and get you started building Conference Central. Good luck. Now it's time to start working on the Conference Central application. Hopefully you've already cloned or downloaded the code from our repository but if not just follow the link in the instructor notes and do so now. We provide starting code for each lesson. You can start with the code for this lesson and keep working on it throughout the course, or, at any point, if you need, you can download the starting code for the lesson that you're in. You might want to do that, for example, if you don't finish all the coding exercises for a previous lesson. As you can see the project layout is similar to the hello endpoints project. There are some more static files for the front end however and the backend is split up into several files. Settings.py for the project specific configuration to web client idea. Models.py for the ndb model, and protorpc message definitions. Conference.py for the main application logic, and app.yaml provides the application settings, same as in the hello endpoints app. At the top of the conference.py file, you can see several imports from models and settings. Let's take a look at this. First, let's take a look at models.py. The profile class extends ndb.Model and defines how the user profile will look like. The next classes, extend ProtoRPC messages and deal with sending data to and from the endpoints app. ProfileForm mirrors the fields of the profile class. ProfileMiniForm has only the two fields that are editable by users and TeeShirtSize holds enum values for tee shirt sizes, because we don't want users to be able to enter free form values in that field. Let's go back to conference.py. Underneath all of these imports is the definition of the endpoint.api. After that we've provided a couple helper functions for creating and returning the profile object and then we have the endpoint.methods. Let's take a look first at the getProfile function. It's defined as an endpoints.method that supports GET requests with no arguments. So, its request type is message_type set VoidMessage. It's response type is ProfileForm and it calls the private method self._doProfile with no arguments. What doProfile does if called with no arguments is first, get the profile by calling self._getProfileFromUser and then returning self._copyProfileToForm. We first want to develop and pass the functionality of sending data to and from endpoints and this is what is implemented here. We'll deal with indication and saving data in data store later and for testing purposes, get profile from user, return some dummy data. So getProfileFromUser creates an empty profile object and returns it. And copyProfileToForm, then goes over the message fields in ProfileForm and copies matching fields from the provided profile object to ProfileForm message fields. You have to use getattr here, you can't just get the values from using the fields by name. Also, note how the enum field is handled slightly differently. Now, if we look at the getProfile method in API's explorer, we'll see that it works with the void request. And it actually returns a response containing two fields, the ones that are defined to have some sort of value. DisplayName, value Test and teeShirtSize with a value of Not_Specified. But look down here into this safe profile method. This is supposed to save a profile that the user has submitted and return the updated profile. But right now all it does is well, very same thing as get profile. Now what this should do is take display name and t-shirt size from a post request and update the output accordingly. It's not going to save anything yet, but it should return the same thing that it's given. And that's your job in the first programming exercise for this conference central app. You should modify the save profile function to use the appropriate messages.message class from models.py, and pick the one that can send two fields, displayName and teeShirtSize. Then for the method to be able to return the changed values, you'll have to pass that request up to the doProfile function. Complete the steps outlined in TODO 1 and deploy the app locally and test by changing display name and t-shirt size in the request. You should see the changed values in the response. Once you're done, check off the completed steps and enter your appspot project ID. So step one is to change the accepted request class. Instead of void message with no fields, we want to take ProfileMiniForm. Step two is to pass the request up to the doProfile function. Now let's save this and run it and check it out in the APIs Explorer. Here's the conference API. If we say save profile, put a display name in. Let's say, maybe my name and a teeShirtSize. Let's say men's medium, because that's the size that I wear. Hit Execute and we see the request going in with these fields and this time, the response comes back with the fields returned to us. It works. Congratulations, you just did a lot of stuff. Let's do a recap. You've wrote your first lines of code and deployed it to App Engine. You got Conference Central up and running. And you've tried out the API Explorer. Don't underestimate the API Explorer. It's a great tool to use to try out the APIs. And it's not limited to your own APIs, you can use it for any Google API. So, all good stuff. But let's now talk about user authentication. Many applications require that a user is registered and signed in before using the application. We will also require this for Conference Central, so let's look at how we do that. It used to be the case that almost every application had to create their own user management system. That would mean that you would have to write all the code from scratch in order for your users to use your applications. Well, good for us. That is not needed anymore. With App Engine, you can use third party authentication with Cloud Endpoints, such as Google Plus sign-in. But also other ones can be used. In Conference Central, we will require a Google account for sign-in. So, how do we know that the user is signed into their Google account when they use Conference Central? That is actually taking care of by Cloud Endpoints. A Cloud Endpoint's API method can optionally take a user object as its first argument and if the user object is not null then the user is signed in, but if it's null then we throw in exception which decline should receive and redirect for sign in. Could it be more easy? Hm, you might say now. But what about security and privacy? Well, that is all taken care of by Google Sign in or a third party authentication provider. So this mechanism is the best of two worlds combined. A simple way to add user authentication to your app, combined with the strong security and privacy standards of Google, all right, that's enough talking, time for you do some coding again by adding user authentication to your Conference Central. Good luck. You now have an understanding of how endpoints simplifies dealing with authentication. There are a couple things you have to do to make sure authentication works, not only in your development environment on local host, but also on F-spot. To do both of these, you'll go to your developers console at console.developers.google.com/project and select your project name. The first thing you'll need to do is to configure your App's user consent screen.a Google won't tell your app anything about the user without the user's consent. So, the user will have to accept this consent screen before the app can work. This screen will display your app's name and give the user a chance to know what's going on. So, go to APIs & auth, and then Consent screen. Select an email address that you want to associate with the app. And, put in a product name. And, that will be displayed over on the user consent screen when the user logs in. Second, you need to make sure that your app's front end will be allowed to access the backend. To do this, under API and auth, go to Credentials. In the Auth section say, Create New Client ID. Then, pick Web application, and under Authorized JavaScript Origins, change the URL field to contain two URL's. One for the https URL of your appspot app, and one for your local host, including the correct port your server runs. Bearing in mind, if it's the second app you have running on local host, because you still have the hello app running, it'll be 9080 instead of 8080. Now, copy this client ID value. Yes, this big long noisy string that ends in googleusercontent.com. And open up settings.py. And, put it here under Web_Client_ID. In the next exercise, you'll get some practice adding functionality to the Conference Central app. Previously, we used dummy data in the get profile from user function. Instead, you'll now be able to get the data from the logged in user, and create a profile for them. Your app is now in the state that the saveProfile method knows that when it is called, it should be passed data that conforms to the fields defined ProfileMiniForm class. Which in practical terms means that the input data can include display name and teeShirtSize. If the input data includes any other fields, they will be ignored. Since we also want to store the user's email address, we'll do that by getting the email address of the logged in user. This of course means that the user must be logged in. You can check that by calling endpoints.get_current_user. If that does not return to user object, the user is not logged in. The endpoints system lets your code handle this case by raising an exception, which the front end can handle by displaying an error or redirecting to the log in screen. So you don't have to change the saveProfile method to get this behavior. But instead work with the getProfileFromUser method where we used dummy data before. Check out the TODO 2 comments. Step 1, make sure the user is authenticated. If they're not, then we'll raise the endpoints.UnauthorizedException. Step 2, use the real data from the logged in user instead of the dummy data for displayName and mainEmail. Now there's one more thing to change if you'd like to use the app from the JavaScript based front end, as well as from the APIs Explorer. Take the same WEB_CLIENT_ID that you used in settings.py and put it in the JavaScript file app.js, in place of this web-client-id here. When you're done, deploy your code on localhost and test it, then deploy to appspot. Check off each one of these boxes as you finish that step and then fill in your project ID down here. So the first step was made really easy. All you need to do is uncomment these lines here. For the second step, I had to change the user profile creation code so that user.nickname is used for the display name and user.email is used for the main email value and that's it. If you set up the web client ID correctly and configured the consent screen correctly, you should now be able to log in to your app on appspot. If instead of getting a consent screen here, you're getting an error message or forbidden error, check to make sure you've entered the URL correctly and that the email and project name are set in the consent screen. You can also check the instructor notes for some help. Congratulations, we're at the end of Lesson 2. In this lesson, we looked at the App Engine development cycle and how to create an App Engine project. We also looked at Cloud Endpoints that allows you to build a single backend that can work with many different frontend device technologies. Together with Cloud Endpoints, we explore the API explorer, that allows you to test your backend and also other Google services. And finally, we got conference central up and running in your App Engine project, which is great, you've got a real application running in the Google Cloud now. In Lesson 3 we have a look at storage. It'll be fun. Join us. So now when you're up and running with conference central you have a pretty good idea of how to write and deploy app engine applications. In the last lesson we also covered usage management and we added it to conference central so only authenticated users can access the application. When you write applications you very often need to store data. And as you know, there are a number of ways you can do this. App Engine is no exception. It can work with a number of different storage technologies. But there is one that is particularly important, it's called Datastore. [BLANK_AUDIO] This is the Big Table restaurant here at Google. You may have heard of Big Table. It's a technology Google created and published a famous paper about in 2006. Big Table allows unlimited amount of data to be stored and searched effectively. And many of the huge services being run here at Google chose Big Table to store data. You know what? App Engine Datastore also uses Big Table. That means that your applications ge the same massive scale. Isn't that great? Hm, but I'm starting to feel a little bit hungry. Hey! I think I'll go in, and get a big bite in the Big Table restaurant! See you in class! [NOISE] Datastore is a database that runs in the Google Cloud, and it's available to any App Engine application. As you already heard, it is built on big table, which is used by many of the huge services being run at Google. So, that also gives datastore unlimited scalability. Please see the instructor comments for more information on big table. Datastore is a no sequel database and to be more specific, it is a key-value, column-oriented store. It is key-value because the way you get and put the information, is very similar to that of hash tables or associative arrays in programming languages. And it's column-oriented which means that columns, not rows, are stored together, which means better performance and scalability for certain operations. This also means that Datastore is not like a traditional relational database system using SQL. The design of Datastore allows you to create applications operating at Google scale. And by that, we mean that your application can scale indefinitely, using distribution in the same way as many of the very big Google services do. This means that your data is not only stored on one computer or file system, but on multiple ones. Even in separate data centers, these type of distribution increases performance as you can retrieve data from these locations in parallel. Your application does not need to do anything extra to get all of this value. This is one of the great benefits using Datastore. You don't have to think about it, because Datastore automatically manages the scalability. However, this also means that you need to design your data model to take advantage of this. And when the data is stored in multiple locations, you also replicate the data. This means, you can always rely on that data being available. Because even if there is a failure somewhere, for example, a disk fails or lightning strikes and destroys a data center. That data is always available from another location. So with the design of data storage, you get both scalability and availability without having to do anything. It's quite amazing. But now it's your turn. Please tell us a little bit about what storage technologies you are familiar with. Select the ones you have used when creating applications. Have you used local file system, shared file system, such as nas or san? Cloud file storage, such as Google Drive, Dropbox, etc. Relational databases, such as Oracle, MS SQL Server, MySQL. NoSQL databases, such as Couch DB, Redis, MongoDB. Or even the Google App Engine Datastore. Or if you use something else, please specify it here. So if I was to answer this quiz, I would have to cross local filesystem, shared filesystem, cloud file storage, relational databases, NoSQL databases, and the Google App Engine datastore. Wow, is that good or bad? So now it's time to cover the building blocks to use when doing datestore modelling. They are kind, entity, and property. Let's start with kind shall we? A kind is a name that defines a particular structure. For our conference central application, two different kinds are, for example, profile and conference. A kind is similar to the concept of classes in object oriented programming. Given a kind you can create many entities of that kind, so in this picture we can do three entities, one for the profile kind and two for the conference kind. And as you can see entities are very similar to the concept of objects in object oriented programming, and each entity holds a set of properties which are highlighted by the red sections in this picture, and by now you can see that properties are very similar to the concept of fields, members, or attributes in object oriented programming. In fact, you can also find similarities with the relational of database modeling. Let's look at the table that summarizes these things. So the kind in app engine datastore can be thought of as a class in object oriented programming and as a table in relational database model, and an entity in app engine datastore can be thought of as an object in object oriented programming or as a row in relational database modeling. And finally, a property in Datastore can be thought of as a field or attribute in object oriented programming, and that's a colomn, in relational database modeling. But, this is on a very high level. Since there are many things that differ in Datastore compared to the other technologies, you shouldn't consider these as direct technology. It could help you out in the beginning, but as we dig deeper into how data store works, you will start to see more and more differences All right, let's do a quiz. Imagine that we are modeling sports league in data store. Which of the following would be best represented at a kind, entity or property? A team, a game, the winning team, a player, a player name. A Team is probably best modeled as a Kind. It's a concrete structure that can have many entities. A Game is also best modeled as a Kind, for the same reasons as for Team. The Winning Team is probably best modeled as an Entity. But, this answer is not so straightforward. It could, for example, be a Property Value in an Entity. A Player is probably best modeled as a Kind for the same reasons as Team and Game. And, the Player Name sounds like it's modeled through a Property Value. I hope you did well on the quiz. All right, that's an introduction to DataStore. Now, it's about time to check out some code, don't you think? Let's do that. Good luck. In this lesson you'll learn how to define entity classes. We'll use an entity class called Profile to save user profiles. You'll learn how to get a user ID for the logged in user so you can use it as the unique ID for the entity's key. We'll explore the entities in the data store by using the data store viewer in the Admin Console. And it's not much good putting data into the data store if we can't get it back out. So you'll learn how to get the profile entities back out of the datastore. So let's get into that. So now let's take a look at how to define and create entities in the App Engine Datastore. To continue the food theme that Magnus started at the Big Table restaurant, let's imagine that you want to store recipes in the datastore. Maybe you like chocolate cake or maybe you prefer rhubarb pie, but whichever kind of dessert you like, your recipe's going to need some properties. What are some things that we'd want to store about a recipe in any database. Well obviously it needs to have a name and a list of ingredients,and some instructions. And it could also have properties about whether or not it's vegetarian, and how many servings it makes. All of these are parts of a recipe, so that would make them fields in a recipe record. Now our data store also needs to know which type each of these values has. And earlier Magnus told you that fields like these are called properties and app engines. Let's add that. You might have noticed that these are an awful lot like the field classes that we saw in pretoRPC messages in lesson two. Like those, these are a way of saying what types the various fields have. But data store entities aren't the same as pretoRPC messages, so these property types have their own names. So if name, and ingredients, and instructions, and so forth are properties, what is the recipe then? Let's go back and revisit what you learned from Magnus with a short quiz. What sort of a thing is recipe? Is a data store entity, or maybe a kind, or is it an object oriented class, or a property? Select any of these that might apply. Well, a recipe is not an entity but that's close. An entity would be an individual incident. Something like a chocolate cake recipe or a rhubarb pie recipe. Those entities are examples or incidences of a recipe. But recipe itself isn't an entity. Just as the ent type isn't itself an integer. So yeah that means that recipes match the definition of a kind. It answers the questions of what kinds of objects or entities are we going to make here. And how do we represent kinds in code? Well, kinds are just theoretical concepts from the app engine data store but classes are the blueprints that we use in our code to make instances from. So class is also a correct answer. Finally we just talked about properties above in our name, ingredients, instructions, and so forth. Those are properties so recipe is not a property. Okay. So now that we know that recipe is a class, let's see what that looks like in the code. Let's start by saying that it is a class. And since we want to store it in the datastore, it's going to be a subclass of the datastore model. That means that I'm going to take a generic data model and define something specific called Recipe to store in the App Engine datastore. Now let's take a look at the properties. Just as we saw for fields and message classes, we're going to have to construct the properties, so let's put some parens in here. Notice that unlike ProtoRPC fields, we don't have to put a number here. But we do have to put for ingredients, because there are multiple ingredients in a recipe is that we'll say that it's a repeated field. Repeated field is basically like an array of values of a particular type. Here, ingredients is going to be like an array of strings. This is looking more and more like real working Python. There's just a couple of things left. First off, where do all these property classes come from? They're provided by the App Engine datastore library, which is called google.appengine.ext.ndb. And since all of these come from ndb, we're going to have to be ndb dot on each one of these. And hey, that's it. What we have here in real Python isn't very much different from the outline that we started with, but now it's fully functional. So that's our intro to using the datastore from Python. I think you've got it now. Let's go back to Magnus to learn some more about properties. As we saw in the previous code example, properties can have different types. These are called property types. For the recipe kind, some of the property names and types were name, which was a string. Vegetarian, a boolean indicator. And the number of servings, an integer. But these are just a few property types. In fact, Datastore supports a lot more property types. And we mean, a lot. The official documentation on developers.google.com lists around 20 of them. In this class, we're only going to use a small subset of these. And the ones you can see listed here are some of the most common properties. Integers. Floating-point numbers. Strings. Dates and binary data. Okay, time for you to do some work. That's right, it's quiz time again. In the instructors column you have a link, go to this link and explore the supported property types. Then answer the following question, which are property types? String, geographical point, name, rating and zip code. Good luck. Hey again. Now that you're well familiar with kinds and properties, let's talk about what goes into a data store kind for user profiles. Let's do in the same way we did recipes. What do we need to store for users of our conference app? Unlike a recipe, where we have one type of user, our cook who is storing recipes. This time we have two different kinds of users, those who create and manage conferences and those who are looking for conferences, registering for them and going to them. But we need to save information for both of those types of users in one kind and we're going to name that kind Profile. So the first thing we need is a unique identifier for the entities key. Now we need to decide what data we need in a user profile. First, we'll have displayName. That's the name that users want to have printed on their conference badges, that might be different from the name on their account after all. Maybe Jennifer Smith wants to be called Jen, or Jenny or maybe Unicorn. Next, we need the user's email address. We're going to call this mainEmail for now. At some point, you might want to extend this with home and work emails and need to decide, which is primary or support another email address for their mobile device to send them text messages. But for now, we'll just have mainEmail. And the next very important thing is your teeShirtSize. People certainly do appreciate having the right sized t-shirt when they go to a conference. Now, it's time for you to think about these. What sorts of properties are each of these fields? Use your imagination and if you need to look up the various property types you can use, check out the docks linked to the Instructor's Notes. For each field, fill in an appropriate property type. Be careful when filling in the answers, because like other Python identifiers, these are case sensitive. Okay. For displayName, well, that's a String Property. [INAUDIBLE] There, right? Names are strings, and datastore supports Unicode strings just fine. MainEmail, well, that's another String Property. Later on, you might want to subclass String Property to make sure that we're only storing valid email addresses there. But for now, String Property is the correct type for this, and teeShirtSize. At this point, this may not be any kind of surprise. This will also be a String Property. Even though this is an enum in the API, to the datastore, it's just a string. So, not too much variety here. Everything ended up being a String Property. But we've made some progress defining our app, and that's awesome. The code for this is going to go into the file called models.py. The name indicates just where we're going to put our data models. You already saw this in the previous lesson when I introduced the profile class. Now, back to Magnus for some more about datastore keys. Welcome back. I hope that exercise went great. Now, let's talk about another important Datastore concept, keys. When you store an entity in the Datastore, the Datastore will assign a key to it. This key uniquely identifies the entity and it's used for many different purposes. They are two ways a key can be generated. Let's take a look. The first one is when Datastore automatically generates this key. Let's say we define the profile kind with these properties. And inserted it for the first time. In this case, the Datastore will automatically assign a key value to it. The other way is that you specify what is called a key name or ID. In this case, Datastore will use that value to generate the key. This is a good alternative when you have something that is unique for each entity. Because it can clearly take us to our front entity based on this value so it's easier to find. But observe that statement. The Keyname/ID must be unique for all entities of that kind. For our profile kind that is exactly the case because we have decided to use the user ID to to uniquely identify each profile identity. The value of having user ID as the key name is that we can retrieve the profile entity directly using this value. And remember, since the user object is the first parameter to cloud endpoints API methods. It will become a convenient way to get the profile, but we'll look more into this later. If you are familiar with relational database modeling, you probably see that this is very similar to the concept of a primary key. There, as well as with Datastore, you have the option of letting the database generate the primary key or use the value of your choice. But now, it's for you to do some work All right so the question is, which of the following properties is a good keyname/ID? Select all that apply, bank account number, date, price, or an email address. Let's look at the answer. A bank account number, that sounds pretty unique. So that could be a good candidate to have as a keyname/ID. A date, is probably not a good keyname/ID, because it's most likely not unique across the entities. But it could be. It all depends on the modeling, of course. The price, does not sound like a good keyname/ID either for the same reasons I stated. An email address. Yeah, that could be a good keyname/ID. Because it's very likely, it's unique across entities. All right. I hope you did well. So here we are back in models.py in our applications source code. So far, you've created the profile entity class in your application, but you haven't actually used if for saving data in the datastore. The next step is to add code to do just that. So you've already worked with the getProfileFromUser function in conference.py. So far, it creates and returns a profile object, but it doesn't actually save the profile in datastore. Let's add the code to do that. Before you can save the profile entity, you need a unique key to identify the user. There are a bunch of choices we could use for this. For now, we've provided a function in the utils module, utils.getUserId, which takes a logged in user record and returns a unique ID. Utils is provided in the additions folder, since it's a new addition to the code base. And then once you have the user ID, you can use nbd.Key to make a key. The first argument is the kind of key you want to make, in this case, profile, and the second is the string you want to make the key from. And then you'll be able to save the profile entity. Doing that is actually super easy since you've all ready created it. You just have to use the put method on the entity you want to save. So in this case, it will be profile.put and that's TODO 2 for this exercise. I've shown you these two, but there are some other steps you have to do around them which are described in the TODO steps. Once we deploy and use the save profile method in the API Explorer, the profile will actually be saved. One thing you'll have to do in the API Explorer is to turn on OAuth for this method. This will authorize this method to access your account. Once you've executed this method, you should be able to see the results in the datastore, under Storage > Cloud Datastore > Query. And there I am. So now mark these steps as you complete them doing the TODOs including the several steps of TODO 1 and number 2. Testing on localhost and finding the profile in the datastore, which properties do you see in the admin tool for your profile entity? User_id, mainEmail, displayName or teeShirtSize. So, when we saved a new profile and look in the data store, we see fields for Display Name, and Tee Shirt Size, and Main Email. We don't see any User ID field. That's because we used the User ID to create the key for the entity, but it's not stored as a separate property. [SOUND]. Wow. Your code just inserted a piece of data into Google's data center. Isn't that exciting? Well keep moving along. [SOUND] Letâ€™s turn to key conflicts. Remember we talked about two different ways to assign a key. The first where datastore automatically assigns a key value for you. And the second was you can provide a key name ID that generates the key. And when you provide a keyname ID, you're responsible for ensuring that they are unique for all entities of that kind. This is important, because otherwise you will generate a key conflict. That's right, we do not want to cause a datastore key conflict. That's for sure. Let's see what they are and how we can avoid them. Let's go back to recipes for a moment. Say, for example, that we have the recipe kind that we worked with before. And we want to insert a vegetarian lasagna entity and a meat lasagna entity into the datastore. Well, if I let datastore generate the keys then it will make sure to create two unique keys. One for the vegetarian lasagna entity. And one for the meat lasagna entity. So two unique keys. But let's say that I instead want to generate a key from a keyname ID that I specify. Then it is very important that I can guarantee that those are unique. In this example, it is very bad to choose lasagna as a keyname ID if both vegetarian lasagna and meat lasagna would have lasagna as the key name. The reason is that when you insert the first entity, it will get it's key based on the key name ID lasagna. But when you later insert the meat lasagna, it will override the vegetarian lasagna, because it also has the key name/ID lasagna. So again, remember this, when you are explicitly setting a key name ID. You have to ensure that they will be unique and in our application, it is safe to use user ID, because the uniqueness of it is ensured by the user authentication system. As it turns out, this is actually quite convenient since it makes it easy to retrieve the profile from the data store. That's exactly the code we will add in the coming exercise. Have fun. So right now, the saveProfile method only returns a profile without really saving it. And the getProfile from user function will change all of the fields back to the defaults every time. That means that if we use it from the front-end every time we'd have to fill in the correct values to replace the defaults. Instead, we probably want to get the existing entity from the datastore and maybe just update one field not all. So the getProfileFromUser function should actually get a saved profile from the datastore, instead of always creating a new one from the default values. Let's see how to do that. Getting an entity from the datastore, if you know the entity's key is extremely easy. You just perform get on the key like this. So now, it's your turn to do this in your app. Since you're already creating the profile key, all you need to do is get the entity from the datastore instead of defining that it's not. And then in doProfile, if you get a save_request, we also want to put the modified profile back to the datastore. And of course, don't forget to deploy your app to local host and test. So, as you complete each one of these check them off and then fill in this missing code to fetch a profile from the datastore. And, here's the solution. You need to retrieve a profile entity, so the class here is Profile. And then, you use the user key to get the entity. Over here in our code, probably see something very much like this, and like that. So now, your app's profile, saving, getting, and updating functionality, should be complete, as well. Good job. So far you've used the API's Explorer to test the functions in the conference's API. However the conference central application also includes a full working web UI. Each of the pages in the web UI sends calls into the backend API. As you implement the backend functionality the web pages in the app will start to work. So far the only page that will work is the My Profile page. This page calls get profile to get the values of the logged in user's profile and calls save profile when you change values and update the profile. Now, the goal of this course is to help you learn how to develop app engine applications not to teach you JavaScript so I'm not going to go into detail on the code for the web UI. However, I do want to point out the code that makes the call to get profile. This is located in static slash js slash controllers.js, and it's the init code for the profile page. You see here, there's a call to gapi.client.conference.getProfile and then a little bit further down, here we have a saveProfile. The Conference Central application includes both a backend application that defines endpoints functions and a frontend application that uses those functions. We'll call the application that defines the endpoints functions a backend API. In this case, the frontend application and the backend API are combined in a single application but they don't have to be. You could have the front-end code in a completely separate application. Or you could even have multiple applications that provide different front-end interfaces to the conference backend API. In a way, app engine doesn't care that the front-end and the backend API in Conference Central are the same application. You still have to authorize the front-end application to use the backend API, even though they are the same application and share the same application ID. So there are a few things you need to do before you can start using the web pages in the Conference Central application. First, we need to tell the backend API which clients apps are allowed to use it. You do that by specifying the client id's, we need to clear the endpoints.api. So we've already granted access to the API_EXPLORER_CLIENT_ID, but now we're going to need to set up this WEB_CLIENT_ ID as well. It is a bit odd granting access to ourself in a way, but as far as the endpoint's API is concerned it doesn't know the web UI is in the same app. So in settings.py here's where the WEB_CLIENT_ID is set up. And, how did I get this big long alphanumerical value? Well, it's just like what we did for the Hello app back in lesson two. We do it in the dev console. So, our first step, go to APIs & auth, and Consent screen, and make sure the email address and product name are set in the Consent screen, and hit Save. Second, under APIs & auth, select Credentials. And under OAuth, select Create new Client ID. You are creating a web application, and then under AUTHORIZED JAVASCRIPT ORIGINS, fill in your app id .appspot.com, and also one for your local host including the correct port like localhost:8080. Once you do it it will show up over here, then copy this client ID and that's what you will put into settings.py. Another thing you'll need to do is add the client ID to the JavaScript code that runs the authentication process on the client's site. So far it hasn't mattered that the web UI didn't have the correct client ID set because we've only used the API's Explorer. But if you've tried logging in using the webpages, you've likely noticed it didn't quite work. So this code you'll find in static js, app.js. And right here under oauth2Provider is a CLIENT_ID and you'll paste that same value in right here. After you've set those things up and saved and deployed, you should find that you're now able to log in to the Conference Central web app and that My Profile will now allow you to both see and save your profile. Now it's your turn to set up the permissions so that you can use the My Profile page in your Conference Central application. After you get everything hooked up, you'll find that more and more of the Web pages start to work as you add more functionality in the coming exercises. So mark off each one of these steps as you complete them. And once you deploy and test the web UI on appspot, then let's go on from there. Okay, so you set up your consent screen. You created a Web App ID and credentials. You added the correct JavaScript origins for the app spot and your local host. You added the client ID to the endpoints.api definition. You added the client ID to app.js. And if you have deployed, then now when you load up your app, you should be able to do a sign in, get a log in screen, go to My Profile, and have your saved profile there. Now let's quickly check your understanding of client IDs. Suppose you wrote an application called Chocolate Chooser, that implements a chocolate API and points API. The app ID for Chocolate Chooser is cho-cho-413, and the client ID is 612 abcd. Now suppose somebody else comes along and writes an application called Meta Menu, that uses JavaScript to access the chocolate API. Their App ID is meta-m-1025 and their Client ID is 2317efgh. Now, you'd like to allow them to access the API, so down here fill in the code that will go into the Chocolate Chooser application to authorize the Meta Menu application to use the Chocolate API. So the answer is that in order to enable the new application to use our backend's API, we're going to put both our and their client IDs into the allowed_client_ids in the endpoints.api decorator at the beginning of our API declaration. Congratulations. We're now at the end of lesson three. In this lesson, we got introduced to Data Store, and kinds, entities, properties, and keys. We also looked at the profile kind and saved it to the data store and retrieved it from the data store. In the next lesson, we will cover data store in a lot more detail. Let's go. Hi, and welcome to lesson four. Conference central is starting to look like an interesting application. In addition to user management that we introduced in lesson two, we also added data store in lesson three. Now it's time to look at data relationships and how to retrieve data and make no mistake about it. There's a lot of moving parts with respect to this. So you better keep your eye on the ball. [SOUND] In addition to data relationships and queries, we will be looking at datastore consistency rules and transactions in this lesson. Now is probably a good time to seriously start to checkout the documentation on the internet as well to become a master of building scalable applications. Let's get started, shall we? Let's start off by looking at two important data relationships when you do modeling in data store. They are ancestor relationships and has-a relationships. Starting off with the ancestor relationship in data store it is possible to specify that an entity has an ancestor entity. There are two important rules for ancestor relationships. First of all, it needs to be assigned when the entity is created. And secondly it can never be changed so those are things to think about when you model an ancestor relationship. And given those rules, please note that the ancestor relationship is not the same as the is-a relationship in object oriented modeling. In conference central, we will use the ancestor relationship for the conference kind that you will create shortly. Where the ancestor will be the user who created the conference. Technically speaking though since the users are identified through their profile entities, so in this case the conferences will have the key of the profile entity as their ancestor key. The other relationship we will be using is the has-a relationship. We will be using this relationship to model conferences which users are attending. To do this we will add a property to the profile entity which specifies the attended conferences. So each profile will have a property that contains the conferences that they are attending, observe that this relationship can easily change. We can deregister from conferences and register to new ones, so it is not bounded by the strict rules of the ancestor relationship. So those are the basics of these relationships and that's also enough theory for now. Time to do some coding. In this lesson we're going to create and save Conference entities. These will represent the conferences that users of our app will create and attend. Then we'll learn how to run queries and use filters to query for conferences. Let's say you've got a trip to San Francisco in June. You might want to check to see if there are any conferences going on in June while you're here. I'll show you how to write the code to run queries like that. You've already learned that App Engine optimizes query results by using indexes. I'll help you explore how to create indexes, and how to structure your queries to make the best use of indexes. Constructing queries and app engines have some differences in querying over a relational database. We'll explore how some of those differences affect the way you write queries. Another key feature our application needs is to let users register for a conference. The registration process involves several steps. If you enter your credit card number to buy a ticket, you want to make sure you get the ticket before the payment goes through. So we'll use the registration process to show how to use transactions to make sure all the different steps happen together. Let's go create some conferences. As Magnus said, you'll now be adding the functionality to create conferences to your application. We use a conference entity class to represent conferences. Each conference entity will be created with an ancestor, or parent, which is the profile entity of the user who created it. Let's take a look at the code for the conference entity class definition. As you can see, the class itself does not have a parent property. Why do we want conference entities to have their creator's profile as a parent? Because this makes it very easy to quickly find all the conferences that a particular user has created. Using NDB, the way you create an entity with a parent is that you use parent equals and then the parent entity key when creating the key for the new entity. First you have to get the profile key, same as you did when working with the profile creation and updating methods. Then you'll create the conference key the same way, but also specifying the parent. Now, when you created the profile entity, you specified that the user_id should be used as the ID for the entity key. For conference entities we still need a unique ID. We don't have anything that we can use for this, unlike the profile. However, the ndb.model superclass gives us the ability to create unique ID's for any entity class. The method for doing that is called allocate_ids, and it returns a list of unique ids. So if we just want one we'll have to access item zero of that list. We can then define a key based on that unique id and the parent key. Okay, so you now have the conference key, but when a user creates a conference they're going to specify a bunch of information that we want to save in the data store. And some of that user input will need to be processed a bit first. So let's take a look at the conference class. Every conference has a name, which is a required field, a description, the user ID of the organizer, some set of topics, which is a repeated field because a conference could be on several topics. It's going to be located in the city, it's going to have a start date and an end date. For searching purposes, it's also going to have a month. Are there any conferences in June? There's going to be a number of max attendees. How many people could possibly fit in this conference? And the number of current seats available, which will change as people register for the conference. Let's also take a look down at the conference form class. This class defines the input parameters to the create conference function, just like profile form defined the input parameters for the function save profile. But the fields between conference forum and conference are not exactly one to one the same. So the code that handles conference requests will have to translate between them. And since we're using the conference key as part of the URL for our conference, this code will also have to translate the key into a web safe text form, that doesn't have any funny characters in it. Now it's your turn to add the functionality to create conferences to your application. Since there's a lot to do, we've provided you templates for the new functionality. To get ready, copy the code from ToDo_1_conference.py into the API class in your conference.py file, and from ToDo_1_models.py into your models.py file. You'll need to copy the conference form and conference classes to models.py, and the copy conference_2 form and create conference objects functions and the create conference endpoint definition into conference.py. Once you've copied the contents of the to do files into your project take a look at the Conference and ConferenceForm classes. Specifically, does ConferenceForm have properties that map to the following Conference properties? Name, topics, parent and month. Check off the ones that it does. So name, yes, both the Conference and ConferenceForm have a name field. The user will specify the name of the conference in the ConferenceForm. And it'll be stored in the conference model. Likewise topics, that's something the user will also specify when they agree to the conference. So parent, no. Parent is the key of the profile of the user who created the conference and it's calculated in a logged in user. It's not present in the classes. Run your app on local host and then use the API's explorer to create a conference. And then go to the Admin console to check that the conference was created as expected. You can also use the create conference page in the app to create and save a conference. The conference will be created because the conference page sends a call to the Save Conference function which you've now implemented. But the Show Conferences page will not yet work because you have not yet implemented the function that it calls, the functions to query an output conference records. So no matter whether you create your conference in the API's explorer or in the Create Conference page, you'll need to use the data store viewer and the Admin console to check that the conference got created. Okay. So now we turn to a big topic when you use Datastore, queries. And we will start by giving you an overview of the types of queries that we will run. Let's do that by looking at Conference Central, where we will build out the show conferences function. In the first tab here, we can see all the conferences. This query is called query by kind, because we will search for all entities that are of the conference kind. In the second tab, we can see all the conferences created by the user. This query is called query by kind, filter by ancestor, because we will search for all conferences that have the user as their ancestor. Remember, this is the ancestor relationship we established between the conference, and the profile that created the conference. And finally, in the third tab we will see all conferences the user has registered to attend. This query is called query by kind filter by property, because we will display all conferences that are in the attended conference property of the profile entity. Remember, this is to has a relationship we created between a profile and the conferences they registered to attend. And one more thing. In this part, you can further restrict the search result by applying filters. And as you can see from the drop down menu, these are properties in the conference kind. So therefore, these will also be a query by kind, filtered by property. Okay. So these are the query types we will look at now. Query by kind, query by kind filtered by ancestor, query by kind filtered by property. The query by kind filtered by property, is a little bit advanced stuff. The other ones are pretty straight forward though, so you'll start off by adding them to Conference Central. Next, you'll learn about querying conferences and it's helpful to have several conferences to query. So, I'm going to create a few and you can do the same. So the only required field for a conference is Name, but we'll generally specify other fields, so that we have more interesting things to query. So far, we haven't an implement that any functionality to true conferences. We can use the data store admin interface to check that our conferences got created. Next, will be implementing the query conferences function to get conferences. You already know how to get an entity out of the datastore by key, but for conferences, we don't know the key, because we let app engine allocate the key for us. So we'll be querying for conferences. So, one thing you can do is you can query by kind. In which case, you'll get all entities of that kind. Note that this q here, this variable is actually a query object and not the result of a query. To fetch a single result out of a query object, you call q.get. To fetch all the results, you call q.fetch. And to fetch say, only five results, you called q.fetch 5. You can also use a query object as a standard iterator in Python. Another thing you can do is put an order on the query instance by naming a particular field. You can also add filters before you fetch the results. So for instance, if we wanted to fetch only conferences occurring in February, we could do it just like that. So let's first, implement this simple way to display all conferences and worry about the filters later when you get this first step done. First, you will need to add a new end point definition for query conferences. And as you hopefully remember from the getProfile on setProfile endpoints, you'll need ProtoRPC message classes for the request and the response. There are some big differences, though. With get and set profile, you're always working with just one entity, one user's profile. But when you're dealing with conferences, you can have several filters and a query can return a list of conferences. So you already have an outgoing ConferenceForm ProtoRPC message class for single conference. But since you need to send several conferences, we'll also need a ConferenceForms message with a repeated field to contain them. The same applies to query filters. A single filter will go into ConferenceQueryForm and several in ConferenceQueryForms for response. So, I've provided some code to help you implement this. Take a look in lesson slash four Additions in the files marked TODO_2. After you've successfully implemented the query conference's endpoint, you should both be able to use it in the APS explorer and in the web front-end as well as long as you don't use filters. So check off each of these steps as you complete them and then put in your app ID here. Okay, so here we go. We're looping over all the conferences returned from a conference query with no filters, putting them in ConferenceForms, and returning that. So this works fine. And now we can go ahead and add some more queries, like showing conferences the user created. Let's do that next. Now you'll learn how to run queries to find conferences that were created by a specific user. Remember that each conference has an ancestor and that ancestor is the profile entity of the user who created it. So here's how you create a query to get all entities that have a particular ancestor. You just add an ancestor argument to the query with the key being the profile of whichever user it is you're looking for. This kind of query is called an ancestor query. But you can also think of it as a descendant query because it lets you get all the descendents of a particular entity or all the descendants that match a particular kind. So for conferences, this will get all of the conference objects that were created by a particular user. Now you already dealt with the response classes for conferences, so all you'll need to do is define get conferences created in the conference API. It doesn't need to accept anything from the request so you can use void message there. But since this query, query's for conferences created by the logged in user, you need to make sure the user is logged in, and if not, throw the endpoints unauthorized exception. That's your task for this exercise, define the endpoint, test it, then deploy and create some conferences. The sample code is in lesson four editions to do three conference.qy. Once you're done adding getConferencesCreated to the conference API, test your code on localhost, deploy it to appspot, create several more conferences, and try querying them, and put your app ID here. So compared to the query conferences method, the get conferences created method, just does a couple more things. It gets the current user, makes sure they're logged in, and then gets all of the conferences with that ancestor. Then it does the very same thing of copying those conferences into the conference forms object, and returning that. So once this works fine, it's time to implement filters. Let's do that next. All right, I hope that went well. We will now talk about the most flexible query mechanism in Datastore. Query by kind, filter by property. Let's say you would like to query for all conferences that are in a particular city, for example Halloween, and with a specific topic like movie making, and then sort the matching entries by conference name. For example, if we were to write this out it could look like this. Retrieve all conferences that are in the city Halloween, and has moving making in the topic. Sort the result by name. This is an example of query by kind, filter by property, since we are using property values in the search and the sort. Let's look at this a little bit closer. We want to have an exact match for the city since we are only interested in conferences in Halloween. This is called an equality filter. Then, we want to further restrict the queries so that only entities which has movie making in the topic are displayed. This is called a member of filter. We are combining these with the and operator, meaning that both of these conditions must be true. And finally, we are sorting the result by name. Let's look at filters and how you can use them to construct a query in more detail. The first filter is the equality filter, that we used for city in our example. Then, we have the member of filter, which has the same symbol as the equality filter. This filter, we use to sort by topic. Then we have a set of inequality filters, not equal to, less than, less than or equal to, greater than, greater than or equal to. And all of these can be combined using the Boolean operators and, and or. That's enough theory to get started. So now it's time for you to add query by kind, filter by property, to conference central. Magnus has talked to you about querying by kind, and by property. In the previous exercise, you practiced querying for all entities of a kind. The next step is to write the code to filter the search results by property values. For example, you could find all of the conferences whose city property has the value London. If we did that in code, it would look like this. Or if you want to be more flexible with filters, you can build a filter like this. You can set a field, an operator, and a value. Build a FilterNode object, using that field, operator and values. And attach it to the query. To practice filtering queries by property, you're going to add a function to the conference API class. You can call this function whatever you like, maybe something like, filterPlayground. The name doesn't matter because the function isn't called by the UI. You're only going to use it within the APIs Explorer. You'll be using this method to experiment and explore setting filters. You can think of it as a playground method for exploring queries. FilterPlayground doesn't take any arguments. So the request type will be void message. And as for the response, well we're returning multiple conferences, so the response type will be conference forms. So, define this function to create a query to get all conferences where the city property has the value London. Run the app on local host, and use the APIs Explorer to test it. Look for the new function name in the list of functions available in the conference service. You won't be able to test your new function in the conference center UI because nothing in the UI calls your function. You can also find sample code in Lesson_4 > Additions > TODO_4_conference.py, including some detailed instructions on how to use it. Now once you get this to work for conferences whose city is London, add another filter on the conferences.topics field, for the string Medical Innovations. Check each one of these boxes as you progress through this exercise. So here is one possible solution. We have the filterPlayground method, which is exposed to the API with the request type of VoidMessage, response type of ConferenceForms. Starts out by building an empty query, then filters the query by city equals London, then by Conference.topics equals Medical Innovations. Let's just sort it by conference name, because that makes it a little bit neater. And then return, the ConferenceForms object, which is a copy of each conference returned by the query. All right. So we covered a lot of material now on queries. Let's do a quick recap. First, we used a query by kind to get all the conferences. Then we combined the query by kind with an ancestor filter. And finally, we applied property filters. Remember, property filters gives you a lot of flexibilities when doing queries. Now, the time has come to look under the hood of Vapor Store, and see how these queries are actually run. The time has come to look at indexes. All data store queries are performed using indexes, and knowing how they work is important when you design and build your applications. If you have worked with a relational database management system, and SQL. You should be aware that those indexes are not the same as data store indexes. Indexes in RDBMS are a query optimization used to retrieve your result faster. While indexes in Datastore are, in fact, required to perform a query on that data at all. So you if you want to query for Datastore data, there has to be an index for that data. Be sure to remember this. So let's look at Datastore indexes by going back to conferences. And look at what happens when we perform a search on the property city. In fact, let's even talk about what happens before the query. When storing entities, Datastore also updates index tables, which are later used to find entities when the query is made. These index tables consist of an index value, that maps to a matching entity key. Taking the city Berlin as an example, the index value would be constructed using the kind, conference. Then the property, city, and finally the value of the property, Berlin. Here are the index values for the cities Halloween, London, and Paris. So again, the index value is a combination of the kind, property, and value of the property. Each index value then points to the entity key where the value resides. So a matching entity where city is Berlin has the following key. And one with London has the following key, and so on. So let's say your favorite city is, Paris. And you would like to get all conferences that are in Paris. Then Datastore would look at the index table where city is Paris, and find all matching entities. Since data store require index tables to perform queries, every property that you query for will need an index table. By default, Datastore creates these automatically. But we will also see how you can control this later on. So now when we have introduced indexes, let's talk about the size of these index tables, since this is important to be aware of when you build datastore applications. Using the developer console, you can see the index sizes under the datastore dashboard page. Here, we display the dashboard for a very simple application. And as you can see, the entities themselves do take up 6.75 kilobytes. However, the built-in indexes for those entities, they take up 57.12 kilobytes. This is actually a common scenario. Indexes very often take up more space than the storage required just to store the entities. The built-in indexes are the indexes that are generated for each of the individual properties, as we talked about for the conference city property. There's also something called composite indexes, but we'll talk about those shortly. As you can see, if you let data store maintain an index for each individual property, it can take up a lot of space. Therefore, it's important to think about which properties you need to query on. So you only generate indexes for those. Let's look at how you can control that in your code. In NDB, most properties are indexed by default. If your entities have a lot of properties, indexes can take up a lot more space than the entities themselves. Unindexed properties also cost fewer write ops than indexed properties. If you want to disable indexing for specific properties, to save some space in Datastore, you can explicitly disable indexing when you declare a property. To do this, when you declare the property, you'll just add indexed=False. And, take a look at the documentation for NDB Property Types, which is linked in the Instructor Notes. And, find out which of the following types are not indexed by default So if we look through the documentation, we'll see that StringProperty is indexed by default. TextProperty is not. DateTimeProperty is. BlobProperty is not, and JsonProperty is not. The only tricky one here is DateTimeProperty, where it doesn't explicitly say in the docs if it's indexed or not. But since the default behavior is indexed for all, we can assume it's indexed. You can also look at your datastore and see that you have Date/Time values with an index. Okay, that's a good introduction to indexes, but there is more and in the section we will cover composite indexes. Remember the single property index we looked at, that had the city as to value. A single property index is also called a built-in index. Data store can in many cases combine results from different built in indexes to provide a response to a query, but there are cases where this is not possible. That's when we need a composite index. In fact we have already used composite indexes in conference central for this query. Retrieve all conferences, filter by city and topic. And sort by name. This is a composite index, because the built in indexes for city, topic, and name, cannot be combined to generate the result. For this to work, an index table consisting of multiple values and combination must be created. This is exactly what a composite index is. So how are composite indexes built? There are two different ways. The first way is to add an index to an index file. [UNKNOWN] a little bit differently depending on which language you use, so you will see how to do this in one of the exercises. The other way is to actually run your application locally on your development server. In this case, if App Engine encounters a query that requires the composite index, it will automatically update the index file. When you later deploy App Engine, this index file will be uploaded as well. So it will also exist when executing on App Engine. All right, it's time for an exercise where you will explore composite indexes It's time for an exercise. This time we'll explore indexes in your app, and specifically, we'll look at composite index creation. We're going to deploy a query that needs a composite index without first running it on the development server. Then we'll fix it by running it on the development server and look at the index file and deploy and see that it works then. So, first, let's see what happens if you deploy an application to app spot that requires a new index, but you don't have it. So we're going to add a filter to our app, asking only for conferences in the month of June. Number six is June. Once you add this filter, save your file and go ahead and deploy it directly to app spot. In this case, do not run it on local host first. So don't hit the run button just go straight to deploy. And once the logs say that your application has been deployed go the API's explorer, make sure that you're on the deployed app and not local host. Go to the filter playground method and execute it. What happens? So if you followed each of these steps, then the result should be that you get an error. Now let's take a look at what that error means, and see what we should do about it. So here's this error that we've received from running our filterPlayground query. We've gotten this 503 Internal Server Error. That's not very informative. We go over to the Developer's Consule, though, and look under monitoring and logs, we can see a request to ConferenceApi.filterPlayground returning an error. And if we take a look into that error, we'll see it says here, raised NeedIndexError (no matching index found. The suggested index for this query is, and then it has kind: Conference, properties: city, month, topics and name. So normally when test the app on localHost, indexes get autogenerated, and they land in a file called index.yaml which shows up in your source directory. As you can see, there's a previous index from a simpler version of this query here. But you can also specifically define composite indexes for all queries that you will run. So knowing what you now know about indexes, which of the following statements are true? How can you make sure that you will have all of the indexes that your queries will need? Could you manually edit the index configuration? Run every multi-property query on localhost before you deploy to appspot? Mark all properties you need indexed with index equals True. Or for single property indexes, you don't need to do anything. So how can you make sure that you will have all of the indexes that your queries will need? Well, one way would be you could manually edit index.yaml for each index that you're going to need. Another way is that you can run every multi-property query on localhost before you deploy to appspot. That would also work. You'd have to make sure you ran every single one at possibly every path through your application. In that case the auto-generator would write index.yaml which would get pushed when you deploy your application. Marking all your properties you need indexed with index equals True, that's not going to work. That will not generate any multi-property indexes. But for single-property indexes, you don't need to do anything. So far, we have described how data store uses indexes to execute queries. Now is the time to look at some query restrictions that exist. As you know by now, data store was designed to be massively scalable. Therefore the performance of queries should only depend on the size of the result set, in other words the amount of data returned in the query and not on the overall amount of data stored in data storage. To meet this objective, data store always uses the indexes to find the matching data. And this leads to two restrictions you should be aware of when you design your applications. The first one is an inequality filter can be applied to at most one property within the query. The following filter is not valid because we are applying an inequality filter for two different properties. The second rule is that a property with an equality filter must be sorted first. The following query is for example not valid. Since the property max attendees is used with an inequality filter, it must be the first property after Sort By, not name. The best way to avoid these restrictions is to think and plan which queries you need during the design phase. That way, you can design around them. So now we'll do an exercise where you will explore these query restrictions. So if you're used to more flexible query languages, like SQL, these limitations may come as a big surprise. To understand them a bit more, let's change up our query here in the query playground method. Let's say that instead of querying for June, let's filter for big conferences. Let's say conferences whose maxAttendees is greater than 10. And save this and then try this one out on localhost, just to prove that it's not an index related problem. Once this is up and running, we'll go to the API's explorer. Filter playground on local host, execute, and we get an error. Once again, the error message we see here is not very informative. But if you take a look at the logs, you'll see something else. Based on the logs, what's the cause of this error? Is it that a composite index doesn't exist? Is it that the sort order is named but the inequality filter is on maxAttendees? Or is it too many inequality filters in one query? So to answer that question let's take a look at our log files. I have here a bad request error: the first sort property must be the same as the property in which the inequality filter is applied. In your query, the first sort property is name, but the inequality filter is on max attendees. So that means that this is the correct answer. So you probably noticed in the Conference Central code that as we build up a query with filters, we've been re-assigning the variable q like this. But there's a somewhat neater way to do this. Every one of these filter and order methods returns an updated query object. That means we can just use method chaining, like this. We can compose a complex query all on one line of code, or a few lines of code with backslashes as continuation characters. So how would you write a query to filter for conferences in Tokyo with at least one, but less than ten seats available, sorted by the number of seats available, then by the conference name, and then by the month? Fill in the query here in Python. All right. So, the first filter that we want to apply is: the conference is taking place in Tokyo, which is to say, conference.city, double equals, string Tokyo. Then, if we want one to nine seats available, that means we want greater than zero and less than ten. You could also use greater than or equal to one, or less than or equal to nine, if you want, but this is pretty straightforward. Then we have three order by conditions. First we want to order by seats available, then by name, and finally by month. And there we go, all on one line, or seven, depending on how you count. So, what are we doing with these filters, anyway? Well, in the app, we want to allow users to search for conferences according to any of these criteria. That's why we're storing an index in them, after all. As usual, the front end for this is already implemented. Next up, we'll be adding the back end logic. Currently, if we add a filter over here, all that happens is the query conferences method gets called, and the filter is just ignored. It just returns all the conferences. But if you look up here in this message, you'll see the filter data that the front end is sending to the back end API. We see filters, fields, city, operator, eq for equals, value, London, just like this, and field max attendees operator gt for greater than value ten. But it isn't actually applying these filters. That's because currently, our query conferences method takes a conference query forms message, which contains all these filters. Right now it doesn't do anything with it. The next chunk of code we'll be seeing will interpret this filter data structure, and turn it into actual data store filters, and you'll be modifying query conferences to use them. So next you'll need to bring in two utility functions. formatFilters and getQuery from todo5conference.py. Lets take a look at those. The formatFilters function takes the user separated values and creates a filter in a format that app engine understands. It also makes sure that users are not trying to run impossible queries, by using an inequality filter on two fields. And the getQuery function uses the formatted filter to form a query, including using proper ordering, when necessary. So to make your app actually take user filters into account, you'll also need to change the query conferences method in conferences.py. To use the getQuery method to get the query, and then return the result as before. Now, go ahead and make the changes necessary to enable this functionality in your app. After you've done that, check off these boxes, then make a couple of conferences in your app. Make one of them named Udacity. Okay, so not a very big change there. But if you deploy this, wait for it to deploy, then go to your app and add a filter, hit Search, and it works. Now, if you happen to notice that a particular filter or combination of filters yields an error, well take a look in your logs, and you might see something familiar from a previous part of the lesson. And you can fix that in the way that we discussed earlier. This is a sandwich. It's not a regular sandwich. It's a little bit bigger than the regular sandwich, but it looks delicious. So why am I telling you this? Well, datastore is like a sandwich. It has layers, each layer built on top of the other for support. We have covered a lot of layers in lesson four and we only have two more layers to cover. So, keep the good work up. As for me, I'm eating a sandwich. We will now turn our attention to the Datastore Commit Process. This process describes to consistency rules for storing data. Datastore has two consistency models, Eventual Consistency and Strong Consistency. Let's look at these in more detail. In this use case diagram we have three things. Your Application, the datastore API that your application calls and the datastore backend, which performs work that your application is not involved in. When your application wants to store an entity, it performs a put operation towards the datastore API. The datastore backend then writes this entity to a log. When the write is completed, control is turned back to your application. At this point the Datastore backend has promised to write the entity to Datastore. Observe however that the entity has not been written yet. It has only been written to a log. The Datastore backend now goes through to work to make everything consistent. It does this by using the login information to update the entity storage and then it updates all the indexes. So observe that when control is returned to your application, datastore may not have done all the work required for the data to be updated. Is this good or bad? Well it's good, because this means that you have less latency in your application. But the question now becomes, what happens if a query is issued that would retrieve the data your application just requested to put into data store? Well, with eventual consistency the data store API will not wait for this to happen. So it only considers matching data that already exists. That is data, that was already in data store prior to your put call. And then it returns that result. That's why it's called eventual consistency. Queries will be eventually consistent with put operations performed to what's the data store API. So let's now look at what happens when in the case of strong consistency. In strong consistency, you're always guaranteed to get the data that has returned from a put operation towards the data store API. So when the query is received by the data story API, it will see that there are pending updates for the data that you are querying for. It will then wait and then get the data once the datastore backend has finished. And then it returns that result. That's why it's called strong consistency. The result from queries will always be consistent with put operations performed earlier toward the datastore API. So which is better, eventual consistency or strong consistency? Well that depends on your application. Let's look at two examples. The first one Posting a Blog Comment. In this case eventual consistency's probably best to use. This is because to update, it probably does not need to be immediate. And there is no operation dependency, in other words there are no subsequent operations that relies on data being completely up to date. What about this one? An atm money withdrawal. Well that's certainly candidate for strong consistency because even though you may not like it, the new balance should be reflected immediately and there may be operation dependency. For example, there may be purchases coming in on that account that depends on an up to date balance. You may now say, so why don't I use strong consistency all the time? Well, remember the use case. Strong consistency causes a lot of wait and locks in your application. So if you want to build applications that scale. You should use eventual consistency as often as possible. Luckily, datastore supports both of these models. You can use the default fast and scalable eventual consistency when required. And when you need to ensure strong consistency, that is also supported. Let's see how, because it's quite simple. You enforce strong consistency when you have an ancestor relationship and your query use filter by this ancestor. If those two conditions are met all children will be queried using strong consistency, in all other cases data will be retrieved using eventual consistency. As we already discussed, strong consistency has a performance and scalability penalty compared to eventual consistency. So you need to look at your application to determine the best option for every query situation. All right, enough theory for now. Time for a quiz. Good luck. All right, let's do a quiz. Deciding on data model. Imagine you want to expand conference central with new functionality, for example adding social components allowing attendees to allow comments, photos, etc. Or adding the ability to purchase tickets to conferences rr add the ability to create conference venues where conferences take place. Which of the following three functions should strongly consider an ancestor relationship? Check the one that applies The answer is, purchase tickets. The reason for this, is that when you purchase tickets, you probably want to immediately see the confirmation that you are registered. To guarantee this, we need to have strong consistency. And in order to have the term strong consistency, we need to establish an ancestor relationship. The other two, add social components and conference venus. Are probably not so sensitive to be up to date. So there may not be, any need for strong consistency. So eventual consistency is probably a better choice for those. Now we're going to cover another area important to use when you're writing data, we're going to talk about transactions. Let's go back and look at a user registering to attend a conference for example. As you may recall, we previously introduced an ancestor relationship between a profile and conference where the profile that created a conference was the ancestor to the conference. We will now introduce a "has-a" relationship between profile and conference where each profile entity will have a list of conferences they will attend. We will call this property "conferences to attend". In addition, we will have a numeric counter in each conference entity which specifies how many seats are still available, we will call this property "seats available". So when a user registers to attend a conference, the conference gets added to conferences to attend and seats available gets decremented by one. So going back to transactions, in this case your application will perform two update operations on two separate entities. So far, every data store update operation we used have been independent of the other. For example, the success or failure of put entity 1 and put entity 2 has been independent of the success and failure of the other. This may be okay in many situations but in many situations it may also be completely unacceptable. So going back to transactions, in this case your application will perform two update operations on two separate entities. On the profile entity, you will add the conference to the conferences to attend property and on the conference entity it will decrement the seats available counter. All of these operations are performed independently in these diagram right now, you may see that they are highly dependent on each other. Imagine, for example, that you have successfully added the conference to the profile entity and then the unthinkable happens. Your application needs to terminate for some reason. For example, it runs out of memory and is faced with a critical termination condition. In this case, when your application restarts the profile entity will have the conference registered to attend but the seats available counter has not be decremented. This means that there will be more users attending than there are seats available. That is not good, we solve this problem by using transactions. This is done by telling data store to begin a transaction. Then you perform your operations and finally, you commit your transaction. If everything goes well, you have successfully performed both update operations and the state of both account have been updated. In transaction language, we call this a commit forward. But if the unthinkable happens and something goes wrong before we have successfully executed the commit transaction statement then anything that has been updated starting from the begin statement will be undone, like it has never happened. For our case that means that the seats available counter was not decremented but it also means that the conference was not added to the profile entity. As you can see we have created a mutual dependency between these statements. They are either executed as a complete group or none of it gets done. Now the time has come for you to add transactions to conference central with a new exercise. There' s at least one function in this app that would benefit from transactions. And, that is registration for a conference. When a person registers for a conference, the app needs to change two pieces of data, the number of seats available at the conference, and the set of conferences, the user is registered to attend. The number of available seats is the seats available field in the conference record. And, we'll keep track of people's registrations by adding a field to their profile, with the conference keys. Transactions let us guarantee that both of these changes will happen in sync, with each other. If we updated the conferences for a user, but didn't update the seats available, then if two people registered at the same time. They could both be trying to take the last seat. Transactions prevent that race condition. Making sure that an api function uses transactions is easy. You just have to add the decorator nbd.transactional to the function. And, if the transaction has to work on, several kinds of entities that belong to different entity groups, you have to use the parameter xg equals True. xg stands for cross group. The default behaviour for transactions is to work on only one entity group. An entity group is defined as, entities that belong to the same ancestral path. So, in this case, a conference that was created by user Joe, and the profile of user Joe, are in the same entity group. While a conference created by Alice, is in a different group than the profile of user Joe. You'll also need to modify the user profile class, to have a property that will hold conference keys. See TODO 6 files, for the additional functions that implement conference registration. So as you complete this, check these off, and then, think about this. What is the correct decorate to use in this case? Is it ndb.transactional, ndb.transactional xg equals False, or ndb.transactional xgequals True? So in this case you have to use the cross group transaction because the function is changing two different kinds of entities, profile and conference. Although for the conference owner those two entities who belong to the same group because the profile is ancestor for a conference. That's only the case for the user who created the conference. For any other user, the profile entity and the conference entity are not related by ancestral relationship and therefore they belong to two different entity groups and that's why you need to use xg equals true. Okay, the time has come to wrap up transactions where we will cover transaction rules. And the two things we will start with are snapshot isolation which defines the read consistency and optimistic concurrency which governs concurrent updates. We'll start off with snapshot isolation. And please observe that this behavior may differ between languages, so be sure to check out the documentation. The first rule is that all read operations in a transaction will return the values datastore had when the transaction started. And the second rule for snapshot isolation is. Updates will not be reflected while in the transaction. So, if an entity is modified or deleted in the transaction, a query will get the original version of the entity, or nothing if the entity did not exit then. That's snapshot isolation, but please be sure to check out the documentation as this behavior is somewhat language specific. So let's turn to optimistic concurrency. This rule is that a commit transaction would only be successful if the values updated by this transaction have not changed since the begin transaction. If the values that you're trying to update has changed since begin, the transaction will fail. Then there are two additional rules you should be aware of. The first is that one transaction can modify at most five ancestor groups. The second rule is that a transaction must complete within 60 seconds, but the general rule is, of course, to keep the execution of transactions short. As short as possible, unless in extremely rare cases, they should be finished within a couple of seconds and in most cases, less than a second. That completes this section One piece of missing functionality from the Conference Central application is a function to return the list of conferences that a user has registered to attend. We've implemented the functionality to register for a conference, and now user profiles have web saved keys of conferences. But you don't want to display keys, you want to display conference names and dates and such. We've provided a code outline of the getConferencesToAttend endpoints method. Your task is to use all that you've learned about dealing with entities and keys to write the code that retrieves these conferences from the data store. You can find this outlying code in TODO_7_Conference.py. As usual, as you complete this steps, check them off here, then answer this question. The instructions for this method ask you to use ndb.get-multi, instead of doing a loop over key.get. Why do you think that is? Do you think it's because the return values fro ndb.get_multi are smaller and use less memory than the ones in key.get? Do you think it's that get_multi makes fewer requests to the datastore, or do you think that it's just the programmer's preferred style? So, regardless of how we fetch it, list of ten conference keys is going to take up the same amount of space. But using get_multi to fetch a list of keys does only one request to the app engine datastore. Whereas calling the get method on lots of keys would do lots of requests. So, this isn't just a matter of personal style. Using get_multi is actually more efficient because it does fewer requests. And don't take my word for it, look in the docs. You'll find a link in the instructor notes. So there's still one more feature that you could add to the project to round out the functionality around registering for a conference. One obvious gap is that currently there's no way to unregister from a conference. Feel free to add the end points method, unregister from conference if you're so inclined. Take a look at the conference registration function. It will be helpful in doing this. Congratulations, we're at the end of lesson four. [SOUND] Wow. That was a lot of stuff we covered. Well, lesson four is also the most content-heavy lesson in this course, so we're really happy to see you here. We covered data relationships, specifically the ancestor and that has a relationship. We also looked at queries, query by kind and then filtered by ancestor and then filtered by different properties. Datastore indexes, we took an in depth tour of datastore indexes, it allows you to do queries in datastore and we've wrapped it up with different consistency modelsa and transactions in datastore. In the next lesson, we will look at heavy duty stuff. You want to know what heavy duty is? Well, be here to join us. I'm standing here by the Google Corkboard Server Rack. This server rack was built in 1999 by Google employees simply because you could not buy the computers required to achieve the scale, reliability, and performance that Google required. Each row here, has eight hard drives on it, and four complete PCs. And there is a cork mat, to shield the electronics from the metal frame that carries the row up. That's why it was called the Google Corkboard Server Rack. When your application runs on the Google Cloud platform, you can be sure it runs on the latest and best hardware. Well, perhaps not this version from 1999. But the latest version that we built. So, in this lesson we're going to cover the heavy duty stuff. We'll start it off by looking at memcash, a great way to optimize the performance of your application, then we'll look at background jobs. In the form of push queues, pull queues, and cron jobs and we'll wrap it up by looking at some of the scaling and tuning parameters for appendium. Let's get started, shall we? So data store is a great way to store and retrieve massive amounts of data. So therefore, it is very scalable. But it would still be faster if the data was served from memory. As we know, we want our application to have as small latency as possible. So whatever we can do to reduce this, the better. This is exactly where Memcache comes in handy. As the name suggests, Memcache is a memory cache which sits next to your app engine instances. Any data you store here can be immediately retrieved without having to read from data store. So when you store data in data store, you can also store a copy of Memcache. That means you can retrieve it from Memcache later, and therefore deliver response really, really fast. This is great, since you don't want your users to wait. Both Datastore and Memcache are shared across any number of instances of your application that app NU starts. This means that they can all access the same data provided that your application updates Memcache when it updates Datastore. The difference between Memcache and Datastore however, is that Memcache is a cache. This means that the app engine platform can decide to free memory at any time, in which case the retrieve operation will fail. So your application code needs to be prepared if this happens, and in that case retrieve the data from Datastore. Let's look at a summary of Memcache. Memcache is of course, not limited to cache Datastore information. You can put any data you want in Memcache. Since the data is retrieved directly from memory, the latency is very low. This help it build applications that can scale to extreme levels. But since it's a memory cache, it also means that the memory could be freed and the data be evicted from the cache. So your code can not rely on it always residing in the memcache. All right, that's the overview of Memcache, it's now time for you to hack in by adding Memcache functionality to the comfort central app. Your conference central application is now fully functional, and you can save and get profile and conference entities whenever you want. But it's not very efficient yet. Every time your code makes a call to get an entity from the data store, that entity can be fetched from disk. That's slow for you users, and it also counts against your daily quota. And that's why App Engine offers Memcache as a way to automatically cache your data. In this lesson you'll learn how NDB takes care of caching entities. And because that's really too easy, we'll also take a look at how to use the underlying Memcache Python API to put entries into the Memcache and get them back. Let's suppose you want to run an ad encouraging people to sign up for conferences that are nearly sold out. You can store those announcements in Memcache. There's no need to save them in datastore because they are very transient in nature, and it doesn't really matter if an announcement gets evicted from Memcache. Another feature of App Engine that we'll explore in this lesson is how to deal with long-running tasks by using task queues. And to do that, we'll use task queues to set a confirmation email when someone creates a conference. And we'll wrap up the lesson by creating a cron job, that periodically checks for conferences that are nearly sold out and updates the announcement in Memcache. The good news is that NDB manages caches for you. There are two caching levels, an in-context cache and memcache. Both caches are enabled by default for all entity types but can be configured to suit advanced needs. For most applications, you can just allow NDB to manage the caches and leave the default settings on. When you've deployed your app to appspot, you can see what entries are in Memcache by going to the Memcache viewer. If you used the application recently, there may already be some entries in the cache. If you don't see anything, or if you see items in cache zero, open the web front end, make sure you have some conferences created, and browse around. For instance, look at conference details. Then refresh the Memcache viewer and you should see that there are some entries there. After you've explored the Memcashe viewer, can you check which of the following information you can see there? So if you looked around the memcache viewer, you readily see the number of items in the cache, and the cache hit ratio. As well as the total cache size, and the age of the oldest item. You won't see the size of the individual cache entries. But if you look in on an individual cache entry, you can find its value. And so indirectly, you could calculate the individual cached entity size. Let's see another way to use memcache. You don't have to set anything up to get ndb to use memcache, but you can also use the memcache API directly and save custom strings to it. So we're going to use memcache for saving and getting announcements about conferences that are nearly sold out. You can imagine, for example, that your web application might display a banner ad to encourage people for conferences that are nearly sold out. There might be other things you would want to announce on your website too. Maybe if you get a special speaker for a particular conference or to announce a special promotion. For the sake of simplicity, we're just going to create announcements about conferences that are nearly sold out, because the goal here is just to learn how to use memcache. But announcements are a good example of the kind of thing you might want to save in memcache without saving them to the datastore. If an announcement gets evicted from memcache, there's no loss of real data. And it doesn't really matter, no business transactions are affected. No one loses their place at a conference. And the next announcement will likely come along soon, anyway. Announcements are very transient and they don't need to be saved for long-term or backed up. So they're perfect candidates to be saved in memcache and only in Memcache, they don't need to go in datastore. That way you get all your announcements without using any datastore quota. Using the Memcache API is really simple. You call the set, get or delete commands supplying the key for the Memcache entry. In the case of the set method, you also supply the string that you'd like to be the value and the get method will return the value for the key that you provide. They key can be any string. For our code, we'll make it a constant, because we'll be using it repeatedly when retrieving and deleting announcements from Memcache. You also need a method that queries conferences to find if they're any you want to put into the cache. An if so, setting an entry in the Memcache API. You want to query for conferences that are almost sold out, that have seats less than or equal to five, but more than zero. You don't really want to expose this functionality as an endpoint, so this should be a private method. Later, we'll talk about using scheduled job or cron jobs. We will need to find a way to call this function via a regular, but protected URL. What we'll do is create a separate file, main.py that will have a function there that uses that cacheAnnouncements function and also sets ahead the furry URL. We'll also add this to app.yaml in the form of an entry forward /crons/set_announcement. Secured by having login: admin. Login: admin means that cron and application developers will be able to access this URL, but not regular users. And it'll let you test it just by going to your app and adding this path. You can implement this on your own or take a look at the provided sample code. The application logic in this case is not that important. Mostly, this is just to demonstrate the use of the Memcache API. So now there also has to be an endpoint method for getting the announcement, so it can be displayed somewhere in the app. You'll also have to finish the code here. And for endpoints, we'll also need a simple message class for the outgoing message announcement. Because you can't just use strings with endpoints, you have to use ProtoRPC classes. So let's call that class StringMessage and have just one field in there, which is required. So there's quite a bit of work to do. Take a look again at the TODO in the provided code. Remember, you can test your announcement setting function by searching for that Memcache key in the developer console. And you can see if your announcement reading endpoint method works by testing it in the APIs Explorer. So, as you complete these steps, check them off and then put your app id here. So in main.py, we have this SetAnnouncementHandler and all it's going to do is call cacheAnnouncement in the ConferenceApi. And then in conference.py, we have getAnnouncement, which needs to get the MEMCHACHE_ANNOUNCEMENTS_KEY from memcache. And if it's not there, this is going to return none so we need to make that a null string instead. So that we can initialize the string message with the string. Once you have all this, and you deploy it on App Engine, you'll be able to call the getAnnouncement method, and probably just get back a null string in the data in the response. And you'll also be able to access the URL /crons/set_announcement. It'll just return an empty page. But it's actually trying and setting the announcement in the back end. If, by the way, you get back a NeedIndexError, well we've seen that before. You just need to deploy the app locally, so that index.yaml gets updated. And then deploy it to App Engine. And then this will go away. So let's look at another feature that is needed to build scalable applications, task queues. So why do we need task queues? Well at this time this picture should be no surprise to you. Our dear users accessing our application from various devices. And our application for turning the response. Technically this response must be returned within 60 seconds from the request. But that's a long time. We've talked about the importance about returning a response as quickly as possible to our dear users. And we don't want them to wait, even for seconds, right? And certainly not 60 seconds. You want to deliver that response as quickly as possible. But let's say that the user activity triggers something big to happen. For example, the user adds a new conference and as a consequence of that 10,000 emails should be sent out to notify other users about this conference. First of all, that's not possible within 60 seconds. And secondly, you certainly don't want the poor user that just registered the conference to wait for this to happen. Remember, the screen update depends on your response so even seconds are crucial to keep your application responsive. Come to the rescue are task queues. This is exactly the problem task queues solve. And it may not be emails triggering the need for this, it could be something else. For example, an image needs to be analyzed or a video needs to be re-encoded. Many things that are not needed to complete the request for the user, could use task queues Task queues allows you to take the job offline, so you can create the task queue and then add a hundred task that each has the job to send a hundred emails. Tasks can be created for many user activity, and be put on this queue. And as we will see later, these tasks are not executed in the user request strip. So you can immediately return a response to the user, but one question remains then. How will these mails be sent? That actually depends on the type of queue you use. But before we go into that, it's time for you to answer a question. So the question is which is best to use, the HTTP request thread or task queues? When you want to generate the report, or register a new user, checking out the shopping cart, or do photo face recognition? The answers are, when generating a report, probably a task queue makes sense. Because that sounds like it could take awhile. When registering a new user, the user probably immediately want to see that the operation was successful. So the http thread makes sense. When checking out a shopping cart, probably the http thread as well. Doing photo face recognition, a task queue. That may take a long time to do. And the user probably, doesn't need to see the result immediately. All right, let's talk about the different task queues and start off with the push queue which is the most common one. The execution of push queues are managed by App Engine itself. This means that App Engine has worker threads that scan these queues and pick up tasks. These threads then call a URL that you have specified when you insert the task in the queue. This means that the execution is broken out from the HTTP request response flow since App Engine provide the threads to execute the tasks. But the code used to process each task is still provided by you, since you provide the URL that the worker threads call. The best of two worlds, a generic execution methodology,. That works with code that you write. So the worker thread picks up task by task and calls your url for each of them. And this behavior continues for all the tasks in the queue until the queue is empty. By default, the maximum execution duration for a task is ten minutes. It is of course up to you when you develop the application to split up the work into tasks that can be completed within this time, for example 100 mails each. There are many different configuration options for queues in the following screen, we will look at some of them. First to note is that the queues of the find in the queue configuration file when you do development. All queues have a queue name. For all applications, half engine order provides a default queue with the name default. This queue can be used without adding anything to the queue configuration file but you may want to change its default options, in which case you need to define it in the file. Other things you can configure are the performance parameters such as, the rate of processing, the maximum rate in case of traffic peaks. The bucket size and the maximum number of concurrent tasks that can be executed. There are also a lot of parameters surrounding error management. So now we've talked about how to define a queue. Let's finish this part by looking at how to create a task and assign it to a queue for execution. The first thing you do is get the queue by the queue name then you create the task and set options such as the task name, the url to process the task, another parameter such as performance and retry options and finally you add the task to the queue for execution. Finally, it is important that you to protect the URL so only administrators can access them through the permission settings. You don't want external users to be able to execute your task logic should they get ahold of these URL's. That was a lot of theory, but that's it for now. Now the time is come for you to do some work because we have an exercise coming up where you will use push queues. Good luck. So when a new conference is created, there might be a variety of tasks that need to be executed, such as starting the process to book the venue, or creating the agenda, organizing the catering, or sending a confirmation email to the organizer. You want to do all of these tasks in the background, rather than try to complete everything before the create conference function returns. In this exercise, you're going to use the default task queue to send the confirmation email when someone creates a conference. The learning goal is to understand how to use task queues. But you can imagine there are many other tasks that need to be completed when a new conference is created. To execute a task in the background using a task queue, the first thing you need to do is define the work to be done. It's up to you to write the code to execute the task. This can be in a separate handler or an endpoints method, or anywhere else that can be accessed by a URL. We've provided the email sending code, however, so you can focus on thinking about queues. An optional thing you might want to do is configure queue in queue.yaml. One exception is that if you're using the default queue, then you don't need to do any configuration. You'll use the default queue in this case. But you can check the documentation for configuration options. And then you'll need to write code to add tasks to the queue. Using the default task queue in Python is pretty easy. To end queue a task, you'll call the taskqueue.add function. In this simple case, you only need to provide the URL to use and additional parameters that you want to pass to that URL. In this case, you at least want the user's email to send to, and the conference information. If you're going to use the non-default queue, you'd also pass in the queue name as well. You can also set other configuration options such as the name of the task, the HTTP method, retry options and so forth. Once you've implemented all this, deployed to appspot and added a new conference to kick off the process, you can then go to the developer console and look at the queues and tasks and their status. As well as delete tasks that are currently in a queue, etc. One thing to remember is that unless you specified how many times to retry a task, if you specified an incorrect URL, it will retry indefinitely so test your queues carefully. And now it's your turn to add task queues to your application. We've provided most of the code in the additions folder, under todo2, you just have to put it all together. See the TODO 2 comments in the relevant files. You'll need to modify app.yaml, main.py, and conference.py. So here we are in app.yaml. And here's the new configuration for sending confirmation email. Then in main.py, here's the new SendConfirmationEmailHandler, which we've added. And then in conference.py, whenever a conference is created, right after putting it into the datastore, we now add the task to send the email. Okay. So let's talk about another type of queue called pull queues. This queue solves another kind of problem. Let's say that you implemented a site that requires humans to review comments. So the users of the site generate comments and these comments need to be reviewed before they are published. Similarly to push queues, you could create a queue. And then insert comments to be reviewed into this queue as tasks. And external people, well in our case robots, could review the comments, perhaps working from home in their spare time. So you have your queue where you add comments as review tasks. Then the different people would pull a review task from the queue and work on it. So let's look at the difference with push queues. Well in this case, App Engine is not executing the tasks. They are instead executed by the external workers. You can say that the task is pulled from the queue by the external worker. That's why they are called pull queues. The tasks that are pulled from the queue are leased by the external workers. The worker must complete the task within the lease time in which case it can be deleted. But if the task is not completed within the lease time, it is automatically returned to the queue. If we compare a pull queues to push queues, the differences are. So pull queues do not have a URL, since your code is not completing the task. Each task is leased by a worker for a certain amount of time. A worker must delete the task when it's completed, otherwise it is returned when the lease expires. External workers pull the queue using a REST interface. And there is no default queue for pull queues, so you need to explicitly create them. That's it for pull queues. Let's have a quiz. For the following examples, which queue would you use, Push or Pull? Select the best option for a Package Delivery Service, a Flight Check In, a Code Review, or Batch Processing of Images. So here are the answers. A package delivery service sounds like a pull queue because each package is probably delivered by independent workers that can pull tasks from the queue. What about checking in for a flight? Well I would like to know that I'm checked in, and in that case it's neither a push queue nor a pull queue. Then the confirmation would be delivered as part of the http response, even if it takes a little bit of time. This is a situation very similar to when you buy things using a credit card on the internet, where you also often wait for the response. What about code review? Well a code review sounds like a pull queue, because you probably have many people that can review the code, so the first one that's available can pull the code review from the queue. And then batch processing of images, which sounds like a push queue. This is because your code is probably doing the processing of the images. Therefore you want to have control over the execution. So we've talked about push and pull queues now. But what if you just want to do something at regular intervals? For example, perform a backup, remove temporary files or send a daily report at midnight every day. That's exactly what a Cron job does. It's a scheduler. An app engine cron job is very similar to a cron job in Unix, if you have worked with that. So, cron is a scheduler within app engine that allows you to specify tasks that are executed regularly. Cron is perfect to use to perform background tasks, which are not directly initiated from user activity. For example, to run batch or synchronization jobs of different kinds and back ups, etc. The cron job is very easy to configure and there are two things you need to specify. First of all, the URL that should be called. This is where you place the code you want to execute and then you also need to specify the schedule which controls when the task URL is called and as with the Queues there is a ten minute limit for dynamic instances. If you want more time than that, you can use resident instances. In this exercise, you'll set up a cron job to periodically update the announcement about conferences that are almost sold out. Since you've already written the function to create the announcement, pretty much all you're going to need to do is to configure the cron job. And you do that in a configuration file called cron.yaml. So you need to create a new file in your project directory called cron.yaml, and to find the cron job. You need to provide the URL and schedule, and option lay description. The URL is the path to the task to be run. The description, which is optional, describes the task so you can figure out later what your code was supposed to do. And the schedule indicates how often to run the task. There are a variety of ways to set the schedule, but you must strictly follow the syntax or the cron job won't run. I recommend checking the format for the schedule in the developer docks. Note this says every 1 hours. It would sound correct if you were saying every 12 hours or every 2 hours but it turns out that even for 1, you have to say every 1 hours, not hour. You can see the defined cron jobs in the cron job tab in the task queue section. You can see the schedule, and when it was last run, and whether it was successful, and you can also trigger to run it manually. And now for your task in the exercise, you should create the config file and make it run, the /crons/set_announcement URL every minute so you can see it working. But after you've tested your cron job, you should set it to run much less often, for example, once every hour or two. In general, you shouldn't run cron jobs more often than necessary, as they do consume resources and quota. So after you complete each step here, check it off. Make sure, again, to change your cron job from every minute to every hour after you've seen it run. And then, as a syntax check, how would you schedule it to run every two hours? Put that down here. Okay. Again, make sure to change your cron job from every minute to every hour, or less than that. And then the answer here is that to schedule it to run every two hours, you'd say schedule: every 2 hours, no surprise there. So now when we have written all this code, let's look at how to scale your application, using modules. Modules are a serious heavy-duty function within App Engine and they allow you to partition your code, have better control over scalability, and do version control and online upgrades. For simplicity, we did not use modules when building Conference Central, but it is something we strongly suggest you start exploring on your own. By looking at the online documentation. All right, let's get started. When you create an App Engine application using modules, it will conceptually be structured like a tree. At the very top, we have the root, which is your application, and your application is made up of a number of modules, in this example two. A module groups three concepts. Code related to the module. The instance class that should be used to execute the code, this is the speed of the CPU, and the amount of memory that should be used. The scalability parameters including the number of instances to start and their life cycle. So a module groups code to be executed, the performance resources and the scalability and instance life cycle you want to have. Each module can then have a number of different versions associated with it. A version is essentially a version of your code being maintained in the module. So, after you have deployed your application into production you can deploy new versions of your code and you can have multiple versions running in parallel, and gradually migrate traffic from the old version to the new. This capability allows you to do version upgrades without having to bring down your application. Something that is extremely difficult to do without App Engine. And finally, there are instances. An instance is what actually executes the application code. And if you want to scale your application, App Engine can fire up more instances to manage the additional load. And since the instance class and the number of instances define the performance and scalability of a module. The capacity of your application depends on the instances class and the number of instances you run. This is configured on the module level, and that is exactly what we will look at now. How to configure a module. Lets start looking at two scaling options for modules. Manual Scaling and Automatic Scaling. These specify different scale up and scale down characteristics of instances. Manual scaling allows you to create instances that are always up and running and that never terminates. Because of this, they don't have the ten minute execution limit with they process tasks or cron jobs. Since they are always up and running, they can work for any amount of time. And that means that they are great to use if you have long running background tasks or cron jobs. The only thing you need to configure for manual scaling is the number of instances you want the module to start. That's manual scaling. Let's now talk about automatic scaling. Automatic scaling is the option that allows App Engine to automatically scale your application up and down depending on the load. The first two options you can specify for automatic scaling is the minimum and maximum number of idle instances you want. What does this mean? Why would you want to have idle instances? Well, instances have a startup time and if you don't want your users to wait for the response while your application needs to scale up by starting new instances, and it could be good to have idle instances available if your application needs to scale up. So your users don't have to wait for more instances to start. Let's now look at latency. Where you can specify the minimum and maximum pending latency allowed for requests. These parameters work as follows. So let's say a request comes into your application right here, and there's no instance available to process it. Then it is put in a wait queue. App Engine now has a decision to make. When to scale up your application using more instances. Well, to begin with, it will wait for some time to see if any of the existing instances become available to process their request. This is called a minimum pending latency, if this time passes, App Engine is considering starting a new instance to manage the request. And if the next pending latency time expires, then a new instance would be started for sure to manage the load. This is how App Engine performs a scale up of your application. Rather than specifying a number for these four parameters, you can all set them to automatic. In such case, App Engine will set them according to the dynamic analysis it performs. Finally, both manual and automatic scaling have a common configuration element. Which is the Instance Class. The instance class specifies the speed of the CPU and the amount of memory to use for each of the instances started. Check out the developer documentation for more details on the options available. As you probably see by now, modules is heavy-duty stuff. And in addition to the scaling options we looked at, it also has versions that we briefly touched on. Versions allow you to perform online upgrades on part of your system without any service disruption. Without App Engine, doing automatic scaling and dynamic upgrades are extremely difficult problems to solve. So make sure that you use this great functionality. There's one more app engine system tool I'd like to show you. This is a tool that helps you understand your application's performance in great detail, much more than the logs do. It's called App Stats, and this is what it looks like. App Stats let's you find out what's going on behind the scenes with each request that your app handles. It shows you how many requests your app is sending to data store memcache or other backends for each request that your app takes in. It shows you latency details. If a request was slow, what was it doing all that time? And when you expand beyond the free service tier, App Stats also shows you the cost of all these requests. So App Stats is a tool for profiling your app, finding out which requests take the most time or computer power, or cost the most money. Then you can use this information to optimize your application. For instance, by adding caching to reduce API queries, or by bundling requests into fewer RPCs like we did with ndb.get_multi in lesson four or by reducing dependency on expensive services. Setting up App Stats in Python requires that you make two changes to your app. You don't need to make any code changes, but you do need to tell App Engine to collect the data. And you need to turn on the built-in URL handler to allow you to access the App Stats console. For the instructions, see the docs linked in the instructor notes. So let's take a look at the data the App Stats gives you. The RPC stats table shows statistics of each RPC, each request that your app sends to a back end. For instance, here you can see data store get operations. You can see the number of calls, the total cost and the percentage cost. All the other back end calls are listed here too, like memCacheGet, and datastoreRunQuery, and mail.send. If your app has accessed another API, like the translate API, you'd see that here as well. Over here is the path stats table. It shows you more or less the same information, but broken down by the URL paths in your application, instead of by back end calls. So if we expand query conferences, for instance, we can see that query conferences makes calls to data store run query and memcacheget. Further down the page you'll see a request history. This shows information about each individual request that your app has received recently. For instance, here, query conferences was called. And here's the time. And they made requests to these API methods. And if you click on one of these requests, you'll see the RPC timeline view, which shows which specific RPC calls were made, and how long each of them took to process. So you can see here, for instance, that user.GetOAuthUser took 17 milliseconds. This sort of thing is very important to understanding your app's latency to users. This information may not look very critical now. But imagine that you're running a service that takes hundreds or thousands of requests every second. That's not at all unusual for a busy and popular app. And you want to understand which requests are slow and which requests cost you more money and where you should focus your attention to make your app faster and cheaper. App Stats is exactly the tool you'd use to do that. Another heavy-duty topic which is good to know about is edge caching. To describe this, let's look at the information flow for your App Engine application. First of all, users that want to use your application are connected to their internet service provider. This provider connects to Google data center. After the DNS lookup has determined that your application is hosted by Google, Google then identifies the data center where your App Engine application run, and starts talking to the App Engine front end. If the content is dynamic, the App Engine front end determines the instance that should manage the request. So these are your App Engine instances that run your application code. But if the request is for static content, for example images or static HTML, the front end can retrieve it directly from the static servers. And in both cases, the response is returned back to the user. So this is a good architecture. But as it looks right now, all the requests have to be sent to the data center, which hosts your App Engine application. It would be much better if more content could be served directly by this data center. First of all, it would ease the load on this data center, but more importantly, since it's closer to the users, the response would be delivered faster. This is exactly what edge caching is all about. Edge caching is a cache that sits in the data center closest to the user. So whenever there is a request, the result can be served directly from the cache if it's available there, rather than going through data center 2. That means less load on data center 2 in your application, and faster responses to your users. A win-win. So the question is, then, what do you need to think about to use edge caching? Well, there are two ways. The first one is to set the cache-control header, in the HTTP response. This should only be done if a subsequent request of this kind would return the same result. The second option is to define as much content as possible as static. Since static content does not change, it's great for edge caching. You can define which content is static through configuration files. A good opportunity for you to look at the online documentation. And remember, as most of the time with caching, there are no guarantees that the content will be cached, but when it is, it will be good for both your application as well as your users. Okay, let's do a little quiz called, how to optimize? So as rows here, we have objectives. Optimize application code, reduce latency for static content, make app more responsive, reduce data store reads and improve performance as user base grows. And as columns, we have different optimization techniques. Edge caching, mem cache, automatic scaling, app stats, and queues. Please select the appropriate optimization strategy for the different objectives. There could be more optimization techniques for a single objective. Good luck. Okay, let's look at the solution. What about optimized app code? Well that's about making the code actually run faster. App stats is the best option for that. What about reduced latency for static content? Well doing this would involve reducing the latency to the user. A perfect example of edge caching. What about making your app more responsive? Well, certainly queues could be an option here to reduce the amount of time spent when you process the request. But that could also achieved using automatic scaling parameters, and of course mem cache to serve content from memory instead of datastore. What about reducing datastore reads? Well certainly, mem cache would help out here, but edge caching could also be an option if the content is suitable for that. What about improving performance as user base grows? Well, that's a clear cut scaling out problem, so automatic scaling. All right, I hope you did well. And this quiz has no exact answers. So be sure to discuss this in the forums as well. Before we leave you, one more thing. You now seen what you can do with App Engine and the Google Cloud platform today. But what about five years from now? Let's hear what Urs Hölzle, Senior VP of infrastructure at Google has to say about the future. In this course, we learned a lot about App Engine. Urs, what can we expect from Google Cloud platform and App Engine in the future? A lot of innovation because what I'm really most excited about, is that we're just at the beginning of something that, I think, is going to be really, you know, we're proud of what we have today, but what we're really aiming for is getting to something that five years from now, is so much better. I think the rate of innovation that we're going to see in the cloud is going to peak up and we hope to be a big part of that. But our goal really is to take what we have and make it so much easier to scale, and so much easier to program, and so much easier to be on mobile, so much easy, easier to analyze data. And then really get a, get a, get a platform for you that is everything that it is today, but much, much better. Really to the point where if you remember, what you thought was great today, and we really have a great product, that, you know, five years from now, you'll kind of remember and say wow, you know, I'm, I'm kind of, I was naive to think that this was great because if I look at today's product. It, you know, was really just the beginning of it. I, that's what I'm, I'm most excited about. Do you have any final words to the students that have now learned a lot about App Engine? Now that you know App Engine, you're actually in a great position to try out other parts of our platform. And one of the things that we're really going to work on is make it very easy to combine App Engine with VMs or with Big Query or with Storage, so that you can mix and match and implement your application not just in pure App Engine. But really combine it with any other part that we have in our infrastructure. And still, have it be very easy for you to both write and run. That's the path we're on. And I hope you enjoyed App Engine and you'll enjoy the other parts even more. That's right. This has been just the start of what you can do with Google App Engine and with the Google Cloud tools. So keep your coding fingers up to date. And stay on the cutting edge of what's possible with cloud computing because every day the Google Cloud platform becomes better and better. So now you have all kinds of tools to build great applications. And scale them on the Google Cloud. So go out there and build something cool. And we'll see you around. Bye.