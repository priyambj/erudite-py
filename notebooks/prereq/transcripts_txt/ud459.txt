It is always wonderful to welcome a Georgia Tech alum back to our campus. Today, we are delighted to have one of our own. Colonel Greg Conti. An academy professor for computer science at the U.S. Military Academy. Where he also directs the Army Cyber Institute. Greg is a graduate of the U.S. Military Academy, but we like to remember him as person who got his PhD here at Georgia Tech in cyber security. My pleasure to be here. So I'm going to start with something we actually discussed at the very beginning of our class, which is the idea of threat modeling and developing a security mindset. Which requires us to think like the people who are going to attack us or their adversary. Now I know you've done some real interesting work in this area. Yes. So how does one think like the adversary? Well first off I'd say it's almost a guarantee to make you less popular if you think like that, but you have to. A lot involves getting into the head of someone you think that would have some purpose that's counter to yours. So if you're designing a system you have to say well who would be incentivized for a variety of reasons to subvert that system? And then try and anticipate who they are and how resourced they are, what type of capabilities they have, and then what they could then, in turn, do to you. You actually did an interesting experiment. Can you share something about that with us? Well, we called it the lessons of the Kobayashi Maru which is a Star Trek reference for our audience. In the Star Trek episode, it was an unwinnable exercise and young cadet, James T Kirk, was forced, or chose, to cheat on an unwinnable scenario. And he reprogrammed the computer, and was able to succeed in that exercise. So, we want to put students in a similar scenario, with the intent of deliberately forcing them to take on adversary mindset. And mind you, the people that we did this were government employees that typically work nine to five. And they don't have this mindset. But they embraced it. What we asked them to do was to take a test. We gave them very little notice, and we told them the night before too, and that they were expected to cheat on the test. And if they were caught cheating, then they failed. They weren't supposed to study. Okay, so cheating was allowed. Cheating was expected, and the test itself was the first 100 digits of pi. So they had to recite the first 100 digits of pi or actually write them down. It actually worked out really well. I'll give you a couple of examples of their creativity. One of them created business cards, well actually had a stack, and came up the next day before the test and was very proudly giving us their business card. The rest of the stack, it turned out, had the answer written on them. So this individual could have walked around in front of us and given one to every single student, and given them all the answer. He chose not to, but he could have because he subverted our trust. Another example was someone who, we allowed students to have soda cans on their desk and they printed out the answer. And put in on the side of the can and when we would walk around, they would put their hand over the answers, and then check the answers when our back was turned. And my personal favorite was a student who would take one of the books from the course, scanned in the back cover, loaded it in Photoshop, and ghosted in the answer. Printed it out, trimmed it down and used hairspray to tack it lightly to the back cover of the book. It was indistinguishable unless you closely looked at it. So they created a whole entire false book cover. But that's exactly what we are looking for. We wanted people to think like an adversary. It really comes down to trust. Where do you place trust? Find those trust assumptions and subvert. So in this case the proctor was the security system in some sense. So you were proctoring all this time when they were taking the test? Yeah, so during the test, my colleague and I were proctoring it. And what was very funny, and granted, we weren't trying too hard to catch them cheating. As we'd walk around the classroom, in front of us, we'd see the students staring up into the sky considering the 73rd digit of pi, and behind us, we'd hear rustling. And then I'd slowly turn, behind me were students contemplating the 67th digit of pi, the 82nd digit of pi. And then behind me I'd start hearing rustling. But it was really a good deal of fun, they embraced it I mean, I could go on. It was just, the innovation was off the scale. But I felt it was a useful and fun exercise. Absolutely. I think security we say is all about our assumptions and guarantees. And assumptions have to be about the nature of the threat you're going to be dealing with. Now, could you talk a little bit about sort of how we go from adversarial mindset to beating diversity and thinking about how to exploit the humans at the other end? Well, I think what our adversaries do And you want an adversarial mindset so you can for this thinking and look for these vulnerabilities before someone else does. I think typically what they'll do is they'll probe the perimeter of a system. Looking for vulnerability. And it's really on multiple layers. You can think of the technical layer, and if the system is secure technically, which is a very rare thing, but let's assume that's the case. Or maybe if they decided the amount of work, the work factor involved to bypass something technically just too expensive in some ways. Then they'll go looking for the ever present weak link, which is the human, and often times by studying the given users Finding their weaknesses themselves, their personal vulnerabilities. Those can then be exploited. So I tend to think against well-resourced and motivated adversary, that the adversary will always prevail given enough time and resources to put into it I'm glad you bring up the idea of the work factor and the resources. I guess we hear people saying adversity is going to prevail, or we need to be paranoid or hyper-paranoid. How do we strike a balance between how paranoid we are and the price we pay for it? Well, one I think, typically we aren't paranoid enough. Actually, I would say first we need to we design things we need to assume there's an adversary. Which a lot of people don't do. Particularly when products are rushed to market. And then I don't think that we assume an adversary is as powerful. We assume the adversary is stupid. Which is, again, a bad, bad idea. And then, I think we can even really be a little more paranoid than that and assume they can do a little bit more than we think. Maybe a lot more, depending on how important the information the system you're trying to protect actually is. When it comes to trade off, though, you could put an infinite amount of resources making something secure. And conventional wisdom says, and actually I would say best practice from industry, is to form a calculation. So the amount of money you're putting into securing a system shouldn't exceed the value of the item you're actually protecting. If you're building a vault to hold a $10 item, something's wrong. So that's part of the calculus. The other calculus, I think, is I personally I don't want to live in a world with perfect security. The amount of pain involved in our day to day lives makes an overwhelming amount of security very, very unpleasant place to be. So it really comes down to risk. Life is about risk. Banking is about risk. Crossing the street Is about risk. So each organization has to determine how much risk they're willing to take. Understand its security cost in terms of time and money and resources and frustration of your actual user base. There have been times that I haven't been able to get into the building where I work because of some glitch in the security system or I forgot my ID card or my pass card in another place. So it comes at a cost. It's a very rapidly changing field. That's where the landscape changes constantly, the technologies change constantly. How does one become a successful cyber security professional? >From your perspective, I guess, I'm sure you have heard about what it means to be a successful cyber security professional on your side? Well I think to be successful, I mean, you can't stand still. To stand still is to get left behind. So you have to have a passion for the discipline. And to really be a constant learner and being able to think creatively and consume and learn and create new things and also be interested in how to take things apart and push technologies in ways that they weren't intended. I also think though to be successful, you have to be trustworthy. Because by studying information security, you're learning effectively dangerous skills that you have to use wisely. People will put their trust into you and you have to respect that. That's really a key, probably the key underpinning the whole discipline. A willingness to teach others about this in a way that's the right language. Taking a step back, I'd say you have to be able to communicate technical subjects to non-technical audiences in a way that's understandable and meaningful. How you communicate with potentially board of directors, there's a whole lexicon of how you communicate with senior business leaders or senior military leaders, or students. It varies, so you have to be very adaptive and be willing to learn and respect the cultures which you're trying to interact with in a positive way. You did your PhD here. Yes. And, I had the privilege of working with you. And, there are certain areas that you are very passionate about in cyber security. I remember you sent me your book, What Google Knows About Us. Is that interest still to occupy some of your time? So, I'm focused really on four varying areas. Usable security, which I think is of critical importance. Security data visualization, the idea of being able to present information to humans, to find insights that machines can't. Online privacy, critically, critically important. And, where I'm spending a lot of time today, is cyber conflict. Typically, what we learn in the classroom is more system-level security. An individual system protecting an operating system, or protecting a specific device. But, we need to think bigger, and scale up. How do you protect a city from attack. That's where we're putting a lot of though into it now. I'm going to ask you to sort of go back, transport yourself back in time, about 10 years back, I guess, when you were here. And if there's anything interesting you recall from the time that you spent with us. I would say that what impressed me the most was the talent of the students that I worked with. The talent of the faculty that I worked with, their passion for the discipline of computer science for information security in particular. The fact that they treated me as a professional colleague. And really I learned as much as I could humanly learn. Didn't sleep much during that time. I would encourage those listening whether they're doing this full time or part time at night, just drink deeply from the well while you have this opportunity. And really when you're done to understand that this a constantly evolving space and that you need to not stop learning. You have to continually feed your mind with new information, new knowledge. I have advised many PhD students, but you clearly stand out in that one respect, the uncommon army discipline that you brought to the academic environment that we have here. And I thank you for coming down today and having this conversation with us. You actually have a website, isn't it? So my personal one is gregconti.com. And actually that's a good point. It's important to have, I think, a professional presence online, something each researcher controls themselves. And then my professional website is cyber.army.mil. Hello, my name is Mustaque Ahamad. I'm a professor of computer science at Georgia Tech, and also a member of the Georgia Tech Institute for Information Security and Privacy. I've been at Georgia Tech for 30 years. And it's been a great journey. My research interests are in computer systems and system security. In addition to doing research and teaching, I served as Director of the Georgia Tech Information Security Center from 2004 to 2012. I also co-founded Pindrop Security that is commercializing our research that we did here in telephony security. I'll be co-teaching this course with Professor Wenke Lee. Both of us have taught this course on our campus many times. We're delighted to bring this course to all of you in this new format. The course, Introduction to Information Security, provides a broad overview of the field of cyber security. Unlike our adversaries who just have to find one way to compromise our systems, we have to secure every aspect of these systems. I'll be starting with topics like software security. Operating system security, database security and so on. Once I'm done with part one, we're going to move on to part two with Professor Wenke Lee. Hello. My name is Wenke Lee. I'm a professor of Computer Science at Georgia Tech. I'm also the co-director of the Georgia Tech Institute for Information Security and Privacy. My research interest are in systems and network security, applied cryptography, and data mining. I've also cofounded company called Amala. It commercializes our research in detection. I'll be covering the second half of this course. The topics will include cryptography, security protocols, network defenses, malware, web security, and mobile security. Mustaque, I'm very excited about bringing this course to students. Absolutely, and by the time we are done, hopefully they'll have great understanding of the basic principles of cyber security, and walk over lots of practical techniques that help us all stay safe in our online world. We're going to start this course by trying to understand why cyber security has become such a huge problem. We're going to do this by developing what we call a security mindset. Once we're done with that we're going to talk about a number of basic design principles that can help us better secure computer systems. And obvious question is why worry about cyber security? Actually, before we get into the cyber side of things, let's talk about when or why do we worry about security. We worry about securing something, or we worry about the security problem. When we have something of value, but there also has to be a threat source that poses some kind of a risk to it. So clearly you worry about security when there's something of value, and you perceive that there's risk that is posed to that thing that is of value. So let's get back to cyber security now. So let's ask those same questions, what is of value in the context of cyber security? And where do the threats come from? What kind of risk are we talking about? Who is the source of the threat? In terms of what is of value, all of us store a lot of sensitive data. If criminals, if they get their hands onto this sensitive data that we're talking about, of course they can monetize it and profit from it. And it is no exaggeration to say that societies actually rely on the internet for really important things. So these are again critical resources that we all rely on. Which could be attractive targets for our adversaries. In this case the reasons may be different from simply profiting from it. To look at sort of a quick example of what we just talked about. In particular our critical infrastructure. People talk about smart grids, they're basically talking about the electric power generation, distribution, billing, all the different things that we do to make sure that we have electricity when we need it. If the computer is controlling the smart grid, whoever sort of takes control of those computers, is controlling a extremely important infrastructure on which the community relies. Obviously every business and government agency now use computers and networks to carry out what they're supposed to do their daily activities. But what happens if hackers, or adversaries, or unauthorized parties gain access to it? So it's an easy argument to make that cyber security is extremely important. It seems like it is important, but is it important only for companies, or is it really important for every one of us? So to make this point, we actually going to do a quick quiz. These are companies that you and I patronize. And all of them actually, unfortunately, have suffered data breaches, which means data they have about their customers was stolen by somebody malicious. So the quiz is really just asking you to check all the companies that you have done business with. I do go to Home Depot, I actually have the credit card because I buy things that I need around the house. I have lot of iDevices and have done business with Apple. I held insurance at Georgia Tech, a BlueCross Blue Shield at parent company at this Anthem. I'm sure I have or some family member of mine has shopped at Target. I have shipped packages at UPS. So for me, it's at least five. I'm sure many of you have Facebook, Twitter accounts and things like that. So, it's not just the data that lives on our computers. But it's the data that lives on the computers of businesses that we patronize. And our data could actually be breached from those companies. And then we could become targets, or harm can come our way from malicious actors. How do we understand the risk that is posed to the cyber assets that we have? So this is going to require that we understand the risk to the online information, and the systems where it is stored, and how they're connected, and who they can be accessed by, and things like that. So these kind of questions are exactly what we call developing a security mindset. A security mindset is really asking the kind of questions that I was just talking about. So if you say, well what exactly, how do you define a security mindset? You have to say, well, who are the bad actors? What can possibly they exploit? What vulnerabilities do I have? And if they are successful in exploiting a vulnerability, what is that attack going to be? So in the security mindset, the first thing we worry about is what is the threat source. In particular, who is the entity that wants to do us harm? So, there are obviously these criminals who are in it for the money, professional criminals who want to profit from the data that they can steal. There's another sort of threat source people worry about. These are activists who use the Internet, in particular, hacking. That's why they're called hacktivists, and they have some sort of an agenda. People may agree with it, or may not agree with it. When you sort of think about Snowden, well he had an agenda. He didn't like certain things that the US government was doing, and that was the reason for him. It wasn't that he was trying to profit from the information that he took, but the reason really was activism. Finally, threats can come from nation-states. And countries are actually doing this, they're doing it for political advantage, they're doing it for spying on each other. So the risk comes from sort of the threat source, sort of the entire spectrum, from a set of group of criminals all the way to nation-states. So the threats are clearly very real. We said old threats exist, what about vulnerabilities? So vulnerabilities could be of many kind. For example, if you use a weak password, that is a vulnerability. Someone can guess that password and then be able to use that to launch an attack which in this case would be taking control of your account. If they are able to take control of your account, well, that is a compromise of your account. If able to do it more broadly to a system, that's a security breech. So an attack is a successful exploitation of vulnerability by a threat source, resulting in this system that has been compromised. Unfortunately whatever it is, are very hard to get rid of completely. And they can be found in software that runs on our computer systems, networks, and lot of times the biggest source of vulnerability people say humans are the weak link. We do things that actually introduced vulnerability into the systems. So the security mindset starts with threats then we have to talk with vulnerabilities and exploitation of those within two attacks and attacks result in compromise and security breaches. We're going to use a really simple example. You can call it a trivial example. To illustrate this idea of a vulnerability an attack. So many of us ride our bikes. And of course it's an important asset to us. It costs money to get one, it gets you around, so obviously, it's useful to you, so, it's the valuable asset. There are threats against it, people steal bikes. Let's think about, if we just leave it anywhere, obviously, there's a serious vulnerabilities in it. The thief can walk and take the bike away. So what we normally do, is we lock it. So as you see in this graphic, this person who is security conscious actually has locked the bike. And they've gone off to class, or to work, or whatever it is, and you look at the system and you say well, are there any vulnerabilities? Vulnerabilities, some we may actually know about, others many not even know about it. It's the threat actor who actually discovers it. And in cyber security we call them zero day. The thief actually is not going to fight the security that you have in place here, which is the lock. And certainly what it is going to do is, it is going to walk away with the bike minus the wheel. So vulnerability here was that this asset that we have here the bike. Of course the people have to buy a new wheel. But what he or she is able to walk away with is actually still fairly valuable, isn't it? It's most of the bike, add a wheel and you have a functioning bike. So the vulnerability we never thought was that we had to secure more then just the wheel, okay. The lock that we put is actually only protecting the wheel. It's not protecting the entire bike. That's the example of vulnerability. One of the better-known breaches, cyber attacks, that occurred towards the end of 2013 was the Target store breach. So in this case, you would want to ask this question, what is of value that somebody was after? And if somebody was after, who was that? What is the threat source? Then we can say, well, what vulnerability did they exploit? So the Target case, essentially what they were after was credit card data that is there on the point-of-sale systems that are in Target stores. The people who are after it are cyber criminals because they want to profit from the stolen information. And the vulnerability they exploited is an interesting one. So, Target stores had an HVAC contractor. The hack actually began with a phishing message to an employee of that HVAC company. Through that phishing attacks, they were able to get credentials that gave the cyber criminals access to Target's network. And once they were on Target's networks, then they were able to get to the point-of-sales systems where they installed malware to siphon off the credit card numbers. So this is a real-world example where the security mindset, essentially we are saying, where does the threat come from? Cyber criminals. What are they after? Credit card data. What vulnerability was there in the system that was exploited? Well, we just talked about it. So an obvious question is why are they doing it? What's in it for them? And I said what's in it for them is that they monetize the data. All right, this is like walking into a bank and stealing cash. They're able to do it online. So one way that this works is that you steal data, you sell it to somebody who's actually able to use it. So if you're going to sell it to somebody. What kind of price does it fetch you? So we have a couple of examples. The security code that you have on your credit card. Credit card number or other information that is stored in magnetic strip. Paypal/eBay account. Some health information about you now that we have electronic medical records. Think about how much would credit card information sell for? How much would an eBay account sell for? Then we'll come back and see what those numbers look like. So the numbers that I'm going to give you here come from a report in the first quarter of 2015. The CVV or 3 digit code we have credit cards actually goes for $2, not very much. Credit card information, actually there's a range for it depending on what kind of credit card you have, so this could go anywhere from $5 to $45. A PayPal or Ebay account was going for $27. And health information could be obtained for $10 in the black market. So the exact numbers are not as important as something that's striking. These values are not very high, well that's okay because they have millions of these. So if you take these values and you multiply the two, you can see that this could be an attractive thing for a criminal who's out to make some money. So we're going to talk about another big cyber security incident that happened in the later part of 2014, and it involved Sony Pictures. There was a movie called, The Interview, and it didn't show the North Korean leader in positive light. Just before the movie was going to be released, Sony Pictures' networks were hacked. We're going to use that incident to explore our security mindset that we're talking about. So the first question is, what was the threat source? What was the goal of the attack, and what happened as a result of the attack? Think about, again, why would someone want to do it, and what did they accomplish? I should say that there was some debate that they're really North Korean government affiliated hackers who were responsible for this attack. So Nation-States would be the answer I would pick here. The goal of the attack was threaten Sony to stop it from releasing this movie. Sony did eventually release it, and the attack actually disclosed a lot of sensitive data. This picture that we have here actually puts a number of different concepts, in terms of how they're related to each other, so let's just sort of quickly look at it from two sides. People who rely on computer systems and network systems, these are owners of the data that's stored, and the systems where the data is stored. They deploy those because those are critical towards the do. At the same time we know that there are threat sources and they're going to pose threats to the systems that we're talking about. But one way we deal with both the threat source and a vulnerability that we may have is deploying defenses. Owners obviously want to reduce this risk. And one way to do that is to deploy the defenses that we have in place. And the risk is to the online assets which are valued by the owners. That's sort of the picture on the legitimate owners and what they do in cyberspace. In terms of the attackers, or the malicious actors, threat sources, they basically increase risk to these online assets that we have. And that happens because they exploit vulnerabilities and launch attacks. By doing that they wish to profit and that's the way they do it. The big picture is sort of the center is what is of value, who poses a threat and what do we do to somehow manage the risk that we have for our online assets from the threat sources. But the key concepts in cyber security, this diagram sort of nicely relates the idea of threats, vulnerabilities, attacks, cyber risk, and so on, and how they're all connected. So, essentially, we're already saying that we need to do something about cyber security. So how can that be done? Making them go away is not an easy thing obviously. One thing we can do is we can make sure that crime doesn't pay. We're actually going to talk about cyber laws. But making threats go away is nice idea, but it hasn't really been all that effective. You can reduce vulnerabilities, but we're never going to have zero vulnerabilities. Complex systems, unfortunately, always going to be error-prone, and some of those errors are going to be vulnerabilities that can be exploited. The three things that you always talk about when you talk about securing or protecting access to information, is the data sensitive in the sense that it can not be disclosed to unauthorized parties. Well that is really means the data has what we call a confidentiality requirement. You want to stop and prevent disclosure. It could be seen, but can only be seen by those who are authorized to see it. Another requirement is what's called integrity, that's really means that no one should be able to corrupt it. So maybe not sensitive in the sense nobody should be able to see it if they are not authorized, but it could have integrity requirement. Only authorized people should be able to write it or modify it. No one else should be able to change it, and that is an integrity requirement. The third requirement we have for data is what's called availability. The data is critical in the sense, what we use it for is critical, so if the data goes away in order to be able to do something that's really important to us. We can't access our online banking services because the server has been compromised, is down, or is a denial of service attack, or something like that. So these are called the CIA, Confidentiality, Integrity, and Availability requirements for sensitive data. So here we're only talking about data which is sort of the cyber side. We should say that cyber attacks could also have physical consequences. So by successfully attacking the computers, we will be able to cause harm for their physical system. Most well-known case of this is the Stuxnet malware that infiltrated the Iranian nuclear plant network, and destroyed centrifuges, and so on. So, that's an example where it's not just this information disclosure, or corruption, but there's actually a physical manifestation of a cyber attack. When we say what should we do? Well, we need to protect data and we need to protect systems. So, this question said data breaches. Remember a data beach is one that exfiltrates large amounts of data that was sensitive and stored on some server. So data breaches violate which of the following requirements, that we had for securing information or securing data? The CIA requirement, one was confidentiality, other was integrity, and then last one is availability. Should data breach actually discloses data to someone who's not authorized that the hacker or whoever else, they're really breaching confidentiality of the data, because it gets disclosed to an unauthorized party. Talk about another perspective, in terms of what the good guys have to do. First of all, we have to worry about what we call prevention. Prevention really is keeping the bad guys out of our systems. The bad guys are the threat sources, they're going to try to attack our system. If we can keep them out, that is prevention. Unfortunately, prevention is not going to be 100%. If that happens, we want to detect the compromise as quickly as possible. People talk about advanced persistent threats, APTs that go undetected for a long time. That means we don't have good systems to detect that kind of malicious activity. Well, if we find out that they have breached our systems or compromised them, then we have to respond to that. So response comes next, and what do we need to do as part of the response? We need to recover from whatever that has happened. If data was corrupted, maybe we have to restore it and things like that. And then remediation basically has to be that the same attack should not happen again. All these things is what good guys have to do so there are really two parts. Sort of, how do you do that? How do you do detection? How do you do remediation? And what exactly needs to be done? So mechanism is sort of the how part. You want to have a flexible set of mechanisms. So policy is always what is the way in which we're going to address something and what's done is through this whole spectrum. Cyber security, again, you're not going to be 100% secure ever but that doesn't mean that we leave the door wide open, so that's what good guys have to do. So what is the estimated value of world-wide losses due to cybercrime? These criminals could include potentially nation states. Think about all that is due to essentially criminal activity no matter where it comes from. So what is sort of the rough estimate of that? We look at the world-wide losses in the year 2014. So this report actually has an estimate of losses. Worldwide it's close to $500 billion or half a trillion US Dollars. So I think the take away here is that this number is pretty significant. Cyber security is big business. What is that we should be doing to address cyber security? How is the task of securing our system, it is going to be addressed by us? What are the things that are available to us? So one way you can reduce vulnerability is by basically following some design principles that are good for security. And when they're more secure, there are fewer vulnerabilities, and less likely to be compromised. The complexity is always the enemy. The well established studies that show whether it's bugs or performance related things, or whatever it is. Those increase with the complexity or size of lines of code for example. So one way to reduce vulnerabilities is, what we call this economy of mechanism. Should be a small set of mechanisms that your system fundamentally relies on. So avoid complexity. Keep it simple keep it small. Chances are that you are going to have fewer vulnerabilities then. Now the design principle is fail safe defaults. Fail safe defaults always say when somebody may or may not need access to some sensitive data you deny it. If they really need it at that point you can allow it. So default should be denied and fail safety fault is that the thing is protected. Access is controlled. Complete mediation says your system should never allow someone to bypass that monitor. The monitor has to mediate before someone is able to gain access. To the resource. The idea is that someone who's there to enforce that accesses are those allowed by policy you have in place then you can bypass it and that's the complete mediation requirement. A lot of people don't believe that you can get security by obscurity. So if someone says well we are secure because no one knows how we do something. Well don't count on that. Perhaps somebody smart can reverse engineer and learn most of it. So Open Design is good because we're not counting on somebody in not finding out how we do things. An extremely important design principle is what's called least privilege. So in systems at any time, essentially when you're running an application or a program, you have the privilege to be able to access a set of resources. Least privilege says you should only have privileges for resources that you absolutely need, and nothing more. It's a damage containment idea. If something were to go wrong, what you can harm is the set of resources for which you hold privileges. We said people are the weak link when it comes to security. And that is because we perhaps expect people to do something and that doesn't come naturally to them. Psychological acceptability says don't ask people to do that doesn't put excessive burden on them. So there are a number of these design principles. Actually they come from a classical paper called design principles for secure systems. You'll have a link to that paper. At least read the design principle section out of that paper. And for each of these principles we should understand how does it enhance or improve cyber security. What security weakness was exploited for this malware to be able to compromise the computers that controlled the centrifuges and so on? Stuxnet, we're going to talk about when we talk of malware,exploited some zero day or vulnerabilities that were not known at the time. So the exploit zero day of course in every system is not going to help you. So it's not the first case. It's not easy to guess passwords, because you can't remotely access it you had to physically get into the plant. So it has to be the second option here. It was an isolated network, so the only way you can breach the air gap is through a humans helping you do that. So either it's poor judgement by an employee or an employee who was a spy of somebody else. So you really had to breach the air gap to access the computers and that requires that people participated in this. The Security Mindset helps us understand why cyber security is such a big concern for governments, companies and people like you and me. We also discuss the design principles that help us deal with some of the threats that we face and the vulnerabilities that exist in our systems. The focus of this lesson is going to be on software security. Software runs on everything. The devices we carry in our pockets. The back end servers. Even the network elements that provide the connectivity that we depend on. We're going to explore common bugs and vulnerabilities and how they get exploited. We're also going to talk about defenses, both what we can do, and what the operating system can do for us. By the end of this lesson, hopefully, you're going to understand the importance of what we call secure programming, and will practice it when you write code. So, software is what controls our computer systems. We get things done by running software that we write, or somebody else has written for us. And, we're going to talk about exactly what are those bugs that actually turn into vulnerabilities. We're also going to talk about how exactly those bugs, or others get exploited. So the vulnerabilities that we're going to talk about come because of memory overflow. Keep in mind memory overflow means that the amount of memory we have for a certain data type is not sufficient so the data type runs over the allocated space. And an attacker is actually able to exploit a program by inserting new code sometimes in certain part of memory. And also then directing or transferring control of that code to the instructions that the attacker has introduced in this memory that we're talking about. Stack is the area where we allocate space for dynamically created data items or variables. So the most common example of when you use a stack is when you make a function call or a procedure call. When we do that, the variables that we need to execute the function code get allocated on the stack. So, as you make function calls, or procedure calls, for each call we create what we call a stack frame on the stack. This stack frame essentially you can think about, gives us the scratch pad or the memory that we going to need for the execution of this function. It's created when the function is called and it is discarded when the function finishes and returns. So what exactly is stored in the stack frame? Part of this we're going to allocate space for local variables that are going to be used by the code defined by the function. Also parameters that we're going to pass. This is data that we're going to pass to the function. Those arguments or parameters are going to be stored in the stack frame. And we also store control information. So remember when you call the function, you are doing control transfer, from where you were, to where this function code is. When you get done with the function, you have to return to the point from where you had made the call. And the vulnerability that we're going to explore today, which is buffer overflow, stack buffer overflow is actually going to manipulate the memory that is in this frame. Before we actually get into the details, I should mention the software vulnerabilities that we have. An attacker is allowed to call a program that you run on your system. The program may have an interface that allows legitimate users to make calls to it. And when they make those calls they are actually going to pass some data, so the entry point here is actually a legitimate call that could be made by users of this program. They pass certain kind of data and the attacker is going to pass data that is not what we expect. As a result it's going to lead to this overflow that we're talking about. This program is vulnerable. This really simplified program what it's doing is something similar to what a password checking program might do. Think about what the program is doing, this is our main function as the program. It has take some arguments. It has bunch of local variables. It has an integer that's value at the end is going to be should login be allowed or not allowed. It has a local variable of size no more than 12. Well that's where we're going to read the password that we're going to ask the user to type. And then we have to compare that with something that we know about the password of the user. So the target password here really is the password we are looking for. Here is a really poor practice here, but the target password is hard coded. Other thing I should mention here is that password checking programs actually don't need to keep your password because then if somebody gains control of that then they know your password also. So these are sort of the local variables that we declare in this function. This is where we get to this code. The code is very simple. It's saying I'm going to get a string. Gets is read a string function get a string that you're going to type. We're going to read that string into this local variable pwdstr that we declared, then we're going to do a comparison of the string that we just read into pwdstr against our target password, which is my password 123. This string compare actually is, this variant says at most 12 character long strings and the result of this comparison, if the two strings are identical then it returns a 0, otherwise it returns a non zero value. So read more about this function to find out exactly what it returns in the other cases, less than zero or greater than zero. So in this case, allow_login = 1, after that we going to say if allow_login = 0, remember that was the initial value, we had shown that we don't allow unless there is a match. Remember the fail safe defaults we talked about? So the default here is don't allow. So that's why we're setting it to zero. So if the variable is not set to 1 as a result of a successful comparison then it's still 0 then in that case we are going to reject the log in request. Otherwise the match was successful. We set it to 1, so in that case, we're going to allow the log in request. But the goal here is to understand how vulnerable our code gets exploited. So we want to see how this code gets exploited. And this simple program that we have here helps us do that. This quiz is actually asking you to go through this code, and see exactly what it's doing. So, where is that data? Is it on the stack In the stack frame that we just talked about. If the answer is yes, then you check it. If the answer is no, then you don't check it. So you go through each line. Understand what the code in that line is doing, and what data it's accessing. If you think it's accessing data on the stack, then you check it. So, lets look at what's happening here in this declaration. We're saying not only give us a variable called targetpwd that is 12 characters long, but assign the value MyPwd123 to it. So initialize this variable with this value that we are providing. So we have to actually store this MyPwd123 into this variable that we are allocating in the stack. So this are the initialized we're going to access it. What does gets do? I have checked that as well. So gets remember, reads a string, that's user input. And stores that into this variable called pwdstr. So remember we declared again at size 12, this character string that's where we're reading. So when we gets is going to ask for input, it's going to take that input and place that input in this pwdstr. As a result, it needs to access this variable and we know this variable is allocated on the stack. So, in particular the stack frame for this function call, so it's going to access addresses when we do gets as well. Next statement, you see several of them checked because they're all access variables that are being allocated on the stack. That's what we're trying to illustrate here. So here, this is comparing two strings. Again, the string strncmp is string compare and tells you the size of the string, the max size of the string. So it saying compare password string that we just was input. Target password that has this MyPwd123, so to compare these what do we have to do? We have to go fetch those values. Character by character or whatever this scheme is that we're using to do the comparison. But we have to see what is in this variable. And what is stored here so to read those values again, we have to access the stack because remember both the the local variables are allocated in the stack frame. Once we do the comparison, if the comparison is successful, we are writing this variable. We are writing one into this allow_login variable. Storing this value into this variable that's on the stack that was allocated in the stack frame is again going to require that we access the stack. And as a result, we have checked it because this again requires access to this variable that's on the stack. In this if statement that we have here, we have to find out what is the current content of this variable allow_login. So we have to read it. Earlier we stored a value in it. If the comparison was not successful, actually we will not do that. So this will not be executed if the two strings didn't match, but in case they did match, we store one. But when we come here we're saying, we're going to check this variable allow_login. If its value is zero, then we're going to print one message, if it's not then we're going to print a different message. So to find out the value of allow_login we have to read it which requires access to an address that is in the stack frame. Then we're doing this printing, so if you look at these are function calls. And I told you earlier function calls result in a stack frame when you make the call. But remember they're not accessing any variables that we have in our current frame. So, I haven't checked these three boxes here because we are not accessing any of the variables that exists over that allocated space in the current stack frame. That is what we are discussing here for this particular function. Remember I said, processes or programs execute an address space. It's a linear address space. It goes from zero to some maximum address, depending on what your address space size is. And a little bit more complicated, we'll get to that when we talk about operating systems. Part of the address space is where the operating system goes, but the other part is where user code and data is going to go. So that's the part we're talking about. There's some place where the code is going to go, that's called segment, that long lived data goes into part of that other space that's called the heap. And the temporary or dynamically allocated data that we were just talking about, when you make function calls, goes into the stack. That's what the stack is used for. So we know a stack is, basically the two things you can do with the stack is push things on it. So there's a stack pointer. When you push something that is put on top of the stack and the stack pointer moves one place. The other thing you can do with a stack is pop something off it. So when you do a pop operation, then the data item or the element that is at the top of the stack, the stack pointer it's pointing to, actually gets popped off, put in some other place, a register or a memory location different from the stack. And then the stack pointer is adjusted to reflect the fact that that item is no longer there. So if you sort of think about the stack as you do push and pop, the stack pointer moves. And you need to sort of see in what direction the stack grows. So when you push things onto the stack, the stack is growing. You're adding more items to it. When you pop things off, the stack its shrinking, and the stack pointer moves as the stack grows or shrinks. So we are going to assume that the stack actually grows from high addresses to low addresses. So the stack pointer starts at some address. And remember that address space starts at zero and goes to the maximum so we have this increasing addresses, address n, n plus 1, n plus 2. Typically it is byte addressable, so each unit for which we have an address is a byte, or eight bits. So the addresses increase as we go down the address space. So when I say the stack is going from high addresses to low addresses, what does that mean? Well, the stack pointer actually absolute address value, it's going to decrease as we push things onto the stack. We allocated some amount of room for the stack, or some amount of space for the stack and grow and shrink. And the stack pointer is initialized to the bottom of that space. So the highest address that we have for the stack reason. And as we push things onto the stack, the stack moves to a lower address. At this point, we have seen the code that we are examining for a vulnerability and we'll soon discover that. We talked about the stack where some of these variables that we have in this code are going to be allocated space. So, to further sort of reinforce what our goal here is to exploit that code, or exploit that program, we're going to do another quiz. And before I show you how exactly how we end up exploiting it, I want you to sort of think about it. So I want you to sort of look at these three possibilities we have. The input that I pass is greater than 12 bytes and ends in 123. Any password of length greater than 16 bytes begins with MyPwd123, which is the correct password. Or any password of length greater than 8 bytes. Any string, only requirement we have here is that it should be more than 8 bytes. So what would happen if we passed input to this program? Extremely important, the vulnerability in the exploit is going to rely on that input. So I'm asking you, think about the three different inputs that we have here in these three choices. And, could defeat, the answer to that is that whatever check that we're providing, that should fail. Okay, defeating here just means the password check should fail. So remember, if we exactly pass this the answer should be yes. If it's anything different than this, the answer should be no. And rejected or allowed should be printed at the end. So defeating means actually producing a result that is not what I just said. Giving incorrect password, and getting an allowed answer, that would be bad. That way you defeat the check. Or giving the right password and getting rejected is also defeating the check that we're talking about. So think about these three inputs. Think about what I just said about, what does it mean to defeat the check that we're doing here. And check all that result in the check getting manipulated or being defeated as we say here. So the first one I've checked here that it could defeat is any password of length greater than 12 bytes. So then what happens when give it a password that is greater then 12 bytes? This get s that we have here, Is going to read whatever string you provide to it. The one we have here is only for 12 characters. So it's 12 bytes long. But when we executing this function says, get string. It has no idea. It's going to accept a string. Whatever length you provide. Since it's greater than 12 bytes. The first 12 characters is going to get stored here. And those are going to compared with this. And where would the remaining characters go? Well remember this variable starts some place In the stack frame that we are talking about. On the stack, it's allocated space. And when the 12 bytes run out, and we have more input, what do we do? We keep going. So, starting address, and then we fill the memory starting at that address, as with the data that we have. So, if it's 20 characters, we're gong to store these characters we get in the string in the next 20 bytes. So think about what happens if this variable allowed login is right after this variable password string. If that's the case, then the bytes of password string, that are beyond 12 okay, the characters in these string that come after the first 12, actually going to going to memory we have assigned to this variable. Initial value is zero, this characters that we're going to read and store in the locations or memory that is actually allowed for that's allocated for allowed login. Well, that value zero is going to change to something else depending on whatever character that we're reading. Even if the string doesn't match here. Because this is whatever string we are saying, any password so it doesn't match. We are not going to assign one here. We're not going to come and sign allow login one here. After the comparison that we do, but some data has already over flown when we read this strength into this variable. And because that's what's going to happen when we check, do this check here to this side if login could be rejected or allowed. Is it going to be zero? It won't be zero because some data. That was input as a result of get as function that we called, has written into this location that was allocated, so we have this overflow. Remember, we started talking about memory overflow, or buffer overflow. So, if you sort of assume that there was some likelihood that this variable was sitting right next to this variable. And if that is the case, then if we had too much data for password string it will end getting stored into this variable that will change it's value from zero to something else, and the outcome of this check will be very different. So even if the password Is different from my password one, two, three, and this check fails and we don't set it here. This is still going to say login allowed, because it's going to find a known zero value. Because this zero value has been corrupted because of the overflow that we have. Because the string that we read was greater than 12 bytes. So it keeps going, writes into the space that belongs to this variable and as a result it's value is no longer zero. It's value is no longer what it initially was, which was in the case zero. So this is the way to think about how we defeated is by changing the value of a variable that is important obviously for making, deciding which way this check goes. Corrupting it by supplying input. Okay, that exceeds the amount we had allocated for this variable in which we we're reading this input. So all our flows always going to be something like this. It's going to all float, over write something, and we going to talk about read only book flows also. But typically, it's going to write and corrupt those values or going to write some values that attacker wants. So in this case, that hacker wants log in to be allowed so, doesn't have to guess the password. By carefully crafting input and over writing this variable that hacker is able to achieve the outcome, which in this case is going to be able to log in by doing this. The same thing would happen in case two because we haven't all four here as well. So any password of length greater than 16 bytes, it doesn't matter where it begins with. Lets say after that because we had checking 12 bytes, we going to check the eight that we have here. So the next set of characters may be different than what the initialized value here was. So even if there isn't a match. There is an overflow and because of that we have the same stuff happened that happened before. Again, our input is not good here. It is larger than the expected input which is no more than 12, so that's why we can potentially defeat is. And we'll see exactly how that is happening. If the password is greater than eight bytes. Well as long as it's not. So it's any password. It doesn't have to start with my or my password or whatever it is. But if it's nine bytes ten bytes 11 bytes. As you keep going. Until the amount of space that we have here. This will not fail. Because we're not overflowing. But if it's greater than 12 as we saw before, then we get into the same. So if the password was nine characters long, the check will not fail. And that's why I did not check this. The password check code is going to output the right value because we don't have an overflow. So, here we talked about playing with or manipulating the logic of this program, which is the password checking, and achieving that or accomplishing that by providing input that is bad. We considered some examples of input where the password is different than this password that was expected for a certain user, but we still able to get this program to say that our login should be allowed. Our login request is proper and should be allowed. So this is what bad input can do to a program. We're not changing the instructions. Before we call the function, we didn't get our hands on this code and modify, okay, that's a lot harder to do. We just calling a function that we're allowed to do, and give it a password, except we're giving it a bad input when it ask for a password and that's how we're manipulating the execution of this code. Let's just look at this code execution that we're talking about. If the attacker actually guesses our correct password, and types that as input to the program, login is going to be allowed. What they're saying here is the attacker doesn't have to do that, doesn't have to guess the password, the correct password. So it's guessing some password, might takes trying different types of passwords and so on. It gives a BadPassWd and we don't have an all flow. Because the BadPassWd here, if you look at it, it's still three, four, seven, nine, ten characters long terminated by a null. We still are good, because we don't have an overflow. And when we don't have an overflow, this actually fits into the space we have for the password string. So, no overflow, and it doesn't match, so as a result the request is going to be rejected. So this is sort of the things you do. You either find a password, then you have access. You can become that user and do what that user is allowed to do. Or you can try a password, but you're not going to be successful by just trying any password. The chances are, the password you type Is going to be, is not going to match the user's password. And match fails, login fails So we looked at that program, what it does, what input it takes, how does it execute on the different values for the input that we provide it. But to really understand how we actually take this buffer overflow to its logical conclusion, which is actually getting control of this program. We need to actually look carefully at how data is stored on the stack. What is the layout of the stack? So remember I told you the stack grows from high addresses to low addresses. So we are at some address, and as you put more stuff on it, we are going to lower addresses. So as we go down the stack, as we push things on to the stack, we get to lower and lower addresses and that's what I meant by the stack growing from high addresses to low addresses or growing up few. Think of your address base starting at zero and going to the max. Then the stack growth is from higher to lower addresses. And lower addresses are higher or up. So that's why we're saying it grows in that direction. So when you make the function call, we're going to push the arguments argcr, with the arguments. So we are assuming this is what the compiler does. You make a function call in your programming language. First thing it does it's going to push. So wherever stack pointer is pointing, we push this argument that takes 4 bytes. So the place it points to the address where the stack pointer is pointing is address 4 addr- 4. Then we push argv and at this point we have to push the return address. Remember we have to know where the return address is, so when we finish executing this function we can go back to where we came from. You then push the return address, so the compiler generates code to push the arguments. And then you make a function call or you make a call. And typically the hardware pushes the return address onto the stack wherever the stack pointer is pointing. That goes and then we get to the stack pointer and we decrement the stack pointer by 4 because the return address [INAUDIBLE] takes 4 bytes. So now we're going to allocate space for the local variables, allowlogin takes four bytes. 12 bytes strings take 12 bytes that we have here. So each of password string and target password we have 12 bytes each. So, you made the call, if the stack pointer was pointing to Addr, then I can say, well the address of allowed login is Addr-12 because 4 for argc, 4 for argv, 4 for return address. Already occupied these 12 locations, so the stack pointer has to move beyond those. And it does that by moving 12 bytes up and as it goes up to lower addresses, we're doing the subtraction that I said. So, now if you look here there's a couple of interesting things. This is password string isn't it? It's allocated space starting here. If it's less than 12 bytes, the input we provide, it's going to stay within these 12 bytes that we allocated for it. So this is the starting address, so if the variable is that address, address +1, +2, it goes like that every byte you store. So these are the higher addresses we were talking about. So, password strings start here, if it's less than 12 bytes, it's only going to go into these 12 bytes. What happens if it's longer than 12 bytes? Some of the examples that we did when we set it to 16 or something like that, it's going to exhaust these 12 bytes. It's going to overwrite the allowlogin and if it's more than 16 what is it going to do? It's going to overwrite the return address. So the way these things are let out on the stack, remember our password string variable address starts here. And as we provide some number of characters it starts storing them. If they're less than 12 we're good. If they're more than 12 we're first going to overwrite allow login. Then we're going to overwrite return address. Then we're going to corrupt the argument values and things like that. Now we can say well, what if I give more than 12 bytes. We earlier said 16 or ending whatever it is. We know that if it's more than 12, it's going to all flow into this. This is what it is going to all flow as. Depending on what sort of input data we provide when this string is read, we know that if we provide more that 12 bytes, we're going to overflow in this direction into these variables as we're talking about. So what can you do if you, sort of, are careful about it? First thing that we want to do is, we want program control to go someplace where our code, the attacker can actually craft some code. There are two things the attacker has to do. It has to have its code that it wants to run, and then it has to find a way to get the program control transfer to happen, such that the program goes and starts executing the code that the attacker has crafted. Here, we should sort of look at this and say there is an opportunity here for the attacker. The attacker has found a way to go up and write into this locations. We know that it has done that, because if it passes more than 12 bytes these 4 bytes are going to be overwritten, that's allowed login. If it writes more than 16 bytes, then the return address is gona be And so if we carefully sort of overflow, and you can find out the return address where our code is. And the overflow actually ends up writing the address of that code into this return address. What would happen in that case? The function, when it's done, will not return to where it came from. Remember it had stored the address where it needs go back in return address. But by carefully sort of overflowing and modifying this return address, that old return address is history. It's gone. It's going to actually return to the address that we have inserted or we have put in these 4 bytes. The 4 bytes where we have the new address that we have put in to this area of memory where we have the return address before. That's where this program is going to return. So this is kind of an ha moment. By providing bad input, the vulnerability is, we're going to know that, we're not checking input. That's the vulnerability. That is what is going exploited by that hacker. The attacker is providing a carefully crafted input. And when we take that input, an overflow occurs. And the way the overflow reaches the return address area, the part of memory where the return address is stored. We are actually going to put a new return address where code that we have created, by we I mean the attacker, the program will start executing that code. So some code that hacker wants executed, if we know where that code is, and we know its address, and this is what we override these four locations with, the return address by passing bad input, exploiting the vulnerability, when the program doesn't check it's input. By passing carefully crafted input, we are now able to take the program from where it was to a place where our instructions are located. So let's talk about a quiz that's sort of revisits this idea of a vulnerability and resulting in this exploit that we are talking about. So we know that there is a vulnerability and what is the reason for that vulnerability? That's what we are trying to answer in this quiz. Okay let's talk about the answers. Remember the quiz was about what is the source. What did you or did not you do when tried to write the code for that password checking program that resulted in a vulnerability? So, the target password was too short, this made it easy to overflow the buffer. That's not the vulnerability, isn't it? It doesn't matter how long the password is going to be. Maybe you can expect people to have passwords that are 100 characters long. But if you assumed the password was not going to be more than 100 characters, and someone gave it 110 characters, the overflow would still happen. So it's not the absolute length of the password that is the Issue. The issue is that we allocate a certain amount of space for the maximum, and the maximum could be anything. So, it being too short is not the issue. The code did not check the input and reject. This actually is the source of the vulnerability. Later we're going to remind you that secure coding, golden rule is check your inputs. We didn't check our inputs. We made an assumption that the input will never be more than 12 bytes. Because that's how much space we allocated for the password string variable. That's where we're going to store the input the user provides. We made that assumption, what could have we done beyond that? We could have, when we received the input, actually checked it. If the input string is more than 12 bytes that's an overflow. And we should have rejected and not gone on and done the comparison and things like that. So the code did not check input and reject password strings that are longer than 12. That is the vulnerability. Correct answer here is, the vulnerability arises because the input is not checked. And when the input is not checked that results in overflow. Over the last answer, the last answer says the code did not add extra, unused variables. First of all, there's no reason why you should add dummy variables, but even if you did by making that input string longer and longer, we can always sort of go past those local with the additional variables. So, just adding additional variables. Now, create some distance between the return address, and where the password string variable is stored. But, by giving longer input, we can go past those. So that is not, again, a correct answer, so the vulnerability here really is in that code that we have. It's poor programming practice, a lot of people unfortunately make that kind of mistake, is the code did not check its input and because of that there was an overflow. And overflow running into the return address. And that gives the attacker a way to direct program control to some other place where attacker crafted code may be available. So the code that the attacker wants to craft is basically code that is going to launch this command shell that we're talking about. And that kind of code is called Shell Code. So how do you write Shell Code? Well a little bit of expertise comes in. So the Shell Code as I said creates a shell, which is going to allow you to execute arbitrary commands. So any code program that we have, the attacker can ask that that particular code be executed. So let's talk a little bit about how do you craft the Shell Code. Say call you're making to launch a shell, you can write the code in C for example. It's a fairly short piece of code, you're just using one of the calls. And then let's say you compile it into assembler instructions. So once you have assembly instructions, remember the shell code has to be machine code. Because these are values and they have to be encoded properly and things like that. These are values that you are going to store in memory. That is where your shell code is. That's where control is going to be transferred. So memory is going to store these binary values. So it may look like data, but actually it really represents instructions of codes. And if your program is instructed to go execute from there, then it is going to be a program. So you really have to figure out what the instructions are and what data those instructions are going to use when they execute. And you're going to craft that as a set of values that you load into this part of memory. And that's going to be based on the instructional codes of the machine code as I've said before. But the way you can do that is by writing the code in a high level language, Assembler, translating that to machine code and that's how you figure out what is the ShellCode that you're going to need. So Shell Code that we going to place somewhere in memory, this is how you're going to get to it. Then I say, we know how to transfer control to it, because we modify the return address when the function ended, it will actually go to this place where the shell code is. So, let's ask ourselves this question. When control transfers to the ShellCode, what privileges are going to be used when this code is executed? We know the code is attacker code. The attacker actually created the code, wasted memory, transferred control of this program that we were running before to the ShellCode, and remember the program was running on behalf of some users. So it's a service, it may be running on behalf of rule or depending on what sort of rule is this or credentials we have some role or whatever it is. It could a system process for example. So, the program was running with certain privileges. This program is now going and executing these instructions that make up the ShellCode. So whose privileges are we going to be using when the shell code or the instructions that make up a ShellCode get executed? These privileges are going to be the privileges of the host program, the program that gets exploited was running with certain privileges. This attacker code now has the same privileges that the host program or the target program that could exploit it had. So if this happened to be system service as I said, is running with root privileges. Essentially, you have keys to the kingdom. You can access arbitrary resources, so the attacker used a vulnerable program used above for all the idea that we talked about, transfer the control to its own code, that was the ShellCode, and the ShellCode lets it launch to command shell, and the command shell now can start whatever program it wants to and that program would run on behalf of the host programs owner or group privileges whatever that it had. So the attacker has come in and become you, either the system or a particular user who is running this vulnerable program. So, this is how the attacker is able to execute arbitrary code with a legitimate user's credentials or privileges. Legitimate user's privileges are available to this arbitrary code that is being executed on behalf of the attacker. That's the best case scenario from the attacker's point of view. It's the worst case scenario from our point of view because somebody is executing code we have no knowledge of with our privileges. And they were able to get into the system because there was a vulnerability that got successfully exploited and as a result the attacker has ability to do everything that he or she wants to do. So this quiz is about how common are these vulnerabilities, or how many vulnerabilities are known. So remember that vulnerabilities we don't know about. Okay we come to know about them only after we are exploited. And those are called zero day vulnerabilities. We had no time to fix them or patch them before they got exploited. But known vulnerabilities, you patch your systems to make them go away in things like that. So we're talking about what systems actually have known vulnerabilities. So the data about this is maintain this National Vulnerability Database or NVD, so you will need to access that to answer this quiz. And it has lot of Information about vulnerabilities that are numbered through what is called CVEs and it tells you what system nature of the vulnerability what can happen and things like that. But here rather than go into those kind of details we're just talking about how many of these are there. So take a few minutes to think about these questions and take answers and then we'll come back and discuss what the correct answers are. Let's talk about answers to these questions we have ,about how many vulnerabilities do we know about in software that is running on our systems. So, first there's how many vulnerabilities do you think are there in the NVD? Unfortunately, the number is the largest possible that we have is the answers is close to 70,000. The last time I checked it was 69,000 something. We're talking about tens of thousands of vulnerability. To that there is millions and million of lines code out there, that have been developed by all kind of people and companies and things like that. But these vulnerabilities that can be exploited by attackers are not necessarily very rare, we know about lots and lots of them so that's answer is 70,000. So then we going to focus just on buffer overflows, we know what exactly buffer overflows are. We going to do our, fix our vulnerability for checking or things like that. We know we can make them go away. Well, the last three months if you look at, and this is sometime in middle of March in 2015, we actually have close to 100, actually the number was 107 I think, buffer overflow vulnerabilities that have been reported to the National Vulnerability Database in just the last three months. If you look at the last three years, the number is about 1,000, a little over 1,000, which means the rate at which these come at us not necessarily decreasing. Because in three months, if you have close to 100, in a year you'll have about 400. In three years we'll have 1,200, so it's in the range. Looks like we're seeing this vulnerabilities becoming known at the rate that doesn't seem to be going down significantly, compared to two years ago or something like that. So the answers are that even when we focus on buffer overflow ,we seeing almost a vulnerability everyday. Close to 100 over three months. That's about 90 some days. So we're seeing almost a new buffer overflow vulnerability every day, leading to thousands of those or close to 1,000 in the last three years. So the takeaway here is that software vulnerabilities. Now we know how they get exploited. They are out there and they provide a very viable path for attackers to craft attacks and gain control of our systems. And we know that, we discovering them almost on a daily basis. And the attackers perhaps know about them. They discover them before you do, and then they're able to exploit them. Because you don't have any defenses or any widely deployed patches to fix them. So far we have talked about stack buffer overflows. Remember we started with the idea of both overflows and then I quickly narrowed onto stack buffer overflows or overflows on the stack. There are other variations of buffer overflows and the number of reasons for those. We want to talk about these different types or variations of buffer overflows before we actually start talking works and defenses against these kinds of attacks that exploit buffer overflow. So the first variation we're going to talk about is what is called return-to-libc. Libc is the library, C library, of a program's use library functions. So, remember when I was talking about buffer overflow shell code, modifying return address. Our assumption was that we're going to return address, it's going to be modified to point to point to some place where we are able to place the shell code. And that could be on the stack itself. In that case you were transfer control to the shell code and execute it off the stack because that's where the code is told, the shell code is told. We're going to talk about some defenses where systems don't allow you to do that. So it's not easy to find room for your shell code on the stack and get it executed. So the variations of where you should return, you don't have to return to code that is explicitly write as shell code, but you can return to a function in the library. So the return address is going to be modified. The return address that we had on the stack, it's going to be modified to point to a standard library function. So the assumption here is that you'll be able to figure out the address of the library function. And once you have that, that's what is going to overwrite. So when you do the stack buffer overflow you make sure that library function address is what gets returned into the return address field on the stack frame. So why do we want to do that? Why do you want to return to a library function? Well if you return to the right kind of library function. And you're able to setup the arguments for it. Or the parameters before the call happens on the stack. Then you can execute a library function with arguments or parameters of your choice so think about the library function called system. The system call, if you look at it carefully, you can ask it to execute bin csh or some csh or something like that. And as a result of executing this function call you would launch a command shell. That's what our shell code was doing before. Now we can get the same result by executing a library function. And the reason it will do that for us is that before we make the call to it we go to it by returning from this function where we were able to overflow the buffer. We're going to set the stack in such a way that the arguments actually are going to be such. So for example I said the system calls should execute bin something. It's going to be such that when hit the system library function or the library function's system, it's actually going to do what you want it do. In particular give you a shell command. A command shell. So when that happens, you actually then going to execute code that you didn't have to craft and place on the stack. You're just executing a library function. And you're manipulating its input by properly crafting that input on the stack. And before you make this library function call. And then it's going to do your bidding if you are that hacker. So in libc, the thing to remember is that the return address is modified to point to a chosen library function and the setup is input in such a way that the execution library function, with that input, allows the attacker to sort of gain control the same way we were able to do before with shell code. Another variation is that overflow occurs. So, by overflow we mean, we write it to beyond the point that this variable getting written, has been allocated space. So, I said data also gets allocated on the heap in a program. So long-lived data, for example, global variables, and so on, get stored on the heap. So, one crucial difference between the heap and stack is that heap does not have a return address. So, you cannot hijack the control flow of the program, and take it some place where you have your own codes, or a library function, or return-to-lib kind of attacks. But in the heap we do have function pointers depending on what kind of language, and how they implemented and so on. So data can be these tables of function pointers. And there you can modify a function pointer, and by doing that you can transfer control, it's actually a lot harder than the stack buffer overflows we were are talking about. Heap overflows require sort of more sophisticated in some sense and require more work, but you can by modifying, figuring out where a certain function is, and modifying the function pointer, you can transfer control to somewhere else where this new address that you place in the function pointer points to. So heap overflow is, again, we're corrupting memory in some sense as we did before, but this happens to be memory that is in the heap part of the address space. Now, so far, any time I talk about overflow, I indicated that we're writing. That we're storing data in some part of memory, and we keep going beyond the limit where we should stop. And so, the overflow is the result of writing stuff. Overflows don't just have to be associated with writing data. Overflows could also happen because we read. The whole idea here is that overflow is because we do too much of something. We keep writing beyond the limit, so we write more than what we're supposed to. What if you read too much? Let's say a variable has 12 bytes but you asked to read 100 bytes. What the read is going to do is it's going to keep going beyond the variable and it's going to go on to the next one and the one after that, and things like that. So wherever in memory you have that variable that you're reading, if you go beyond that, you're going to get into other variables. Now, there is a fairly well-known vulnerability in the open SSL code which is used to secure all kinds of online transactions, and secure communications, and so on. It was called the Heartbleed Vulnerability. Actually, it was an overflow related vulnerability. But it wasn't writing into the stack beyond the memory that we had allocated for the variable into which we are writing. It was actually a read overflow vulnerability. Actually, this OpenSSL Heartbleed did, is that it kept reading beyond the variable that you're supposed to read. And there was some juicy stuff beyond that to do with keys and things like that. So if you read too much, you're going to get additional data beyond what's really should be in this variable that you're trying to read. And so, the additional data that you got could be used to figure out sensitive keys and things like that, and that's what the vulnerability was. But the vulnerability comes from the fact that we're reading beyond the variable that the code was supposed to access. As a result, the overflow occurs while you are reading. You go beyond the boundary of the variable and keep going. So that's called a Read-only Buffer Overflow, and the example that we just discussed the is the Heartbleed Vulnerability that occurred because of that. Buffer overflows don't have to be on attack only. They can happen on the heap, and buffer overflows don't have to be just when you write, they can also happen when you read. It's just going beyond the boundary where you should stop. You don't do that, that's the overflow. If you're reading some data, then it's read-only buffer overflow. So we have talked about buffer overflows and how programs that have this vulnerability can get exploited, in particular letting attackers or giving them the ability to execute arbitrary code with the host services privileges. It's not a good thing. We shouldn't write code that has those vulnerabilities, but if such code is out there deployed on systems, we need to find ways to defend against attacks that are going to exploit these kind of vulnerabilities. So we're going to talk about a number of defenses that help us counter attacks that rely on these buffer overflows. The first one we're going to talk about is your choice of programming language. What language are you writing your program in? There are actually going to be languages where this kind of problem goes away, you don't have buffer overflows. And these languages are languages that are strongly typed, that do lot of memory management they do automatic automatically, so they do bounds check. Remember one of the problems that we had was that we declared a string variable of size 12 and we said you can read any amount into it. There is no way unless I explicitly inserted a check, the language didn't do that, unless I explicitly inserted a check that said giving it more than 12 bites, that's not good. The language didn't stop me from clobbering whatever was next to the password string that we had. Not all languages do that. There are languages that say you have an object of size this, this is how much it could be, it can't be any more. So what type of variable it is, how much memory is required for it or is allocated to it, and how much memory you can access when you access that variable, this bounds check that we're talking about. And if you manage this memory automatically, the problem of memory overflow, or abusing memory the way we did when we have buffer overflows, that goes away. So, languages that do are called safe languages, because you can rely on type safety and you don't make these kind of mistakes. So there are several examples of these languages, Java, C++, and others. So if you write code in these kind of languages that have type safety and that do the kind of things that we're talking about, buffer overflow would not be a problem. The buffer overflow typically comes because we don't do either strong type checking or this bounds checks that I was talking about, which is what low level kind of languages are notorious for that. But these object oriented strongly typed languages make that problem go away. If you go for this defense where you use languages that are safe, obviously buffer overflow becomes impossible because of all the checks the language is going to do, so we set bounds checks. So the checks have to be done at run time rather than you doing that explicitly in the code that you write, that language is doing for you. So, why don't we use these kind of languages for everything? Well more and more we're doing that but one draw back with these kind of languages typically is that, that's going to be either some sort of performance degradation because we just talked about the checks that had to be done. And they force you to do certain things in a certain way. So flexibility that generally you have you have with low level programming languages goes away. So if you're looking for really performance or all the flexibility in the world, maybe that's why you don't use these languages that do this work for you to make the buffer overflow kind of problems go away. So what if you could not use typeset languages? You could not use languages like Java or others that do this checking to prevent all memory overflow kind of attacks. What can we do in that case if you're stuck with let's say languages for whatever reason, we talked about a few, where this is not done automatically, what should we be doing? So when we are using these so called unsafe languages that automatically don't prevent buffer overflows then it become the responsibility of the programmer. If you're writing code it is your responsibility that you deal with this problem of potential buffer overflows. One way you can do that is by checking ALL input, okay? So secure coding mantra is that trust no input. All input is evil and you should be checking that it can conforms to whatever your expectation was. Don't rely on the underlying system to do those because we're talking about using languages that do not do automatically. So secure coding is an extremely as you write programs. You write them to implement certain functionality. You may even write them with some performance goals in mind, but at the same time you need to keep in mind that you have to write them so they cannot be exploited. You have to code securely or use secure coding practices so your program doesn't have the vulnerabilities like the ones we've been talking about. And one way you do that is by checking ALL input. The other thing that you can do is, you can use functions that are safer that do bound checking. So remember in the code we could have string compare or string copy. You could also have certain side string, though you shouldn't be dealing with a string that is longer than that. So their functions that would place a limit on the length of the string this function would manipulate. So you can use there number of safer ways to use certain functions that are unsafe and the patterns and ways of using those that are safer where the likelihood of a vulnerability is a lot lower. So these functions, all though the language doesn't do it, the functions that you do will do the checking or the bounds checking that we're talking about, and as a result using those leads to code that is safer more secure. And the last thing you can do after written some code that's not in a typeset language, is to use tools that exist that would actually analyze code that you have written and flag potential vulnerability. The way they able to do that is, they can look for certain code patterns. They can look for certain unsafe functions, and can warn you that the code fragment that you have where these things appear, an unsafe function appears, could be potentially vulnerable to some sort of exploit buffer overflow or something like that. So these tools, you just run your code through these tools and there's a lot of them that are out there. Some commercial, some you can get for free. And these tools they rely on sort of database of certain kind of patterns that are known to be unsafe. And if they run into your code that matches those kind of patterns, they can flag those. And they can do more sophisticated analysis to potentially discover the kind of vulnerability that we're talking about. So there's no excuse for writing code that is not secure. You should be checking your input. You should be using safer functions that do that checking automatically. And then you should run through your coded through these tools that can flag potential problem spots. And you should go back and look at that and see how you can actually make those potential vulnerabilities go away. One problem with automatic tools is that they may have a lot of false positives or if they miss out and they have false negatives. So you can't have tools that will have zero false positives or false negatives, but the tools can certainly help us get rid of many possible vulnerabilities in the process producing code that is a lot more secure. So even when you are stuck with programming in a language that's not going to help you much with the kind of attacks, vulnerabilities that we've been talking about, there's certainly things you can do as a programmer who likes to do secure programming by checking input, using safer functions, and then running, checking your code through these tools that we're talking about. And we'll have a long list of these tools actually you can go and learn about. So when you talk about software vulnerabilities, it is affected by what language you program in. So there's safer programming languages compared to programming languages that are less safe where it's easier to make or have bugs that can be exploited or bugs that become vulnerabilities. So one idea is strongly typed languages. They're basically every piece of data that we have must have a type, and type defines what you can do, what kind of operations you can perform on the data. So strongly typed languages allow only that operations to be performed which are defined for their data type. Weakly don't care so much, they may sometimes allow you to do something that is not consistent. Java is a strongly typed language. I should say that there is no strict separation in terms of where strongly ends and where weakly type starts, but Java is strongly, C or Python or something are not as strongly typed languages. So this quiz is how does strong typed versus weak type languages, now how do they help or hurt in terms of software security is concerned. Or whatever it is that can arise in software that can impact its security. Is it helped by strongly? Certain things that we talked about here in the options actually are relevant to potential vulnerabilities. This quiz question is actually asking, how does that strongly versus weakly typed language either make these things possible or not possible, okay? So if the first option, for example you think this is possible with strongly typed then put S, otherwise put W, and do that for all three options. And strongly typed languages attempt to pass data that is not of the right type. Remember we said strongly typed, just make sure that when you passed it for example, what you're passing has to match the expected type. Or the target type where this data is going. An attempt to pass the data would either be called. This shouldn't be allowed by a strongly typed language. Okay? Because of typed mismatch. So either it would be caught at compile time statically, or it gets caught at run time, leading to an error. So a strongly typed language will actually detect that mismatch and will catch any attempt to pass incompatible type of data if you try to do that. So this is possible in strongly typed. Okay, strongly typed languages don't allow you to access array data beyond it's, outside of the range of the array. So the second one, we have to perform a bounce check to make sure that you don't spill into memory where some other data type lives or some other data item lives. So you shouldn't be able to access this other data time by just sort of overflowing the bound, ray-bound, that we have here. So this checking has to be done in strongly a type line which is weakly typed actually may not do that and as a result of that they may allow something like this. A W would be the answer. Pointer arithmetic basically says, pointer is an address in some sense, isn't it? Arithmetic allows you to move up and down from some point or value into various parts of memory, and depending on how you manipulate the pointer, you can get to sort of arbitrary area of memory. So this is allowed by languages like C, by weakly typed languages, but strongly typed languages do not allow it. So they would not allow you to do point arithmetic, and it's going to be impossible. And so the answer is it's impossible for strongly typed. So the answer here would be S. So the tools that I was talking about you can find those owasp.org website. So remember we're talking about source code analysis tools here. So we are making assumption that the code that you write, you have the source code and you want to make sure that it doesn't have those vulnerabilities. So you going to run it, so you obviously have the source code. You run it through these tools. A lot of times if the code is not written by you, it's coming from somebody else. You may only have the binary. And then some of those tools that requires source code, reduce source code analysis obviously won't be helpful. So discovering those vulnerability and binary is said reverse engineering and all the other skills come in, but attackers for example will take binary code and try to discover vulnerabilities in it. And we do that, one of the lab courses we teach here. People with attackers mindset, given a piece of binary how do you find out if has let's say, some potential overflow kind of vulnerability or not. But if your source code, you writing the code and you want to deliver secure coding and so on. If you work for a company part of their software development process may include reviews and passing those codes through these kind of tools and so on. It's a problem that we all recognize, we all recognize the importance of writing secure code. And these analysis tools can help you identify potential vulnerabilities and mistakes that you may have made. That could result into these kind of things that we've been talking about. So, spending a little bit of time with these is a good idea and that's going to help you write more secure code. We said buffer overflows are bad, so, what can we do beyond just relying on the programmers or coders to avoid those kind of vulnerabilities? One way we can do that is, remember that one of the tricks that hacker uses is to override the return address field that we have on the stack, in a stack frame. Whatever the function you were executing when the bad input has to received, the return address is modified. That should not be during the execution of that function. There is no reason for that return address to be modified because this return address is where we are supposed to go back. And where we go back should not change while we're executing this call function. Okay, so how can you detect if the return address has been modified? And if it's been modified there is an overflow and if you can automatically detect that then you can stop things right there, program can terminate or something like that. So how can we detect any modifications? So one technique is what is called stack canaries. So these are sort of like coal mines. We have those birds that tell you when the harmful gas concentration is high or something like that. What we can do is just before the return address in a frame we can store when the function call is made. At that time we can store a canary value in a location that is just before the return address. If you're going to overflow into the return address, you're going to come wire this canary value. So if the return address gets overwritten, chances are that the canary value is going to get overwritten also. So all you had to do is, before you return from the function, you check if the canary value has changed. You knew what the canary value is when the function call was made because that you had placed it in this location just before the return address. So you're going to what that location contains now with the value that you knew. If they don't match then there is an overflow and potentially the return address has been modified. And if it's been modified, then you shouldn't go on. We have detected a buffer overflow and that potentially could change program control flow and lead to attacker code and so on. So that modification should basically indicate to us that there is a problem here. So a canary value basically just says, if it had changed this is done automatically isn't it? The programmer has to do nothing. We compile it the way sets up the stack and sets up the data structure within the stack frame. That's where we add in this and we checking it automatically. So the programmer has to do nothing to get this extra security that comes from this defense that we just talked about that detects buffer overflows. So stack canary is one way that you can do that. The couple of other techniques. In fact the next one we're going to talked about is hardly used. A number of operating systems now and so one thing that's going to be different about the next two techniques we'll talked about is that they going to use. Support from the operating system/hardware that we have to deal with buffer overflow or to thwart buffer overflow attacks. So the first one, as which I said many operating systems use, is what is called Address Space Layout Randomization. ASLR. So remember, in an address space, some place stack is space where stack is allocated. So, stack starts at certain place. Somewhere the library code goes, another place where the heap goes, and so on. What if we randomize the places where these areas of memory start. Remember that hacker had to actually guess where key information is. So if I wanted to do return to libc attack, I had to know where the library is, where a certain function may be, so I know the address of that function. And I can use that as I overflow the return address field in the stack frame. So, I need to know these address. And what ASLR does, it makes hard, because of the randomization that we do, it makes hard for the attacker to find these important locations. Where the address that the attacker is interested in is going to over the information, address of the information that that hacker is interested in is not know. So as I said. A number of operating systems these days actually do that to protect the programs and processes from these buffer overflow kind of attacks. So this just makes it harder for the hacker to successfully craft an attack where can return, or manage, or direct control of the program to the certain known place that could be used to let the attacker execute arbitrary code. This one other defense against stack buffer overflow. When the shell code itself is stored on the stack. So remember if I don't do return to libc then my return address has to point me to some place where the shell code lives, or is stored, and you could write that shell code into some area of the stack itself. And then you can execute off the stack. There's no legitimate reason for us to execute instructions that are stored on the stack. So one thing we can do is we can make the stack non executable. Basically says the system is not going to allow you to fetch instructions from the stack area or the stack segment. If you do that then you can't execute on the stack. So your shell code cannot be on the stack. You have to find some other place to put your shell code and maybe that's why you have to try tricks, like return to libc and things like that. And use a library function to transfer control to the code that attacker ones executed. So here I guess we talked about sort of security is increasing assurance. Making the attacker's job more difficult. And making the legitimate, in this case here we're talking programmers, their task somewhat easier. So ASLR obviously is making the attackers job more difficult because key addresses that they need to know, they don't know exactly where they are in the address space. And they can't put code on the stack and hope to get it executed because we have the hardware operating system says not supposed to fetch instructions from the stack area of the address space. So these are techniques for actually dealing with buffer overflows. As we said, one makes it harder for the attacker, by making it difficult for him to find the right addresses, or be able to use the stack flow shell code. Now that we have talked about number of defenses against buffer overflow attacks, let's look at some questions that help us understand what these defenses can do for us and what they cannot. So the defenses that we talked about included stack canaries, address based loud randomization. So, the first two questions about that, and the third question is whether a certain buffer overflow vulnerability, read only kind in this case, would be avoided, or would go away, when we can't execute code off the stack. So look at the defenses and what they can do for the kind of exploits we talked about before, and once you have a chance to do that we'll come back and talk with the answers. So, the first question says, does the use of stack canary prevent return-to-libc buffer overflow attacks? Remember return-to-libc buffer overflow relies on the ability to rewrite the return address in the stack frame. We rewrite it with the address of a library function that we returning to a library function. But we have to write or modify that return address. And we said the use of a stack canary is to detect if the return address on the stack has been modified. Because when that gets modified, the canary value is going to be modified as well. So the answer here should be the yes, because a stack canary does allow you to detect change or modification to the return address, so that's why the answer is yes. Second question, does ASLR protect against read-only buffer overflow attacks? Well, ASLR makes it hard to guess where certain addresses of certain variables are, so for example a certain function in a library because library starts at some random address. When we talk about read-only buffer overflow attacks, we already have an address. We start reading from that address and then we keep going. We overflow into whatever is after it. So, we're not trying to guess an address. We have an address and reading starting that and reading more than they should. But there isn't a problem that we have here of guessing an address where we should start. So because of that, ASLR does not protect us against read-only buffer overflows and that's why the answer is no. The last one is actually read-only buffer overflow but that's the open SSL heartbleed vulnerability. Can that be avoided with a non-executable stack? Non-executable stack remember, is the code cannot be placed on the stack because it can't be executed, it can't fit instructions from the stack area. The answer to this particular question is no, because the OpenSSL heartbleed vulnerability read more data. It didn't actually execute code. The overflow was reading certain data that was sensitive that it was not suppose to the function was not supposed to access it. So, there's no execution. We're not executing code to get to the sensitive data, and because of that it's had nothing to do with non-executable stack. So the vulnerability that overreads or reads beyond the one that it's supposed to, we can't prevent that or make that go away just because we have non-executable stack. So here we talked about a number of defenses and where they work and when they may not work. And going back, write a secure code, use a typeset language or use a program in a language that makes buffer overflows go away. If you can do that, you have to use a language where that's a possibility then check input. Use tools to look at potential places where there may be a vulnerability. And then hopefully, the system that you're working on has defenses of type such as ASLR for which you don't need to do anything as a programmer. Or things like stack protection where you can't execute off the stack. We saw many of the common software vulnerabilities or bugs and how they can be exploited. We also learned about a number of defenses, including what we can do and also what the operating system can do for us. At this point, hopefully, you are well aware of some of the secure programming practices that we should all be using. The operating system plays a really critical role in protecting resources in a computer system. Resources such as memory pages, or files, that might contain sensitive data. It makes use of hardware functionality and uses that to isolate itself from un-trusted user code, and also can isolate one user's process from a different user' s process. So we're going to see how that is done by the operating system. Operating systems play a really important role in computer systems. In fact, when we talk about computers, we often say the operating system running on a certain kind of computer, a Windows machine, or an iOS device. We're going to see that operating systems play an extremely important and critical role when it comes to protecting and securing resources that we have in our computer systems. So let's look at what a computer system is actually. So we actually start with the hardware. Your hardware consists of a CPU, perhaps you have memory, and other IO devices, so that's the hardware that we have. Now if you wanted to use the hardware directly, it's going to be pretty difficult. In fact no one does that. What we really do is run a really important program, actually operating system is kind of a program, that handles the low level hardware resources that we have. So your operating system may be Windows or Android, or a number of these operating systems. What we generally start with the hardware run the operating system on that device that we have. And the applications that we directly deal with, that we run on our systems, so this could be your browser, let's say, or this could be your mail client. So these applications then actually running on top of the operating system. One way to think about this picture that we have for a computer system is that hardware is where we have the real or physical resources. We're going to run an operating system to control access to those hardware resources, and those resources then are going to be made available to the various applications that we have. So, if any of the these resources had to be protected in case we have different applications running here. The reason the operating system plays a really important role is because the operating system gets between these applications and how they can access the resources. Applications rely on the operating system. So what does it really do for you? First of all like I said before, the hardware is actually not very easy to use if you had to use it directly. If you wanted to program your applications, and had to work with the hardware directly it's going to be infinitely more complex. It may be already be hard, but it's going to be much harder. So what the operating system does is that it creates easier to use and high level abstractions for the resources that we have that the hardware provides. For example, we know that data that must be persistent, it's stored on the disk in disk blocks. Well we actually don't directly work with disk blocks and keep track of where on disk a certain piece of information may be. We have things called files. Think of file as a high level, where we're going to talk about virtual resources that are created or supported by the operating system and made available to you. So these high level abstractions that we can use to access more user-friendly resources in some way. Make it easier for us to write our applications, or build our applications that are going to run on this computer system. Now the resources that we're going to provide. The high level resources, the operating system makes available to the applications. Well, they going to be implemented using the real physical resources that we have that the hardware provides to us. The resources, the physical resources are going to be shared across these different applications. Obviously when you have that kind of sharing, we need to access those resources in a controlled fashion. So the hardware resources are actually managed by the operating system, and access to those is going to be controlled via it as well. And that actually is one of the fundamental reasons why operating systems have such an important role to play, when it comes to protecting resources and securing access to them. The last thing that is really interesting and we're going to spend a good bit of time on that, is all the different applications or different processes going to run on the same system and share the resources we have. The physical resources that we have between them, but the operating system makes each process believe as if it's the only one running. And it's able to do that by providing what we call isolation between these different processes. In some sense one process or one application does not need to be aware of another application, unless it explicitly decides to interact with it. It's sharing these physical resources, but think about the operating system making sure that what is being used by one application doesn't get used by somebody else, or another process or another application. So, having this isolation, sort of giving each application, sort of the feeling that it is the only one running. We should say that if they share, of course, not each process has all the resources, so there may be some performance implications. But, if you don't worry about performance, basically each process can believe as if it has the resources, and the computer to itself. So, the operating system is actually going to create this isolation between these different applications, which is really important. Because applications may not trust each other, and if they don't trust each other, they don't want any sort of interference from other processes that are not trusted by them. So this isolation that the operating system provided isolation is actually going to guarantee to us that the process doesn't have to worry about other processes, or other applications that may be there in the system. Why do we need to trust the operating system? And I'm going to talk about exactly what does it mean to trust a system, but that sort of differentiates an operating system from other applications that may not be trusted. So, why do we need to trust it? In fact, not only we need to trust it but we actually also call it a trusted computing base. So let's see why do we call it a TCB or a trusted computing base. It is the base in some sense. If you don't directly want to deal with the hardware, we set applications on top of the operating system. So the operating system really is the computing base that is seen by the applications or the user processes. The need to trust to the operating system comes from the fact that we giving it the keys to the kingdom. Keys to the kingdom here are direct control of all the physical resources. So the operating system in some sense is able to access anything that we have and then it's the job of the operating system to make sure that these resources, or whatever, high level resources we implement using the low level physical resources get accessed by the correct users in the system or the correct applications. And if you're going to make someone sort of the in charge or the controller, you better be able to trusted that it's going to do what it's supposed to do. Otherwise, the results that you going to have, you will not like them. So that's why there's need to trust an operating system. So what the exactly does it mean for us to be able to trust the operating system. So what are the requirements that should be met by this trusted computing base that we say sets an important role in protecting our resources? First of all, the operating system absolutely has to be between the untrusted applications and the physical hardware resources that we have. This is called complete mediation. Think about the request coming from an untrusted application and this request is for a resource that is being requested by the application. It has to go through the operating system. The operating system has to mediate this request and you can think about why that's necessary. It's necessary because we had the operating system has to check that the resource of the request actually has access is allowed to gain access to this resource. And for that check to happen, the operating system has to become between the request and the resource. So that's called complete mediation. Okay, you can't get to any resource that need to be protected without actually going through the operating system. So trusted system is always going to take a look at any request before it actually reaches the resource, and the source of that request is able to access that resource. So the complete mediation requirement is the first one. The second requirement a trusted computing base has to meet is, what we call, it has to be tamper-proof. In fact, we going to spend bunch of time how to achieve that, or how to meet that requirement. The reason it has to be tamper proof is that we're talking about untrusted code and in the trusted computing base of the trusted operating system. Well, if the untrusted code can tamper with the trusted operating system, you can't trust anymore. The untrusted code tampers with it, changes it to do whatever it wants done. In particular again, access to resources that perhaps were not meant for this untrusted application, which may be compromised, could be malicious. Third requirement is the so called correctness requirement. If you going to call a system trusted, you going to completely rely on it to make sure that you're protected resources get used in a proper way. Then however it does that, whatever functionalities implemented by the trusted computing base, it should be done correctly. Because if there is a vulnerability or a bug or some error in the trusted computing base or the operating system. I said it has the keys to the kingdom. So, we said operating system has to be trusted. Now we know what it means for it to be trusted. You shouldn't be able to bypass it. Okay? That's complete mediation. You have to go through it. You can't alter or change it, if you are an untrusted application. And whatever functionality it implements, that's done correctly. One of the things the operating system does is protect resources that need to be protected. How does that work? Well, the way it works is because we give control of the resources to the operating system. We say you can't directly, untrusted applications can't directly access it. So, operating system has control and it's actually going to control access to these resources, and we know it can do that because of the complete mediation requirement that we have. So, anybody who wants access to such a resource actually has to come through the operating system and when they come through the operating system, the operating system is going to be able to say yeah this is okay or I'm not going to allow or grant access or I'm going to deny this particular request. So how does the operating system actually sort of mediate these requests that we're going to have for various resources and that needs to be protected? First it must establish who's making the request. What is the source of the request? Request always has a source. The entity that's making the request and it has a target which is the resource that is being requested. So the operating system has to know who is making the request and what is being requested. So the who part is typically answered by this thing we called authentication. We're going to talk more about it in a future lesson. But authentication essentially establishes on whose behalf a certain application or a process is running. So that's the user who's making the request. Once we know who is making the request then we have this thing called authorization or access control. This is really checking, looking up if this source of the request is allowed to access the resource for which the request is being made, okay. You may be allowed to access a certain file or you may not be able to access a file because it doesn't belong to you. So the check that we're going to do is called the access control check. So the trusted computer base of the operating system really does It doesn't decide who accesses a given resource or not. That's called select policy. You have to decide who you're going to share your resources with or what other applications can share a given file that you have. So that's the policy. But the fact that when a request for the resource comes, being able to do that access check. The mechanisms are essentially that part which intervene between the source of the request and the check that we going to do to determine if the policy that we have in place is going to tell us whether the resource can be granted or not. So the mechanism is essentially this check that we are performing. And the goal of address computing base is to implement and do that in a structured fashion the checks or how the mechanisms for doing these checks that going to allow a variety of policies to be implemented or supported in the system. The protection of the resources when you talk about it. Always we have to know where the request is coming from. Whether the the requester is allowed to access it and that's what the trusted computing base has to do and it does it by providing these mechanisms to support variety of access control policies. Let's talk about a question that actually comes from an ad that was run on TV by a well known computer vendor, I will not name it. But that ad basically claimed that its systems didn't suffer from any of the ailments that the competition system had to worry about. So this claim that was made by this vendor who said it's system was superior, and when it came to security could have been well founded, or it may have been a marketing thing. Here we want to sort of dig deeper and to sort of think about vendor A and vendor B, and they both supply operating systems. And vendor A's making this claim about better security that its system offers in some sense. So what could be the basis of that claim? So we haven't talked that a lot about how one would validate this sort of a claim, but we know what a trusted system is and what requirements it must meet. And we also talked about the threat model, that is who targets these kind of systems, and one of our design principles said it is easier to get things right when they're simpler, okay. Complexity is the enemy, leads to vulnerabilities that can be exploited and so on. So I want you to use your knowledge about threat modeling, about design of secure systems, and the trusted computing system requirements to think about the options that we have here and check the ones that you think there is some reason to believe that it may be true. Once you do that, we'll come back and talk about the answers. Okay, let's talk about the options that we have here. So it is true at the time this ad ran, this is a few years ago, that vendor a, its systems didn't have as many, sort of, reported attacks as vendor b's. Or maybe you can call this vendor a and vendor m and you can figure out who a is and who m is. So it's true that it didn't, but both of the systems were running complex rich operating systems that supported similar kind of applications. So both tried their best to meet the requirements that we listed for a trusted computing base. So it's not quite true that one vendor's operating system was significantly more secure than the other vendor. Okay, let's look at the second one. The second option says, the two operating systems were similar as far as security was concerned but one was not as big a target. So this is about threat modeling that we're talking about. Who is that coming off to you? And it is too at that time vendor a's systems were not as big a target as vendor b's, because vendor b actually had most of the market belonged to it. So the attackers obviously were going after these kind of systems, and vendor b's system was a much bigger target than vendor a's. One reason was, as we said, market shares. So in this case, I'm going to say, this is one of the reasons why the claim that vendor a's systems didn't suffer as much as vendor b's was true, because vendor a's system was not as big a target, okay? So the last option says, the more secure OS could be much simpler than the other one, okay? So this is our design principle for secure systems. So that's what I want you to think about. But I said both systems are actually full faced operating systems. They supported same kind of applications and neither one of them actually we can say was much simpler. By the way this used to be an ad that Apple ran. And the sickly system that suffered from all the ailments was a Windows system. So I gave it away Okay, so actually I haven't told you, so far, we're going to talk about it, what a system call is. But let's sort of think about what would be a system call. We said the operating system controls access to protected resources. So if the operating system is going to control access to protected resources, and if you are one of those applications that needs this resource, you have to ask the operating system. When you ask the operating system the way you do that is by making a call to the operating system, and that's called a system call. As I said we haven't discussed this, but I want you to sort of think about this question here that says system calls allow application code to gain access to operating system, and particular resources that are controlled by the operating system. A system call is different than a regular call. When you run code, you make function and procedure calls all the time to do control transfer, to go from one part of the program to another part of the program. Here we are going from the user part of the program to the operating system, and when you make a call like that it is special and it is called a protected procedure call. So a protected procedure call is different. And the question that you should think about at this point is how different is it? One way to sort of characterize the difference is the cost of a call. So I want you to think about the cost differences between these two kinds of calls. When the system call is how a user or application code actually goes into the operating system to gain access to some resource it needs, and things like that. So the answer to this question actually is the second one. One reason, we're going to get into some details as to how exactly it's different, but you can think about you're doing control transfer. You're going from one part of the program, that's your user part, to the operating system. So you're doing what you would have to do, saving, for example, when you're going to return and things like that. Pass arguments barometers, but you're also switching the so called protection domains. You're going from user space, so user land we call it, to the operating system. The operating system is trusted. It's a distinct protection domain then the application code that we're running. And when you do that, now you're going to be able to do things that you couldn't do before. Okay. So, one more question which has to deal with one of the requirements we have for a trusted computing base. We know that's complete mediation, which means untrusted code cannot bypass the operating system or the the trusted computing base when they need access to a protected resource. We're going to say that you can't bypass it. And so, you'll have to come to the operating system. And then, the operating system has to figure out who is making the request. How does the operating system know that? Well, we know that the Operating System keeps track of applications or processes that we have on the system. Because it has to make various resources available for this processes or applications. So, in addition to if knows on whose behalf a given application is running. And when I say that I really mean. So the user would launched this application. So the answer to this actually is, first one, processes are going to run on behalf of users and that users have to login to the system and launch this application or these processes. System, the trusted computing base as the responsible for complete mediation, is going to keep track of these processes as we said before. So when a process is going to make a request, and I keep saying process, or application interchangeably, an application essentially runs as a process. So when the process makes a request, the system knows on whose behalf that process is running and the user on whose behalf it's running is somebody who must have logged in previously. And after logged in, we had launched its process for that user. The resource that is target of this request, we have to keep track of who all are able to access it. But when we get to Access Control we'll discuss how that is done. How is the operating system able to meet the requirements that we have identified for it? And one of them is this requirement that the trusted part be isolated and not be tampered with by the untrusted application or the user code. For us to be able to do that, we're going to see that we going to need help from the hardware. Okay so, remember operating system layered on top of the hardware that we have. But, the hardware has to implement certain functionality and that has to do with protecting memory where the trusted system is going to reside. And so the hardware is going to help us protect the trusted part of the system from untrusted applications. And that is actually going to be the reason we're going to able to isolate it or make it time for proof. When you talk about isolating then, we also ever talk about, well something has to be different when you're executing user or application code. Worse is when you actually go into the operating system or when your executing their trusted software that makes up the operating system. Well the hardware actually's going to do something else for us that processor that is executing actually is going to execute in different modes. Execution modes say either I am executing user code that is in our trusted, or I am executing system code. So the processor is actually aware of what kind of code it is executing. So one obvious thing we can do when it's executing user code for example the process I'm in user mode. I shouldn't be touching anything that belongs to the operating system or the trusted base that we have. The trusted computing base that we have. When I'm executing in the system mode. Then I'm allowed to access the code and the data that makes up the trusted system. So the hardware is sort of aware of what is going on at a given time. In particular what execution mode is it running in. The execution mode for this goes by a different name it is also called execution rings. So ring is sort of think about a privilege level in some sense. When you are in the system mode, you have the highest privileges and there could be many of these in fact several processor architectures support more than two. And I'm just talking with user and system. But, the innermost ring, where the trusted computing base resides, the most privileged. As you move out of that, you, for example, could move into the user ring. And then your privileges are reduced, because you can't access what can be accessed by being in the trusted computing base. So this execution mode that we're talking about, whether you're in system mode or user mode, you can also say you are in the ring that you reside in when you are in system mode versus the user ring, or mode. So it goes by different names, either execution modes, system, supervisor, a variety of these names. But the idea here is that the processor for the hardware knows that it's, you know, execution state is such the state indicates the mode that we're talking about. So it's either running with higher level of privileges, it can do certain things that it's only allowed to do when it's running the trusted code, or it's executing the non-trusted or untrusted code when it's in user mode. So again, memory protection but then this processor being aware of what its execution mode is, that's something else that we're going to need to meet the trusted computing base that we had identified for ourselves. So, if you are in different modes, we said you should be able to do more things when you are in the system or more privileged mode, compared to when you are in user or less privileged mode. Well, certain hardware instructions, they're called privileged instructions. They can only be executed when the processor is in the inner most ring or the most privileged ring or is in the system mode. Or ring zero it's called. Zero is the most privileged and you go out, zero, one, two, three and become less privileged. So privileged instructions are special instructions that you cannot execute when you are in the user mode. Okay lot of times these instructions sort of help you figure out what parts of memory you access for example. Or these are hardware devices that we have need to control. These kinds of instructions direct access to the hardware we said is not available to user code. Similarly what parts of memory you can access can not be manipulated at user level. If it could be then you can go access somebody else's memory. So those kind of actions require user instructions that can only be executed in the system mode. And these instructions are called privileged instructions or instructions that can be executed in high privileged level, a ring or more that you're in. So one of the ways in which we're going to be able to meet the requirements that we identified for a trusted computing base is by making use of a number of different things the hardware does for us. It has memory protection, it gives us these execution modes we're talking about. It also has special instructions that cannot be executed when we are not in the right mode. So, with this support from the hardware, we will see we will be able to do isolation or the tamper resistance that is one of the requirements for a trusted computing base. Okay so we already talked about system calls. In fact, I said think about the cost of a system call compared to a regular call. So system calls allow you to go from user mode to OS or the system mode. So let's talk a little bit more about system calls. First of all, they're used to transfer control between user and system code. Or execution that must happen in user mode and system mode. The interesting thing about these calls is that they cannot be arbitrary. These are a set of calls that user level code is allowed to make. Okay, this is defined by the application programming interface, or the API that your operating system provides. So, you have these calls, and these calls have to come into the operating system in a control fashion, okay. So we said these calls are different than regular calls. They have to come through what used to be called call gates, which is special ways in which you can enter from user to, it's a defined way, transition from user to system level. You come through these call gates. And what happens as a result of that is, well we know that it's a call, so we going to return at some point. So we have to keep track of where we going to return and the user code once the system call completes. But the processor execution mode has to change as a result of that. We went from user mode to system mode so the privilege ring or the mode is going to change. And we actually going to be able to, we'll have to change some memory mappings, data structures that keep track of those, because we going to be able to access memory now that we couldn't access before. So some registers have to be saved. Others have to be loaded. And things like that. So all that work has to be done. So now we are executing in a different protection domain. In fact, this used to happen through, sort of an interrupt or a trap into the system from user mode. But in x86, we actually have explicit instructions. This is different from your regular call return. These are execute a system call. That Sysenter enter the system or the operating system and when you're done you return by doing another special instruction called sysexit so there are these instruction that help you implement this system calls that we're talking about. Keep in mind that they are more expensive because now we're using, we're doing this work that we didn't have to. We have to check what kind of call it is, arguments that are being passed. Same information when we come back. We change memory mapping so we can access things that we couldn't before, use the special instructions rather than the regular ones and so on. So that's what makes it costlier or more expensive. Okay, so let's revisit this idea of isolation. When it comes to protection and security, isolation is actually your best friend. Okay, that means that I'm isolated from somebody else and they can't do anything bad to me. Untrusted user code has to be isolated from the system code, okay. We talked about trusted system code tamper-proofness that we're talking. You shouldn't be able to alter it and the only way you can go in to operating system is through system calls that are defined by the operating system and so on. So how are we going to achieve this isolation of user code? The reason I wanted to think about this is what does a processor do? It says fetch the next instruction. To execute it you need some operands that are somewhere in memory. You fetch those you execute the instruction and you keep doing that forever. But I'm using let's say running executing some user code. Why can't I say the next instruction I want to switch to somewhere in the operating system or the next data item I want to bring is a data structure that resided in the operating system? If you tried to do that we said you wouldn't be able to do it. Because we have this execution mode we're talking about, and things like that. So, we're going to use that hardware support to achieve or accomplish this isolation, separating, or separation, of user code from the operating system code. So let's see how we actually going to be able to do that. Well the way to do that is again I said. It's going to rely on the hardware to protect memory. So remember isolation I said the way. Executing in user mode or executing user code and at that point if the hardware can protect memory where the operating system is. What does memory protection mean. We said the process in which read write request or load store request or execute those instructions to fetch the next instruction or fetch the data that it's going to operate on. It's going to generate an address where it wants to do a read or write. Well, the hardware says, you're not allowed to access that part of the memory. Okay, hardware support is saying, it is part of memory that belongs to the operating system. Okay, that's what the operating system code and data is. If you're running in user mode, you are not allowed to generate an address and complete a read write in a memory location that belongs to the operating system. So hardware support memory protection essentially says, if the process happens to generate an address that is in the operating system, the hardware support we have for memory protection is going to stop that memory access from completing. Although it's generating an address, and it's a memory location, the hardware that potentially can do a read or write there but it currently knows that it's user code. And user code directly can't access the information that we have in the operating system part. So the hardware is going to essentially stop that access from proceeding. So hardware is going to sort of keep you contained in your space. And we get to that, it's not just OS user, but it's also across different user processes, but it's going to contain you within the area of memory that belongs to you. And if you try to touch something else. Which you can. It's going to stop you from doing that. And the hardware is going to do that. All this hardware support we have for memory protection is actually going to do that. So now that we have talked about the importance of isolation of user code and the hardware support with the process execution mode, memory protection, and so on. I think it should be clear that if you're the bad guy, and I did say that the operating system has the keys to the kingdom, the best thing you can do is actually compromise the operating system. We said well if using user code, it's hard for you to do that, because if you tried to touch something that belongs to the operating system, the hardware is going to stop you. So attackers actually have been really creative, because the operating system of the trusted computing base being such a big target, they have been really creative in terms of how can they gain access to the operating system and the information that it keeps or maintains to control access to resources. Now, I want you to explore these three different options that we have here and then we'll talk a little bit about them. So one thing, if your attacked, you can think about is, how can I actually get into this process when the operating system itself is coming to life. And the operating system, data structures are getting set up and information is getting initialized. How can I do some mischief at that point. I know once I start running user code, once the operating system is in place, the hardware is going to stop me from going to the areas where operating system cordoned data is, unless I do a system call. But can I get onto it in the early process, it's sort of what you would call infant mortality kind of thing. Attack it when it is coming to life. So the way to think about it is that when you actually power-on a system, a computer system, it runs a set of instructions. And those instructions are what is called firmware. So these firmware instructions actually run and the execution of those instructions leads to finding where the operating system is, where it goes, loading it and setting it up. So, I said get onto it early in the process. Well, there was an attack like that, which was demonstrated against Mac OS. Macs have this Thunderbolt interface. So what you can do is you can have a device that you have maliciously prepared that device, you can connect it through the Thunderbolt interface, the very early stage when the Mac is booting. So this is called bootware or Malware that you can inject at the boot stage. So, because the macs have this extensible thumbware framework, you can inject new thumbware by connecting this device to the Thunderbolt interface that we were talking about. And you can change the firmware, when it's running. You can actually run this malicious instruction from the malware that your malicious device has that you connected to the Thunderbolt interface. So there was an attack like this actually we are going to provide a link where you can go read more about it. And interesting thing about this attack was that there's no protection against it. The second one is actually really I said people get creative to go after the system. And this is another one that really exploited some property of the dynamic RAM that we have. So we talked about memory, the DRAMs, dynamic RAM, and the attack actually exploited, if you read some locations, the RAM locations, the RAM addresses repeatedly then the way it is refreshed and the charges that hold the value that stored in that location work. Actually this was demonstrated that by reading aggressively, reading again and again in certain locations you can actually flip some bits in an adjacent location. Okay if the bit flip happens to be the right way, in particular in a location that is storing information protection and information about memory, then you can flip it and make something that will be readable, you can also make it writable. We're going to talk about these protection bits, that you can only read certain parts of memory, but you can write also. So this attack, again, I want you to read and maybe the details. Here is not important. But sort of think about it by exploiting some vulnerability that is DRAM. The way these DRAMs are designed, someone is able to gain access to the OS part of the memory and change something just by doing reads. The operating system doesn't stop you because you just reading ertain locations. But those reads actually have this other effect. In particular, this undesirable effect where they end up changing a bit, flipping a bit in another location in the operating system. Which, sort of, again, I I should say that this is not easy to pull off you can obviously corrupt or affect integrity of that location but to change it exactly the way you want to do this idea of again privileges that you didn't have. For is quite challenging. Again this is reported attack. If your OS is written in low level language so it performs reasonable whatever it is. Typically not type safe languages. And if you're not careful you can have buffer overflow possibilities too. Buffer overflow means you can alter the return address. Maybe you can insert some code that you can transfer control to and that is going to lead to exploitation of the operating system. And you're in the operating system running your code, that hacker's code, and you can do whatever changes you want to do. Of course the isolation is lost in that case. So there are instances of attacks like this too. Okay, so I talked about memory, I talked about isolation. Transferring control from user to OS part of the system that we have, a memory where OS coordinator structure is. We want to sort of dig deeper into it and really think about how do we define what data and code a certain process or application can access. I said one of the jobs the operating system has is give you high level obstructions. Okay, so we don't talked about particular memory location that we happen to have right now. With each process we said sort of can believe as if it has the whole computer to itself. In particular, each process wants to feel that it has the memory that it needs, where its data and code is going to go, it has to itself. And actually, each process does have something of that sort for itself, and that's called an Address Space. Address Space is sort of a container. It's a sequence of memory locations. It's a collection, which is a sequence of memory locations, that define a collection of those memory locations, it defines a space, and each of this location can be address so that's what makes it an address space. because each process has an abstraction of memory that is an Address Space, an Address Space actually is going to be a unit of isolation, so when we talked about isolation. Now we're talking about sort of isolate what from what else, and we're going to talk about isolating Address Space. Address Space completely defines the data and code that is there for a given application. The data and code is going to reside in the bunch of memory locations and those memory locations together make up an Address Space that belongs to a particular process or application. So that's why we have to sort of [INAUDIBLE] solation. We have to dig deeper a little bit and think about the Address Spaces. So if a processes essentially going to think that it has the memory to itself, it's going to think of this memory as a contiguous set of locations going from some starting address zero to some max. And it's going to think that it has this set of memory locations, not only the contiguous, but they could even be more than the total memory that we have, physical memory that we have in a computer. If you done your operating systems class, you heard about virtual memory, which most operating systems actually support virtual memory. That could be more than the physical memory that we have. Address Space that we're talking about, when a process is a contiguous container of memory, okay? That can be addressed, which is going from zero to some maximum address that we can have and it doesn't have to be constrained by the physical amount of physical memory that we have. So the address space actually usually, it it's a 32-bit architecture, basically means addresses in that case are 32-bit long. Or it could be 64-bit architecture. In that case they'll be 64-bit long. They go typically form zero through two to the third minus to minus one, or two to the 64 minus one. Your addresses and number bits, and based on that you can have two to the n different locations that you can address with that size of an address. We said that Address Space is a contender for anything that has to be stored in memory for a process. Remember, process has to fetch instruction data, so that information has to be in memory before we can execute it. So we have to prepare an Address Space for a process before it starts execution. And we tell the process, here is an Address Space for you, you can address the location zero through something, and you decide to put your data and code in these locations. Okay, so lets look at what happens, I call this Logical Address process. So we always talk about physical addresses, those are addresses that point to locations in physical memory. Locations in address space are logical its a logical or virtual address because it's going to map to, okay so this mapping that we're talking about here, it's going to map to some physical memory location. Eventually everything has to be stored in physical memory. So we can say logical address 2000 in process one's address space is currently lives in physical location 5000 or something like that. So process the address space that we're talking about and the addresses that make up that address space, zero through I said two to the n minus one, n is 32 or 64, these are called logical addresses. This is in these logical addresses, or the logical space that we define for a process. The process is going to put it's code, and it's going to put it's data. So, when we talk about an address space as a unit of isolation, so that if you start with this process had its own container of memory where it can address different parts of it in particular different locations where it's going to place its code and data, and these are logical, or in case of virtual memory, virtual addresses. Eventually, remember the operating system has to use physical resources to implement these abstractions or virtual resources. Well address space is a virtual resource that abstracts memory. So this address space or whatever that we have here, the information that we stored in this address space, eventually we have to use physical memory, okay, that's RAM or physical memory, to actually implement this abstraction we call an address space. So you enter the logical address, you say I want to access logical location 2000. We have to say well that currently lives somewhere in this physical memory we have here. So this code and data eventually has to be placed in different places in the memory that we have. This address space has to be backed up, physically has to be supported with the physical memory that we have. And this physical memory could be multiple processes having their address spaces concurrently being or existing in this physical memory that we have. So one of the things, isolation we're talking about is that if I'm giving up some physical memory for address space to do process A, and I give some physical memory to another process B, it then uses that memory to store some of its stuff from its address space. I have to isolate the two address spaces, I actually have to isolate both physical memory can process A access versus what physical memory can process B access. So the operating system has to do that. I think isolation, the way to think about this is when process A is running, it's adding space is in some parts of this physical memory, it should only be able to go to those parts. When process B is running, it should be able to go to only those parts that are currently allocated to it. And they never run into each other unless they choose to share, which is always sort of an exception. So when you talk about isolation, think of an address space that's what a process thinks it has, implementing that address space by storing pieces of pieces of indifferent parts of memory, and then making sure that the process can only go to physical memory that belongs to it. And that's how we get this isolation. So one of the things we want to do is how exactly does it work, so to get some sense of that. Address translation process. And the reason that translation has to happen is because process thinks it has this big address space, contiguous going from zero through some x. And then we said, well the physical memory, one process may not have all of it at the same time. Or physical memory maybe much smaller than the address space size itself. So logical address or virtual address is actually going to be different from the physical address that we're talking about. So this is actually, if you think about it, this is a logical address here. And addresses that we have here are going to be addresses into physical memory. Right, that's your real memory that you have in the system. So what we're going to do is, we're saying, well, this part is going here. That part is going here. So, maybe 1 goes at the top here, maybe 3 goes down here, and things like that. We said we have to take data and code that is in the address space, and we have to place that into physical memory, okay? The way we sort of do that is, we don't want to do it for each byte or each word, because we'll have to keep track of what's going where. Okay, so one way to scale that process is is that we divide this address space and the memory into some size chunks. Okay, so if we do paging, it's divided into what's called pages. A page typically may be 4 kilobytes, for example, 4K size. So we would say, well, the address space really is a page number and displacement within that page. So this is page 0, and if your location 10, then it's page 0, 10, okay. So 0 is the page, and 10 is displacement within that page. If your address happens to be let's say, 4K plus 3, then it's going to do a second page because second page starts at 4K. First 0 through 4K minus goes in first page, and then the second one starts. So then we'll say it's page 2 and maybe displacement 5 or 2 or whatever it is. So the logical address you can think of that as a page number and a displacement. Essentially a page table says, this, it is a mapping isn't it? So it says this logical page is currently stored in this physical page. Okay, so if I want to think about it, a page table, essentially, a page table we have all these entries. And it's logical base 0, 1, 2. We're saying this may be a 3. This may be at 5. This may be at 8. And maybe 3 is currently not loaded. Virtual memory, not everything is in memory at the same time. So this a page table that actually we build, and the operating system is responsible for actually building this and protecting this. Remember, at this point, sort of a flow that we should understand, it's really important. A process, maybe executing in user space, generates an address, okay, saying I need to fetch the next instruction. That's where my instruction point or program counter points, or an operand, or whatever it is. And I need to access this memory location. It sort of starts with a logical address. The system now says, well, where exactly is this information that the process needs? We need to locate the physical address to which this logical address maps. For that, what I'm going to do is I'm going to take the logical address page #, and I'm going to look up this page table. So this address translation requires that you access this page table, and there are ways to speed this whole process up. So it's saying if page number is 3, or let's say the page number is 2, then I go down here and I say oh, it's in physical memory page 8. So the address that we have, if it's 3 and some displacement, d, could translate to 8 and the same displacement. So it's say memory page 8 and go dig down d locations or d bytes in that. This is what is called the address translation process. So an important observation here is that we can only access physical memory that is reachable through this page table that we have for this process. I should say page table a lot more complicated than multilevel, things like that. But let's just sort of stay with this basic idea of how they help us get this isolation which is important to us because the process has to start with an address. Address has to be translated. Translation is only possible through this page table. So whatever the page table points to is all a process can access. Another observation here that we should be clearly understanding, is, the page tables have to be maintained by the operating system. They have to be managed by their trusted computing base because whatever goes into the page table, we can access that. So the operating system should make sure that whatever goes in the page table of a particular process is only memory that belongs to that process. So, that's the address translation. It gives you a little bit more information about what part of memory a given process is able to access, and it relates to this isolation thing that we've been talking about. So this is this address translation, and what you can access in memory is really powerful. This is how we're going to be able to protect the data and code of a given process. And the way we're going to be able to do it is, the operating system, I said is responsible for managing those page tables you're talking about. Remember you always start a virtual address. So process is going to start with virtual address, which has to be translated to physical address that should fall in a physical page that we're talking about, physical frame. So the operating system will not map a virtual page of a process A, but remember you're going to start with a virtual address that has a virtual page number and displacement within that. It will not map a virtual page of process A to a physical page that has been given to process B. Remember these pages are reusable resources or time to get assigned to different processes so that currently a certain physical page has been assigned to process B. The operating system is not going to set up the page table of process A, such that a page table entry in that process A's page table, can point to the space that belongs to B. That's our protection isolation whatever you call it. We have essentially partitioned the physical memory that we have, among the different processes we have in the system. And we limit these processes to only the portion of memory they have access toward a given time. By making sure that they can only reach those portions through their phase tables, and the operating system is again going to manage their phase tables. The trusted computing base you trusted to do this right, so we keep a different processes separate from each other. Which is what results in getting protected from each other and process A doesn't trust process B. So as I explained process A cannot access process B's memory because it has no way to read the memory that belongs to process B. This will work as long as this page table is correctly managed. And as I said before they're managed by the operating system which is trusted so let's hope they are correctly managed unless somebody finds a way to attack and successfully compromise the operating system. So protecting processes from each other and protecting the operating system from untrusted process code, same exact mechanism. The page table, when you're running in user mode, doesn't allow you to go to any physical pages that belong to the operating system. It's really done through this memory management that we just talked about. The memory management is important enough that we actually hardware wired support for it. Typically, you have what is called a memory management unit, or MMU, that helps you take those logical or virtual addresses and resolve those into physical addresses efficiently. Except we had to go through the page table then to access memory location, you have to access page table entry and that adds overhead. So the hardware actually can store some of that in things called TLBs, or translation lookaside buffers, that are associated memory that allows you to do this mapping fairly efficiently. Memory management is translation, and isolation, what you can access, and all that, there's hardware support for it. So there's another interesting thing that we should be able to think about here. So we have these units we call pages or frames, and we have this table we call a page table. And we said we go through a page table entry to see where our logical page currently lives in physical memory. So that page table entry tells us where in physical memories. But we can have some other information along with that. We can, for example, have whether this page that we're trying to access is for reading only, is it for writing, or it's for execution only in case it happens to contain instructions or a code. So you can actually have this kind of protection information on these pages, in the page table entry, that can limit the type of memory access you can do. So we said protection sort of, now we are saying there are two parts to it. First part is, where in memory a given process is allowed to go. So where can its logical addresses be translated to in physical memory. But now we are saying another level of protection is, in what manner can it access, even if it's allowed to access a given location in physical memory. Is it reading only? Is it writing only? Is it execution and things like that? So the way we do that is in the page table entries we keep this information, this protection information about the ways in which memory can be accessed. And that is also used when you're trying to do, let's say store, and the target address is write protected. You can't do that, that instruction would not be allowed to go through by the hardware. Well all this talk about memory protection and pages, and read write execution informations, and things like that. Probably a good point when we can revisit this Stack Overflow quiz we did when we talked about software security. So Stack and we exploited in a variety of ways. So the first question here in this quiz, is go back and think through how we could exploit the Stack if we don't have memory for protection. And then we're going to come back and see how memory protection mechanisms that we just discussed can actually help you stop that kind of exploit. So the first one is just how do you exploit the Stack. So remember what happened when we're talking about smashing the stack for fun and profit, you overflow the buffer and go down where the return address is stored and then you change that if you going to alter the program execution. So actually had to do two things, you had to change the return address and then you had to send the return should happen to your shellcode where the exploit code is. So you overwrite that, the return address, so this is going to be true. It's correct. The second says pushing data onto the stack to overflow the stack into the heap. The heap and stack are sort of separated, the buffer overflow, although the heap overflows and stack overflows, but going from stack to heap is not what we discussed, so that's not correct. And popping data off the stack doesn't do anything for you either, because you need to actually alter the return address and go to exploit code. Neither of these two, second and third options are going to let you do that. So, the way to think about this is overflowing the buffer to change the return address. So essentially what you're doing now write into a location. So keep that in mind. Where is this location? On the stack. So all the memory protection stuff that we have talked about, how can we use that to prevent malicious code execution on the stack? Okay, remember, you're going to alter the return address, then you're going to insert some instructions where you're going to transfer control and things like that. So one of the things we're going to do is, what if we have a non-executable stack? We actually talked about that's one of the protections for code injection, because you have to inject your code when you overwrite the return address to point to this code. And then you execute that code, that's how a successful exploit works. So now think what we can do if we made the stack non-executable, right? So this is important, non-executable stack, which means you could go right into it, you can store some instructions into it. So in some sense you're able to inject the instructions onto the stack, but you will not be able to to execute those, okay? Because remember we're talking with address consolation and space level entries, and read write execute permissions in each of those pages that you can access through the space level entries. Well, if we say it's non-executable, non-executable is you can only read or write [INAUDIBLE] location when you want to fetch an instruction off the stack. This is your injection code that we are talking about. You can't execute off of that, so setting the permission to non-execute actually is going to prevent malicious code to inject code on the stack, and use the buffer overflows to transfer control to it, and successfully exploit a program by doing that. A simple thing we can do is, you can't execute off the stack, so your injection code can't be on the stack. So, this sort of protection, actually more than operating systems, make the stack non-executable. So, the way you understand that is, this protection comes because that portion of the address space where the stack lives. The page table entries that are used to translate addresses from the range of addresses with the stack is, those page table entries have execution permission turned off. So you can't execute off the stack because it's going to require that you fetch an instruction from a certain memory location where the execute permission is not there. So memory isolation protection, that discussion we've been having, gives us a simple way to protect our execution on the stack by making the stack non-executable. And that avoids or prevents this malicious code to use a buffer of overflows and use code injection on this stack. Okay, so one nice thing about the memory protection stuff we've been talking about is that it could be used to create these fences or isolate these different applications from each other. I was talking about process A cannot access memory that currently belongs to process B. But the same mechanism actually could be used to isolate the operating system. Remember, we wanted to tamper resistance for the operating system. You shouldn't be able to alter from untrusted application code, so we can isolate the operating system from untrusted application code using the same memory protection mechanisms that we were talking about. So the way we do that is we're going to say the operating system, also called the kernel, is going to reside in a portion of each processes address space. So the address space in which the process executes now has two parts. Where the user code and data is going to go, and where the operating system is going to go, operating system code and data. So the OS resides, and that part of the address space is common across different processes, is going to be a portion of the address space. So think about the address space. There's a user portion, a user area, and then there's the OS, or kernel, area. And this holds for each and every process that we have in the system. And remember that we talked about system calls and things like that. Whenever you want to go from the user portion of the address space to the system portion of the address space, that transfer has to be special and handled by the operating system. It can only be done through the system call that we're talking about. So one way I want you to think about it is that, but so this is the address space that process A executes in. Then we said there is this fence or boundary. The OS goes here, the user code goes here. And when you are here, when you're executing this portion, you're not allowed to access this part of the memory. Remember the execution mode that we're talking about? So the hardware is going to save you in the user mode, and you're asking for an address here, that's not going to be allowed. So the way all operating system is isolated from application code, which is untrusted, is essentially putting those in separate places, and making sure when you're here, you can't go into the operating system. Well, we said that call gates are controlled entry points through which you can come in, but then we're coming in by making a certain a call, the operating system understands, and the operating system is going to do whatever checking it needs to do. And then you're going to be executing in the operating system while you access anything here. And when you're done, you're going to return back to the user level. So we use the same memory protection mechanisms you're talking about, base tables, protection bits, what different base tables we have for different processes for the operating system. So, remember, one of the things that I can't overemphasize is the operating system, or the trusted computer base that we have, has to manage these page tables because page tables control what portions of memory you can access, and in what manner. And the operating systems says, this is your code, you can execute it, but you can only execute. Or this is your stack, you can read/write it, but you can't execute from it, that's a non-executable stack, and things like that. So this is how the operating system is going to actually protect itself from untrusted process codes, and also allows these untrusted processes protection from each other. So process A can't, for example, corrupt the memory of Process B because the way the address translation and the protection mechanisms work. Let's consider a more concrete case. Consider some of the operating systems we currently have or had in case of MS-DOS and Vanguard. So what exactly I said in the OS portion and the user portion of the address space, how does it work in these systems? So for example, if you look at the address space layout for 32-bit Linux, lower three gigabytes is where the user code and data is going to go. So address space is going to be four gigabytes long, with 32 bits. So we're going to separate that and say one gigabyte for the kernel and three gigabytes for the user. The lower three gigabytes, that's where the user code/data is. The top one gigabyte is where the kernel goes, and there's this fence between these two. The kernel portion can only be accessed when you are in the operating system or in the kernel. Another thing to keep in mind is that these portions of address space that we have, access to those address spaces also is related to what execution mode or privilege ring you are in, so x86 has the zero through three. For you to be able to access the kernel portion, you have to be entering zero or in system mode if you are executing user code in ring three, you can't access this portion of the address space that we have, which is the one gigabyte. So this connects back the hardware support that we have and the address space like we have been talking about. So these other operating systems, depending on what particular version, may have some differences, but typically this is how it works. The operating system is common, so this kernel one gigabyte has the same data and code for every process, but the three gigabyte is for process, obviously. This could be different for different processes. And we said, if you're executing this part of this three gigabyte, then you are in ring three, if you're executing in the kernel, you are in ring zero. This how operating systems manage access to various part of the address space. So modern operating systems actually separate the trusted computing base from untrusted. It wasn't always like that. If you look at MS-DOS, and actually now it's in a museum, Microsoft even, the first version they made before is called Available, actually, written in assembly language, it didn't have the separation that we're talking about. There's no sort of fence or wall between the user and system code, so any process could actually alter the operating system, because you can freely march down into the OS space. And if you happen to be virus, you obviously could hook anything, including here, DOS interrupt handlers actually are always in the operating system. We are talking about dealing with hardware device, and direct access to hardware it through privilege instructions, not from user level, those instructions have to be privileged. Though, here you could actually change the kernel, change the interrupt handlers, there was no protection that modern operating systems provide through this isolation of the operating system from application code So this is just, in picture form, the address space 0 to 4 GB. This is the mode 3 GB we're talking about, which is the user space, user core and data and it can be accessed in user mode, or the ring that corresponds to user mode execution. And the high 1 GB has to be kernel mode, and that's where the kernel data, and kernel code goes. So, this is just the split that we're talking about here showing, drawing the address space starting at 0 going to the max, which is to the 31 minus 1. Okay, so we're ready for a quiz now. We talked about sort of system mode and user mode or previous level that is needed when you're in the kernel, and user mode. So in this quiz we're going to talk about a couple of different functions. These functions could be executed either in the operating system, so you have to figure out if this function belongs in the operating system, or it could be executed in application code that is running outside of the operating system, and that would be in user mode. Depending on what this function is, if it can only be carried out in the operating system, which is kernel mode, then you should check operating system. If you think it doesn't belong in the operating system and can be done at the user level, then you should check user. Okay, so there are four different functions In this quiz that you are to think about. And for each, decide if it's OS, system mode, or user mode. Goes in that system portion of the address space, one we provide at the top, or can it be executed in the user part of the address space? Okay. So the first one is switching CPU from one process to another when a process blocks. When you go from one process to another process the address spaces are going to change. Yes. The address spaces are going to change because the incoming process now is going to use its own page table to access its data and code. Where can we change address spaces? Remember as soon as you change address space, you changing what portions of physical memory can be accessed by whoever is executing. Address space change always has to be in the operating system because that's how we protect memory. Okay the operating system manages what page table currently can be used by the process that has the CPU and is currently executing and so on. So that has to go in the operating system. Page fault handling we didn't talk about virtual memory that page fault handling comes in virtual memory. So if some virtual page is not currently in physical memory then the page table entry is going to say it's not in memory. It's not a valid frame that this logical or virtual page maps to. In that case, we have what is called a page fault. We have to bring this from disc, or swap area, or whatever it is. Find some free space in memory, and then update the page table of this process to point to this new page in physical memory where this information was just brought in. So if you update page tables, again you have to be in the operating system. If you could update a page table from user space than you can update with a different frame number or physical memory, some other place. And then you can go there, even if it doesn't belong to you. So in user mode you should never be able to update page tables, and that's why it belongs in the operating system. The third question we have, in changing who can access a protected resource such as a file? Changing permissions. I can't do that at the user level. If I can change permissions for files that belong to a different user, Alice can do this for Bob's file. Well, she could go access Bob's file even if Bob doesn't want her to access it. The way we should do it is, go to the operating system, request that can we have access to it? If Bob has done something that allows Alice to gain this access. That's fine, otherwise the operating system is not going to add this permission for Alice to the file. So, this is another function protected resource accessed to it, the operating system should do it. Last one, setting up a new stack frame when an application program calls one of its functions. We know that happens at the user level. If we call functions back and forth, every time we call a function, a stack frame is set up and that doesn't require intervention of the operating system. So this could be done at the user level. Okay, so keep in mind that memory protection isolation relies completely on how page tables are managed. Page tables are used to implement address spaces. So management of page tables, whether we update, when a page fault occurs, or switch them, all that has to be done in the operating system. That's the only way we can get the isolation that we're talking about. We had three requirements for a trusted computing base. Isolation or tamper-proof was one of those requirements. And we've been talking about isolation and memory protection. All that is done to get that tamper-proof requirement. Untrusted code cannot be altering the operating system data and code. So that was all to do with meeting one of those requirements, which is tamper-proof. The other requirement is complete mediation, that you should not be able to bypass the operating system and go directly to a protected resource. So how do we implement complete mediation? Well, the way we do that is we make sure that no protected resource, whether it's a memory page or a file, could be accessed without going through the trusted computing base or the kernel. Okay, so the TCB is also the kernel. The way we do that is by making the trusted computing base, or having it act as a reference monitor. Any time you have a reference for a particular resource, the TCB has to come into the picture before you can get to the resource, okay. The reference has to be monitored by the trusted computing base. So there should be no way for you to bypass, where one way you can get around complete mediation is by that bypassing the trusted computing base, there should be no way for you to bypass this. So how is that implemented? Well, we sort of used the same tricks that we've been talking about so far. If you are executing user code, you are in user mode. You're not in system mode. And we know that protected resources are implemented by the operating system. And so that data structures, the information that is necessary for us to access those protected resources lives in the operating system part of the address base. And we know that while you're executing the user code, you can't access that. You have to switch to the system mode through a system call. And since you can't directly access it, you don't have the information that is necessary for you to access a resource without going through the operating system. Which helps us with this complete mediation property we have. In addition, the user code can not access physical resources because often times they require execution of privileged instructions. We're talking about service interrupts and things like that. There's no way for you to get to a disk lock. Or change the register that points to a page table, because those are instructions that are privileged. And if you tried executing them in the user mode, your execution would be aborted. You won't be able to continue that. So complete mediation, again relies on the same hardware mechanisms we've been talking about where the isolation mechanism we discussed the idea of privileged instructions that we discussed. Complete mediation actually comes about, or we have that, because of one other thing the operating system does. So at user level, we have what we call virtual resources. We don't have physical resources. There is no way for user code to actually name or be able to target a physical resource, or ask for a physical resource. There you have only virtual resources, and the operating system actually gives you an API in how those virtual resources can be used. An example of this is that for storing persistent data, we have file abstraction, or virtual resource for storing persistent data is a file. So you don't access a disk lock. The operating system does disk IO, and schedule requests to read the blocks, and services interrupts, and things like that. It can interact through the disk controller. Through those instructions we're talking about. But you cannot from user space. You can only ask for access to files either opening them or reading them or writing them. And things like that. So at the user level, we don't have ways of directly talking to the disk controller, the low level hardware functions that operating system has access to. We only have these virtual resources. As a result of that, remember virtual resources are what the name says. They are not the real resources, they have to be implemented using physical resource. The file has to be implemented with a bunch of disc locks where the file data is stored. So the virtual resource has to be translated to physical handles or resources. For example, buffers or file data is stored on things like that. And this translation is done, from virtual to physical, implementation is done by the operating system. If you want to access a disc block or a phase of memory, you have to start with either a file descriptor, or you start with a logical address we're talking about, or the virtual address. And the translation from virtual to physical is the first step before you can get to the physical. Only the operating system knows how to do that. Or has the metadata, or has the control information, that can be used for this translation. So this level of sort of indirection that we add where you have one kind of namespace and the underlying resources have a different namespace. And the translation can only be done by the operating system, or the trusted computing base. Well that helps you get complete mediation, because you first need to go have the translation done. Okay, then you have to go to the trusted computing base, so the complete mediation gets implemented right then and there, because you're coming to it, and asking for a physical resource. So it could be the reference monitor. It can check at that point, that whatever that you are asking for, do you have access to it? So, talking about virtual resources, there's a lot of talk about virtualization. We talked about cloud computing, and virtualization and things like that. Once you sort of have these virtual resources and implement them using whatever underlying physical resources you have, so there's no end to what kinds of things you can do. We said we had to go through this translation, and someone, maybe it's, but that can be done at many levels, so I thought we'll talk a little bit about virtualization, and the context of the security and protection is our main interest here. So we're not talking about virtualization for the sake of virtualization, but how does this concept actually relate to isolation and relate to protection of resources and securing them against unauthorized access and things like that? So, main motivation for virtualization comes from the fact that operating system is large and complex. And the different operating systems, different applications may require different underlying operating systems. So one use of virtualization commercially is actually to not be limited to just one operating system and be able to run multiple operating systems. But as I said before we going to look at value of this idea when it comes to protection and security. So, remember one other thing when you have a single operating system supporting all of your applications is that if the operating system is compromised, then it's going to impact all applications. Every application that you have running in that system is going to be affected by this compromise of the operating system because they all have this one underlying common operating system. Our picture was, we have all our apps, then we have our operating system and the hardware. If something goes wrong here, of course, all of this is affected. So I quickly want to sort of introduce how virtualization actually helps us in limiting the damage that can be caused by a compromised or hacked operating system. Okay, we said, maybe you want to run more than one operating system on the same underlying hardware but this is going to help us with security as well. So, limiting the damage of a hacked operating system. Let's see, how can we do that when we have virtualization? Okay, the way we do that is we change the picture where we had the hardware and the operating system, well we introduce something between the hardware and the operating system. And that is called a Hypervisor. On top of the Hypervisor, we support what is called virtual machines. And these virtual machines have their own operating system. It's called the guest operating system. And the guest operating system then can support a number of applications that run within that virtual machine. So if you, we look at our earlier picture, we have the hardware, then we have the Hypervisor, then we have this virtual machines. So this is the OS here and the application here. Compromise of an OS here only affects these applications in this virtual machine. It doesn't do anything to the application that may be running in this different virtual machine. With the same operating system or a different operating system. What we really have is isolation between virtual machines. So earlier, we had isolation between processes or applications, and they all shared the same operating system. Now we have isolation between different virtual machines. Each has it's own operating system or guest operating system. So I can do my package in one virtual machine while browsing potentially dangerous places on the web in a different virtual machine. Anything bad that happens in the second virtual machine, or VM2, should not affect your tax stuff that you're doing in your first virtual machine. So the tested computed base really here is the Hypervisor. In the picture that we had before where we had the hardware and the trusted computing base and then the application. It changes with virtualization where we have the hardware, Hypervisor, and then the virtual machines and those virtual machines have their own operating system. So here is a good picture of how virtualization is related to security and isolation and things like that. This is your physical hardware. On top of that, we have this hypervisor. It's also called a virtual machine monitor, or VMM. And on top of that, we have this different VM. So, this is VM1. This is VM2. This is VMn let's say if you have one of n of those. If you somehow compromised this operating system, it only effects the user applications we have here. It does nothing to the user applications we have running in other virtual machines. This is what we're saying, is your trusted computing base. As long as you can trust this, the isolation across virtual machines is going to work. So, compromise of this operating system is not going to affect these other virtual machines and the applications running in those. For us, virtualization is important because our focus is in security and you see this isolation of these applications. Earlier they all shared the same operating system. Now they only share a Hypervisor. Why might that give you better security? We're going to talk about that next. Okay, so why virtualization in hypervisor may give you better security, we're going to get back to the third requirement that we have for a trusted computing base, which was getting it right or having the correctness property. And correctness is really important because compromise of that trusted computing base means attacker has access to everything. Remember our keys to the kingdom statement that we had before? You compromise the operating system, you gain access to all the physical resources, every memory page, every block on the disk, whatever else is coming in out of this machine and so on. So compromise off of TCB is really bad and so we don't want that and that's why we want the correctness requirement. Unfortunately getting the trusted computing base, meeting this correctness requirement is extremely hard. Operating systems are complex, there's all kind of concurrency, resource management, handling, managing devices or dealing with devices that we have lots of them and things like that makes it complex. And we know complex is not good for security or for correctness. So one thing you can do is make the trusted computing base smaller. Hopefully, if it's smaller and simpler, then you have a better change of getting it correct. So the third requirement of a TCB. You can see the connection with virtualization. Hopefully, the hypervisor is a lot smaller than the operating system. Because all it should do is partition the physical resources across the various virtual machines. And a lot of the guest management of how those resources get used by the applications can be done by the guest operating system. The details right in the main point to take away from here is that hypervisor could be smaller and simpler. And if that is true then securely or having correctness of that, for that obviously going to quote it securely, is important with the operating system because it gives you access to everything. It's written in languages and not type set, so keeping that program smaller and simpler is going to help you get it right and have that correctness requirement that we're talking about. So, with a hypervisor, you may have a better chance than a full fledged operating system. So I think we going to finish it off with a quiz. And that's going to bring back sort of all the TCB, trust with the competing race requirements they're talking about. And give us a chance to think about them. So the first one is saying an attack that exploits a vulnerability in an operating system turns off the check. The access control check, that we're talking about. That determines whether the target resource or the resource that's been requested. Access to it should be granted to the source of the request or not If you turn that off, which TCB requirement are you violating? Well, you actually go into the operating system, so it's still mediating. You have actually turned off the check. I'm going to say that you tampered with the TCB, so it's the tamper proof requirement has been violated. As a result of that, it's no longer functioning correctly. So correctness is not going to be there, but I think the reason that the violation that we first run into it is, that it's been altered or tampered with, because we have turned off this check. So the answer to this should be the Tamper-proof. Next question is related to correctness, and we know size and correctness don't go together. Larger the size, harder it is going to be for us to do it correctly. So, we sort of go back in time, saying, start with MS Dos, to the more recent Windows operating system, they offer richer functionality. They are much more complex as a result. So the question is really rough estimate of the multiplier, the lines of codes. I'm going to assume that lines of code in some sense capture the complexity of a system and the larger the system is more complex it is, more lines of code it's going to have. So, multiplier is x if recent Windows OS is x times the number of lines of code and the recent Windows operating system is x times that the lines of code we had in DOS. So, how much more complex has this system become? 100 times? 500 times? 10,000 times? I think if I remember right, the first version of DOS was about 5,000 lines of code. I said assembly code, but about 5,000. Windows currently is running in the tens of millions range. If you look at more recent Windows-based operating systems, 50 million or whatever it is, we're talking rough estimates here, so it's tens of millions. So if you go from 5000 to tens of millions, the complexity has grown by a factor of 10,000. So one of the things people say, why do we have these vulnerabilities? Well, it's complexity, and it's complexity because we have all these fancy features, and richer interfaces, and APIs that are more general, and things like that. And that adds to the complexity. So it relates to correctness. Getting operating systems correct is a challenge because they're complex. And here is an example of how the complexity has grown over time. So when you have virtualization, of course, the trusted computing base becomes the hypervisor. That comes between the hardware and the virtual machines. So one popular open source hypervisor is Xen. So this one is saying, we know that Windows is millions of lines of code. So it's saying well, in compared to that, if you have a hypervisor. How small is it? What's the rough estimate for the lines of code that the Xen hypervisor has? Well, 10,000 is kind of too small but it's not quite a million. The answer is 150,000. That's relatively small for something that has to essentially take the hardware resources, do the direct management and make those available to the various virtual machines and so on. So this Xen is about this size, which is relatively small compared to tens of millions that other operating systems. Their size is in those multi million numbers. So we saw that the operating system is really the trusted computing base, and it has to be isolated from untrusted code. We saw the hardware features and how they could be used to achieve this. We saw how hardware support sets its process execution modes. And memory protection are used to meet the complete mediation and tamper resistance or tamper-proof property that we require from a trusted computing base. In the last lesson, we saw that an operating system is really a reference monitor. Every time a request is made for a particular resource, it must say, what is the source of this request? Authentication helps us answer this question. In particular, it tells us on whose behalf a process is running that is making a request for the resource. We will explore multiple authentication methods and how they can be implemented in a computer system. We can understand the importance of authentication by going back to the discussion we had about what a trusted computing base does. So we have resources that need to be protected and we can do that by having a trusted computing base that is a reference monitor. Every request that comes for a resource has to be monitored. And when you monitor the request, the question you have is, should this request be able to access the resource for which that is the target of this request? To answer that question we really have to establish who is the request coming from. The source of the request is, we're going to be able to identify that because we have authentication in the system. So let's look a little bit more closely at what authentication really is. Authentication, we said we have to establish the source of the request. So we have to ask the question, who are you, if you happen to be the source? And of course, you can't just claim to be anybody. When you're going to claim that you are Alice, you have to provide some evidence to us that is going to convince us that you really are Alice. So authentication is knowing the identity of the source and establishing that it indeed is that person who is making that claim about the identity. So once we establish the source of a request, convince ourselves that the user is who he or she claims to be, the next part is authorization. So in authorization we're really establishing whether the source of the request does have the permissions necessary for the resource that they want to access. So this permission check is the authorization process and once authentication and authorization is done, of course then we allow access to the resource when permitted and the source of the request is able to make use of the resources. So let's dig a little bit deeper into this question about what is authentication? We just discussed that the operating system or the trusted computing base, OS is that plus more perhaps. So the OS of the trusted computing base needs to know who is making a request for the resource that is protected. So access to that resource has to be secured. So we know that in a computer system a request actually comes from a process. This process may be running one of your services, your browser, a mail client, whatever it is. So processes making the request, but we know that processes are run on behalf of users. So a given process that is making a request must be running on behalf on a certain user. We call the user also a subject or a principal. These are the active entities that actually initiate request or cause actions. So these terms are used interchangeably, but we'll stick with user. So process runs on behalf of a user. Authentication, which is our topic, essentially is going to help us answer this question that we have, is that if a process is making a request to know who's making the request, what user is making the request, we must answer the question on whose behalf is the process running? So the requesting process makes the request by making a system call. It comes through the operating system, but on whose behalf is it making that call? To establish the user on whose behalf the process is running, of course we have to start with authentication. In the beginning, when a user comes to the system, the user is going to authenticate himself or herself. So that's how we, for example, start a login session. You login to the system, or start the session that's going to launch one or more processes that you need to run, and then in the end the session is going to be over. So when you start the login session, prior to launching the process that is making the request, of course you're going to make a claim about your identity. You're going to tell the system, I am Alice, and then the system is going to ask you, well give me some evidence so I can believe that you really are Alice. And when that evidence is provided we need to verify it. The verification actually tells us that the user is actually who he or she claims to be, and then that user launch an application or start a process. And that process is then going to have, sometimes we say credentials of this user, because this user was authenticated. In other words, those processes are going to run on behalf of that user that just authenticated, and whatever resources this user is able access, those processes will be able to successfully gain access to those resources. So really, requests when we talk about, do come from processes, but processes run on behalf of users. And the system knows what user a process is running on behalf of because of authentication, because that comes before the process gets launched and makes the request. What are some of the goals of the authentication process itself? So remember, authentication in a nutshell is a user convincing a system who he or she is, so using some technique or some method. So obviously we're talking about the user associated with a certain identity. Username for example, wanting to authenticate herself to the system. Whatever authentication method we use, what do we desire from it, what should be its goal? First of all, when a legitimate user tries to authenticate herself, the system can demand some evidence but when the right evidence is provided, the system should allow the login to complete successfully. Well, that is called availability, okay? The system is available to the user who's able to provide the right evidence to support the claim, for example, the user is Alice. So the system should be available to the legitimate user who is able to provide the correct evidence. In other words, we don't want to have any false negatives. So let's just spend a little bit of time on what do we mean by this phrase false negative that I have here. So negative refers to the outcome. So if the authentication process is successfully complete, that is a positive outcome. A negative outcome is when we deny the authentication request or the login request. So if, for some reason, believe the evidence is not right or whatever it is, so we're going to say login unsuccessful, for example, or authentication unsuccessful. If it's the right user providing the right evidence and the system still has a negative outcome, well that is incorrect. In fact, that outcome we see is not a correct outcome, that result is false. The user was the right one with the right evidence. Whatever method we're using, perhaps, has a problem and because of that it's not able to allow the user in. So if the outcome is negative and that's done incorrectly that is called a false negative. So false negative basically means when we incorrectly deny access or incorrectly decline an authentication request, we don't want to have that. We have an authentication method, we don't want it to deny authentication requests for right users who provide the right evidence. Well the other possibility is the bad person, Eve, is trying to impersonate her. So Eve walks up to the system and says I am Alice. So it's the user principal that is associated with identity, which is Alice. It's really not her and it's Eve, so the user or principal who is not associated with Alice identity, we don't want that person, or Eve, to be able to login. Well, if you had a guarantee, then we have authenticity in the authentication process. If we authenticate someone, we know for sure that it is that person, so that's the authenticity part. And another way to sort of think about this is that we don't want to have any false positives. So positive here refers to the outcome of the authentication process. Positive means we allow someone to login or authenticate. And false means we actually make a mistake. We do that incorrectly. If Eve is able to login as Alice, that is not right. That is incorrect. So that's a false positive. So we don't want to have any false positives. The second goal that we have for authentication is that the right user with the right identity and evidence should be allowed in. That means we have no false negatives, and somebody who is going to impersonate a user of the system should not be allowed in which means we should not have any false positives. So these are the goals of a good authentication method that you would want to implement in your system. Let's look at our authentication quiz. You should check the answer from the choices that we have here. And this question really is talking about a lot of devices that we have. Those are our personal devices. So the device, it's your smartphone or your laptop. And they're not shared between many different users. Even in this case, we do use authentication. So in a smartphone you may have a PIN or a pattern to unlock it. On your laptop you may have to type a password to gain access to it and things like that. So if it's not shared and we don't have multiple users, why do we have authentication here? The fact that you authenticate malware infection, malware finds a way to get onto your device and it's running there. So who authenticated, it really doesn't matter. So that first option is not the correct option here. The correct option is actually the second one. One threat with these personal devices is that they can be stolen, misplaced and things like that. And if the device is in the possession of somebody else, the only protection you have is that they authenticate themselves as you. So your PIN or pattern that you have for unlocking your phone, sort of protects in case the device gets into the hands in the wrong person. So authentication that really is addressing the thread that the device may be in the hands of a person who's not authorized to access it. So now let's talk about how authentication can be implemented. It could be something that the user knows. So this has to be a secret, a secret that is shared between the user and the system. The fact that you are able to produce that secret that's associated with the user, that means you are that person. That is the evidence that you are who you claim to be. A password for example is a secret, isn't it? We say ask people to pick a password that others can not guess or easily guess. Well, that's your secret. So one way to do authentication is that users have a secret that they can produce to the system and the system is able to check that the secret really is the one that is associated with that user. The other possibility is something the user has. So think of this some sort of a token or a smart card or something that you use and the fact that you have it means you have possession of it. So, possession of that token, we have to talk about how this could be used for authentication. The fact that it's with you, smart card or token is with you, a fob, for example, people use that. That's with you, that means it must be the user who you're claiming to be or the user who should be possessing this particular thing. The first thing is something you would know. The second is something you have on you, or with you. The third thing we can think of is something the user is or something you are. So this could be for example your finger print. This could be your voice, for example, people talk about voice biometric, or their smartphone devices that you can talk to, for example. So this would be a biometric, something that is unique, hopefully, to the user, because we don't want somebody else to become you, so this biometric has to be something that's specific to you. And if you're able to produce that, the system you're able to give that to the system and the system is able to check that actually it is your fingerprint then the authentication is based on something the user is. Something you know, something you have, or something you are. These are sort of the three basic methods that are used in almost all the authentication techniques that we use in computer systems. So we are going to again look a little bit more deeply into how authentication is implemented, using one of these methods that we talked about. A secret you share with the system, or something you have, or something you are. So the basic sort of sequence of steps that we're going to have is that the user is actually going to come to the system and is going to request access to it. And we know that is going to start with request for authentication, you first have to authenticate yourself. The system, the operating system here is going to take whatever claim about identity you make, so for example you may be saying this is my log in name. So it's going to read those keystrokes and that's who you are. And if you're providing, for example, a password, then the operating system at this point is going to run this login program and the login program is going to check that information that you provide. For example, the password you provide Is really your password. Is the password we have associated with this user? If there's a match with what the system knows and what you provide, that is a true positive. If it's the right user then that's a true positive, the outcome is positive. Here, so the user is authenticated and when the user is authenticated, this is how you can now initiate any further actions you want to perform. So this is sort of the case where the right user provides the right kind of evidence. The log in program is able to check and successfully log in the user. Well, the check could fail and if it fails then the system believes that the user has not provided the correct information. And if the user has not provided the correct information, it's probably not the user who is being claimed. The identity that is being claimed probably doesn't belong to the user who is asking for authentication. So this could be impersonation for example. So in this case, the system doesn't have the right evidence, doesn't have this match, so it's not going to authenticate the user. So in this case, authentication fails. Maybe you retype your password. If you're not the right user, then that's a good thing. So then, that if you are not the right user, even you are trying to login as Alice and the system says no, that's good. That is a true negative actually because outcome is negative and it's the right thing. Okay so Eve tries to login as Alice and we end up here. That's a true negative. When Alice tries to login as Alice. We come here, that is a true positive. So this is how the authentication process is implemented. Couple of things to note here is that we have to read the evidence or capture the evidence. We have to compare it and then decide whether there's a match or not and go either this path or the other path. An attacker correctly guesses Alice's password. So the method here is something you know in particular you know, a password. So he was able to guess Alice's password and is able to login to the system. So is this a false positive or a true negative is the option that you have to pick? So we're going to pick the first option here. It's positive because A is able to log in. So login process is successful or the outcome of the login process is positive. Okay, that's why we're going to pick positive. Why is it false? It's false because it's an incorrect outcome because E was able to log in as Alice, stolen or guest password, whatever it is, we actually have sort of a security problem here isn't it. Alice's account is being compromised. We allowed authentication, that's the positive. And it's an undesirable thing that we just did, so that reference to the outcome not being desirable, which makes it false. So this is a false positive. It will authenticate even if it's authenticating as Alice, which is undesirable and the positive is actually false here. It's not a true negative. Negative would have meant that the authentication process doesn't succeed. Request is denied, which is not the case. So here again you choose the correct answer. The question is saying many online banking systems including the one I use. Actually they send you a pin when you login as a SMS to your smartphone. And they ask you to type that pin. So you had to provide that pin in addition to The password that you might have. So is this an example of authentication with something you have or something you are? So which case it is. Well it's not something you are, there's nothing biometric about it. It's a device that you have that receives a code. Since there's a device that allows you to authenticate yourself, and device is something you have, this option would be the right one. Okay, so the smartphones are essentially being used as something you have part of the authentication method that we're talking about. Somebody is trying to compromise the security of your system, somebody who is not authorized is trying to access your sensitive data. How can they attack, if what's they're asked to attack or what they're allowed to attack is the authentication part of whatever security you have in your system. So authentication is how you start. So the target of the attacker here is going to be how you authenticate. In particular, what they're going to try to do is defeat the authentication system. So Eve is going to try to be successful in authenticating herself as Alice. In that case, you would have successfully compromised the authentication method. So we're going to do this threat modeling saying well what can they do. What can the bad guy or that hacker do. Well, let's just sort of talk about the password method. So remember the password method is something you know. And we all know what passwords are. So let's say passwords are being used by users to login to a system. What kind of threats do we need to be concerned about? Well, it's a secret that's shared between you and the system, well, someone can try to guess that secret. So one threat is the not having good passwords that can be easily guessed by attackers. That's one threat we have to be concerned about, authentication that is based on passwords. So this is kind of interesting. It really is not talking about, how can you be a user, guess their password, or steal their token. But it's talking about impersonating a real login program. So think about you come in, and we're talking with passwords, you provide your password to the system. How do you know you're really talking to the system? How do you know it's not a program that is impersonating the system? All it has to do depending on which system you have, it has to display login, call in, or something like that for you to provide your login ID. Or login name and then it has to display password colon so you can type your password, or whatever the interface that you have. The is a Trojan horse, this a program that is impersonating the system at the other end. Then what this program is able to do is, as you type your password, it'll be able to steal it. So what's the last one? We're talking about someone trying to steal your password, isn't it? If your computer is infected, in particular if it is infected with malicious software called keyloggers. They are able to capture the keys that you press, you are going to provide your password by pressing a bunch of keys. And so keylogger grabs your password, okay, so if your are doing a password method, bad guy has to get hold of your password, so they become you. They can guess it, they can steal it, either using a piece of software that fools you into believing that you're giving your password to the operating system. Or it could run as the real login program is running and steal these keys that you press or the characters that you press that make up your password. So trusted path means that you really talking to the trusted computing base or the operating system, okay. So path that connects you typing the keys and where these keystrokes are being captured. That path is really connecting you and the trusted computing base and there's no one else in between. If that is the case, you have a trusted path. How do you make sure you have a trusted path? We're going to talk about a couple different ways. This trusted path really has to be provided by the operating system, maybe some combination of hardware as you look at combination of the operating system and the hardware as we're going to see. So for example, if you login to Windows, you have to press this key sequence, Ctrl+Alt+Del. The idea is that you press this key sequence. This sequence cannot be trapped by any other program. Great, this is going to take you to the operating system. So the display and the keyboard has to be connected to the CPU on which the operating system is running in a way that there is no one in between. Display is not under control of somebody else. And somebody else is not able to capture the key strokes and perhaps even alter them and things like that. So generally these are sort of physically connected, isn't it and they're in close proximity, they're right next to your CPU, both the display and the keyboard. So we assume there's no one else in between. But if you're paranoid we can do sort of couple of different things to make sure that really we're not talking to somebody else but the operating system. People had to propose a bunch of different ideas to make sure that when you're authenticating yourself you have a trusted path. They said there's a special part of the display that only the operating system can write. So for example you could indicate on the display when you're talking to the operating system and when you're not talking to the operating system. Or actually have a light on the keyboard. Okay, that will light up when you're talking to the operating system and it will be off when you're not. So this way the user would know that when it's providing, I don't know, his or her password, it is actually going over a trusted path to the operating system. The other entity, or the entity at the other end actually, is the trusted computing base. So one challenge with these kind of things is, do users pay attention to something you display, or a light on the keyboard or something like that. If it's a red, flashing light maybe they do, or something like that. But the idea here is I think the concept of a trusted computing path is something we want to understand. Here are sort of let's guess about, and since you are in a security class, you may have somewhat of a different mindset. But let's try to think what kind of passwords a lot of people use. Okay. What are common passwords? So, check everyone you think made the top ten list. Actually, password is a pretty popular password people use. It did make the top 10 list. 123456 actually has also made the top 10 list. Again, maybe it's the ease remembering it that's why people use it. 123456789 is also on the list. Here someone is picking a longer password. It's not weird if it's a stronger password or better password, but it's commonly used. Qwerty is faster to type, well actually that made the list also. Some of the other ones that we have here actually are on the longer list but not on the top ten. What else could be on top ten? A lot of sports fans, baseball, football, made the list. Actually 12345, 123456, 12345678, 123456789. All these are on that longer list. So, I think one takeaway here is that people put their ability to remember their password, perhaps ahead of how good those passwords are. And they continue to pick passwords that are pretty weak. And we're going to talk about why common passwords are a bad idea. So now let's talk about how password-based authentication can be implemented. So we know the evidence that's going to be provided is a password user is going to supply, and then they're going to type or touch the screen or something like that. And that's going to follow, you're telling the system who they are, which typically they provide the user ID. So they tied their user ID and password, let's say. And the system now has this task of, well, how do I know that this is the correct password for this user, and if it is the correct password, authentication will be successful. So how can we perform this check? So you would say well an obvious solution to this problem is that we ask users to share their passwords with the system. So the first time this is called registration or enrollment or some out of band method by which. Of course the system has to know something about your secret. If it doesn't, it can't check it. So let's say the secret is password so we have a set of users with passwords. And the system stores those passwords. It's trusted system we, let's say, give it our password. And if we do that, it's not a good idea and actually we won't have to do that. But if you do that than how can we do that check that we're talking about. I will see it's pretty straight forward. All we have to do is impair the type password with the one that's stored. So where do we store it? We have to store it in a file. People come back, you turn the computer off and then come back and login so this information has to persist. Your computer multiple sessions, so persistent information is stored in files. One thing we should be smart enough to do is that only the trusted root or admin be able to read this file. The login program is going to run on behalf of them and they are the ones that are going to check and somebody's trying to authenticate themselves. So this file should be made readable only by the admin or the root. The most user we have in the system. But what if we make a mistake? What if permissions are set incorrectly? The permissions are set incorrectly somebody else may be able to read that file. And if they're able to read that file, they're able to learn everyone's password, which is not a good thing. Even if the permissioners are set right, why should an admin be able to know all the passwords? Okay, every user's password, if they know it, there's no reason for them to have access to passwords of all users in the system. And the one reason we don't want to have that is that in case you have security breach, the passwords are going to be exposed, everyone's password is going to be exposed to that hacker. So it's clear that something about the secret has to be shared with the system, the password has to be shared with the system. But storing those secrets in a file, even if you do access control, is not a good idea. So can we do better? So indeed, we can actually do better, and the way to think about that is that we don't store the passwords themselves, but store something that is derived from it. It's not the actual password But something that is possible to get hold of only if you had access to the password at some point. Or at least in the beginning or the enrollment time or when the password was changed and then you're not storing the full password but just something derived from it. The question is if you're storing something derived from what is it and how do you use it. The result that we derived from password. One way to do that is to use what is called a one-way function. This called are called hash functions. Hash functions are widely used to take an arbitrary length of input and produce a fixed size output that is fairly unique to the input. And one-way basically says that it's easy to compute given the input of the hash value is easy to compute, but it's very hard to compute the input if you just had the hash value. So such functions exist, we're going to talk about some examples of those. So thing to remember here is that given a password it produced a hash value of that password. And going from password to hash value is straightforward going in the other direction which is from hash value to the password is really hard. So what you're going to do is you're going to use a function like that so rather than storing the password we're going to store something derived from it. In particular, we're going to store a hash value, and that's what we're going to have, and even the hash values that we have, those have to be stored in a file as earlier we were talking we're storing the passwords in a file. Even this file should be readable only for the perlist user or the admin user that we're talking about on whose behalf the log in program is going to run. A hash function basically as we said, the input is a password, this box is the hash function, whatever computation goes on here is going to happen here. And the output, this password could be any length, could be a past phrase that combines a bunch of words, letters, digits, whatever it is. But it produces the hash value is of a fixed length. In this direction we said, it's one way so it's easy to go. It's easy to compute this function given this input and for us to produce this output. If it just had this and wanted to invert it going the other direction that's going to be really hard, which is the basis of which we're talking about here. Given that we're going to derive something from a password using a hash function and store that, let's talk about a couple of threats. because of the security mindset we have, anytime we sort of make a decision, we want to think about its security implications. So what are some threats? We are going to assume that the one-way property of the hash functions holds. You make some assumptions so we are going to assume that inwarding or going the other direction from hash value to a password, we're not going to worry about that. Let's say we're talking about common passwords. You know that password is a common password, or one, two, three, four, five is a common password, but this hash function is known. This one-way hash function we're talking about, we're not doing security, web security, so it's known. We can compute the hash values off the common passwords. And we're saying these hash values are stored by the system, those are the derived values. If you can get hold of the hash values the system has and if you can find the hash values for the common passwords among those stored values, it's not hard to see how we can find out passwords of certain users in the system. So if the hash value of 1, 2, 3, 4, 5 matches with the stored value, we know that user's password is 1, 2, 3, 4, 5. The other kind of attacks we can mount, one is called a dictionary attack. So it's like a dictionary has lots of words and think about a long list of passwords, a dictionary of possible passwords. Words and maybe they're mutated in a certain way, you add a few digits or something like that, but you have this called space of passwords or the dictionary of passwords, and you can try to brute force each one of them. That's called a dictionary attack. An offline attack is one where we're not interacting with the system. We're not trying to log in each time, we try a different password. If you do that, then you are, that's called an online attempt we are making. We are interacting with the system so when you do an online, sort of guessing attack, the system can stop you after a certain number of tries. It locks you out. So, to avoid that problem we can sort of offline in our free time, we can take the hash values, take a dictionary and sort of compute for each dictionary word the hash value and search for that in the set of hash values that we have. When we find a match we know our password. And we can do this as I said without interacting the system. We interact with the system only when we discover a correct password. So those are called offline attacks and dictionary attacks, sort of brute force attacks. So one thing even when we store derive values in particular hash values here these kind of attacks are always possible. Common passwords, the hash value, dictionary or offline attacks. The assumption here is that the attacker does have access to the hash values. And the hash values don't immediately allow him or her to find out the password. They can try different, or try to guess the passwords that match and when they run into a match for the hash value then they know for sure that this is the password. So these are the kind of attacks that are possible even when you only store hash values. So we talked about a trusted path, remember? So, between the user and the trusted computing base he or she wants to log in. They want to provide the password to the trusted computing base. Okay? So they need to have a path to the trusted computing base, and that's what we talked about when we were talking about a trusted path. So this question's saying what problem would arise if we don't have a trusted path? Well if you don't have a trusted path, what is the problem? So not the case that the user, so let's say what the problem could be that we may have a trojan, okay, that creates an interface that looks like what the operating system to be do we normally would have. So, for example, would ask you for user id and then when you type that it may you know next ask you for your password. Trojans may sort of do exactly that. You just don't know you're talking to the trojan so if it does this that steals your password and logs you in anyway. Then it's not the case that you are not able to log in. What's really happening is that you're giving your password to a trojan who is going to steal it so you're giving your password to a malicious program because if you don't know for sure that you're talking with a trusting computing base that means you don't have trusted path. So possibly, potentially you're giving your password to some malicious code. The question here is why did we add the shadow password files instead of protecting the earlier publicly readable file, which was /etc/passwd? Why couldn't we just make that readable to sysadmin or admin or root user, rather than separating the hash values into a shadow file? So the reason we have to separate and we couldn't just make the etc password file readable only for root is that actually it included public information that other programs or utilities used. So if you turned off information for that file, in particular the etc password file then certain utilities will not function properly. So this actually was the reason for shadow files. So hash function used for computing hash values. If it's a good one, and serves the purpose, then what properties or characteristics it should have? Or what requirements it should meet? Should it be efficient storage of the hash values? Unique hash values for different passwords? Or should going back in the reverse directions be hard? So these are the options. Chose one or more that you think make sense. We already talked about the one way requirement of these kind of functions. So given a hash value, we can't go back to the password itself. So this is obviously a requirement. The inverse should be very hard to compute. Actually, we do want different hash values for different passwords, because what would happen otherwise? If user one and user two had the same hash values, then user ine can actually log into user two's account, using user one's password, because user two's hash value is what we compare. So, when user 1 claims to be user 2, types his password, the right hash value is produced, right for user two. So user one is able to get into user two's account if we don't have the different hash values for distinct passwords. So second option is also a requirement. Provide more efficient storage for, that's not really a big deal. We said passwords produce a fixed size output. Could be a couple of hundred bits, 128 or 256 or something like that, but storage itself is not a big deal. So what can that hacker still do? So this brute force guessing, they'll have to get hold of the shadow password files. They'll learn the hash values. So let's say there is a compromise of the system and that file is exfiltrated or something like that. In fact in the past year or two, there have been several examples where LinkedIn and Adobe and others had these password, derived values from password files, those were stolen by hackers and actually made widely available. So if you have these hash values, let's say if you get a hold of them, you're doing brute force guessing of passwords. How hard is this task? One thing we can do is we have some hardware software and whatever hash function here, we're talking MD5. That's one of the hash functions that is used. Publicly available software that we have, and certain kind of hardware that is common and quite popular for graphics. GPUs are graphics processing unit, and they're particularly well suited for doing password kind of calculations or brute force kind of attacks we're talking about. So a GPU with available software can do 10 to the power 8. So that's 100 million hashes it can compute and check per second. Okay, so if the password is only, let's say six random upper or lowercase, okay, 26 lowercase, 26 uppercase, all digits, that will give us 62 total number of characters. If you have 62 possibilities, and it's 6 characters long, the first character, there's 62 choices. The second character, there's 62 choices, the 6th character, there's 62 choices. So total number of possible passwords we can have is 62 times 62, 6 times, or 62 to the power of 6. If you actually do your calculation, you see how much that is. Divide that number by 10 to the power 8, because that's how many we can do per second. If you try to find that out, you'll see that this calculation, or this brute force attack, guessing attack we're talking about, can be carried out in about 10 minutes. One thing we do is we increase the size of the password. We ask people to pick longer passwords. So from six if we go to eight, of course, the effort for that hacker is going to go up. It's going to go up from minutes to days. But if you take eight random characters of the same kind, you would actually get 62 to the power of 8, because there are 62 choices for each. If you add special characters, the calculation would change a little bit, but if you calculate this number, again divide this by 10 to the power of 8, it's going to give you 6 days. You don't have to limit yourself if you're the attacker to one GPU. If you add GPU, this kind of computation can be done in parallel and you can reduce the time if it's too long. But the idea here is that brute force guessing of passwords, even when they're reasonable respectable length, is certainly doable for an attacker. Passwords are not really random. So when somebody's trying a brute force attack. They don't have to sort of search for things in the dark. What they really, if they're smart, they would start with more popular passwords. So the 12345 or password or whatever it is. So you absolutely can sort of reduce the amount of work that you have to do by trying out more popular passwords first. In fact, what you can do is create what is called a rainbow table, which is nothing but potential passwords and their hash values and as a table that you create. The hashed value table for each possible password and so this is, you don't even have to run your hash function when you're doing this brute force guessing. You're just doing a lookup in a rainbow table saying this password let me see what it's corresponding hash value is and does that match some entry in the rainbow table without having to compute the hash function. You can have these other tricks you can use if that hacker is use you rainbow table or be smart and try more popular passwords first. The couple of other things related to passwords and their guessing and things like that, we said, what if two users pick the same password? If you have common passwords, then of course 12345 is taken by lots and lots of users. How do we avoid this problem? In one of the quiz questions we say, what would happen if the hash value is the same. It's sort of related here. If they do have the same password, they'll have the same hash value. And by looking at the hash values, we can see that from one person's password we can login as the other user and that would be a problem. Fortunately, we have a solution to that, we add a random number before we do the hashing. We take the password strength, add this random salt. The random salt will be different for different users, that makes the hash value different for different users. If two users pick the same password, without the salt, the hash value will be the same, but with the salt, the hash value will be different. And we will know whether the passwords are the same or they're different because the hash values could be different, either because of the passwords being different or the salt being different. You'll have to store the salt because later on when we do the check, we're going to get a password. We'll need the salt before we can run the hash again. So we can compare it with the hashed value. So we're going to need the salt. Where do you store the salt? You store it in the file where you store the hashed value. Whoever has access to the hashed value would also have access to the salt, which is fine. Actually not a problem. So this checking, when we have salt, is done by combining or computing the hash function with both the salt and the password. To see how that is really done, let's just look at how unique started for example. So you provide a User ID and your password. We use the User ID to select the entry in the password file for this user that we're talking about. The salt is stored here. The hash value's stored here, isn't it? This is sort of the data structure we keep. To log in, you're going to supply your password, so we start on this side later on. So this is what is set up initially for create the user account or set up the password. At that time, we determine the salt and computer hash value and things like that. So later on a user comes types the password. Well, they're saying who they are. You give your user id, your log in name. Based on that, we're going to find the hash value and the salt. So the salt and the password are going to be put through this hash function. And it's slow because if somebody's doing brute force, it pays to slow them down. So we passed these two things through the hash function. So this is typed by the user based on the claim by the user, who they are. This is what we find in the file that we have, where we store the salt value. So the two are passed as input to this. The result is compared with a stored hash value. If the two match then we allow authentication is successful. If they don't match, then authentication fails. So this is what happens when we have makers of salt to deal with different users making the same password and the result having the same hash value. And that would be a problem. When you add salt, the rainbow based, rainbow table based attack, think about what would happen to that. A lot of us use PINs on our smart phones or ATM access and things like that. So if the PIN is four digits, how many different PINs are possible? If somebody was doing a brute force, they may have to try all possible values. How many of those possible values are there? For each position we have ten possibilities, zero through nine. So position one, there are ten possibilities. Two, there are ten. Three, another ten. So, it's ten times ten times ten times ten, because there are four digits in the pin. So that's ten to the four, or 10,000 is the answer that we get. So, there are ten thousand PINs if you have four-digit PINs. So if you're doing a Brute Force Attack, that's a fairly small number as we saw how many operations we can do per second. So here we're saying, password has six characters, it must include an upper, lower letters, digits and some special characters. We're adding those. Plus or semicolon or something like that. In the worst case, how many attempts must a brute force method make to determine a password when its hash value is available? You have the hash value of the password, you know the password is six characters long and you know the structure of the passwords. They can have upper, lowercase letters, digits and some special characters. So how many total such passwords are possible that we may have to try in the worst case? It's going to be some number multiplied 6 times. So, the exponent is going to be 6. And n to the power 6 whatever that number n is. It's not the first option. So let's say 26 letters upper and lower case will make that 26 times 2, or 52 digits, makes it 10 more. That's 62, and the special characters, if you add 10 of them, that's 72. The answer is going to be the last one. It's 72 to the power of 6, because each position we have 72 possibilities and there are 6 positions in the password, 6 characters in a password. The question is are these patterns random or there is some inherent bias that people have which makes certain patterns more popular? So in other words, what's the one, two, three, four, five equivalent when you're talking about this unlock patterns on touch screens? So check the correct answer or answers. Is it true that users often start at the random point but then fall back to a common pattern? Okay, actually users typically start at the left upper corner. Because stuff in English and many other languages, we go left to right. So, sort of, that bias is still there. So there is, people really don't start at a random point. I won't check that answer. There's a bias in starting at a point near the top left, okay? We just said that so we're going to check this answer and the next one is the ease of moving from current to next point. Okay, going right or down rather than diagonal in the opposite direction or something like that. The ease of moving from your current to next actually also introduces bias, an easier pattern on you and how you sort of enter it. So second and third options make those patterns less random. There are more common patterns because of ease of use reasons, because of the bias in sort of where people like to start, and things like that. Although we use passwords almost universally, very commonly, there are plenty of problems with passwords. People been predicting the demise of passwords for a long time because of these problems, but they sort of refuse to go away. So what are some of these problems? For passwords to be strong, and by strong we mean harder to guess, they need to be long and they need to have these upper, lower digits, special characters and things like that, so we are increasing the space of possible passwords but that increases complexity for users and usability suffers. Even if you have good passwords, there are problems with sort of the trusted paths discussion that we had. This is sort of related to that. Phishing and social engineering attacks, really users don't authenticate who is really that their giving their password to. So this idea of mutual authentication, you trying to convince that it is really you. It's Alice. Do you assume the system is the true system? Do you ever ask the system to authenticate itself to you? In particular if you're using a remote service, your bank or something like that, how do you know it's really your bank? It could be someone else that just looks like your bank. And those are the sort of fphshing attacks that we have. Unfortunately once a password is stolen, it can be used many times. Not a one time use sort of a thing, and they can gain access to your account again and again. And the way we address that is by having policies that say you must change your password every 90 days, or every six months, or whatever it is. Of course, usability's going to suffer because of that. And finally, humans are humans, and they have a hard time remembering lots of passwords. These days it's not just one system that we have. You have your bank, you have your mail, you have various merchants, retailers, and things like that, so you have lots of passwords to remember. You don't want to have the same password everywhere because when one gets compromised, you're vulnerable to all your services that you access, somebody can get to them with that one stolen password. So if you're going to have different passwords, well that's going It's going to be hard for us to remember so many of those passwords if they're really hard to guess, and they're long and complex. Well all of the many problems, there are of course, some best practices when it comes to passwords. So when it comes to Sys administrators, obviously the password should never be stored in the clear. So you only store hashed values. And we use salting, random salt, to avoid the problems, multiple users having the same password and things like that. We have to limit access to the file where these passwords are stored, the hash values are stored, so shadow files we talked about and so on. And we want to use a function here that is not fast. We have to do it only once it's in it once the user logs in. We don't need to run it a million times. So it doesn't have to be fast, it can be slow. On the other hand, somebody doing a brute force attack has to run it again and again. They have to run it millions of times. So using a slow function actually is helpful, and current implementations actually do that. They use slower hash functions. What users can do is, a lot of password managers these days, we're talking about having lot of passwords, one for each different service you want to use and hard to remember all of them. Password managers actually can generate complex high entropy. We didn't talk about the word entropy but entropy is sort of high entropy means it's harder to guess, so they can have these difficult to guess passwords and they can keep track of those. You basically have one strong password to access the password manager and they can keep track of your other passwords. Since passwords have problems and we said that's only one of the ways in which you can do authentication, let's look at some of the other ways in which authentication can be done. So the second one was something you have. So a token or a smart card or something like that, obviously if it some thing you have, then you must have it on them. If you don't have your token or your smart card or your smartphone, if that's been used as the something you have, you can't obviously access the system. And people sometimes forget these things and don't have them on them when they need to access especially banks, when they give you these tokens or send you some sort of a code through them. If you don't have that with you, you're not going to be able to access your account. So, not having them would cause a problem. So you do have to have them that that's one of the requirements when you use this method. So obviously, if it's something that you must have, it should be able to talk to the system, or the human has to enter the PIN or whatever code that is sent to them. But if it's a smart card, the smart card has to talk to the system, which requires you have to have a reader, for example. You should be able to swipe the smart card and it should be able to communicate. So that adds the hardware requirement which increases cost. Even if you have it at the other end they should have a reader. If they don't have it then you can't use it with that system. How is authentication implemented with something you have? If you have a smart card we were just talking about, then it can talk to the system, then there could be some sort of challenge/response kind of a thing. So the system can say, if you really belong to the user that is being claimed, or claim is being made about the identity of a certain user then that user's smart card has a certain secret. Use that secret to sort of maybe respond to my challenge, encrypt it or something like that, or do something to this challenge. And the response should demonstrate that the secret that is stored in this smart card is the one that is used to generate the response. So, sometimes, if you have some processing this smart card. Smart card can do this sort of computation, generate a response that could be sent. So this PIN and chip kind of cards, the chip can do this kind of work, and that's how this would we implemented. The problem with this is that there is added cost, and of course sometimes there could be misplaced trust in this kind of tokens or cards that we're talking about for something you have. There's of course the very well known case of the RSA breach, where the master key was compromised. And as a result of that, all the SecureID tokens that RSA stronger authentication relied on, you couldn't count on doing correct authentication using those. So there's added cost we are talking about but there's also vulnerabilities or attacks that can limit the strength that authentication, the stronger authentication that these kind of tokens provide. So, something you have, you don't need to remember those complex passwords we're talking about, but we need to be aware of the fact, and that they have their own set of problems. The third method we have is something you are. That's the biometric method. So, what exactly is that? So obviously, there are a variety of biometrics and it should be unique to you. Somebody else shouldn't be able to use a biometric feature or set of features to impersonate you, so what are some examples? Fingerprints are one. You swipe your finger for example, touch ID and things like that and they look for the pattern in your fingerprints and based on that they say it's really you. When it comes to keyboard if nothing else, the way you type, or the keystroke dynamics, how fast you are, how long you take from one key to the next depending on what the keys are and things like that. Sometimes that is used as a biometric features derived from this speed at which you type, or pattern of your typing, could be a biometric that's unique to you. Actually seeing devices that do voice biometric authentication. They extract a set of features from your voice which hopefully is unique to you, and that can be used when you talk to the device, it knows that it's really you. And, of course, they're more intrusive and fancier one like retina scan and things like that, which look for sort of unique patterns based on the blood flow in your eye, and things like that. So these are basically what you are. The way these work, is that you're going to claim it's you and you're going to swipe your finger, or you talk in case of voice. But the biometric measurement should be the same each time. If it's different then of course we're going to have a false negative. It's you but you're not able to login because you have a cold and your voice is a little different. So maybe you don't have exact of these measurements. Maybe you have a probability distribution for your other features that we extract from your biometric measurements. And of course we're going to have false positives and false negatives, as we have with each authentication method that we discussed. Here, if your biometric measurements change beyond a certain limit, then of course you may not be able to login. Resulting in this false negative that we were talking about. So the basic way of implementing biometric is there has to be a biometric sensor, there's some user interface where you swipe your finger or camera takes a picture of you or where you talk to it, or wherever it is. That the interface, the sensor, then from the biometric reading we're going to extract a bunch of features that actually describe the reading that we just have from the sensor. And that's passed to a biometric database that stores these features, or something that derives from these features. So PIN is here, as sort of a separate thing we may have, in addition too, PIN is something you know, so that's the secret. But say if you just sort of going with biometric, you would not have the top line here. So the biometric database seems like a hash values for passwords. We have something that describes the biometric feature set and feature values for a given user. So once you extract the feature we compare those. If there's a match between these two, then we allow authentication. If there isn't a match then we're going to say that authentication is not allowed. So the idea is that you still have the enrollment problem. Somebody has to have a description of your biometric features, which is the database where we'll be storing it. The sensor has to read it, the features have to be extracted, the comparison or match has to happen. And that's how biometric authentication works. So that when you're typing a password, we sort of reading through the interface and the sensor and then it's sort of similar to what we had before. We talked about these different methods, something you know, something you have, those are called factors. The first one is sort of the secret you know factor. The second is something you have factor. The third is who you are factor. So it doesn't have to be just one factor. You can actually have a multi-factor authentication. One of the quiz questions, the smart phone and PIN for example, the password and the PIN that is sent or the code that is sent to you, the password is something you know. The PIN that is sent to you on the smart phone is based on the smart phone which is something you have, for example. So you can use more than one factor to actually make authentication stronger. So, as we said we use more than one method, you type password but also send code via SMS, and this code goes to your phone, actually Gmail when you create an account uses this. These are called phone verified accounts or PVAs, mainly to provide protection against bots that create a lot of these accounts. So this is multi-factor, because it's not just a password, but also this code that comes to a device that you have. So that's the second factor. Your ATM card and PIN that you use to get money out of an ATM machine is actually a two factor. The PIN is something you know, the card itself is something you have. So these are two factors. There's another example of a multi-factor authentication. The other methods that use things like where you are, your location and IP address of the machine, OS fingerprint, these sort of add a little bit of more to the authentication process. So when you have multi factor authentication of course that attacker must defeat both to successfully compromise the authentication method we have in place. So the authentication we talked about sort of going to the system, having a trusted path to the system, providing the password, fortunately, a lot of times you do authentication over the network where the system may be thousands of miles away from you. In fact, a lot of remote services we access whether it's our email or our banking system and so on are of course, we don't walk up to them. So, what happens in this case? We don't have a trusted path to the system, remember we sent physical wires from your keyboard to your machine right in front of you in close proximity, That's how we get that trusted path. The network is open and, obviously, the same thing doesn't hold there. So we don't have a trusted path, especially when we access in remote services. So network authentication introduces a new problem. But how do you deal with the absence of this trusted part that we've been talking about? Well, actually, we need crypto to secure the network communication. So when you talk about network security in a different lesson, we're going to address this. Of course you have to worry about different kind of threats, such as man-in-the-middle and things like that. But the reason I want to mention, although we're going to cover this in network security topic, is that authentication relies on this fundamental requirement of this trusted path. In a network, you're not going to have that. So you'll have to do something else about the lack of the trusted path, or some method you have to find to secure the network over which you're going to send the evidence for authenticating yourself. Remember multi-factor means you're using more than one factor. You could be using a card and a password or a PIN, so if you do that the first question says the authentication, multi-factor authentication method will likely reduce false positives. Is that true or false? So I'm going to say it's true and the reason is an attacker who is able to login as you is the one who is going to cause a false positives in it. Somebody else logs in as you, that's a false positive. For them to succeed in logging in as you, now they have to successfully attack two different factors. They have to guess your password. They have to steal your card. If they have to work harder, you increasing the complexity for them, reducing the likelihood they succeed at it, so you're going to have fewer false positives. I want you to read about a chip and pin based authentication attack that's known, was reported by some researchers. And essentially the idea is that when you're chip and pin, there's multifactor, two factor authent, pin is something you know. Chip is in a card that you have. So it's two factor something you have and something you know. So what is the main weakness in this sort of an authentication method based on this research paper and stories that have come after it was presented? Lost card is hopefully is not as big a problem because there's still a PIN, isn't it. So just because they have a card, you lose it, the PIN still protects you. Cloning of cards is hard because of the chip that we have is in it. And the way it works, obviously, in the old days all you had to do, the credit card number was encoded in a magnetic strip that was very easy to clone. But these chip-based cards, cloning is harder, so that's not a weakness. The third option is what this link, or the story, or the paper I mentioned. The protocol actually is implemented had some problems. Supposed to generate random numbers and answers in each transaction. Supposed to have random nonce but the implementation had some flaws, because of which, it didn't do that. And those could be exploited. So this is really an Implementation issue, in the protocol, that is based on this chip and pin cards. And that is the weakness that we are exploring in this question. So once you read about the Apple's Touch ID attack, question is can a similar attack be mounted when voice biometric is used. The answer to this is, yes. They're different ways of doing voice biometrics. Either someone can record your voice and play it, that's one way, or they're also sort of voice synthesis kind of attacks people can do if they have some segment of voice from you that can build a model for you and so, similarly, these could be repeated. So just because it's something you are, in the end we're extracting a set of features from you that we make available to the system. If somebody else can grab those set of values, well, then they can fool the system into believing that is really you, and defeat the biometric indication method. So, it's interesting read how you defeat fingerprint, the touch ID that users fingerprints but similar tax people have explored on voice space authentication and so on. So yes we make hopefully we make the task harder for the attacker but there is never a full proof, completely 100% secure authentication solution. We studied a number of different authentication methods and their implementation in a computer system. Our security mindset approach requires that we consider any threats that may exist against these methods and their implementations. So, we did consider the strengths and weaknesses that exist for each of these methods and how we use them in computer systems. So authentication helps us answer the question, who the request is coming from. But then we have to check if the source of the request is actually authorized to access the resource they're asking for. Well, this is the access control, or authorization problem. The focus of this lesson is going to be on access control and how it can be implemented in a computer system. We're going to talk about how we control access to resources. These are protected resources. Any program can make a request for such a resource, and we need to decide if the request should be allowed to go ahead, it should be granted, or we have to deny it. So we've been talking about the importance of the trusted computing base. In fact, when someone is asking for a resource, they are generating a reference for that. They have to reference the resource they are interested in or they want to use, so the trusted computing base has to act as a reference monitor. No request for resource should go without the trusted computing base being involved in checking that request. Obviously, it'll have to see the source of the request is, what the target, or what resource is being requested. And then we're going to discuss how do we decide if it should be granted. As a quick example, let's say John is a student in a class and there's a file that has the grades of all the students who are enrolled in that class. John, obviously, can make a request to read that file. Not only he wants to see his own grade, maybe he's curious about how other people did on a certain test or an exam. So John could make a request to read a file. In this case, we not only have to monitor the request that is going to come from John, but our access control that we going to do should decide that John should not be able to see other people's grades. So, in this case, the result may be denying the request. But the idea here is that references have to be monitored, and then we have to make some sort of a decision when a request comes, whether it should go ahead or it should be denied. So we talked about authentication. Authentication basically tells us when an application or a process makes a request, on whose behalf that request is being made. So the example that we're just talking about, it's a process that John launched, then it is going to have John's User ID or UID. And authentication is how we know that this particular UID should be associated with the process. Authentication tells us about the source. Authorization, or access control, which is the topic for this lesson, is going to answer the question knowing the source of a request, authentication is going to tell us that. Once we know the source of a request, in this case a User ID, and the target of the request, which is a file, and what they want to do with this resource is read this file, should we let them do it or not? So access control is similar to what we have in real world. You want to access something, and somebody has to decide, in this case, the TCB, or the reference monitor that is implemented by the TCB, whether that access is one that should be allowed to proceed or go forward, or it should not. So let's quickly talk about how do we actually decide this. Resources that we have in the system, for example, files are created by certain users or subjects. You can, perhaps, think if Alice is creating a file, maybe she has the ability to decide who should be able to access it or not. The intention here being, it's her file, she chooses to decide who to share it with. So in many systems, actually, the idea of an owner of a resource is defined. This is the subject who creates the resource, and it is at the discretion of the owner how that resource can be shared. There are other kind of systems where this may not be the case and we're going to talk about that. For example, if you work for a company, the company may not allow you to decide how you can share certain sensitive data about that company. How do we control access to resources with a context of authentication that establishes a source and the idea that these authorization check or access control check has to happen if the resources have to be protected. Really, for us to be able to answer this question we had, whether a request should be granted or denied, we have to know who is allowed to access what resources in a given system. So if you think about this, there are really two parts to this problem, or the way we're going to address this problem. In part one, someone has to specify who has access to what. So this is called an access control policy. So there may be many ways to decide who gets to access what. There has to be a policy we have in place that is going to define this for us. So to answer this question about who can access what, is you tell the system how the resources should be shared, or who should be given access to what resource. Once you do that, then it's the part two is about enforcement. The system has to monitor each request for these resources, and based on the policy that tells us who can access what, it should make sure any accesses that are allowed are consistent with the policy that we have. So enforcement basically says there is no way for you to go and access a resource when that access is not allowed by the policy. So define the policy that's in part one, and then enforce the policy, that's part two. Now you can see the importance of complete mediation. Clearly complete mediation says, no one should be able to bypass and get to the resource without going through the trusted computing base. If you don't have complete mediation then we can't perform this check, access control check that we are talking about. That is possible only when the trusted computing base is involved in a request and that is why complete mediation is so important. We said there are two parts, we had to define our access control policy and then we have to do enforcement based on what is in that policy. Clearly that policy defines who can access what and things like that. That kind of information, we going to abstract that in a data structure that is called an Access Control Matrix, or an ACM. If it's a matrix, in this case an access control matrix, that is going to abstract all the state that is relevant for making those access control decisions. Well matrixes have rows and columns. To define a matrix you have to say what rows it has, what kind of columns it has, what does it store in each element, or each cell of the matrix. In an accesss control matrix, rows are defined by the users or subjects that we have in the group. So rows actually correspond to the sources of requests or the subjects or the users. So this access control matrix, if Alice is a user in the system, there is going to be a row for Alice. Well other thing that a matrix has is to define this two dimensional matrix we are talking about is we have to define its columns. Rows correspond to users. Actually columns going to correspond to resources that we have in the system. So each resource that needs to be protected. Remember, we need to know who is allowed to access it or not. So this matrix is sort of telling you for a given user, that's a row, and a given resource or object, what can be done. So the columns are going to correspond to all the resources that need to be protected in the system. So an access control matrix, since I'm talking about rows and columns and it's two dimensions. Any particular entry, or cell, in this matrix can be defined by what we have here, [U, O], with the row that is corresponds to user U and the column for object O. You start with a user, continue until you get to the object that we are talking about. So that element of the matrix or cell of the matrix, is actually going to define what kind of access rights user U has for object O. So if this was a file, and the access rights can be read/write, execute or whatever subset of those access rights then we're saying the entry in the matrix, if you look at the row for user U and object O, it's going to say if you can read this object O. R is present in that entry. That means the user can read object O's matrix. Access control matrix is based on users we have in the system, resources we have in the system, and the state actually it captures is who had what kind of access for the resources of the system. That's what each entry of the matrix is going to answer a question for a given user and a given object or resource. So let's look at this matrix a little bit more. This is a matrix, the size here is m x n. That's because we have m rows from 1, 2, 3 all the way going up to m. And we have n columns. So if you look at 1 2 all the way up to n. The first row we know corresponds to the user 1 that we have. Similarly, the second row is going to correspond to user 2, user 3, and the last one is going to be for user m. Similarly if we look at the columns, remember these were for objects or resources. So this is for Object 1, this is for Object 2, and all the way to Object n. So we are talking about ACM U, O. So that's going to be some entry like this. So if you take a look at it, this is are ACM 3, because it's user 3 that we are talking about and object 2. So this entry A32, which is basically going to describe how user three, or U3, is allowed to access object O2. If it happens to a file, it could be read access or write access or execute access. Things like that. So this is what we just discussed for the case where there m users, n objects. So this is an m by n matrix, a row here corresponds to user, a column here responds to object. So again if you focus on a particular entry in this one, remember, this is what axis does user 3 have for object 3, A33 is. So this is going to be a subset of the axis right. How this object can be accessed. Object O3 for example here, user 3 we have here. So depending on what kind of object this is, so we've been talking about file objects that contain data which can be read or written, so. The access rights for a file may be read write and execute. So this can be some subset. This could be nothing. That means user 3 could not access object O3 in any way. If it just has read then we know that reads are allowed. User 3 cannot write or execute a file that is object 3. If it read write execute then of course user 3 is allowed to read or write or execute this file that corresponds to object O3. The file is created by a certain user. So the file belongs to user Alice, let's say. She then becomes the owner of this file. And let's say here she can choose to decide who should have access to this file, and in what manner. So if confidentiality is what we are concerned about, then what kind of access should be controlled for this file that contains this confidential data? Okay, so that's the quiz about. Remember, entries in an access controlled matrix cell in a matrix ACM that we just defined, are some subset of access rights. So here is the question of saying, if confidentiality is important what access rights should you focus on? What should be present in that entry in an Access Control Metrics or what right should not not be present in it? So think about the 3 options, and so let the one that you think makes the most sense We know that confidentiality is all about disclosure of sensitive data, which when somebody being able to observe or read the data, not about writing it, that is integrity. So if you just concerned about confidentiality or disclosure of this data, what we really need to do is control read access to the files. So this is the answer that is correct in this case is that read access has to be controlled. So the scenario here is kind of little interesting one. There is a particular user. The user does belong. User is Alice, she does belong to this group. The system specifies that members of this group can read file foo so that's a positive access right. Think of that as plus read says in the column it's going to be for file foo, the row is going to be for this subject All-Students we're talking about. And if you look at, in the entry in the access control matrix corresponding to this subject and to this object or file. Then it's going to say, there's. They're allowed to read it, but Alice is denied access. Maybe Alice is no longer a student, a member of this group that we are talking about. So if you look at Alice's entry for this file foo, okay, so the role would be Alice's role in the access control metrics, and the column is foo. If you go in there, then access is denied. So we have a negative read that says Alice should not be allowed to read this part. We're exploring a couple of different questions here or ideas here. We're exploring the idea of a group and users being part of a group. The idea of a positive access right as well as a negative access right. And actually the important thing that this question is exploring is what happens when you run into a situation where you kind of seems like you have a conflict? Where Alice is a member of this group and the group has access. But then if you look at Alice herself, she is denied access to this file. So what does enforcement do in this case? That's the question you are exploring in this quiz, and there are two options. When you run into the presence, in the access control metrics, of conflicting access rights, denial is the wise thing to do. Because the policy does say that Alice should not have access. That takes precedence over the fact that she happens to be a member of some group that does have access. Define something called discretionary access control, it's called DAC. In these kind of policies, who has access to a resource is at the discretion of the owner of that resource. So Alice owns a file, it's at her discretion whether she's going to allow Bob to read it or write it, or Bob is going to be prevented from accessing it. Furthermore, she can actually say whether Bob can propagate the access to somebody else, access he gets from Alice. So in this question we are saying, Alice actually is going to grant read access to this file foo, to Bob, but she's not allowing Bob to propagate this access to anybody else. And that is discretionary access control model is basically an Access Control Matrix that we have. She is going to put a read access in the cell or the entry that we have for Bob and foo, okay? So the column for foo and the row for Bob, if you look at ACM, Bob,foo there's going to be read access rights there. So the question is asking, does this ensure that a third user, Charlie, will not be able to read the sensitive data that is in this file foo, or is there some way for Charlie to gain access to this data? First are the says, does this ensure that can never read the data? So the first one says yes, Charlie does not have access to read this file foo. So Charlie made a direct request to the system, saying, I want to read this file. The system is going to check the access control matrix information that we have. It's not going to find the information for Charlie, so it's going to deny. So Charlie is not going to be able to gain access to the file directly. But things are a little bit more interesting. So what can happen here is that Bob is able to read this file. So what can Bob do here, is read this sensitive data from file foo, and Bob can create a new file that he owns. And he can then tow the read data into this new file that is owned by him. So the data is now being moved from file foo into a new file that is owned by Bob. And that's possible because Bob could read the data into some buffering memory. And then Bob is writing that buffer into this new file that he just created. The new file that Bob has created is owned by Bob and he, of course, can give permission to read that new file to Charlie. And this new file contains the same data that was there in Alice's file, foo. So the answer here actually is going to be the second one. It does not ensure that Charlie will never be able to read the data. This is called information flow problem. Can the information that you shared with someone else flow to another user? And discretionary access control, unfortunately there is no way to prevent that kind of information flow. And so we can't make this guarantee. The only guarantee we can make is that Charlie will not be able to directly read the file foo. How do we implement access control using the access control matrix? This is what we want to discuss. So first of all the access control matrix is going to be pretty large. There could be tons of resources, depending on what kind of system you have. If it's a large shared server there could be lots of users. So it's a large matrix, but we know that the matrix is going to be fairly sparse. So sparse matrix here really means that many of the entries in the matrix are going to be null. because most users not going to have access for a given resource. Resource is owned by somebody. He or she's going to have access to it and maybe they choose to share it with a few other people. So the question is how should we represent this matrix in the system? Actually sparse matrices represented in one of two ways. You can represent them either sort of column measure or row measure and that's something we can do here in this case. So we can focus on each column of the matrix, and if most of the entries are null we can just ignore those and we can make a list that says, well for object Oi, user ui1, this user's access rights are this value. Okay, so we have these entries, so when I said matrices, when they're sparse, they can be implemented through link list for example. So this could be a list of, and it's going to have a small number of entries. Because most users don't have any access for object Oi. So if a given user has access, what kind of access we can have, sort of this pair, and then the user id, and access rights, that makes up the first node in our list. Next user is going to be ui2, who has some other set of rights, and so on. So we can just go down the column and see what users actually have known null access rights. And build these entries, that become nodes in a link list or something like that. So we can organize this information either by a resource or an object. Here we'll have to keep track of what users have access to this object, and what kind of access they have for this object. So that's one way to do it. When you do that, this list I'm talking about, or the data structure that we're talking about which is this collection, this linked list kind of a data structure. That's a collection of these entries, what user has what access rights. This is called an access control list or an ACL. So if you focus on a resource or object Oi and grab the information who has access to this resource and what kind of access, that defines an access control list or ACL. To keep in mind is that ACLs are for resources we have in the system. So ACL is always associated with an object or a resource. There's another way to think about how we can implement it, and rather than focusing on columns, we can focus on rows. And we know that rows correspond to users. So we can go down horizontally, you know, across a row and say, what would we see. If it's a sparse matrix, this user ui is not going to have access for most of the objects. But he or she will have access to some objects. So the first object we run into for which ui has access is this object oi1. So the list we are going to generate here, which is going to say for this given user ui, we do have access and these are the access rights for object oi1. The next object for which we have access, or this user ui has access is oi2 and those access rights are defined by rights2 and so on. Basically we are ignoring the null entries in the row, and the entries that are not null are for certain objects. So these are those objects, and what kind of access for each of those objects, that's what we capture in this list. Well, this kind of a list is called a capability-list, in some sense what a user is capable of. A user is capable of accessing oi1 in this manner, oi2 in that manner, so it's called a capability-list that is for a given user ui. Remember, ACLs are for an object, capability-lists are for a user. Again, C-list is going to be associated with each user we have in the system. Remember, we have a row in the access control matrix for each user, and each row really is a C-list. It's a list representation of what's in that row. So, C-list are for users. ACLs are for objects. The example here says we have a sort of system that has three users. The users are A, B, C, because rows correspond to users. And we have three resources as well, we calling them X, Y and Z. These are three objects of the resources we have. And if we look at the matrix says well A can read write execute. Object X can only read object Y and has no access to object Z. B can read write Y and can read execute Z and things like that. So this matrix actually tells you who can do what given the three users and the three sources we have. So if you want to implement the information that's in this access control matrix. Well using ACLs then we look at what's at the top here. So what would ACLs or ACLs correspond to object. So the first object is X, first go on you find user A who has read write execute permission. So that's an entry in the list that we're going to, actually in this case the list is only going to have one entry because B has no access, C has no access, those are null entries that we don't need to represent in this list. So the ACL for X it's just going to be this one entry which says, user A can read, write, and execute this object. Still talking with ACLs, if you look at Y, the second resource we have here. Actually, here we're going to have three entries in the ACL, one for each user. A is going to be able to read it, B can read, write it. C can read write it, too. Similarly for the last resource, we're going to have two entries. A has no access, but B and C do, so we can read and execute it and she can read and execute it as well. So this information that we have in the access control matrix gets represented with three access control lists. One for each object. This is for X, for Y, and for Z. Now another way to represent this very same information is to using capability lists or C-lists. So remember C-lists are for user. So we have to have a C-list for user A, one for user B, and one for user C. If you look at A, it has access to both X and Y. So the C-list would have two entries. If user A has access for resource X and the access rights are read write execute. Similarly user A can read resource Y. So this is the C-list of user A. Similarly C-list for user B is going to be no access for resource X, so nothing for X. But for Y we're going to have read write access and similarly for Z we're going to have read execute access. So this C-list is going to have two entries. And the C-list is going to look the same for user C because B and C have same kind of access. The given set of resources that we have here. ACLs, you go vertical per resource. C-list, you go horizontal or row-wise per user, so we are talking about how you implement an Access Control Matrix. Well, one way is to define or have this list that we have, and we'll talk about where these lists get stored or maintained, but the ACL or ACM information gets captured either in ACLs or in C-lists. Remember an ACL is per object or per resource? It tells us user ID, what access that user has, and the next user ID and what access that user has, and some number of entries like that. And by the way, access control entries, or ACEs is what they're called. So where should ACL for an object be stored? So first of all this has to be stored in the trusted part of the system. It has to be stored in the operating system of the trusted computing base because it actually determines who can access a resource that needs to be protected. If it's not in the trusted part, then some untrusted code or application potentially can change it but where exactly does it go in that system. So we know that it is a list of these access control entries. And this is for a given object or a resource. Such a list exists. And ACL exists for each resource. So we have the resource R is going to have an ACL R. One natural place for us to store this ACL, is where other information about the resource or the object is stored. So the other information typically what we call is meta-data about that resource. So if the resource was a file for example meta-data might say the size of the file, where on disk it may be stored, who the owner is, and things like that. We can store the ACL along with that other meta-data that you have. So we're going to do an example for the Linux system, Unix systems, how file system access control is implemented. And we're going to see that ACL information actually gets stored same place where meta-data about a file is stored in the operating system. We said meta-data has bunch of other information where ACL is going to be stored as well, so then the question is, how do you use this information? So remember for an object resource if somebody's going to use it, the object or resource has to be activated. So it's ready for use. The meta-data has to be acquired, something has to be set up in the operating system. And once that is done, then the request is going to come. So at that time we have to perform a check, access control check to decide if the request should be granted. So how do we perform that access control check? Let's say it's coming from Alice. The request source is, we know is Alice's UID, and the request is for file foo. We're going to go to the meta-data for file foo, where we're also going to find ACL for file foo. And once we have that ACL, we basically have to traverse it, looking for an access control entry for Alice. And then see if the access rights in that ACE grant permission to this object to Alice in the manners of what kind of request is being made. So for example, it's read of a file, and does the read access write exist in the ACL or the ACE that we have in the ACL for user Alice. So traversal basically says when a request comes from a given user, we have to go down the list to see if an ACE exists for the source of the request. And if it does, does it include permissions consistent with the nature of the request. And if that's the case, then we can go on to access. If you can't find such an ACE or the access right doesn't exist, so there is an access ACE that says you can read but the request is for write, then also we are going to deny it. So now you understand how ACLs can be used. The most common systems actually implement ACLs. Operating systems have, in researches I have studied, C-lists as well and we'll see there's some nice properties they offer. So how would you implement C-lists? So first of all, where would a C-list go? Where would it be stored? Remember, C-lists are per user. C-lists we get from each row of the access control matrix, so it's very user. So the natural place for it is not going to be where we kept ACLs. The other question one can ask is what exactly is a capability? A capability, remember, says you are capable of accessing that resource. You've been given permission to access that resource. So typically a capability implementation details differ, but is really a handle for a resource. A reference allows you to find or locate the resource and be able to use it. So if you can go use the resource, you shouldn't be able to forge a capability for that resource. Capabilities are sort of identifiers you can think of them or references or handles. But one property the system has to guarantee, is that these capabilities or handle that we're talking about are unforgeable. So think about the user has capabilities for a bunch of resources. List of those capabilities is what defines a C-list for that given user. The way where C-list is going to go, we have to have a catalog for each user that we have in the system. Similar to ACLs for each resource or object that we had before, here we have to have a catalog of capabilities, and system has to store the catalog of capabilities for each user that we have in the system. The main capability systems that have been explored and one of the interesting ones was a system called Hydra. It was sort of a research project done at Carnegie Mellon. This actually, you could have C-list stored in objects themselves. So everything was an object. It was an object based system. And objects stored a C-list. So when you came to a certain object, well you found new set of capabilities. And you could then use those to access objects that you couldn't access before. You can access this new objects while you are within this object. And you could take capabilities back to the object it came from sometimes and not other times, and things like that. The basic idea is that capabilities are how you decide, or the system makes the determination that you can access or not. So you, the user has to start with a catalog and as you execute maybe new capabilities come your way. And that was this example we're talking about. So one thing we didn't talk about is actually how does sharing happen. The whole idea of who can access what, and giving access to somebody else or the example or the quiz question we've been doing, is how does sharing happen? With ACLs, we didn't talk about it. But someone who has the right to let somebody else access a resource and make a system column have this new user or this user that I'm specifying should be able to read this file foo and what we're going to do is create a new ACE for this user, if one doesn't already exist, and going to add this new access right to that and then put that ACE in the ACL. So the new user will be able to go and access this file in the future. Capabilities, what do we have to do? Sharing is going to requite propagation of capabilities. The new user which should be able to access this resource In the future. We have to provide this user with the capability for this resource. So the capability has to be propagated. But remember they shouldn't be able to forge it, shouldn't be able to mint a capability unless it has been passed to them or someone had chosen to share the resource with them. With capabilities sharing is going to happen by propagation of capabilities. I'm going to give you a capability, you're going to find a way for you to gain access to a capability that you can then use to be able to use the resource. Remember, the one difference here is that possession of a capability means you can access the resource. There is no access check required as was necessary with ACLs. So when there's more than one way of doing something, well of course we have to decide if you're implementing an operating system or something like that, should I go with ACLs or should I use C-lists? I said most operating systems use access control lists, but some were done capability oriented or capability based. So there must be some pros and cons. One must have some nice things that perhaps the other one doesn't offer and then you have to see what is more important to you and based on that you're going to make a decision. Let's look at couple of these possibilities. Metrics that you perhaps can use to decide whether you want to go with ACL or C-list based implementation access control. The first thing we worry about is efficiency. So in this context, I said the couple of things when we're talking about implementation with ACLs and C-lists, with ACL I said you have to traverse the list. You have to go down the list looking for an Access Control Entry, or ACE, for the user who made the request. Traversing a list is there is some processing overhead required and the compute time it's going to take. Capability list I said it's a handle, shouldn't be able to forge it but once you have it it's been propagated to you. You present it to the system and system knows that by mere possession of this capability, you should be able to access the resource. So when comes to efficiency, C-Lists is faster. We don't need to traverse anything but ACLs have this negative thing were we have to go down the list. So efficiency would say c-lists are better. But lets look at another thing that's good to have, and that's accountability. So let's say if I ask you who all have access to this sensitive file? With ACL, what you're going to do is you're going to find the ACL, you'll go down the access control list, and you have every user and what kind of access they have for this file. So information about who can access this file is available. All of it is available in one place which is the ACL. So ACLs are actually good when it comes to accountability because we can go look in one place and be able to answer that question. How about C-list? Remember we said C-lists are stored either in objects or catalogs for different users and so on. So to find out who all have the capabilities, who are all the users who hold the capability for resource, I have to look on every user backlog. I have to look into every object that may store this capability, for example, if you want that capability to be to be used from that particular object. So now accountability is going to be hard because as we said we may have to look at all of the catalogs and things like that. So C-lists are not so great when it comes to accountability. Revocation is essentially you give someone permission to access a resource but in the future you don't need to share the resource with them anymore. So at that point you will revoke their access, remove their access. With ACL, how can we do it? So let's say Alice decides to revoke Bob's access for file foo. Well, it's easy. We know that there's an ACL for foo. And Alice makes a call to the system saying remove ops permission. The system locates the ACL, finds the ACE for Bob and in there removes whatever permission that's been revoked. So the ACL revocation is actually easy. They have this desirable positive property because we know how to go turn of the access right in the ACE, and all the ACEs are in one place. How about C-List? How do we revoke with capabilities? Well that's interesting, it's actually hard because we know the capability sits in Bob's catalog, and Alice can't go remove capabilities from Bob's catalog at will. How exactly does she do it? There are interesting ways in which you may be able to do it and things like that. But with C-lists actually revocation is not easy. So if you look at accountability, of course, C-list look better. If you look at efficiency, C-List look better. But if you look at accountability and revocation, ACLs look better. So there obviously these trade offs we have to worry about and because of that I said most operating systems actually choose to go the ACL route but we're going to see how maybe we can get the best of both worlds. So here we're asking, is the access control that's happening. Is it kind of based on access control lists or access control entry? Or is it being based on a capability? A scenario is Alice going to a theater purchasing a ticket, presenting the ticket to whoever is doing admission control at the theater. So the ticket, is it more like an access control entry, or is it more like a capability. That's the question. The ticket really is a capability. The possession of that ticket allows Alice to go in the theater. Be able to access the movie in this case. The reason it's a capability is, it's not an Access Control Entry. It's that, the person who's checking the ticket doesn't even need to know who Alice is actually. The mere possession of the ticket. Remember that sounds like what we were saying about capabilities. Mere possession of a capability actually allows you to gain access to the resource. In this case, the ticket really is a capability. We don't even know the user ID of Alice or the person who's doing admission control doesn't even know who Alice is so it's not an ACE. Because an ACE remember requires the UID and what kind of permissions they have. So this quiz the ticket is a capability. In this case, how does the ACL traversal happen? That's what the question is about. You have a couple of different options. Remember in this case, we want to grant access when there's a positive access right, and no negative access, okay? Because negative takes precedence over positive, that's what we have discussed. So for someone, there's a number of cases you can think of. If there's no positive and no negative access, then access should not be granted, because there's no access. If there's positive and no negative access then access should be granted. If there's both positive and negative, the negative takes precedence and negative should not be granted. So in this case, how does this get implemented is what the question is about. The first one says, we are traversing the ACL and we run into an ACE that says, this user does have access to this resource. You can stop right there. So this talking about termination of traversal that we are doing. Saying, you stop right there and grant access to the user who's making the request. Oh, this is not correct, because we said, there is a possibility that there is another ACE that contains a negative access. So we said all students has positive, Alice had negative, so then we have to go to Alice's ACE as well. So we can't stop at the first one, positive one that we run into. So this one, is not a correct answer. The next one, a negative or deny ACE is found, you actually can stop right there because negative takes precedence. If there's a negative ACE, then maybe a positive somebody else in the list, it doesn't matter, okay? Because this is going to take precedence anyway. Or, sometimes we may have to traverse unless use or implement a list in a smart fashion, that actually Windows does do that. But and it does that by putting negative ACE's in the front of the list. But if you don't do that optimization or that smart thing that I just mentioned, then you have to actually traverse the entire list to make sure that there is no negative or deny access ACE anywhere in the list. We sort of compared ACLs with C-lists and revocation was one of those things. So remember, we want to remove some users' permissions. So is it easier to do that with ACLs or C-lists, is the question. It's easier with ACL, because your traverse find the ACE for the user for whom you want to remove the access right and update that ACE or actually remove that ACE from the list if the user has no more access rights left. So we, you know, we know what resource we're talking about for which accesses are being revoked. We have the ACL for that resource, which has all the ACEs in one place, we find this user's ACE, take care of it. C-lists, we said, you know, capabilities are in the user's catalogues and things like that, but you need to be able to remove capabilities from certain places, which is harder to do, so revocation is easier with ACLs. Now we are going to talk about how actually Access Control is implemented in real operating systems. We actually going to talk about Unix-like systems, but there is similar things done in other operating systems. Let's focus on Unix-like systems. And so we have to think about users, resources, access permissions and things like that for us to understand how access controlled is handled by this system. So in Unix, actually every resource for which access needs to be controlled looks like a file. So in your next like system we also know that users have UID. Unique identifiers for the users that we have in the system. Users can also be groups and certain users could be members of a group. There is a special group. Every user in system that's the world group. And we look at access control for a file, we're going to say, well, who is the owner? What kind of access does the owner have? Is there some group that access to this file? What kind of access that is, and what about everybody, people who are not owner or members of this group? Since resources of files, they can be read, written or executed, these are the three access rights. So at this point, we know, we think about your access control matrix instruction, rows would correspond to each UID and each group ID that we have, GID and a special group world that we have. Column is going to correspond to each file that we have in the system and if you look at an entry in the access control matrix it's going to be a subset of read write execute. Unix system, the original ACL implementation was actually, had a compact fixed size implementation. Remember, ACLs are ACEs, so that's for each user, so the owner is one of the users, then the group would be the next one, and then everybody is this special world group that we are talking about. So actually had only three maximum of three ACEs. Actually had three possible ACEs and ACL it wasn't implemented as a link list. It wasn't implemented. It was implemented as a bit mask. In particular, we need nine bits. So the nine bits encoded read, write, execute for owner. Let's say the bit is on for read for a given write, then the user has that write. So the user can read it. This is how the compact fixed sized representation, perhaps in those days seven memory was important, so they implemented ACLs using this essentially nine bits. Little bit more complicated, but that's the main idea. A few things, few extra bits we'll talk about. Now this does have its limitations, for example, you can only have three subjects or principle whom access can be either just the owner or just one group or everybody. If we had 10 different users, you want to get information to six or three or whatever it is, we can't have an arbitrary size access control list with this representation we had here. So operating systems actually now provide implementations of full ACLs, and it's available in many operating systems, Linux, MacOS, BSD, and so on. I did say that I'm oversimplifying, and there are a few other things. So there are a few other bits that can go with this other information that we are talking about. In particular, there's an interesting thing called setuid, and maybe we want to talk about this a little bit. Think about a game program that needs to update a file that stores the scores of different people who have played this game. So when these people are playing the game, of course, we want to be able to update that file. When your score increases you want to update your score that's in there. So, these users while playing the game, and they play the game, need to have write access to that file. When they're not playing the game, we don't want them to have write access because then they can go change the score without playing the game, so we don't want that. So how can we solve this problem? These systems I'm talking about, this problem is solved by having something called a setuid bit. So the way to understand this is that we have Alice and Bob, and Bob owns the game file, and Alice wants to play the game. So the game file is actually owned by Bob, and Bob is going to give permission to Alice to execute, being able to execute this file. By the way, the score file is also, Bob is going to own the score, and is not going to grant write access to Alice because Alice should not be able to update that file unless she's playing the game. So you can think about the score file, Alice can read it, her score and somebody else's score, how people are doing, but she can't write it. Bob can rewrite it because Bob owns the game file that we have here. He's some special user, or someone who is running this program that we have. People who want to play the game. So, setuid bit is set on the game file, the executable file that we have. And we said Alice executes this file when she wants to play the game. So setuid bit, when it's set on executable file, actually does an interesting thing. When Alice executes this file, the User ID temporarily changes it from Alice to Bob. So it says when a setuid file is executed, the User ID, effective User ID, at that point is not the user who actually executed the file, but is owner of this file. In this case, the owner of the file is Bob. So during the execution of the game, although Alice is playing the game, during the execution of the game, the temporary or effective UID is Bob, and because of that, even though Alice is playing the game, she is able to update the file. File is updated back that is in this game file. And while we're executing the game file, as we said, the effective User ID becomes Bob. Alice launched this program, but the program is owned by somebody else. This change in UID is possible when the executable file that Alice executes, the game file here, has a setuid bit set. So a setuid bit is used to change the UID temporarily during the execution of the program on which this bit is set, and I hope I motivated why you may want to do that, in this case, for example, the game file that we are talking about. So we have been talking about implementation of access control and the resources and request for those resources and we said well in Linux based systems, resources look like files. So let's actually see how the resources are used and you know what kind of calls we make for accessing them and what happens during those calls. So, first of all, of course, resource has to be created. So the process that creates this resource is the owner. That owner can have whatever set of access rights they want to have. In a Unix-like system, you probably know the number of ways in which you can create a file, for example. So once a file exists and we want to access it, you do what is called open the file. S this is sort of like prepping the file. If it doesn't exist, then file gets created. When you do that, file has a name and mode here says, do you want to read this file or do you want to write it or execute it. What are you opening it for? How do you want to access it? Once that file exists, the first thing we do is open it. When you open it the operating system returns to you a descriptor which is a small number, and we'll see what the use of that is, but as a result we'll see that to be able to access or get the data that's in the file, write it into the file, we're going to need the file's descriptor. So the descriptor comes when you open the file, you prep it or get it ready for access. So a system call you make saying, this is the file I want to access, this is how I access it. The operating system prepares the file for accessing, returns you this file descriptor that you then hold on to. So if you want to read this file in the read call, this is another system call that you have. You have to go to the system. You specify what file using a descriptor. This is what was returned here. It's a small number. So this could be file for example. If you're reading you are to specify where the read data should be put, how much data you are reading. That's the buffer, and the size. Data is going to come from the file. Where is this data? Well, the file we know, unless file point or that points at the next place from where your data is going to come and it is advance that you read. So whatever data follows the file, current file pointer, we are going to pull that data, lays that in the buffer and that's what the recall is going to do. Similarly there's a write call, except that the data flows in the opposite direction, so the data is here then it's going to go into the file. And how much data, that's what we specify in this call. So an open is followed by one or more read or writes. And when you're done you close the file. And once you close the file, again the input here is the descriptor that says what file that you had open before. Then this file we are basically telling the system is that we're not going to need to access it anymore. So the system can, if I'd set up some data structures, it can essentially free those up at that point. A resource is a file. Users have UID, we also talked about group ID, and to access a resource, this is what we basically have to go through, file exists, they'll open it, then then they have to say read or write it, and then you close it. If you forget to close it and the process terminates, it's implicitly closed, but this is how a file gets used. Talk a little bit more about what happens when you open a file, and how does access control factor into it. So, process here is making these calls we're talking about. It first opens this file F, without specifying the mode here, but maybe this is read. This line is going from user to the operating system, system call, this is the boundary between the two. This is untrusted, this is trusted, as we talked about. So, this is all in the operating system. So basically, the file got stored on the disk, and files that are actively being accessed, the operating system keeps track of those information about a particular file is in a file control block. So if you've done the operating system's class, you have some idea of how file systems are implemented. But for each file that we want to access, we you know meta-data that we're talking about for a file you know what's the file pointer, where should we read next, where on disk the file blocks are. Other kind of information. We keep track of that in the file control log. In Unix, well that's called an i-node, and there's a table of these. That's where this i-node table stores the meta-data about the active or currently open files. So this is, here, I'm saying it file meta-data table. A file that you're going to open has to be active, it it's not active, it's going to get activated. So, there's something interesting happen, when you open a file you tell the filename, the operating system's actually is going to go and say, is this file currently active. If it is active then there is going to be a file control block for it, okay. That's where the meta-data for this file is going to be. So a process also has another data structure that's called an open file table. Okay this is kept by the operating system. It's per process data structure we're talking about. So in this open file table that we have for this process that is making this open call, basically what we want to do in the open call is that we want to see if the file is active. If it is then there is a meta-data block or file control block for it. In the open file table, there is some set of entries that we have. This is a fixed sized table. We're going to use one of those entries to actually store a pointer to this. And the file descriptor we return is actually nothing but an index into this table. So i is the index in this table that we're returning. I think it was about access check when do we perform. Actually in Unix-based systems, access control is done when you open the file. At the time the file is opened remember we're going to go in, we're going to find out if the file is currently active or not, we're going to locate its file into a block where the meta-data is. In particular the ACL is here too. With the ACL is here, then it was nine bits that I was talking about. Depending on the user or group the user is part of. I'm actually going to look at those ACL bits that we're discussing. And if the mode here is read, and you have read information, I do all this. Okay? I'm going to grant you access Mode is right and the right permission is here. I'm going to do all of this and give you write permission and to place data in this file. So access check is going to happen during the open calls execution in the operating system because we locate this information where the ACL information is and we're going to look that up. And if everything checks out we set this up and return this file descriptor. So the reason we do it at the time it's open is, that when you come to read, you specify a file descriptor. So, it's going to say read I here, for exam, that's what you're saying. Well, once I come to the operating system, it's the that remember is an index into a table, that tells me, it's this particular entry in the table. Follow the pointer that it points to, that's where the file meta-data is, and that tells you where the file data is. I don't do any access check at the time I do a read or write. I just sort of quickly follow these pointers and get to the file. If I'm doing reads then I find the file data to return to copy that into the buffer. If I'm doing a write then I copy the data from the buffer into file cache or something like that. It gets copied. So there's no check done during either read call or a write call. The check was done when we did the open, okay. So the fact that the script was returned to you at that time we did the checking and now you're able to do that. If we opened it for read and you tried to do write, of course. Of course we're going to stop that. Okay. That goes without saying. We don't have to go look for the particular to do the transversal that we are talking about. So we want to talk about something called a Time to Check versus a Time to Use problem. Time to Check is when you open the file. Time to Use is when you either read or write it. So a time to check, time to use vulnerability arises because access check is performed separately from use. Typically when you say you want to use a resource, you perform the check. But the model that I just showed you with the open call coming first and then read writes happening later on, and you can do as long as the file is in the closed and the process is running. Okay, so it could be a while before you do your read or write after the open was done. So when the two are separated, this vulnerability arises. And based on what time to check versus time to use, think about what these are. I want you to look into these two options that we have. What is the reason for this TOCTOU vulnerability that we're talking about? So let's look at the two options we have here. First one says, file permissions can change after an open call. Remember checking was done when an open was formed. This call was made, this call complete, that's when we down to the IO table or the file meta-data table, setup the open file table entry, return an index, all the checking got done. Well at that time, this user lets Alice was able to read the file. Later on, Bob who owns the file comes and turns read permission off for Alice. But Alice's process is still running. And Alice's process to actually read the open before Bob was able to revoke this permission. And Alice's process hasn't closed the file yet. Will Alice be able to continue to read this file? Yes, because we're not performing any more checks. The checks were done at the time when we opened the file, okay? So it was separated from the use. So the check was during an open( ) call, okay? So sequence here is Alice did open, sometime after that Bob comes and removes the permission, and sometime after that Alice is trying to read this file. Alice is doing read not open now, so there's no check happening. So if file permissions change after an open ( ) call, it completes, but before it's closed, Alice has the file open, hasn't closed it, because if she closes it, and then it has to be opened again, we'll check again. And when we check again, Bob's change might be seen by the system. So if permissions change between the time we check and the time we use, well, that actually is the reason for this vulnerability talked to vulnerability that we're talking about. And the second option is saying, the file permissions only change when the file is currently not opened by any program, then this vulnerability will not arise. We talked about how file system implementation works in Unix-like systems or Unix based systems. And said open returns a file descriptor which is a small number. And the question is can a process share a file with another process by just sharing that file descriptor? The answer here is false. You cannot do the sharing by sharing the file descriptor numbers because those descriptors point into a per process table we're talking about. Descriptor five value FD being five means the fifth entry in process one's open file table points to something. Okay, but process P2s open file tables is entirely separate. So fifth entry in there may be pointing to nothing or may be pointing to altogether different file. So if you try to use the descriptor, it won't take you to the right place, in particular the file that process P one is trying to share it. Well it has the operating system data structure set up to get to it, and these are per process, so the same thing as done for P2, it's not meaningful to share file descriptors in Unix based systems, so the answer here is false. We talked about the SetUID bit and in particular a game file and a game score file and things like that. So this question is about the SetUID bit. In particular executable file F1, so that's a program file has its SetUID bit set. And it's owned by user U1. User U2 executes F1. So we are assuming that U1 who owns F1 has given permission to this file to U2, which user ID is the effective UID when F1 is being executed in this case. When the setUID bit is set, the UID temporarily changes to the owner of the file that is getting executed on which the said UID which was set. Owner is U1. So although the process that we're talking about was started by U2, during the execution of this file F1 One, user ID is going to change to U1. And it's going to change because F1 has the SetUID bit set. And when the SetUID bit is set, the effective UID becomes whoever the owner of this file F1 is. So in this case, the answer is U1. In some systems we have what is called, Role-Based Access Control. Role, sort of, is what you do, your function perhaps that may learn what kind of files you can access if your human resources or payroll, or you are in the different department. The set of files you can access would depend on what your role in the organization is. So there is role-based access control or RBAC that is implemented, it's a little difference so I thought we'll just spend just a couple of minutes talking about it. In role-back Access Control, remember, rights are for being able to access different resources, so those are access rights. Those are associated with the role, they're not associated with the user. So the way to think about the way you going to define your policy, access control policy is you're going to say, what roles do I have in my system? And for each role, what kind of resources do they need to have access to? So I would say people in this role can read these files or write these files and things like that. So this part of the policy basically says, access rights are defined for roles. And users who going to log into the system or authenticate themselves to the system can then take on some roles. So RBAC, the way to think about this is that we don't have direct access rights for users. Users must be activated into one or more roles. And once they assume one or more roles, based on what those roles give them access to, that's what the user, or the process running on this behalf of this user, can access. So we're talking about enterprise setting. Access may be based on job function or role of a given user. That's where this might make sense. Project manager may have access to all the files. The developers or people doing QA or something like that may access files in different ways and so on. Payroll manager and HR function may be able to look at files that have people's salaries and so on because they are payroll managers and things like that. So this is how roles will be defined. Access rights associated with roles, as we said before, and users when authentication happen and then we know the user is was logging into the system, we have to then so to start with authentication, but then be able to activate one or more roles for them. So role activation is something else that has to be added to this process. The policy defined here and there are two stages, activating a role for a user and based on that deciding what the user can access. Now there's some benefits that come when we have role-based access controls. What could be some of these benefits? So first of all we said our policy defines what roles have kind of access for resources in the system. So the policy doesn't need to change when, let's say a certain person leaves the organization. Policy is associated with roles. It's not associated with users so users coming or going they don't require changes to the policy the organization has. When a new employee comes, basically we think about what role is appropriate for them. And as soon as we decide their role, based on their function and what resources they should have access to automatically happens, because that role, along with it has had an access right to various resources. And that, as I said, happens automatically as soon as we decided what role this new employee can take. An interesting thing, least privilege, remember, is one of the design principles that we had, which said you should always execute with the smallest number of privileges or access rights that you need to do what is being done at that time. And it's a really damage containment thing. If something goes wrong, you don't negatively impact resources that you have no business having access to at that point. So how does RBAC help you with that? Well actually it does, because user can start in one role and access a subset of the files that are only available to that role. The user can then switch roles and then go access a different set of roles, a different set of files associated with a new role. If you don't have RBAC and a user has a UID and has access to everything, he or she can never access. Roles sort of give you this ability to control, saying if you're in a certain role at a given time only the resources needed for that role should be available at that point, and we can do that with RBAC. Being able to implement least privilege is a good thing. That the design principle for systems we want to trust, so RBAC actually enables that. So there are actually systems that implement RBAC. SELinux, we're going to come back to it, is Security-Enhanced Linux actually supports RBAC, and there are others as well. In a system, our systems that don't support RBAC but allow user groups to be defined, benefits of RBAC can be realized with groups. The question is can the benefits of RBAC be realized with groups? True that groups are basically give you the same benefits that RBAC gives you or is it false? And the way to sort of think about this role is sort of more based on a function or job, which is closer to what kind of resources you need for that. If you're a payroll manager you need access to payroll files, for example. That's what you're talking about. So roles are sort of characterized more by function and the resources that go along with that function. Groups, on the other hand, are basically what set of users or what UIDs or what subjects have some sort of similarity so they can be grouped together. Okay, so roles are sort of more resource-related versus groups are more subject or user-related. So that's sort of the way to think about the two. And then the question is can we get the benefits. So depending on how you actually implement groups, role activation could be like activating a group for yourself. And deactivation is leaving the group and having permissions associated with the group. So only when you activate your members in the group you able to access it. So in theory, the answer is that you could actually do the kind of things you can do with roles with groups as well. But there's sort of this, basically the difference is in how the two come about that we talked about. But the way, actually, groups are typically implemented in Unix-based systems, actually they don't work the way that I was talking about. You don't join a group and then leave it and things like that. If you used how we implement groups and define access based on those groups, if you want to say I want to go and basically use that to get RBAC, you're not going to be able to get that in systems that we have. So there is a difference between the two, and I said the difference is somewhat subtle. Possible to sort of think, subject-focused then centric, then you are sort of in the groups. Or object or resource-centric, then you are with roles based on the job function. But if we look at how we implement groups, and what kind of functionality that you want to have with roles, I would go with the false answer here to this question. We talked about another design principle, which is fail-safe defaults. What it implies is that when access control policy is silent, if it fails to be explicit about how someone has access or not, what should the system do in that case? What is the fail-safe default when explicit access is not defined. So these are two options and if you are following fail-safe default, which one would you pick, is this question. Well actually fail-safe says you should deny it. If access is not provided explicitly, then the default is deny. So the only way for someone to gain access to a resource is that it is guaranteed to them. If we're silent about access being granted, then basically we assume it should not be, so the default is deny. And the first option is the right one. We saw that Access Control is a fundamental requirement for computer systems that protect resources. We started with the Access Control Matrix Abstraction and how we can implement using Access Control Lists and capabilities. We also explored how operating systems implement Access Control We discussed access control in the last lesson. But that was only one kind of access control called discretionary access control, or DAC. That allows you to decide who you want to share a resource with when you create the resource and own it. If you work for a company, or a defense or intelligence organization, you don't get to decide how information that you access can be shared. The company or the agency will mandate how various types of information is accessed by different users. For this, we're going to develop a number of mandatory access control, also called MAC, models. Once we're done with, this we're going to revisit the idea of a trusted computing base, or a TCB, and explore how we can evaluate the level of assurance that is provided by such a TCB. In the last lesson, we focused on access control, but in particular what we called discretionary access control. The idea there is that access control is about checking if the source of a request should be able to gain access to a resource. We know that who the user is because user comes and authenticates himself or herself. So in discretionary access control, we said whoever creates the resource is the owner of that resource. So the owner can also selectively grant access to other users. For example, Bob may both be able to read and write, and Charlie can either read or write or maybe just has read access or something like that. So the idea of a discretionary access control model is that resources have owners who are users. And these owners get to decide, it's at their discretion, who else should be able to share these resources. In this lesson, we're going to examine if this model has any limitations or any problems. Are there situations where it's not going to do the job for us? And if that is the case, then what other kind of models do we need to develop to meet those applications, or those needs that arise when we talk about access control? In particular we're going to focus on two problems that DAC has. The first problem is that although the owner of a resource, owner of a file let's say, owner happens to be Alice, controls who this file can be shared with, and Alice gives Bob access to read this file. The problem arises because Alice is not going to be able to control if the information is going to stay with her and Bob only. So the problem is, can Bob share this information further with somebody else? One problem that we have here with the DAC model is that although we can control direct access to our files, but how that information can be shared with others we're not able to fully control that. In particular, this problem is called the information flow control problem. So information was shared with Bob, flows from Alice to Bob, but can it flow further? And in the Discretionary Access Control model we're not able to control this flow going from Bob to Charlie. There is another problem that Discretion Access Control has and this is because of the idea that resource is owned by the user and it is at his discretion who it can be shared with. Well, in many organizations at companies it's really not at your discretion. You are an employee of a company and the company actually controls how certain type of data should be shared in that company or outside of that. In other words, the employer of the company where the user works actually may be able to mandate or will mandate actually how certain kinds of sensitive data can be shared. So to address these two problems, we actually going to explore another model for access control. It's called the Mandatory Access Control model and it's called Mandatory Access Control because it is not at the discretion of the user. Discretionary Access Control is at the discretion of the owner of a resource. But here organization is going to mandate how data can be shared and that's why it's called Mandatory Access Control. In this one we're talking about a company, in particular payroll data, who gets paid how much. And so we have a file that stores data that is created by a user who works for the payroll department. And obviously this user is an employee of the company. But let's say it has a compensation information. By the way, there is a large company right next to the Georgia Tech campus had a situation where people's salaries were leaked. And that happens when somebody shares the information, either by mistake or by design, with others of whom this information should be shared. So here we're talking information of that kind. Access to it has to be controlled. So what kind of policy is going to help us provide the right kind of access control for it. There are two options. It could either be DAC or it has to be MAC. So I want you to think about it and then we'll discuss which answer is the right one. Here we talked about the nature of the data, okay? It's sensitive data who gets paid how much. Company obviously doesn't want to disclose that to people for whatever reasons, even within the company or outside. So the user who is creating this file is obviously should not be able to share it with whoever he or she feels like. Even if they try to be cautious and careful, remember, they can make a mistake or they may not be. We talk about going back to threat models, talk about insider threat. Someone can be unhappy and do something that could harm the company. So it shouldn't be at the discretion of this user who is creating a file. This kind of data really needs to, the company needs to, not just mandate but control how it can be shared. So, really, this is an example of where a mandatory access control model would be the right one for controlling access to the data that is in this file that has people's salaries. DAC and MAC, these are two different models. And this is a scenario where DAC is not going to be able to address or meet the needs that we have for how this data should be accessed. Well, what exactly is mandatory access control? Why do we need it? I think we already talked about that, although the users, it's people who actually create information and run applications that manipulate such information. But people work for a company or a garment agency, we'll talk about that. And it's really their employer and somebody that's trusted by them already acting on behalf of them, should really be able to specify how certain kind of data can be shared. First of all, in taking the power of deciding how the data should be shared from the user, and, essentially, you can think about the user being told the right way to share this data is based on this policy that the company has. And this policy could be, it may depend on the kind of company that we have. So, for example, hospitals with electronic medical records and stuff like that clearly have patient records. And we know that patient health information, their health conditions, information about them is highly sensitive, and who can see it has to be limited, and how it can be shared has to be limited. And it's not really up to just the hospitals, they would want to do it. But there are regulations that force them to do this or force them to control how patient records can be accessed. So in the context of so are held information. We have HIPAA, which is the regulation the government imposes on anyone, entities like hospitals, that collect information that goes into patient records, medical records. And of course, there's a need to share that information. If you go see a different doctor, they need to have access your medical information. But how access to it should be created? It's not the particular person who does data entry for a patient's record. They don't get to decide who else can see. This has to be decided by the hospital that's informed by this regulatory policy called HIPAA. This is protecting people's health information, privacy, and things like that. We said in mandatory access control, it's not just the user owning a resource and being able to control, but it's really the user's organizational company. Hopefully, this example information, being medical records, convinces you that sharing of information has to be mandated by someone other than the user who's creating it So, before we talk about this different kind of models, and we will talk about a number of them, let's sort of explore what is new thing that we need for implementing them. So, we're obviously, I said, going beyond discretionary access control. Well, implementation of this mandatory access control model is going to require some new functionality, some new mechanisms the system has to now include. So one of the new things that we're going to talk about all the time is, what is called sort of these labels that we're going to associate. That's a key new implementation requirement for defining these models and for implementing these models. But idea is that we're going to see that both users and resources of documents are going to have certain labels attached or associated with them. And they're going to get used in the way we make access control decisions. So what's the purpose of a label that you want to attach to a document or associate with a user? The label is actually going to tell us how sensitive certain information may be. Labels also actually have something that describes the nature of the data. What topic, what area does the data come from? We talked on payroll as another example, before. So labels could also include sort of a category of the nature of the data that we have here. And that's sort of useful in answering this question, who needs to know or needs to have accessed? If you're in the payroll department, of course you need access to payroll information. Labels with documents, or data files, or users, are going to capture something about the information that's contained in those documents and the users who need to access them. So these labels are actually going to get manipulated each time access to an object or document is requested. So TCB is actually not going to associate, but it's going to use these labels associated with user and the object any time the request is made. So when I say the labels look right, of course we have to sort of say, here's the label for the user, and here's the label for the document. Should the user be able to access this document, to be able to read it? So we have to sort of relate or compare the labels, and we're going to see results of that comparison. But you're going to say, here is user with this label, here is a document with this other label. Does the user's level imply that he can access this document? So we have to relate, compare, the labels that we have. Now exact nature of what the labels look like, and how we are going to be able to compare them, and what the result of the comparison is, well, that's going to depend on the particular model or policy that we're going to implement. We're going to look at several different examples, and each one of them has a very different kind of a label, and how you make use of that label. A label goes with, not just the document, but also with a user. And we'll see examples of those. So in the Department of Defense, we're talking about the labels actually going to include. For a user, it'll be their clearance level. For a document, at what level the document is classified. And compartment is this category that we're talking about. What kind of information is contained in this document? Of course, in the commercial world, we don't have clearances and classifications. So these labels have to look different. And the concerns are going to be of a different kind, also. For example, we may have conflict-of-interest concern. So access control must be such that when somebody has a conflict, they don't get to access a certain document where a conflict-of-interest, or COI, may arise. Similarly, sometimes we may have separation-of-duty requirements. The same person can do two different things, when allowing them to do that requires a level of trust or potential for fraud and things like that. So, the needs would be different in commercial versus DoD, or garment DoD intelligence agency. So we actually going to explore this idea of labels, that a mandatory access control policy would use in a DOD, Department of Defense kind of environment. A little bit more by looking at some examples. So information is going to have classification and users going to have clearances, okay? So the highest sensitivity typically is top secret, that's the innermost one then secret, confidential and going out we have this unclassified we said this information anyone can see. So most sensitive, least sensitive, this is. We said, the level at which has somebody is cleared or the classification level of the document. So we said label is going to be a sensitivity level. So for example, it could be top secret here and a compartment that describes what kind of data is contained in the document. So are we going to, let's say look at documents one, that is top secret. And let's say these documents have information about various arm stock piles. So this document let us talks about nuclear and chemical weapons. You hear about them in the news, so this is clearly extremely sensitive. So we going to say that is top secret and the document contains information about both nuclear and chemical weapons. We have another document that has information about let's say conventional. So maybe it's sensitive but it's a notch below. So let's say this is only secret. So we have two labels here. L 1, L 1 sensitivity level it's TS comma the compartment is nuclear, comma chemical. Remember this compartment here looks like a set, okay? So their document may pertain to multiple topics or areas. Each one of them could be in this and that's the description of the document using the set of terms that we have here. So L1 is (TS comma {nuclear, chemical}). L2 is secret (S comma {nuclear, conventional}). Using these labels L1 and L2, we're actually going to be able to decide axis control in the end comes to being able to read and write documents because that's what you do to files or documents. So we actually, when disclosure or confidentiality is an issue. There is a model, Bell and La Padula model or BLP model that makes use of labels exactly like the L1 and L2 that we have here. And actually the model is defined by a set of rules that tell you when, based on these labels, you can read or write. But implementation of the mandatory access control, is always going to require some kind of labels. We're going to use this quiz question actually to reinforce the idea that mandatory access control applies in nonmilitary setting as well. So hospital is found to be lax. Unfortunately, there are examples like these that you read about in the newspapers almost on a regular basis. So they were lax in securing access to patient records and they have a breach. And a breach means basically someone who's not authorized to access those records is able to do that or is able to gain access to them. That's not a good thing. So the hospital has violated some kind of a policy. And that policy, really, we talked about in healthcare regulation, is what requires that you handle data with care or in a certain way. So this quiz question basically is what policy may the company have violated? And there are two options. Think about each one of those and then we'll discuss the answers. Since we talking about health information, remember the BLP model was talked about in the context of military or intelligence setting where we have. We're going to see that confidentially is the reason for it, but information that's classified and things like that. So this is an for the health care environment that we're talking about. For health care, we actually have HIPAA, that's the regulation. And HIPAA is the Health Insurance Portability and Accountability Act, I think goes back to the mid-90s when this was put in place to protect patient privacy and so on. This HIPAA requirement is why hospitals must have these policies for controlling access to patient records and those policies are often mandatory access control nature that we're talking about. Just because someone did data entry about a patient record, of course they create that file but they don't have the discretion to decide now who all they can share it with. It's governed by the hospital's policy on how patient records should be accessed. So that's what makes it mandatory. So the answer here would be HIPAA because we're talking about patient records in the hospital setting. There's an interesting story in the newspaper a while ago, about how exclusive are these people who have access to classified information? It looked at various types of clearances people have, so one of them was how many people actually have secret clearance. The question is, in the US, there are various types of clearances, but how many people actually have such clearances? Is it pretty small, 10,000, or runs in the millions? I want you to just go with your gut and come up with a guess. We'll see whether you're right or not. One thing that was interesting or surprising in this story that I just mentioned is just the large number of people who seemed to have these clearances. Actually, the answer here is the highest possible number we have in these different options. Close to 5 million people currently hold clearances. So it's not a very exclusive group of people. If you talk about 5 million people who have access to secret, or depending on their clearance level maybe top secret, information. The idea here is that people need to have clearance levels, but it looks like a lot of them do actually have that. Now we talked about the need for labels when you want to talk about MAC, or multi-level security. And we even said, well, user has a certain label, file has another label. And we have to say, we have to check, if user with this label L1, should he or she be able to access the document that has label L2? So we have to sort of compare these labels. So here, we're going to explore how that comparison might work. So let's just talk about labels of the type that we discussed in the context of the military intelligence community and how they access information. So, sensitivity, how sensitive data is. We said that, we had those circles, top-secret is the most sensitive, secret, next level, then classified, and unclassified. We had restricted in the middle there, but I'm just picking these four examples. So what does this greater than mean? So we are saying that, these levels are actually ordered totally. And by total order, what we mean here is that if I pick any two of these, I can tell you which one is more sensitive than the other one. So TS is the most sensitive. If you took TS and any other of these levels, then we know that this is higher, or is greater than, the other level. Similarly, secret is greater, or higher, than classified. So this greater really means it's higher level of sensitivity. And total order basically means, when you pick any pair so the relations and various kind of orders that you can define for the elements that are in a given set, that's where it defines a relation. So the total here means you pick any two elements of this set, and there is an order between them, so one is always going to be more sensitive than the other one. Here we see that through this greater than sign, symbol that we have. We do know that labels have a level, sensitivity level. But they also have a compartment. And we said compartments are sort of sets of different topics or categories. And how do you compare sets? You take set S1 and S2. S1 may be contained in S2, S2 may be contained in S1, or neither set may be contained in the other one. So if set S1 has some elements that S2 doesn't, and vice versa, then neither set will be contained in the other. So when you have sets, the way you compare them is containment, or subset, or is one a subset of another one? And we know that that order is a partial order. So total order is when, pick any two, and one always is greater than or higher than the other. When you talk about sets, we know there are going to be pairs of sets where neither one contains the other one, and they're not going to be ordered, so that's called partial order. So we'll explore this a little bit more through examples. But if a label is going to have two parts, one part is totally ordered, one part is ordered sometimes and not comparable at other times, so that's the partial order. So when we talk about actually ordering labels, we had to look into the label. We have to look into the sensitivity level. What are the sensitivity levels? If the labels that we're comparing is L1 and L2, then we are to look at L1's sensitivity level and L2's sensitivity level and how they compare. And then we're going to look at the compartment of each one of those, and then compare those compartments. And by comparing both parts, then we going to be able to decide how to order the labels. And this idea of comparing labels, we're going to explore that through an example. L1 is uppercase L1 is lowercase l1, comma Comp1. And similarly L2 is lowercase label l2, is lower case l2 which is the level, sensitivity level of label two, comma, comp two. So the comparison is going to yield, what we call what label dominates the other label. Okay so L1 can dominate L2, or L1 can be dominated by L2. Or, maybe neither one dominates the other one. So these are the various possibilities. So L1, label L1 is your dominate, label L2 if, level 1 is greater than level 2, and the categories or the topics that make up compartment one contain the topics or categories that are in compartment two. So essentially, we are saying lL is going to dominate if it's higher sensitive level, and the categories actually are a superset, or contain the categories of compartment two. It it's dominated then it's the other way, okay? So in that case, label l2 is the dominating one, so obviously level l2 has to be greater than level l1, and the containment that we had before has to be in the other direction. Okay, so this is the subset relation that we have here. For the two labels to be equal, of course, each part has to be the same. So the levels have to be the same, and the compartments have to be the same. So if L1 and L2 are not comparable, and what do we mean by that? That means it's one of the first three. They're not equal, and neither one dominates the other one. Then we say that they're not comparable. L1 doesn't dominate L2. L1 is not dominated by L2. And the reason for that is, we're talking about a label having two parts. So levels are obviously totally ordered. So you cannot be, no you're not comparable. It's not because the levels that you have within your label L1 and L2. It's actually going to be because the compartments are sets, and these sets are going to be, neither one is going to be dependent on the other one. So, they each have some elements the other one doesn't have, and because of that, this containment property is not going to be satisfied. And, because of that, neither of these are going to be holding, and when they don't, then we say that these labels are not comparable. So one thing, if you take these labels and arrange them sort of what kind of structure you get. Mathematics, actually their name for that structure, it's going to form a lattice. And when you have a lattice, you have the elements that are the least upper bound, and the greatest lower bound, and things like that. So you can always find labels that will dominate any set of labels that you have. So that would be finding an upper bound, and you can find the smaller such upper bound, so that's LUB and things like that. So when you compare these, instead of arranging what's dominated by who, we get this structure because of the partial and the total order we have on the different parts. The structure you get is what is called a lattice. Comparison among labels, just look at a quick example. L1 is TS and itss compartment has these three categories, A, B and C. L2 is secret with A, B and L3 is secret also with B, C, D as their categories. So does L1 dominate L2? TS is dominate S and A, B, C contains A, B. So, we said that each, for L1 to dominate L2, the sensitivity level has to be greater, which is the case, and its compartment has to contain the compartment of L2, which is the case. So this is, yes. Does L2 is less than L1? Yes, the same reasoning we did. It's less because this is less than TS and this is containing that. How about L1 and L3? As we know, TS is higher than S. So when you look at the first part, just the sensitivity level, that's the total orders, so this is greater than this. But B, C, D is not contained in A, B, C, okay? So there's this element D that's not contained here and because this is not contained here then L1 is not greater than L3. Similarly, L3 is not greater than L1, and the two are not equal because the compartments are different and the sensitude level is different. So, when neither one dominates the other one and they are not equal, then they can not be compared, which is the third case that we're talking about. And this one, we talked not about the labels that we were just discussing but we're talking real numbers. So these are number, integers, rational numbers, we have real numbers, it's all the numbers. So take any two numbers and there is this thing relation called less than it. So, number 1.5 is less than 2.5, for example. So, if you look at the less than relation and real numbers, does it define a partial order or does it define a total order? For numbers, the answer is actually going to be total order. If the two numbers are different, then one is definitely going to be greater than the other. Or, in this case, actually we're talking about definitely going to be less than the other. We don't have partial orders when you just look at real numbers and the relation is less than because you take any two Numbers. As I said, if they're different, than one is definitely less than the other. So you'll be able to order them. When you can order any pair of these numbers, that means you get a total order. In our next question, actually, we are talking about the kind of labels that we have seen, that we discussed. So, two labels. Remember, I keep talking about labels. The uppercase L labels are the secret or top-secret sensitivity level, so L1 is secret comma, this is pretends to since this comes from the DOD, has a different command, so let's hit towers Asia and Europe. L2 is something about Europe and South America. So one is secret, the other is top secret. So there are three options and I want you to look into each and if it is true, then you check that box. So let's look at, does label L1 dominate L2? It doesn't because secret actually is not higher than top-secret. The domination fails right here. This doesn't contain this either, because there's no South-America in here. So L1 doesn't dominate L2. Does L2 dominate L1? Well, top-secret actually is higher than secret but the compartment here doesn't include Asia, so the containment doesn't hold. So L2 doesn't dominate L1 either and the two are different, so actually, in this case we're going to run into the third option, which is the correct one. Neither L1 nor L2 dominates the other one and they're different, so they can't be compared. So this is Sensitive Data Quiz. Again, select the best answer. Assume that label L1 of a document D1 dominates label L2 of document D2. There are two documents, D1 and D2. The label are L1 and L2, and these labels are defined the way we've been discussing. So these are, let's say, DOD kind of documents. So sensitivity level could be top secret or secret and compartment is what kind of information is contained in this document. The question here is that L1 dominates L2. It dominates L2, so we know L1 is greater than L2. This is sort of what is the interpretation on that domination. I guess that three options sort of present three different interpretations and you have to say based on what we understand about label domination which one is the correct interpretation. So if Label L then dominates Label L2, this is because either the level is higher. So, for example, this could be top secret and this could be secret. If that's the case, then D1 would contain more sensitive data than D2. Okay. So this actually is correct. When D2 is dominated by D1, which is the case here, because L2 is less than L1, or L2 is dominated by L1. It cannot contain more sensitive data, so the second option is not correct. The third option says the data contained in D2 has a narrower scope as defined by its compartment. So what exactly does narrower scope mean? Remember, categories define what the more categories the data, at least the way we define a label comparison, so that one set contains another set means this information is about more topics or more areas or something like that. So, since L1 dominates L2, the compartment of L1 has more elements in the compartment of L2. Okay, remember L1 compartment, if I use this notation, is a superset of L2 compartment. So in this case, we know that the categories that we associate with the data that's in D2 are going to be fewer. So that's a narrower scope and we can say this is true. We've been talking about labels and their use in mandatory access control. Now we're going to get to a concrete model. This is a model that deal with confidentiality and it was introduced earlier is the Bell and La Padua model. Project that developed this model was funded by the Department of Defense, so the kind of label we're going to talk about are the type we've been discussing, where you have classification and clearance and things like that. Couple of things to keep in mind here is that confidentiality is the issue here. We're concerned about the disclosure of information. So more sensitive information we don't want that to be disclosed to someone who is not cleared at that level. So that's going to be one. And since we're talking with DoD the other thing to keep in mind is the labels what they're going to look like. As I said, the DoD means it's going to assume that information or data that's contained in files is classified at levels like TS, top secret, secret, and classified and so on. The second part of a label is a compartment or category that as we said before. It depends, what is the document about. So is it chemical or nuclear or conventional or two kinds or something like that. And the rule for read says user with label L1, top secret user, for example. Who's working on, let's say, nuclear and conventional. So that would be this user's compartments. So user with label L1 can read document with label L2 only when L1 dominates L2. So this domination that we defined before when the label of the subject or the user dominates the label of the document, then they're able to read it. This is called the read-down rule. You can read documents that are classified at your or a level that's below yours. Or further down from your level. So that's the read-down rule. It's also called the simple security property. So how about writes? Because you both need, you need to address both read and write. So user with label L1 can write document with level L2 when L1 is dominated by L2. This is saying the user has to be at a level that is lower than the level at which the document is classified. Which means if the user is secret, he can write a top-secret document, he cannot write an unclassified document. This is the write-up rule. So you can read down, but you have to write-up, and it's called the star property. So let's just see what's the sort of rationale for writing up. Remember, our focus is on confidentiality or disclosure of information. This model says more sensitive information cannot flow. Well, the information flow control requirement that we had. So more sensitive information should not flow to users who are not cleared at that level. It's basically focusing on disclosure. It doesn't say anything about who can write information. That's sort of an integrity question and we have to revisit that perhaps. But if I can write at my level or at a higher level. If I could write at the lower level, then what could go wrong? Well, I may have in my possession some information that is more sensitive. If I write it into a document that's at the lower level, then someone who's cleared at the lower level can in the future come and read that document. Which means we would have information flow from a more sensitive level to a level and to a user who should not have access to that. So, you can't allow write downs. You can question why would an unclassified user have information that is top secret? But if they did write a top secret document, a top secret user would read it, as information flow from unclassified to top secret. But that doesn't, while with the confidentiality requirement that we have. So think about, a request comes. User has the users label. The target is a document that the user wants to access, had its own label. We can do this comparison if the read request is, user is making a read request. Then the user label better dominates the documents label, to satisfy the simple security property of the read-down rule. Now, we want to revisit that question of information flow. And remember, recall that information flow is a problem that cannot be addressed by discretionary access control, okay. So we want to see how mandatory access control, or MAC, or multi level security, in particular the BLP model that we are focusing on. And so how does the BLP model help us solve that information flow problem that we had? So we say we have an object, this is a top secret object. So we have a top secret object 2 is TS. This is more sensitive information. We never want this information to go and end up in an object that has level is only secret. So this is more sensitive object. 2 is more sensitive than object 1 because we said TS is greater than S. So this is highly top secret information. We never want it to end up in an object that is only classified at the secret level. Okay, so how can this information flow from here to there? There are two paths. Path one is that a top secret user reads it. We know that the read down rule means what? For this to read, for someone to be able to read this, the user has to be TS, otherwise they can't read it. A secret user can only read secret or below. So, a top secret user can read this object. So, this read will be allowed. And then they, can this information that they have pulled, remember the Alice, Bob, Charlie example in discretionary access control we had? Where Alice gave read access to Bob, who copied the data into a file that he created and then passed it on to Charlie? Well, can User 1 do something like that? Can it create Object 2, in which it may be able to write this information that it just read from Object 1. For someone who is not top-secret to be able to read this information, the object we create has to be secret or lower. TS then is just like here. They won't be able to read Object 2, they won't be able to read Object 1. This object is at the lower level, then the write rule says what? You can only write up. You can't write down. So all the top secret user can take this information, read it. But they're not able to write it in an object that can be read by somebody who's not cleared at top secret. So this path doesn't allow informational flow to get to a lower level. What about a secret user? We know the secret user can, once it's here, can read that. I guess we know that a secret user can't read. So, this path we said is not possible. This path actually ends right here because the secret user is not able to read a top secret document. This label would not dominate the label of the object. So this path is broken here, this path is broken right here, so this information actually never flows in here. And information from Object 1 flowing to Object 2. Secret can read this object and can write up, yes. But that doesn't violate any confidentiality requirements. So secret information can now be read by a top secret person. This information, made its way from here, through user 2, into this object. The top secret could already read this, read down. So, in this direction, information of flow doesn't make more sensitive information available to a user, who is cleared at a lower level. The system would not allow information that is read from this object to be written into an object that is at the lower level. That's a guarantee that comes with BLP and the MAC model that we didn't have with DAC before. So this is how we're able to control information flow, and that's how we fix one of the shortcomings that we had with discretionary access control. Unclassified document contains data that is not sensitive. So it could be read or written by anyone in a system that implements BLP. You can read down, so anyone can read it all the way from the top to the unclassified levels. It can be read. Can it be written by anyone? Well, so it can be read by anyone or written by anyone. The question is asking, is this statement true? Unclassified document contains nothing that's sensitive, so it can be read or written by anyone. It is true. Okay, so pick the right option in this quiz. The answer actually is false. The statement is false. It's true that it can be read by anyone because of the redound rule, but it cannot be written by anyone, okay? So this is not correct. Okay. This part is wrong. So it can be read or written by anyone. That's, that's not true. It can only be read by anyone. So read is yes, true, anyone. So to write, it can only be unclassified. Okay. No one at the higher level is ever to write into a document because if they can write into this document then the information can flow from a more sensitive level to this level. Our next quiz question talks about classified data, and in particular, let's say this is a top secret document that is what we're talking about. So it says BLP allows an unclassified user to write a top secret document. Is it true or false? The answer actually is true, because remember we have write up root. You can write at your or higher level. So unclassified user is down here, classified secret, top secret, you can write at any level. Write up rule is the star property that we have. The answer here is true. Again, if you think something is not right here, think about the BLP models, focus is on confidentiality or disclosure of information. An unclassified user has no sensitive information and gets written at whatever level. They may want to choose to call it top secret perhaps, but that is not going to disclose any sensitive information to somebody who should not have access to it. So, the write up rule is the reason why the answer to this quiz question is true. As you sort of talk about a more complete definition of this model, there is another requirement which is called the tranquility requirement in this model that says classification of a subject or object does not change during a session. You can't change the label associated with this. So this principle is needed, one of two reasons. Okay, so the two options we have here. So try to give the reason why we need to have this requirement, or need to have tranquility principle has to be satisfied. So you have to choose the right option, and sort of explain why. The correct answer is the first one. Information flow is the reason why we have this requirement. Let's say your label can change, then let's say you were, some full example is that you are top secret, so you were reading top secret information, and then your label changes to secret light set. You are the user. Well, now you can write at the secret level, so top secret information that you could read previously can now be written to documents that only have secret classification level. Then the user, later on, who's cleared at secret level, can come and read this document into which information might flow from top secret to the secret level, as a result of the change that happened in the label during your session. So, if you do that and sort of do a couple of other examples, labels change during a session. Then, where you were reading, where you can write, all that changes. And because of that, we could have information flow from a pierce object 02 that we have in the example to a secret object 01 and we don't want that. So, information flow is the problem. To reduce overhead associated with change of classification level, you do not change the labels, and the reason for that is nothing to do with overhead associated with changing them. It really has to do with information flow, and the information flow that may violate the model that we have. So we've been talking about sort of MAC models and BLP, Bell and LaPadula is one example of that. That's the Biba model. Anytime we talk about a model, you have to say what's the nature of the labels? How do you compare them? What kind of rules do you have? Similar to what we had in the BLP model. The Biba model actually is dual of the BLP model. And this duality is because rather than focusing on confidentiality, which is what BLP focuses on, Biba focuses on integrity. Integrity is defined, somebody being able to write or corrupt data. The interpretation here is that, when you say it is top secret information, it is really top secret information or something like that, or the information indeed should be at that level. So think about the sort of, quality of information rather than disclosure of information that confidentiality focuses on. So it's dual because the rules actually are opposite of what we had in BLP. When you're concerned about information quality or integrity you want to read-up. Because anything below you is lower, I mean information with lower integrity, you don't want to read that. So reads in BLP you have read-down, here you're going to have read-up in the Biba model. BLP you have write-up, in Biba you have write-down. So if you're at a certain level, well, the integrity of the information that you can create and then write is either at your level or or it's less integrity than the level at which you are. So you're able to write down. Again, we're not concerned about confidential disclosure here. We're concerned about information integrity. Okay, so let's look at an example to clarify this a bit. Let's say integrity could be high integrity, medium integrity, or low integrity. So saying information quality, think of your favorite newspaper that you trust a lot, let's say for me, it may be the New York Times, the information that appears there is highly likely that I can trust it. Your supermarket tabloid, let's say at the other end. Apologies to anyone who may be a fan of those. When you're talking with confidentiality, anyone, it's not about controlling who can see it. It's controlling how good the information is. So here it's high, medium, or low. Compartment could be similar to BLP, depending on what the information is about. Really, it's the topics of the document that contains that information. So label here. So anytime I quote a model I said I want to talk about, what does the label look like. So, high, medium, low is ordered again, totally ordered compartments, partially ordered, because they are sets. And, the model says low integrity information should never flow into high integrity documents. But what about policies in commercial environments? First of all, when you go work for a company, they might do a background check on you, but they don't clear you at secret or top-secret levels. So clearance is really not common when it comes to these types of environments. However, there are other requirements that may arise in these kind of places. If we talked about, we go back to our payroll. If you are a payroll department employee, then you can access applications that have to do with payroll. But others should not be able to do that. So one requirement could be that data is accessed by certain applications and only certain users be able to gain access to certain kind of applications. Requirement may be that non-payroll department people don't have access to the payroll application. Other requirements that arise in these commercial environments is conflict of interest, is pretty common. So if you're a law firm, have two customers, and there's some competitive relations in between those, and a lawyer working on one case should not have access to data that belongs to the other company, when those two companies compete. Separation-of-duty typically is good for reducing the likelihood of fraud. So if one person can do multiple things, it's easier for them to commit fraud. In commercial environments, clearance is not there, BLP's not very useful, but perhaps, the need for mandatory access control is there. So we want to look at some models that may make sense in this setting. So we can look at two different policies or models that make sense for commercial environments. We're going to start with orders called the Clark-Wilson policy. So Clark-Wilson policy says that users should be able to access certain programs or applications. Objects are constrained and certain objects can only be accessed by certain programs. So the way you think about the policy is that somebody says this user should have access to this application, and somebody says this application should have access to these objects, so that's what the user can then gain access to those objects. And only when they're running those applications. So if I want separation-of-duty, for example, where the same person should be able to do things that could potentially lead to fraud, then we can just say that user Alice cannot execute both of those applications. So, you can do one and somebody else will need to be able to run the other application and that's how we separate the two duties associated with the two applications that we have here. So, Clark Wilson essentially was developed for commercial settings. The second one is more about the conflict of interest requirement, how we address that. And the name of the policy is called Chinese Wall Policy, and we'll see why this is an appropriate name for it. So the deals, as we said, with conflict of interest and what you can access or cannot access depends on what you have seen so far. So what you've seen so far, if it has no conflict with the next thing you're asking for then it's okay. If not, then we're going to deny access. So how do we decide when do you have a conflict or the absence of that conflict of interest? To explain this conflict of interest and the Chinese Wall Policy a little bit more, let's say we have a document on a system. Now some documents are financial documents that pretend to different banks. So Chase, Wells Fargo, Bank of America, whatever it is. So this grouping here, these are all the documents that are for Bank of America. These are all the documents for Wells Fargo and so on. Okay, similarly we have other documents that pertain to, let's say, oil companies. So some documents are Exxon, others Shell, Chevron, and so on. The reason we have this outer box around all the bank documents or oil company documents is that this is a conflict class. Chase has a conflict with Wells Fargo because they're in the same kind of business so they compete with each other. So the conflict class is, you know, these documents form the conflict. Similarly, Exxon would have a conflict with Shell and Chevron. So these documents are in this other conflict class. Okay. So Chinese Wall Policy basically says a user can access any object as long as he or she has not accessed an object from another company In the same conflict class. So let's say user Alice accesses Exxon, okay, a document pertaining to Exxon. She can because so far she has accessed nothing, so there's no conflict that we can see. As soon as she accesses a document from this class, from Exxon. She cannot access anything in these other classes that are in the same conflict class. So she can't access something that belongs to Shell or Chevron. Can she then go and access a document that pertains to Chase? Yes, that's in a different conflict class. There's no conflict between Chase and Exxon. Alice can access this and this. But as soon as she does Chase she can't access Wells Fargo, so you can basically access any oil company and any bank but not two oil companies and not two banks at the same time. So we're basically creating these walls around sets of documents, okay. That a conflict exists across those and that's why it's called a Chinese Wall Policy. Let's do a quiz about Clark-Wilson. Why is it a mandatory access control? There are two possible answers I wanted to think about. So remember, mandatory is how things get accessed is not at the discretion of the owner or the user who creates a resource or a file. So we think Clark-Wilson is a mandatory access control. So why is that the case? What explanation do we have for that? Okay. So look at both options and see if either one or both of them help explain why Clark-Wilson is MAC. If any user can decide what files to be accessed, then of course, there's no mandatory aspect to it. They should be able to decide about their files, not any file, but that doesn't make it mandatory access. The fact that only the company can dictate, somebody on behalf of the company, for example, sysadmin, what programs can access what files. That's what makes it mandatory, okay. So somebody, other than the user who creates a resource, is required to specify how it can be used. That's what makes it mandatory. And here, the company or company's representative, someone in the security organization, is deciding it which makes it mandatory. So we talked about a number of commercial policies. For example, a Clark-Wilson and Chinese Wall. So here we have a scenario. In this quiz, we have a scenario where there's a law firm that has two different client companies. And the interesting thing about these client companies is that they compete with each other. And lawyers work on cases related to many companies. So some of them work on company A, others work on company B, or whatever it is. And they need to access the documents. So to ensure proper access, in this case, which policy should you be using? That is the quiz question. The two options are Clark-Wilson and Chinese Wall. Clark-Wilson, remember, defines as transactions or procedures that you're able to execute, and you can define who should be able to do what and things like that, based on what they're doing in the organization. It doesn't address the conflict thing. The conflict thing is addressed. So, the correct answer is Chinese Wall. So remember, we talked about conflict classes? So similar, for example, airlines all because they compete with each other would be in a single conflict class. And then we said, as soon as you access one company's information, you're not allowed to access from any other company the same conflict class. That was the Chinese Wall policy. That's what would work here in this case, because if lawyers lurking on this competing companies, if the same lawyer does that, that is a conflict. We want to avoid that, and you avoid conflict of interest by using the Chinese Wall policy. So we discussed role-based access control and we also talked about discretionary and mandatory access control. So, this quiz question is asking you, if role-based access control, which is used often in commercial settings, is an example of mandatory access control and if it is, then how do you actually justify this statement that RBAC is in fact a mandatory access control policy? The correct answer says, well who has what roles gets defined by the company itself, okay. So whoever is responsible for secure access to its resources, trusted sys admin or whoever it is. So what role you can take on is mandated by the company and what you can access depends on the role. So, it doesn't depend on so, remember, discretion access control, the owner decides who can access their file or data that they created. Here that's going to be decided by roles have access and who can take on what role is going to be decided by the company. And because of that, the fact that the company decides how accesses are going to be allowed, that's what makes this a mandatory access control policy. So we talked about mandatory access control models, multi-level security models. And talked with the BLP model for accessing classified information and that's done by individuals or users who are cleared. The one question you can ask, well, does anyone care about this model? And if they care enough do they implement? What operating systems actually implement this? So there are four options, consider each one of those and then we'll talk about it. Well, MacOS and Windows are the commercial systems that most of you use. Neither one of them actually support BLP. These vendors choose not do that, for business reasons, or whatever. So security enhanced Linux or SELinux actually does do that. The NSA was of course keen on having operating systems that supported BLP-like models because they have information that is classified. And if the vendors are not going to do, they did support an open-source effort to build security enhanced Linux to include the SELinux kernel module that actually supports these kind of policies. So, SELinux is out there now. You can actually use it. SCOMP is a system actually that goes back to late 70s, early 80s. This was an effort between Honeywell and DOD and the goal was to do sort of hardware. So this is Secure Communications Processor, that's what this stands for. Was to build a system that actually can get an A1. So in a TCC criteria that we had, the version a, a1 it supports mandatory access control, verified design, and things like that. It's only the highest one, so that's what this system aimed for, and so it did obviously implement mandatory access control and in particular BLP-like models for it. First question we sort to have to ask ourselves is how do we know that a certain system that claims to be a TCB or Trusted Computing Base is actually, can actually be trusted? So trusted is actually not the only set of qualified we use. We call systems secure systems, trusted, I've been using this word, but also high assurance systems. So, security you can think of this as a binary property in some sense, either something is secure, or it's not secure and something can go wrong and while the security requirement. Trusted is typically sort of higher level of confidence that you have that it's going to do the right thing, what it is expected to do, this level of trustworthiness, trusted our reliance on it and things like that. High assurance, basically, is a similar sort of idea. It does what it's supposed to do with a very high level of assurance. I should say that the hardware that we have, you need some place to stand on. Hardware is obviously something that we're going to trust, and software that has direct access to the hardware, which makes up your operating system, the trusted computing base. We have to trust it and anything else that will be able to make claims about the level of trust that we have in the system is going to depend on the trust that we have in this underlying trusted computing base. So enforcement of a security policy because we implement access control in a certain way, or isolation of processes, isolation of untrusted code from trusted code. All that is going to rely on this level of trust that we have or the level of assurance that we have, that the trusted computing base would do what it supposed to do, and nothing more and nothing less. We said the Trusted Computing Base has direct access to the hardware. Hardware, we said, we can trust, that's not something we're going to, although you may have reasons to, to, be suspicious, of hardware itself, but we're not going to go there, so we're basically talking about how can we trust software. The software is supposed to perform, a set of functions. We've been discussing some of those, whether in a recently accessed control, or kind of policies that are supported and so on. So if it's going to implement certain functionality, it has to be functionally correct. Okay, when it says it implements a certain model, it implements that model correctly. The functional correctness basically means that it does what it was designed to do. So it implements a set of functions and that is done correctly. Now, to implement those functions, it does have data structure. So data state that it makes use of. Also, it may be protecting other data that belongs to different users and programs, and things like that. If it's trusted software, we wanted to implement the function it's supposed to do correctly, as well as maintain And the integrity of that data that it is going to rely on. And we want it to maintain that integrity even in the presence of bad input. Okay, so we do explore some interface or API, hence the bad guys can try to use it in ways that we didn't expect it to be used, give it bad input for example. So the integrity that should be maintained by the trusted computing base actually is maintained in the presence of a correct model, but an adversity may even try to provide bad input. Obviously, there is sensitive data that it uses and it protects for other users and applications. It has to address the confidentiality or disclosure of this sensitive data, it shouldn't be disclosed to somebody who doesn't have access to it. And it should do that in the presence of un-trusted software obviously. We're not going to assume that all software is going to be trusted. Okay, so we have our trusted computing base and then untrusted software that is run by applications so on. So this disclosure that we want to control for sensitive data can make assumption that all code is the system is trusted. So the last couple of requirements we identified in terms of integrity, confidentiality, implementing the necessary functions really talk about what it does. Our confidence basically says how well does it do that? Okay. Confidence, we can't demand a proof that's going to be really hard, a formal proof. But, as we said earlier there could be things that increase our confidence in the system's ability to do all these things that we're talking about. So where does this confidence come from? Well, a lot of things in real life, it could come from the fact there experts of these kind of stuff and evaluating this kind of a system. And maybe they analyze it, they spend time with it. And it's their word based on which we actually derive our confidence and the trust that is assured by the system. Finally, we can think about this as our requirements or needs, in terms of trustworthiness, can be maybe captured in a statement. And that statement says, well, this is the kind of security or trust we expect the system to provide, or this is what we expect the system to enforce. And then the system, someone who builds the system, design and builds the system. So let's say there's a vendor who provides the system. We said TCB functionality, normally operating system is where it goes in typical systems that we have, so an OS vendor, for example, may have to then tell us our requirements are met. We come with some sort of a requirements analysis, or a statement that sort of captures our expectation. And then somebody has to give us a checklist or a proof of some kind that allows us to believe that our expectations are met. And so the best possible scenario is that all this could be done formally. We actually have a proof that whoever is providing the trusted computing base actually meets our requirements, our expectations that we capture in this statement. When we have formal proof of it, that would be ideal, but would it be possible always? This is one of the questions we're going to explore as we go through this lesson. Trust in the trusted computing base is going to come from a number of things we expect it to do. We talked about design principles for secure systems. And this system is something that everything relies on it. So it better be secure to the degree possible. So it should be using some of those design principles. So let's go back and review a number of those design principles that we talked in general for secure systems but we're going to see that they also apply to the trusted computing base. One of the design principles was least privilege. Any time we are executing, the idea is to execute with the fewest possible privileges, and the idea is that if something was wrong. So, remember trustworthy means that a high likelihood that things go the way you want them to, but there's no guarantee. There's no proof, as we said, so if something were to go wrong of course we have damage containment and fewer things will be affected because what can be affected, what resources can be abused is based on what privileges you hold at the time. So holding as few as you need for doing what you're doing at the time. You want to use that principle even when you design the trusted computing base. Economy is the other thing. Economy is, we also said, keep it simple. It's really more important when it comes to trusted code. You want to keep it simple and as small as possible the other design principle, we're going to look at a couple of these, is open design. The idea is that trust is not going to come because somebody tells you take my word for it. We're not going to have trust, or confidence, when security is by obscurity. We don't know how something is done, but we basically have to take someone's word for it. That's not a good way to design a trusted computing system. You do want to have an open design so security is because you understand exactly how that level of security is being accomplished or achieved. We definitely want complete mediation, and that means that every access, that one of the basic requirements for a trusted computing base, that every access is checked, any attempt to bypass the system, to go to a resource that needs to be secured, course is going to prevented. So we want complete mediation, if that's not there then of course you will have no trust in the trusted computing base. We also talked about other design principles. Fail-safe defaults is the default that gives you safe behavior, so when it comes to access control, the default should be denied. And finally ease of use or psychological acceptability. Basically, we have to make the right assumption about what users are able to do and what they're not able to do. If you make sort of assumptions about users that are unreasonable. They're going to avoid security that gets in their way. They're going to find ways to work around it, and that's going to result in a lower level of trust. I decided to sort of revisit these design principles. One of the things we're talking about here is the trusted computing base and where trust comes from. Well, if it's designed with the design principles in mind the number of those that we have discussed here. Of course, that's a question you're going to ask. And if somebody answers in the affirmative saying, yes, we follow those principles, that's a good thing Is this principle applicable to a trusted computing base? Such a system has to be basically trusted. Everything that we want relies on trust that we place in the system. So, is this useful for a TCB is the question that we have here. We obviously talked about this principle being useful for applications and services and so on. So here, the question is that useful for TCB? So the idea here, if something goes wrong, then this is useful. This one says, maybe nothing goes wrong when there is a TCB, when we're talking about the trusted computing base. The second option says, yes, it is relevant because TCB only provides high assurance and not a 100% guarantee. If you think about these two options, we said TCB is also a system that is fairly complex for a variety of reasons. We're going to do our best and use the design principals and do other thing for analysis and so on, but it's trustworthy. It's not 100% secure. So basically, it gives us this high assurance, but something could still go wrong. It helps to have this design principle. So if something can go wrong, even if it's a small likelihood, having this proves it's useful. So I think, the idea here is that you can't assume just because we have the word trusted computed base in TCB, that it is guaranteed secure. So, we have a vendor who claims some mystery techniques, proprietary techniques that we're talking about. So there's all of this debate between open source and closed software that is not open source. So proprietary is techniques to help ensure high assurance, but is not going to disclose those to you. So they cannot be disclosed. So what principle does it violate? Remember, security wells security is what open design principles says you shouldn't rely on. So open design principle is the one that is not being followed in the system, so that option two here is the correct answer. So if this is how the system comes out of the box, the question here is, is it violating a design principle for secure systems? And if it is, which one that is. So depending on your choice, one or both, you want to check those. So actually the answer, here, is really talking about default settings. This is default, don't encrypt. And if you want encryption, which is the right thing when you have to mess with the settings? So the defaults are wrong here. That is why I would pick the second answer, which says fail-safe default principle is what is being violated here. Because the defaults, essentially, come such that this system is not secure. Okay now that we talked about what we want in a trusted computing base, we want it to be trustworthy, we want it to be high assurance and we said even in a follow designed principals. So let's talk a little bit about how do we build a trusted computing base. Well first of all, if we claim it to be a trusted computing base, it has to implement certain key requirements or features that you associate with a secure system. In other words, it must implement certain security Relevant functions. Certain functions that are important for securing access to resources. So, for example, it has to address authentication. If it doesn't address authentication we will know who's making a request for a resource that is protected in the system. It has to do access control. Access control for whatever resources that we have, could be files, could be other kind of objects that we have. When it comes to access control, we talked about two different kinds of access control. We talked about mandatory access control. It may support that. It may support labels that can be used to implement the BLP model for example or some of the other mandatory access control models that we discussed. It has to support discretionary access control. So somebody who creates a resource that has some control over who can access it and who cannot access it. The basic idea here is that when you say I'm going to build a trusted computing base, the first question you have to think about is, what are the core functions I have to provide? Some core mechanisms that have to go into the system and these are some examples of those. So let's dig a little deeper into sort of how we're going to implement some of those functions, or those features. First of all, we have to protect the data that is used by the operating system, or the transferred computing base. It must protect itself. Remember, one of the requirements we had is that it should be tamper proof. So, no one should be able to tamper with it. And that's only possible when it is able to protect itself, against that kind of tampering. So let's talk about some security features of the trusted operating systems that you have, in terms of protecting data and other kind of resources. So one thing that we address in these kinds of systems, is what is called object reuse protection. Think of various kinds of objects or resources, a memory pages, for example. Disk blocks. And so what happens when you allocate a memory page to one process. That process completes and then goes away, and then that page is allocated to a different process. But what happens if that page had some sensitive data that belonged to the first process? This also applies to disk blocks for persistent data. When a file is deleted, we may free up certain blocks where the file data was stored, and these free blocks are free to be assigned to somebody else. And they could be, but there may be data already in these blocks. In fact, there is. The one thing we can do, if we are concerned about having trust in the system, is that when a process is allocating either disk blocks or memory, then we should be aware of the fact, that something could be left behind in these reusable resources. Well, what can we do if something is left behind? If this the system is trusted, it can zero out these object before they get reused. So before the memory block is given to the new process, we make sure that it doesn't haven access to the data that was stored in that memory block or page by the last process. We can zero it out. If we're talking about a file deletion, or data that's written out in the disk, we can override it with various patterns of zeros and ones. And we have secure deletion, for example. And that's basically writing it one or more times with some garbage, random zeroes and ones, to make sure that all data that was there is no longer there. And you can sort of carry it even further for example, when you discard your disk, do you actually destroy it, do you degauss it, or you can do physical destruction. Because one of the things order a number of years ago, a bunch of students at MIT actually went to some place, and bought some old PCs at almost no cost, and then pulled out the discs in those machines, and they contained all kind of sensitive data. Because even if you, either people don't delete files, even if you delete, if you're not doing some of these things that we discussed here, which a trusted system should do, the data would still be there. And if the disk gets in the hands of somebody else, then they will be able to sort of gain access to the data. So if you're building a secure system, you have to worry about this idea of reuse of objects or resources. In particular, memory objects, disk blocks, and how information that's stored in them by a given process of the system has to be wiped clean, before those resources are re-allocated to somebody else. So let's talk about some of the key features that we have to implement. Or some of the other functionality that the Trusted Computing Base must have. We have to have a complete mediation. We have to guarantee that there's no way to get to a resource without going through the trusted computing base, okay, or the reference monitor that it implements. We have to have a trusted path. Remember, trusted path is needed because otherwise what could happen? Well, some malicious program, a trojan for example, can spoof the interface of your trusted system and ultimate trusted system is your trusted computing base. So it could look like the operating system and you may think that the log in is being, you're interacting with the OS. A trusted path allows you to guarantee that when you think you're talking to the trusted computing base or the operating system, you indeed are talking to that system. Which, of course, prevents programs, malicious programs in particular, Keyloggers. A quick example of they're trying to log your key strokes or keys that you press to figure out your password or whatever it is. So, it prevents these kind of programs from tapping the path from keyboard where you press your keys to the system where the key presses are captured. This path has to be trusted, and a result of that trust is that malicious programs are not going to be able to gain access to it and be able to tap and see the information that is being sent over it. The next thing we have, in terms of functionality features, we didn't talk about that. A lot of our focus has been on the prevention side, the idea that, you know, authentication makes sure it's the right person. Access control says the right person gets access to a resource that is getting requested. The thing is that, no matter how hard we work at prevention. Things could go wrong. Somebody can steal your password, for example. And they can login as you. If we keep an audit log of what has happened, what objects have been accessed, when, by whom. Well this sort of an audit log can come in handy when, and some abuse or misuse might actually take place. So, prevention, if it's not 100%, then you have to go to detection, and having a log will help you with that detection. Typically, detection is that for some definition of unusual, you see some unusual activity in the system that potentially means an attacker has gained access to it and that unusual use can be, an analysis that you do, that detection is possible when you have a recorder log of what the system did. Keep in mind that this will only be useful if we actually look at the log and you're able to correctly assess when there is an abuse. So although audit, we're not going to talk a great deal about that here, but Lamson, one of the pioneers who worked in security, actually had this thing called the gold standard of security. This was based on the three AUs. The authentication, the authorization, and finally audit. Periodic table symbol for gold is AU. So the three AUs actually define the gold standard for security. And the third AU typically is audit function that system. So I said it's useful only if you look at it, but also it's only useful if malicious programs are not able to tamper with it. Okay, so it's integrity obviously is going to be important. So we've been talking the trusted computing base, what it has to do. The fact that it has to be small, but it has to be sort of the kernel or be at the center of everything, isn't it. We talked about, it should do complete mediation. You have to go through it to get to physical hardware resources. Or, we shouldn't be able to bypass it. So that means this is something you always go through and this is at the hardware core of any system that we have. So, the Kernel is that core, the inner-most. The most trusted, hopefully something that you can trust for things to be right. And this kernel is where the security mechanisms have to be enforced. So the security kernel is sort of at the center or the core. It has to enforce anything to do with security. The fact that all it's at the center and small. It's just a kernel, and it's not everything, an entire blob, hopefully that helps with good isolation. Small size is good for verifiability so kernel design basically says look for the smallest possible system where you can put your security mechanisms. The reference monitor that controls access to objects where all references are monitored. Enforcement all security mechanisms access to protected objects or resources, one of those. The reference monitor function of course has to be wanted. The security kernel has to address and the security kernel sends good isolation. Actually, it becomes the core of the trusted computing base or the trusted computing base, so it has to be tamper proof. You shouldn't be able to bypass it, always gets in the way, it's always invoked. That's our complete mediation function or reference monitor that has to check every reference. And going back to the small size we had before, we want, the small size is going to help us be able to analyze it and understand it and make sure that it does enforce or implement the security mechanisms that we need. And those implementations are correct. So the idea here is that trusted computing base, one approach is the security kernel approach where you bring the security relevant functions together and that goes at the core of the system that you're trying to build. So you can say, well, it's awkward kernel design at the heart or the center or core of it. What really goes into it? What is included in the trusted computing base when it is done by using the security kernel kind of an approach. So we said, well, last one we're saying everything that is necessary for the mechanisms, that are necessary for correct enforcement of security policies we want to have in the system should go there. We know resources are implemented using physical resources we have, so any I/O that's necessary to access your physical resources, primitive I/O has to be handled in there, which includes clocks, interrupt handling, at least capturing the interrupt, and being able to directly access the hardware resources with whatever capabilities that are necessary for it. In memory, for example, we said we have TLBs or speeding up the address translation process and things like that. So who can manipulate those control part of memory, somebody can use. So that kind of stuff, if you're doing manager access control, then there's label checking, that the model is going to rely on notion of labels and making sure secret information doesn't flow into an object that is unclassified and things like that. So that's part of your security policy and the correct enforcement, of course, has to be implemented by the security kernel. And direct access to hardware resources and their manipulation, and support for these various models for access control we have has to be in that. The other thing we did is sort of virtualization. One way we get sort of a complete mediation, and you can bypass it, is that untrusted code is, has handles who worked with resources. Unless we're talking about capability systems. So this virtualization, were putting this mechanism going from virtual to physical resources helps us isolate the hardware from untrusted code. It also allows us to do the separation Now we talked about what the trusted computing base has to do. Whether we take the security kernel approach or we don't but we largely sort of focused on what it has to do. When we say revisiting assurance, what we're really talking about is how well does it do that? As the example we used before, somebody's trying to sell you an operating system, which is as we said is the defector trusted computing base. And they're making all kinds of claims about how trusted it is. When we say we want to get a sense of the level of assurance, somebody has to sort of convince us. So let's just sort of quickly talk about a couple of sort of ways in which assurance can be provided. One thing they can do is saying you know we did implement it is a software system but we extensively, thoroughly tested it. So testing is normal testing. You do it for right functionality, things like that. But there's also also pen testing or penetration testing. Penetration testing basically refers to sort of you come with the adversarial mindset, saying what vulnerabilities can I discover in this system? If you can, better is to actually have a proof that cannot be refuted. So you have a formal verification. It's a theorem that we prove saying this is what my system is supposed to do and here you can check that it does exactly that and nothing else. Other things we can do is, you know, some sort of a checking that developers have actually implemented all the requirements that we had. And we talked about a bunch of different functions that are trusted computing base or have requirements for a trusted computing base. So, when it comes to assurance and it comes to testing, a word of caution here. We should look into what exactly does testing give us in terms of assurance. So remember, testing is good for showing that there is a problem. Testing obviously cannot demonstrate absence of a problem. You can't say, well I ran all these tests, so there are no problems. Tests can only demonstrate problems that you can find. And the variety of testing, the testing is something that we do fairly frequently in the software development process, in the QA process, Quality Assessment process. And one kind of testing, for example, is regression testing that says when we enhance a system, add features to it, patch it, well it's a new system, it's a different system. So regression testing says, you run the test that we have to make sure that these alterations that we did or these changes that you made don't break functionality or it could improve performance. However, the key take away from here is really the first two bullets which is testing is important. It's useful to discover problems. If you've done a lot of testing, it increases your level of assurance or confidence, but of course it doesn't guarantee that there are no more problems that may be left with the system. So when talking about the level of assurance that we have in the system and we said, well, we can do testing. You can do formal verification. So let's talk about why that is harder, some of the challenges that we may face when we're trying to do that. Well, when you're doing tests, you have to then rate the right set of tests to your test cases. Test cases are supposed to show the existence of a problem. If you don't have a good set of these test cases, then a problem that you may have may go undiscovered. In other words you want to have good code coverage. You won't execute the code that we have in the software system and the kind of execution paths we may be able to take when the system is actually deployed. The problem with this is that there could be a very large impact on exponential number of different executions that are possible. And these different execution paths we're talking about could change when we have different execution environments. So we talked about the other kind of testing, which is the penetration testing, okay. So that is the adversary actually or somebody else trying to find one way. Here when we're doing testing as a good guide, we have to make sure that every possible execution path we know that is good. So you have the burden of showing that no matter how the system executes, things will not go wrong. If you're doing Pen testing, the problem is different. You're not concerned about every possible execution path. You're actually concerned about some execution path where a problem shows up, some vulnerability you discover that can be exploited. One way we do that is if ethical, hopefully, hackers, they attempt to defeat the security measures that we have in place. If they're successful that means that we have a problem with the security measures that we use in our system, okay? If they can't find any, that doesn't mean that somebody else can't find them two weeks from today, but if they can defeat the security measures, then we know that we have a problem. Keep in mind that, even with pen testing, as in testing, we only talking about showing the existence of a problem that is there. That a problem exists. We can never demonstrate the absence of a problem. And that is formal verification. Formal verification is essentially the mathematical specification of what the program is supposed to do, and what security assertions have to hold, what security properties have to hold. And it's a proof, and the proof is about the correct behavior of the program or any executions that the program may go through. Through. So, how can we do this? Well, there are automated theorem proving techniques, things like model checking and all. What they typically do is, at our program or code that we have, we'll have a bunch of state variables with some initial assignment or values. And program instructions or statements or the code that we have actually change these state variables. Depending on what the instruction or the statement is, it's maybe assigning new variables or altering, or computing and things like that, whatever it is doing. But the state changes and then we have some boolean predicates. The correctness is sort of captured in predicates that hold on the state variables that we have. And so essentially, model checking or theorem proving, basically is saying that certain security assertion we have is going to hold as the program executes or as the state variables change, due to the execution of the program. The assertions remain true. Okay, if you can show that for everything that the program does of course, all of its statements, starting in some initial state. That is a proof of correctness, or formal verification that whatever that we wanted actually holds. Well the problem is that model checking, the same sort of difficulty we had with testing is that the state space can actually explode. And worst case complexity is,again, exponential. However, there are strides being made in this field, Verification. The problem that we have with it is would it scale to really large systems. And your operating system, of course, is one of those large systems. It doesn't quite scale to that size system, but as I said, strides are being made. We can check where these kind of properties, and this kind of work actually. So the model checking pioneers won the highest award that computer science has, the Turing Award, in particular in 2007. Because of the importance or impact this kind of work could have in, not just building or writing software that implements the functionality we desire, but also implements the security requirements that we want to have or security properties that we want to have in the system. We discussed the need for reducing the size of the trusted computing base. If we do that, we have couple of different options here, so you should check each one that you think smaller size TCB would help with. Okay? So, for example, tier testing or verification, if smaller size helps with each of these, you check those options. So small size or reducing the size of the trusted computing base, will it help the testing? Yes. Remember, testing, you need to have code coverage. Larger the code you have, harder it is going to be for you to cover. All these possible execution paths. So testing would be easier. Verification we talked about state space and explosional state space and exponential. Well, size is small, that's going to be easier, too. Finally, isolation of the TCB. Depending on how isolation is being done, in general if it's smaller, fewer interactions with other components and things like that. So small size is actually good for everything. So I'm going to go ahead. The details as they say, the devil is in the details. So you could say well what exactly how, what isolation you talking about. But in general, we say reducing the size is a good thing. The next question is, what makes testing challenging for a system like a trusted computing base. So there are two options and, you look at each, and, see if it makes sense. The first option actually is the only one that is correct. As the system goes in complexity, it's going to be difficult to cover all executions because in testing, you want to cover as many possible executions as you can. So, size, complexity grows, this is going to be more difficult. Testing can actually never show the absence of a problem. So that it can not do. So, we're not going to pick the second option here. It can show the existence of problems, but not the absence of problems. So, this and that we have here makes the second option an incorrect option. What is the key problem with model checking is what the question is asking. Two options, consider each, and check the one you think explains the problem with model checking. Model checking actually is correctness proof. If it is correct, that means there is no problem. Actually, it cannot show absence of a problem. It's true for testing but it is not true for formal verification. If you're able to verify, you are able to show that there is no problem. It can show the absence of a problem. If we can verify means there is no problem, and that means this is demonstrating or showing the absence of a problem. So the first option is not correct. It would have been correct if there was testing, but we're talking about verification. See the problem really is the second option, which is that because of the exponential nature of the work that you have to do, it doesn't scale to practical large size systems. Lot of people are faced with this problem of how do you evaluate the level of security that comes with the system. No one more concerned about this then, for example, the Defense Department. They have a lot of sensitive data. So they actually address this problem of how do you do security evaluation? How do you establish how trustworthy or level of assurance, level of trust that you can associate with a given system? And they actually came up with a trusted computer system evaluation criteria. This is work that goes back to actually the 80s. And this is called the Orange Book or TCSEC Requirements. Trusted Computer System Evaluation Criteria. What you see here is, actually it doesn't the color. Actually it is orange. Copy the color page of that report. Now one of the people who were involved in designing that, Roger Shell, who signed it, my copy. So I just provided you a picture of that. So the question is if you are sort of tasked with this, develop a sort of violation criteria that we can use to assess the trustworthiness or security of the system. What would be those criteria? I think some of the discussions we have had for example about functionality and how well it's done and so on, should actually factor into these requirements that we're going to have. So the Orange Book that I mentioned sort of captured the evaluation criteria, and came up with sort of way to place a system in a division, and within a division, a certain class. So, the four divisions were D, C, B and A. The less than really refers to the least trustworthy and we'll see why D was the least trustworthy. And A was the highest level of trust. And within that, for example, the division C had C1 and C2, so C1 doesn't do certain things that C2 does for it to be more trustworthy, okay? So this ordering that we have is going from least trustworthy to the highest level of trust. B1, B2, B3 means three classes within the duration B. Well, D really didn't have to do anything. If you didn't do, think about your old MS Dos, if you didn't isolate the trusted computing base from untrusted code, basically untrusted code could hook operating system functions in whatever way. There's no protection against tampering, bypassing, or things like that. See, one of the things it has to do is do authentication and access control. Okay?. Access control. It only has to do discretionary access control but of course to do access control it also has to do authentication and if it does that then it addresses a bunch of other things. Just sort of giving you quick highlights here but of course in a TC isolation and stuff like that has to be here too. So that places you in division C. If you implemented mandatory access control. For example, BLP like model, where that will move you up to division B. And you can verify, use formal verification techniques. Specification in verification. If you went that far, then that's what will be needed for you to get into division A. Most systems that we use that the security features that they provide, commercial systems that are widely deployed, they're basically in the C division and C1, C2, or whether they move up to get B1, of course, they have to do mandatory axis control. So we said SELinux, for example, does implement Mac So C1 and C2 actually depends on what functionality. So here it, shouldn't be confused by the fact that this we're talking about functionality with discretionary and mandatory access control. This is actually how well effort you put into making sure there are no problems. So it's how well you you do, you testing, testing, based on all these auditing support and audit logs and things like that. You will have different classes, but the main idea here is that most systems that we use widely these days, they basically division C some implement Mac features and they get into this division B1 that I have. If you want to move up to B2 for example, what you may have to do is, you have to, whatever security model that your system is, underlies your system, you have to have a proof of correctness for that model. Or you also have to have a speck for the TCB. It doesn't have to formally done, but you have to have a narrative specification of the TCB and what it does. Of course if you want to move up to B3 or A1 beside that's formal verification that you have to do an, that the TCB is implemented correctly and things like that. Well it started as a US DOD effort, at or the orange book, then with time multiple countries sort of recognized this importance of software procurement, they had to worry about security that they can evaluate for those software products. That evolved into what's called the Common Criteria which is Is an international standard. Common Criteria came from efforts in Europe, in Canada, and the US one that we talked about. The idea Is that you are consumer of the system or user of the system, you're going to specify what needs you have or what requirements you have. Vendor going to sort of implement a solution and going to make claims about that solution meeting your needs or your requirements. And in particular, we're talking about sort of security requirements or security properties. So whoever is going to evaluate is going to determine whether the vendor actually, the claims that they make are claims that you can believe. And these claims actually allow you to say, well, my needs that I had specified are met. So there's three entities, you have some requirements, a vendor who provides the solution makes a certain set of claims how they meet those requirements, and somebody who evaluates has to then assess those claims, and the reason we should be able to trust whatever those claims are about. Based on that, we're going to give it an assurance level, which rates the trustworthiness of the systems. Evaluation assurance level or EAL, we will be able to use that to rate a system, in particular, how well it meets the claims about security properties that are important to us. So similar to division D, we said, basically, it didn't do very much EAL1 is the most basic one and EAL7 is the most rigorous or the highest level of trustworthiness that you can get. Many widely used operating systems don't support mandatory access control, like BLP models. And hence, cannot be in a TCSEC division higher than what? Okay, so the two options here, D and C. So you're saying, if you don't implement MAC, I know you can't be higher than either D or C. Well, if you can't be higher than D, then you can't be higher than C either. See which option here makes sense in terms of, if you didn't do something, what division could you go into? So in TCSEC we know that D was the basic one. Really had to do nothing. For C you had to implement some security functions, including discretionary access control, but you didn't have to do mandatory access control. For mandatory access control, if you did, you could go to B. If you don't implement mandatory access control, you can't be higher than C. The best you can do is C. So if you don't support MAC, you cannot be in a TCSEC division higher than C. This is the answer. Automatically also means you can't be higher than D, so this is somewhat poorly formulated question, but you could be higher than D actually. You could be in C, even if you don't implement MAC, which is higher than D. So the only option that you should be checking here is the second one, which is C. The question here is, what did this vendor do, VMware in particular, that enabled it to get EAL4 but not higher? The two options sort of explore that, and you pick the one that you think is the right one. So we know that formal techniques and design and validation and all that, that moves you up the scale. In fact, EAL7 is the formally verified most rigorous requirement, so since they only got EAL4, it is more the systematic review and testing process. It doesn't have to be formal techniques, okay? Systematic review and testing process was used by the developers, and that's why they ended up at EAL4, which is not bad. So the two takeaways are what you do and how well you do it. Many OS vendors don't aim for the highest certifications. AL7 or A1FDC sec is what you are using. So if they had the highest they would have to formally verify design correctness. They would have to implement mandatory access control, all that stuff. So they're not doing it because of what reason that two options capture those, and you check the ones you think makes sense. The first one is, there is no market demand for such certification. Well, security has never been more important than it is. So if you can sell someone, you can convince somebody that you're selling more security or higher level of trustworthiness, there will be demand for it. So I don't think the first option is the right one. The second one is cost/benefit tradeoff is really the reason. As you move up the certification chain, of course, is you are to do more and more things and you have to do them in a better way. And there's a cost for doing more and doing extensive testing and verification and whatever that is required and that cost. And so that the benefit that comes from that cost, those tradeoffs are the reason that people don't choose to go for the highest possible certification. So we explored a number of mandatory access control models, ranging from ones that address confidentiality and integrity to those that are designed to meet the needs of commercial applications. We then revisited the idea of a trusted computing base, or a TCB, and how we can evaluate the level of assurance that is provided by the TCB. We saw that it not only depends on what the TCB does, or its functionality, but also how well it has been tested and verified to ensure that it is free of vulnerabilities. We've been talking about the importance of protecting sensitive data. Actually, a lot of our sensitive data is stored in databases. When you hear about a major data breach, chances are that a database was compromised. So, in this lesson, we're going to explore issues that are unique to databases. How do we do access control for data that is stored in a database? Are there new kind of attacks that are possible, and if yes, are there new defenses that we need to use to protect the data that is stored in databases? Databases must store sensitive data, and there must be something different about securing data that is stored in a database compared to what we've been talking about before. So what we discussed before is data stored in files, for example. When we did access control we said you have read/write permissions on files. Databases we're going to discuss are somewhat different. They also store data, but it's different and the fact that they store lots of data that is sensitive, that's what makes this topic important. So databases actually do store massive amounts of sensitive data. This could be, if you are a company, could be your customer data. In fact, when we talk about some company getting hacked and large amounts of their customer data, such as social security number, or date of birth, or address, credit card numbers, when those are being stolen by hackers, databases store data that we can see needs to be accessed in a certain way. And the reason we're able to do that is because databases can store what is called data in a structured form. And the kind of examples that I just talked about, the data that we have about customer records or health records, that data does have structure, and the structure actually influences how it's going to get accessed. So, I may ask, for example, information about all customers who purchased some number of things in the last two months. Asking this kind of a query or formulating this king of a query or a question That we want to run against the database, we make use of the structure that is there in this data to do that. So, it really do is that think about a database storing structure data and then we can basically access it through programs called queries that we submit to it. These programs are queries written in a language that is designed for accessing data stored in databases. One common one is SQL, which stands for Structured Query Language. So we're going to talk about accessing data from databases using SQL quite a bit. This is another thing that is different about databases. Databases also store data that is persistent, like files. And of course we need integrity of the data. One way to do that is we have this transactional nature of queries of programs that we run on the database. A transaction means either the program runs completely, does everything it's supposed to do, or whatever it did gets undone. Transaction either, we have the zero or one property, either it's done completely or not done at all kind of a thing. By running a partial transaction, for example, a query could leave the database in a state where it is not consistent and that impacts the integrity. Finally database is another thing that's kind of different about them is that we can create a virtual database, what is called a database view. This is really derived data that's there in the database. So for example, you may want to choose a subset of the information that's stored in the database. So you can run a query against the database, and the result could be another sort of virtual database, also called a database view. And then users can have access to that view. Oracle, one of the largest database vendors, actually sponsored a database security survey. I think it was 2014, to basically ask people in various companies, organizations, about threats to data that is stored in the databases. And they list the number of threats that they're worried about, and this question is actually about, what are some of these threats, which are the most serious or the biggest threats? The two options I have here are external hackers, and the second was insider and unauthorized users. The people who are responsible for securing the data are actually more worried about insiders and unauthorized users. The next question, actually, is about why databases are so attractive, or why they are attractive targets for hackers. There are a couple of different options. And you should think about each and mark if you think that is a good reason that would explain what makes a database highly attractive to malicious hackers. The first option should be checked. If it is attractive because you're not just getting information about a few people, but lots of people. Finally, query languages that are used to access data can be abused to gain unauthorized access, okay? So we talked about software security and vulnerable software and things like that. And, actually, we're going to see that I mentioned that SQL is used to write the programs for the queries that we run against databases, and people exploit the way we formulate those queries for example, and I will again access to databases via that. So the attack rectors, and if they make it easy for them to gain access to the data, of course that makes the target attractive because it's easy to get to it. We're going to focus on relational databases because this model is what the databases that are widely used, Oracle for example. The databases that are widely used in real world, they actually use this relational model. So that's one reason we're going to focus on it. It's also been around and investigated heavily and so on. So what does a relational database, what's the data model here? Well, a relational database consists of what is called relations. Another name for relation is a table. Remember a table is basically, it's sort of like rows and columns. It's a metrics kind of a structure. So relation is nothing but sort of a table that has a bunch of columns, and bunch of rows. So what columns we have, what kind of data items we're going to have in the various columns, in the database or in a relation. That's defined by a schema, that we have for that relation. A schema basically says what each column is. And what is the nature of the data? Is it a name, it's an address, it's a social security number, whatever it is. The columns defined by what we call attributes. So the values that we store in the various columns actually are attribute values. Think of about the schema defines what these various columns of the table are or the various attributes are. And value for a given attribute is the attribute value. We talked about columns or attributes, but of course, then we have rows in a table. The given row is going to contain value for each of the attributes as defined by the schema that we're talking about. So, it's column, row, cell of a metrics in our table basically has a value. So, these rows are called Tuples. So tuples basically, as I said, are attribute values that make up that row of the table. And what these values are, again, is defined by the schema that we have. It's a table and the attribute values and the rows are called tuples and things like that. Some of those attribute values are kind of special, they're called keys. The various types of keys, primary and foreign, and stuff like that. But keys are basically values that you uniquely define a given tuple or a role. So, if the database stores information about eight employee there's going to be a tuple for each employee. Information about that employee is going to be stored in the table or the relation given employees information will define one tuple. And if you identify employees by, lets say, their social security number, then that's going to be the key value in that tuple. And it's uniquely identifies the tuple because social security numbers are different for different users. So we're going to look at an example of a relation or a table that we just talked about. This example that we have here is- the name of this relation or table is Employee because it store Information about employees that we have, maybe in a certain company. So, the columns or the attributes schema that we're talking about is defined as follows. You're going to have an employee name, going to have an identifier for the department in which they work, that's Did, the salary we don't have the value, but we have a code for the salary. We have the employee's ID and then we have their phone. The information about a given employee consists of their name, the department, the salary code, their ID, and their phone numbers, so you can reach them. Now, in this particular organization, we have a bunch of employees. Robin, Neil, Jasmine, Cody, Holly, and so on. So for each of them we're going to have a tuple. So if you look at sort of a given row in this table here, that is what we are calling a tuple. And we uniquely identify those by the employee ID. So in case of Robin for example 2345 is the primary key value for employee Robin. So we are going to see primary keys are how we actually say what set of employees. You may want to get information about them. Things like that. So this is the key, and these are the key values that we have here. So as you can see, for each employee that information makes up a tuple in the table. The collection of those instructed organize this way. That's what makes a table. The various attributes that we have which define what's stored in this table is based on this schema that we have here. I have something else here which a foreign key and the reason I included that here is because sometimes you may want to do operations across different tables. So foreign key basically is going to be key in some other table, and we can do what is called joins for example. Put data together from multiple tables. Well you can say, this is the foreign key that should match a primary key in another table. And then I want to pull information about Robin's department salary code and employee ID from here and her Social Security, address, other things from different table and things like that. So there maybe the department IDs that keep for example and won't be unique if you're trying to pull Social Security numbers and things like that. So foreign key is essentially a primary key somewhere else and somehow, sometimes you may have to sort of combine information from multiple tables and we use these keys for doing that. And so some of the fields may be the attribute values that we have here. You're not going to use them as the key for accessing Tuples from here, but when you referred another relation and operation across multiple tables, that's where they get used. Again, if you've done your database scores, you probably have learnt a lot more about it than what we need here. Now that we have seen what a relational database looks like, we said it's a collection of relations or tables, and we looked at a table, and tuples, and at the various attributes and so on. So, what kind of operations can you perform on tables? Of course you can create a table. We're talking about selecting certain information out of a table of relations. So for example, I can say everybody who is in a certain department, we can read that information out of that relation. That's what a query selection will do, and we going to see examples of that will return. So you can select information, you can of course insert tuples, or rows, you can update attribute values that are in there. You can actually perform joint operations across multiple tables and, of course, you can delete tuples, delete tables and eventually delete a database, that may be a collection of tables. You know, create, delete are sort of similar to what we have seen before, for example, files. But some of the things like select, insert, join, these are kind of new to how you can manipulate and access data that is stored in a data base. What I do for, we said we write queries or programs to access data that is stored in a data base and those programs or queries can actually make use of these operations that we're talking about. So this is saying select, star means everything, so all attribute values. >From the employee table that we had where the department ID is 15. So we are saying basically tell us, give us the information about every employee who works in department that has ID 15. So this is an example of how I might access information that is told in the relation or the table that we just saw. And this is an example of a read query, because all we're doing is reading selected pieces of information that is there in that table. And returning that to whoever is making this request. So actually if you are going to run this query against the table that we just saw. You'll see that the department ID is 15 for two employees, Robin and Cody. So we are going to get the tuples for Robin and Cody. In response to, when we run this query against the database that we have. So selection has happened because, of course, we had a lot more employees. Now we're only selecting the ones who work in the department with ID 15, who happen to be these two people. And these are return of the programs, the queries that return in a language, a query language. We talked about SQL before, and we'll see some more examples of that. But basically these programs or queries that we're talking about in this language allow you to use these basic operations. That you can perform on tables of relations and you can use these operations in the programs that you write or the queries that you submit to access the data that is of interest to you. So now that we talked a little bit about the relational data model, and talked about tables, and talked about keys, and so on. So let's try a quiz. Two tuples in a relation or table, we are also calling this a table, can have the same primary key value. Is that possible or not? Is it possible to have the same exact value for the primary key in more than one tuple in a table? The answer to this question actually is no. Remember we did say that primary key uniquely identifies a tuple. Uniquely identifies means that given primary key value, there should only be one tuple. If it's your social security number or your employee ID, of course, that has to be unique. So a view is a virtual table. It's a derived table from data that is in table that you store. You can run a query on it, and actually can produce set of tuples, maybe certain attribute values have been removed, or something like that. And that table is what is called a database view. So, the question here is saying, we can use such a virtual table, or derived table, or database view, to enhance security, because one of these two reasons. So choose the answer you think is the best one. Let's look at the answers. The first one says we can exclude a query that we run to derive the data from a table that we have in the database, could exclude certain sensitive attributes like social security number or something like that. And users can only be given access to the view where this sensitive data is not there. This is a new derived virtual table we're talking about, so it doesn't have all the attribute values, in particular the highly sensitive ones. So this answer is the correct one. The next one is not correct because it says view can only be accessed by a single user. We actually didn't talk about that, doesn't have to be. Have a virtual table, or, it depends on what sort of access control you have in place, but it doesn't have to be accessed by a single user. Now that we have an idea of what a database is, in particular, a relational database that we been looking at, well, now we want to go back to what is relevant to securing it. What are the things that we have talked about? Remember, we are going to have users who want to access the database, we would have to do authentication so we know who the request is coming from. And once you do authentication, then comes access control. So, let's talk about how access control is done in databases. So actually, the two basic commands and different variations of SQL. Details may differ but we are basically talking about the key concepts. So you can grant or revoke. So this is sort of an example of how access could be granted. So, this says grant either set of privileges. So, for example, selecting some tables. So, privilege may be to do a select, for example. So you can grant a privilege on a table or you can grant a role to a certain user. That could be in the context of a given table. That's the resource we're talking about. This is what you can do with the resource. This access has been granted to a certain user, it could be granted to a role. We talked about role based access control, so if you have that it could be granted the role. Public is granted to everyone. So this is the world, every user that we have in the system. So some of these things are optional, for example what's in this parenthesis here, so if you don't specify a table, that's basically saying on all tables. Okay, so you can either specify a particular table or if you don't then this is optional part here. If it's omitted, then that would mean all the tables. Similarly there's an optional thing here that says identified by password, so that means if you ever revoke this access that's being granted as a result of this. You will need the password to do that. So somebody else can't revoke it if they don't have the password. And another thing that you can do here which is also optional is, that whoever you granting this access to. The user or to the role, they can actually further propagate, so that's the GRANT option, saying, well, I give you access, you can give this access to somebody else, if this is specified. If you have the access with the GRANT option. So an example of this would be for example a statement that says you grant the select privilege on any table, so we are omitting this choosing a particular table, because this is optional, to, the user part here is Alice. Remember we didn't include the grant option here, that means Alice will not be able to further propagate this select access that she has on any table to somebody else. We're going to talk about securing data, of course, you have to worry about access control. So access control, to make good things happen, of course authorized users need to have access. So this is how you can grant access to them. We talked about both a mandatory access control and discretionary access control. If your company controls centrally who should have access to what kind of things they should be able to do with various databases and tables and the databases they have, then of course it'll be centrally managed access. So that would be mandatory access control. Discretionary means if you created a table, then of course you are the owner and you can decide who else can access it. But this is how access is granted. So we saw the example of a privilege, but privileges can also be operations such as you can insert, update, delete tables and insert new tables and things like that. All the different things we said we can do under the basis. These are operations you can perform and basically access control says a certain user role is allowed to perform that operation or not. So the other side is, the side of grant is of course you may want to revoke. So revoking is again privilege we were talking about. The example we had was select, but it could be one of the other ones or you being able to take on a certain role optionally on a table. And you take it from a user or a role or from everybody. Granting is giving someone access, revoking is taking that access back, and an example would be, revoke the select privilege on any table, so we omit this from user Alice. Granting access rights to perform various operations you can do on a table, and revoking those, this is how we manage access control in databases. In this particular question we're saying Alice had SELECT access. So, someone has, if it's centrally done, admin or whoever's responsible for controlling access to various databases, has given SELECT access to Alice to a table. And she can propagate this access to Bob. So Alice wants to share the data she can pull out of the database through the SELECT operation with Bob. When can she do that? There are two options here. So you choose the one that you think would allow Alice to propagate this access rights she has to Bob. If Alice was granted this access with GRANT option, only then, can she properly fully propagate it to Bob. If she wasn't granted access with a GRANT option, she cannot propagate it. So, the second option is not correct. She cannot always propagate. The access right must come with a GRANT option, for it to be propagated. Cascading authorizations occur when access is propagated multiple times, as we said, from Alice to Bob to Charlie to whoever else. And it's just a matter here where Alice is granting it to Bob, who further grants it to Charlie. Now Alice is revoking access from Bob. So, we're saying, what should happen to Charlie's access? Remember, Charlie got access because of Bob. When we revoke it from Bob, how should we handle Charlie's access? That's the question. Question's asking, should we also revoke Charlie's access or we should not? Well the answer is yes because remember that Charlie got access because Bob had access. When we revoke it from Bob, Bob no longer has access. And if Charlie had access because Bob did and Bob doesn't have it, then Charlie shouldn't have it either so this revocation also has to cascade. I should say that sometimes somebody can get access from multiple such grant paths. So Alice, Bob, Charlie, but it could be John granting access to somebody who grants it to Charlie. Then they're two distinct paths and then relocation becomes a little bit more interesting. We talked about discretionary access control and mandatory access control. So here it says database access control is managed centrally by a few privileged users. We may have used the term trusted users in the past. It's saying is this an example of discretionary access control, DAC, or mandatory access control, MAC? Well, this is actually a case of mandatory access control, because, if you create a relation, well you don't decide who has access to it. If that was the case, then it would be discretionary access control at your discretion. But we know that database and tables that are there in the database, access control is done by trusted users centrally. So that is mandatory access control. We said databases, we need to talk about securing access to them, because there's something different about databases, while the structure in the data and how we access it. And we talk about access control, which is sort of something that we had discussed before. So, are there sort of unique kind of attacks that are possible on databases? So, that's what we want to explore next. What kind of threats are possible, because either the structure of the data that we have in the database, or the way we access it through the query languages like SQL, for example. So, we're going to talk about a couple of different possible attacks. The first one we're talking about is what is called SQL injection. We'll see that these are essentially someone exploiting vulnerabilities in the code that makes up the query that is submitted to a database. Query is written SQL, then it's an SQL query and attack is called SQL injection. So what really is an SQL injection is a malicious command. This is a command this is presented to the database. So that the database is going to actually run it. Malicious because it's going to allow someone to do something that they are not authorized to do. Results, of course, are not going to be consistent with the kind of security that you're looking to provide for this database. These kind of injection attacks when successful, they can disclose large amounts of data. We talked about disclosures of customer data and so on earlier. So if you do that, of course, you are impacting confidentiality of the database. You're extracting data from the database that is being made public or going to somebody who shouldn't have access to it. These injection attacks can also corrupt or delete the data that's there, some set of tuples in a table or something like that that could impact integrity of the data. So injection attacks can corrupt, which is integrity or disclose, which is confidentiality, and both of these bad outcomes are possible when it kind of attacks. We're going to discuss what they are, are successful. For us to understand what these SQL injection attacks are, we have to sort of learn something else, which is a lot of times the databases are in the back end of the system. The front end is some sort of web application environment that you have, okay? And the user interacts with the web application, and presents, we're going to do an example in a minute, some sort of request to the application. All that application then translates to a query that goes out to the database. And the query is typically generated by a script, based on the user input. So user wants something done, provides input for it. There's a script that takes that input, generates a query, and then submits that SQL query, for example, to the database. So this is the kind of environment that we're talking about. And what happens is that there is a vulnerability in the web application itself. This is web application, it's coded, it's software. So, this is software vulnerability. And SQL injection attacks, basically, exploit that vulnerability to craft this injection attack that we're talking about. So, just to make this idea of SQL injections concrete, and the script and the web application and so on, let's do an example. So, the example is the database that stores data about what is shipped where and things like that. It's a collection of the orders, for example, that a company may have received. Okay, orders that, whenever an order is placed, we create a tuple. Of course, one piece of information or attribute in that tuple is going to say where the item is going to be shipped. And this is, let's say specified by the users. User says, we're saving something, and I want it sent to the city. The way the web application supports user attraction is maybe using a form. And you fill the form with this needed information including the city that we're talking about. So, the user interacts with the form, fills in the various fields, including the city that we have here. And then, we said this web application now has to generate a query. So the script code that generates the query is included as sort of the example here. So the code is going to be SQL query that we have to generate. So the script basically is saying from the form you read the user input, which is the city where we're going to ship a certain item. So this statement shipcity is basically saying, from the form you read this value and that's if you want to send it to New York, for example, that's what we're going to have here. Once we have that, we're saying generate the SQL query, okay, and code. So, the query's going to say select everything from OrdersTable where the attribute Shipcity is what this input is provided to us. So we're going to add that. We'll see the resulting query is actually going to be select* from OrdersTable where Shipcity is, if you put New York City here, it'll be shipcity = New York. Okay, so that's an example. I think the only thing that's important here is that the user, the injection attack, the way it is going to work is that somebody is going to provide bad input. So we talked about buffer overflows through bad input before. It's not quite the same, but it's kind of like that. And injection is going to occur because of bad input. So the idea here is that you are getting the input here from the user. Based on that query, and this query is what's going to be submitted to the database. What we just said, with the web application and the reading input from a form and then generating a query, let's do an example based on that. So let's say this web application is running, and user is going to provide input. The input they provide is, let's say the city Redmond. So if that's the case, the script is going to generate the query that says select star from OrdersTable. If you go back and look at the code where basically it's going to replace the placeholder that we had for Shipcity by this input that was just provided by the user. The script generating this. And this is what is going to get submitted to the database. Now let's get to what kind of mischief a user who's malicious may do. So, what if the user, instead of providing Redmond, is going to provide something that says Redmond Code, semi colon, drop table orders table, drop, this is the particular table that we're talking about. What if they entered this whole thing? Okay, so before we look at this maybe a little bit of SQL here. So semicolon separates the statement, so this statement will end here. And the next statement is a drop table, orders table and drop results in basically, assuming that you have access to do that, deletion off this particular table. So what happened here is that the user is actually entering a lot more input than what we expect, which is just a city name. So what happens when the user actually provides this as input? They're injecting this SQL injection that we're talking about, you'll see where the name now comes from. In this case the query that we're going to generate is going to be this, actually. So, what this is what we got before. This is what we're going to get now because here we're going to replace the Shipcity variable value that we read is this one. So now, essentially we're giving the database this query. And now you can see what really has happened here. The malicious user that we have is able to inject code to delete the table, and that is what we see here after the semicolon, we see a DROP OrdersTable, and that's what we generate, and this will result in deletion of this table. So, this is the injection. We're injecting code through input, that is normally expected by the web application. The vulnerability we have in the web application is that it's not checking input obviously. It is accepting this input for cities where the item is shipped. Getting a name of course, but it also there's lot more stuff that comes after that. It is accepting this input, and this input is not correct input when we looking for just the city name. So, by providing this input, it's able to inject malicious code to drop the table or to delete the table. This is going to impact its integrity. And that's actually a concrete example of what SQL injections are So input checking is a defense that's generic for all software security. That applies here as well. And there are actually other kinds of vulnerabilities when it comes to web application security. OWASP is the non-profit open web application, security project. They have a list of top 10 vulnerabilities. They also talk about proactive controls that can help you address those vulnerabilities. And they always talk SQL injections, or SQL injections. So to avoid that of course, parameter checking, or argument checking, input checking, is something that is a good defense. In this question, we're saying in the following script is used to generate the query, so the query is going to be, we want to select account information from a table called users, and we want to do it for those users for whom we going to have the login, a password, and PIN. These three values going to be red, all these arguments going to be red from a form. And once we read them, we going to generate this query that is, this SQL query that is going to get submitted to the database, what input is necessary for it to generate the correct query. So there are three options here. Mark whichever ones you think are applicable. So it's really understanding this little script code that we have here that is used to generate the query that we going to submit. So remember, I did say that something has to be read. So the input may be through a form where the user types in. And then we're going to use these three variables that we have. So these login, password and PIN the values that are input, those are going to be added to this query that we are producing here. So, this query to run properly, one thing to notice here is that there's an AND, so, the login value has to match. The password value has to match. And the pin value has to match, okay? We're sort of doing three-factor authentication here, if you like. So for the query to execute correctly, because of this conjunction we have, all three of the values have to be correct. Okay? So we have to have the correct login name, we have to have the correct password, and we also have to have the matching PIN for that login name. So all three are necessary. All three must be provided correctly because of this AND operator that we have. So for each each tuple that we have in the table, and the table here is users, we have to see that these three values match the values that are there in the tuple. And when they do, that tuple is going to be selected, and we're going to return the account's attribute from that. This is how the script is going to generate a query that is going to produce the result that I just talked about. Now, we're talking about what the user is going to type in. So the user for the login field we have in the form, that's the value that goes to this variable that we have here, is actually typing something strange. So what your types is, in these codes, it's the space code, or 1 = 1. And it's important that these two dashes here that we have, they really say that everything that follows is a comment, and is to be ignored, is not to be executed. So the way to think about this is that, I'm not typing anything for password. I'm not typing anything for PIN. For login, I'm typing this information that I have here. So, if I did that, this script that we have here is going to generate a query. And that query is going to be run against the database that we have as we did before. So the quiz question here is asking, well what is going to happen when this query is run? The first one says the query will fail because the provided login is not a correct user. Okay? So obviously what we're providing here is not, doesn't match in any tuple, the login value that is starting, restoring. But let's look at it a bit more closely. Although there isn't a match, but then we're running into this or. Okay? This is a logical expression. Boolean expression. But if either is a match, which will, you take a tuple, the way you would do select is that you look a row in the table or a tuple, you look at the login field and see if this value is going to match. So what value are we going to try to match? Well, we only match the value up to this point. There is no match here. So that's going to be false. And then we continue on this, and here we say there's an or, 1 = 1. Now this is always true. 1 = 1, always evaluates to true. So we're either saying there's a match, or 1 = 1, well together this is going to be true. So, although the match fails, but because of the or, the full expression that we have here is going to be true. And we're actually not even going to check for password and PIN because whatever we put there or did not, actually follows this two dashes that we said, actually is a common field, anyway. So the query is actually only going to select, be that select accounts from users where log in equals this, Okay? That's what the query, that's going to be submitted to the database. That's what is going to get executed when it's submitted to the database. So the query actually will not fail, and the reason for that is that in this or expression, although the match fails, this side is going to be true, so the overall expression is going to be true. So the first one is not correct. It will not fail because it doesn't, login doesn't have to match. 1 = 1 is why this is going to be true. So the first one is not the correct answer. The correct answer is the second one. An injection attack will result In all users' account data being returned. This evaluates to true which means that tuple is one from which we should be returning that request for information, or the information that's being selected. Remember we return information that is selected from all tuples, for which this expression evaluates to true. Now, we're going to talk about a very different kind of an attack. This is called inference attacks and that can be performed or mounted against databases. So the definition of an inference attack is actually fairly straight-forward. Inference attacks occur when somebody's able to use queries that they're authorized for, they're allowed to execute those queries. But by executing those queries, they're able to gain access to information for which they're not authorized. They're not allowed to access that information directly. So the way they gain access to that information is by executing a set of queries that are authorized, and then making an inference based on the results that are returned by those queries. So a concrete example here, since we're talking to students in a course, is let's say a database that contains all your grades. So schema, maybe we have an attribute, that is the studentid, maybe the student_standing, whether a junior or senior and then scores. These might be numerical values between zero and 100, let's say, on exam1, exam2 and then the final_grade. The tuple, if you look at it, it's going to have a studentid, either junior or senior, their standing, the two exams, course and the final_grade. So if we consider this database, where we have a tuple for each student in the class, tuple as we just discussed, what kind of query can you allow on it which any user should be allowed to submit and get the result from it? We're going to look at an example of inference attack with the database that we defined by the schema that we just discussed. The query that we're going to consider here is a query that returns the average score on an exam. Okay, so any student should be able to find out. In fact, that's the first question after an exam, when you bring back the grades, students ask what was the average. So any student should be able to submit a query that returns the average, of the score, on that exam. And the average basically is, we take every student's grade, sum it up, and then divide it by the number of students. And that shouldn't tell us anything about a particular student. Remember what you're not authorized to access is somebody else's grade. This is an authorized query and our idea of inference attack is that when you use a query like this to gain access to information that you normally are not allowed to see. If you are an attacker, your goal would be to gain access to information that is not available to you. So that would be to find the exact score of some other student. It's not your score that you already know, but if you are an outsider, of course you're targeting a particular student, and you want to find out his or her exact score. That's the goal of the attacker or the information that we want the attacker to get access to. Inference attacks sometimes require some additional outside or external information. So maybe the attacker does know when somebody takes, some person takes the exam late. Could be because they were sick or something like that, so the whole class took the exam on a certain date, and after a few days after that, perhaps, this student is going to take the exam. And this information is available to that hacker. So that hacker is going to do in this case is run the average score query, before this one student, the one who takes the exam late actually takes the exam and his or her score is added to the database. So before the exam is taken, the score may be zero or something like that and after the exam, it's replaced by the correct value. So that attacker is running the average query which we said should be allowed before this one student takes the exam. And the attacker knows when the student has taken the exam, maybe tries a few times, eventually sees maybe a different average value. Or, after a certain amount of time, we know that the exam must have been taken by everyone, including this one student who had missed it earlier. So it does the average again. Okay, so now I have two averages, and let's say this student is Alice. So the average score, not including Alice, and then average score of everyone that includes Alice. So based on these two values that are returned by this query that is allowed, I can now figure out what Alice's average is. Actually it's fairly easy to do that, if there are n number of students, the total score, if you add them all up, is n times the average. So here, since Alice hasn't taken, so it'll be n-1 times the average of the score before, the total of the scores before, and this is after Alice takes the exam. The difference is Alice's grade exactly. Okay so the idea here is that these two queries both are allowed, because they return the average. The database system allows that hacker to run each one of these queries by running these queries at particular times. You have to think through when exactly you want to do that. We are able to make an inference and find the exact value of the target score, just by the difference that I just mentioned. So, this is the example of an inference attack where you use authorized queries to gain access to information that you're not authorized for. So, which is the exact score of the target, in this case the target was Alice So let's look at another example. Maybe you say, well, the one I just gave is not that realistic. What if everybody takes the exam at the same time? Well, that particular attack will not succeed in that case. So we're going to craft a different kind of an attack, using the same authorized query which returns the average score on an exam. So let's say in this case, we have only one student with a standing that is junior. Because the course, let's say, is a senior course. And one smart junior student chose to take it or enroll in it. So it's a senior class and we only have one student who is a junior. It's useful information that we know where there's groups of students. Okay, you may ask for the average scorer, male students, female students, students who are juniors, students who are seniors, and things like that. Because in general, if there are a lot of junior students, maybe you want to know how did you perform relative to your peers who are also juniors. So let's take the query that says, which also specifies the standing of a student, but it's still asking for average score. Let's say that's allowed. So here we get average score of students when the standing was junior. So that's where our query says, where standing equals junior. In that case, we're still running the average score query, so it will take the tuples where the standing is junior, and then compute the average on those and return that value. Well, that's a problem if it does do that. Because in this case, the query's actually going to disclose the average. If you're doing average for one value, then you are actually, this average discloses the value itself. So this query, if there's only one student, of course the result it's going to return is going to be the score of the student who had junior standing. So this is another example where I think the reason this happens is that we're computing the average which is an aggregate, which is authorized as we said before, over a small set of tuples for the exam score values in tuples that are selected. In extreme case, there's only one of these tuples. And in that case, it's the exact score. So this query again allows someone to make an inference, saying the student who's standing a junior, this, the average that we get is his or her score. We saw examples of inference attacks. The two quick examples when the query was an average of an exam grade that was authorized but the result was disclosing somebody's grade, which we didn't want that to happen. So obviously, we want to have defenses against these kind of inference attacks. These defenses are actually pretty hard. In general, if you can submit an arbitrary number of such queries You can always make an inference based on the results those queries return. But there are some things that we can do to reduce the likelihood of somebody finding sensitive information by using such queries with at least limited amount of effort. What kind of defenses can we have in place? So remember the way we do these aggregate queries that are allowed is that the query sort of says, select a set of tuples. So, for example, that case we had where we looking for Junior's standing, we're going to pull out all the tuples where the standing attribute value is Junior. So we're going to pull out all the tuples that match this query in some sense. And then we're going to aggregate the exam score. So one way to make sure that inference is more difficult is that we don't do that when the number of tuples that are selected is too few. The idea here is when it's too few, extreme case when it's one, then we know that the average actually discloses the score that we have. But if it's a small number of tuples, the average is going to be close to the score of every person who is selected or whose tuples are selected. So average for a small set essentially tells us what those values are and we don't want to do this average score when the number is too small. Well you can think of other cases where if you asking for some property, and that selected set of tuples is very large, then that sort of holds for everyone. And if it holds for everyone, then it holds for a given user as well. So both too small and too large actually lead to this inference problem. I'll let you think about the too large part. So you should see the aggregation is happening over a set of tuples. And does it include every tuple, or almost every tuple, in the database? Or, does it include very few? And depending on the nature of the query that we have, you may say that, we can't run that query because of the number of these tuples, that it's going to aggregate all. So that's one way to do that. The other defense that we typically do is saying well, we can transform the database and can remove all the identifying information. Think about the exam score that I was talking about. Let's say we dropped the student ID and standing, even if you're concerned about too few students with a certain standing, and then we just post in the exams score without the names or the IDs. And the way we do that is the process is called de-identification. De-identification essentially says drop or remove all the values that identify a particular user. Now, de-identification alone is not sufficient. Sometimes we have to do what is called anonymization. So, think about the example where there was only one student with junior standing. Although we dropped the student ID that directly identified that student. But the standing field or attribute will tell us, even without the ID, who that particular tuple is about. So one way to do that is you can do some sort of anonymization. And one way to do anonymization is what's called generalization. So you replace an exact value by a set of values or a range. So, we may combine junior-senior standing and we can just say upperclassmen or something like that. And if you only have juniors and seniors, then that would then avoid that problem we had where the junior standing identified a particular student. So, even when you do the de-identification and anonymization it has to be done with care. Depending on your data set that you have, or what's stored in the database, this may not be sufficient. Think of an example where we're looking at people, with certain diseases, who live in various zip codes. So, well someone may be able to make some sort of an inference based on a zip code and publicly available information, addresses, and things like that. So, we may want to drop the last digit of the zip code, or the last two digits of the zip code. So that's one way to generalize it. Okay? So, Georgia Tech is 30332. If we replace the last two by zero zero, then perhaps the three digits we have match with a much larger area. But what if, even after you do that generalization, the disease that we have is the same across all those different zip codes where the first three digits are the same? The disease values are not diverse enough, let's say, if they're exactly the same. Everybody has some sort of infectious disease or something like that, which CDC maybe looking at on how those infections are spreading out and stuff like that. Generalization won't do anything on that case, isn't it? Because it's the same value C same disease D for example that we didn't want someone to see that this particular zip code had incidence of that disease. That's why we do the realization saying well it could be any of the four or eight or whatever number of zip codes when you replace the last two digits by zero zero. But if the values, the disease values are not diverse then just generalizing or anonymizing the zip code value doesn't go very far. So in a generally, either this transformation involves de-identification, generalization, anonymization. There are things called k-anonymities so they're at least multiple tuples and you focus on certain subset of the values that could be used as a quasi identifier are the same in the database And then we have to see that across the same set of quasi-identifiers that we have, they're multiple values, so we can't say that given an identifier that we have here, for sure we know that this must be the sensitive value. Look at the two options and choose the one that you think is right. The question really is saying, is inference attack possible? So either yes or no. So think about, we're doing queries where we're computing the average and returning it and we're doing that only for students that come from a certain state. So, what if there's only one student from, say, a smaller state. Wyoming for example. So if there is only one student from Wyoming, this is kind of like the Junior standing discussion we had. There was only one student, the Junior standing. So there's only one student, we ask the query, saying give me the average of students who are from Wyoming. The average is going to be the same exact score of that student, one student, who comes from Wyoming. So, depending on how many students, if there are too few, extreme case only one, then, of course, the average is the score. So, Yes is the answer. It will not be possible if there are lots students from each state. But we didn't say that here so I would pick the first answer. We have the same database that we were talking about, but we're going to do a different kind of attack here. We're going to de-identify it by removing student ID, so de-identification, you take the database, drop the attribute that is student ID, and, if there are names, maybe we remove those, too. Furthermore, the field that has the state of the student because we just now saw in the previous question that, if there are too few students from a given state that could be a problem. So we're going to do generalization by replacing state with sort of the US region. East Coast, Midwest, West Coast, whatever way you want to do it. We're not asking for a specific state. That is too small and has only one or a small number of students. So this generalization is essentially replacing state by a larger geography, hoping that there'll be more students who would come from the geography. The generalization ensures that there are at least two students from each region. The problem where we only had one from Wyoming is going away. Are inference attacks still possible? Again, yes-no possibility, so think about it. So we've done DI identification, we've done generalization, and then we're logging these queries that return the average value. Can let's say Alice and Bob both come from the Midwest? Is it possible for Alice to figure out Bob's grade? I'll seek and ask for the average score of students who come from the Midwest. So she knows the average of her and Bob's scores. She knows her own score. Based on that she's going to, it's easy to see how she will be able to compute Bob's score. So the answer is yes here. It is possible when there are only two students. Remember? We said a generalization ensures that there are at least two students. So let's say from Midwest that we're talking about, the two students, Alice and Bob. Okay so, Alice can get the average for herself and Bob. There were only two coming from this region. And she knows her own grade based on she can make an inference about what Bob's grade is, can you exactly compute Bob's grade. So yes, an inference attack. It's possible even when we do this with de-indentification and generalization. If you do generalization course grant generalization with there more and more people, this likelihood would reduce. But clearly the example that we have here, this is certainly possible. We saw that how structure of the data that is stored in an interrelational database changes the way we do access control. We also saw that certain kind of queries, in particular queries that return arrogate results, can leak potentially sensitive data via inference attacks. So we studied this new kind of attacks. Inference and injection attacks. And ways in which we can address those attacks for database systems. We covered a number of topics so far. But you'll see that all of them have focused on problems that we address in the context of a single computer. Unfortunately, a single computer is not very interesting by itself. It has to be connected to a network. And when we connect computer to our networks, that brings in a whole bunch of new security problems. That's where we're going to go on to the second part of the course. We have a world-class network security researcher who's actually going to take you there. For part of this course, we're going to cover a number of major topics in network security. We're going to start off with malware and network defenses. Then we're going to move on topography, security protocols, web security and mobile security. There are several reasons why attackers would want to carry out their attacks through malware or malicious code. They can achieve automation, scalability and deniability. For example, they can do these malwares on the Internet, let the malware spread and carry out the attacks on their behalf. In this lesson, we going to give overview of malware. And cover several kinds of malware. In the next lesson, we going to discuss the more advanced malware. To get you to start thinking about malware, let's do a quick quiz. What are the estimated yearly losses due to cybercrime worldwide? Is it $100 to $500 million? Or $500 million to $1 billion? Or $100 billion to $500 billion? According to the Center for Strategic and International Studies and reports from various antivirus and security companies, the correct answer is $100 to $500 billion. This is a huge amount of money. For comparison, drug trafficking results in about $600 billion a year. So cybercrime is in par with drug trafficking. And you may wonder why such a huge number. Now of course, this number includes the direct financial losses, such as when a credit card number is stolen, or bank account is compromised. But it also includes the cost due to productivity loss, such as the need to compare the computer and systems after a cyberattack. And it also includes losses due to intellectual property when valuable information is stolen. There are two major types of malware. The first kind of malware needs host program, meaning that they have to be embedded in the host program in order to run and spread. The second type of malware is independent, meaning that they themself are independent programs that can run by themself. We will study some of them in this lecture and cover the rest in the next lecture. When we say a malware needs a host program, we mean that the malware is embedded in the existing program so that you can enter program, runs on the system and then spread from there. There are several ways for malware to imbed itself into a program. For example, trap doors, logic bombs, trojan horses, viruses, and malicious browser plugins and extensions and scripts and so on. And the independent malware does not need a host program, because this malware are complete programs by themself. Examples of these malware include worms, botnets, and advanced persistent threats, or APTs. We will discuss botnets and APTs in a later lesson. Now let's discuss some more details of the various types of malware. The first is trap doors. Trap doors is also known as back doors. It is a sequence of instructions in the host program or system that has been embedded by a programmer and can be activated by the attacker. Essentially, a trap door provides a secret entry point to a program or system, and this secret entry point is typically known only to the programmer and the attacker. A backdoor in a program typically works by recognizing some special input command, such as a sequence of input specifically crafted, or a special user ID. For example, an attacker can gain access to a system through the back door without providing the proper user authentication. A famous benign version of a trap door sometimes called an easter egg is the fly simulator in the 1997 version of the Microsoft Excel program. The user when entering undocumented series of commands, can gain access to a flight simulator program embedded within Microsoft Excel. A Logic Bomb is, essentially, a trigger planted in a program. When the triggering condition is met, the planted code then execute. In such a way, malicious activities can be activated whenever a condition is right. For example, a branch of the program will launch, denounce service attacks to whitehouse.gov only when the current time is the specified time and date. And that's an example of a logic bomb. Trojan Horses get their name from a tale from the Trojan Wars. It is said that the Greeks wanted to enter the well fortified city of Troy. Rather than launching a direct assault at the city and suffering huge losses, they devised a wooden horse and they hid their soldiers inside the horse. Then the left the horse out side the gates of Troy as a gift. The Trojans thought the horse as a gift to acknowledge that the Greeks had been defeated. And so they brought the horse into the city of Troy. At night, the Greek soldiers hidden in the horse came out. And they let their fellow soldiers waiting outside the city come in as well. As a result, the Greek soldiers passed all of Troy's defenses, and destroyed the city, and won the war. In the context of malware, a Trojan horse is a piece malicious code embedded in a utility program that a user will run frequently. That is, when a user runs this useful program, the malicious code, or the Trojan horse is also executing. An example of a Trojan Horse is a login program that performs key logging meaning stealing user login and password and pass along such confidential information to an internet server. The login program will still allow the user to log in by calling the real login subroutine because otherwise the user would notice. Many malicious browser extensions or also perform key logging and phishing, in addition to some useful functions and these are the latest examples of Trojan Horses. Viruses is perhaps the best known type of malware. A virus infects a program, by modifying the program code so that when a program runs, the virus code also runs. It then self-copy into other programs, and thus, spreads itself. There are many four stages in the life cycle of a virus. The first is a dormant phase. This is the phase when a program has just been infected by a virus but program has not run yet so the virus has not been triggered or spread. The second stage is propagation. This is when the malware's being sent around or spread. For example, the malware can come as an email attachment and the email attachment is being sent to many users. The third phase is the triggering phase. This is when the host program is being run. And as a result, the virus is also triggered to run. For example, when a user clicks an email attachment that contains a virus, it's triggering the virus to run as well. The fourth stage is the execution phase. This is when the virus code runs, performs some malicious activities, and most importantly, it looks for targets to infect so that it can spread. For example, in a case of email attachment, when the virus runs it can search for users in the address book, and then send email attachment with the virus to users in the address book. And that's how he can be propagated, triggered, executed, and again propagated, triggered, and executed. And this is how virus spreads For this quiz you write in the box the type of malware. And the choices are T for trapdoor, L for logic bomb, H for trojan horses, and V for virus. So the first malware, an email attachment that when being opened will send itself to all people in the user's address book. As we have discussed a little bit earlier, this is a virus. The second malware, a customized keyboard app that logs user input and sends it to a server on the Internet. This is a Trojan Horse, because while it performs some useful function, it also performs some malicious activities. The third malware, part of a program that will only run if the computer is at the user's home and it will upload Microsoft Word documents to a website. This is a logic bomb, because the triggering condition is the place or geolocation or IP address of the computer. Which is at the user's home. And when the triggering condition is met, it performs malicious activities. The fourth malware, login program with an undocumented option, for example DEBUG, that would allow an attacker to supply any username and password to gain access to the computer. This is a trap door, because it allows an attacker to gain access to system without going through the proper security check. Let's do another quiz of malware, that requires host programs. Here in the box, you specify the type of malware that would be best for the given task. So first, spy on employees of a specific company. You can do this with a trojan horse. For example, the trojan horse can come in the form of a utility program, such as a company calendar, that also spies on the employees. The second task, cripple an organization's computers. This is a logic bomb. For example, a logic bomb can be inserted into the company's computer servers so that when the time is right, the server will shut down. The third task, quickly spread information and drive traffic to a specific website. This is a virus. As we know, virus can spread quickly, for example, through email attachment. And, when triggered, it can perform a number of malicious activities, such as driving traffic to a website. Now, let's discuss some of the details of viruses. First, let's take a look at the structure of virus. A virus infects a program by modifying the program code. That is, the virus code has to be physically inserted into the program file. Logically, when the infected program runs, the virus' codes run first, then the original program will run, so that the user will not suspect that the program has been infected. And then at the end, there could some virus code that does clean up to avoid detection. If you look at the infected program, the first line controls that the virus program will always run first. It is critically important to put this control in the first line of the infected program, because this is the only way to guarantee that the virus code will always be run whenever the program executes. It is also important to put a marker in the infected program. Such as putting a special flag in the second line of the infected program to indicate whether the program has been infected by the virus or not. Otherwise a program can be repeatedly infected. When the virus code is run, it typically first finds other programs to infect. Of course, it will check whether a program has been infected already by looking at the special flag. In addition to infecting other programs, the virus code can also perform other malicious activities on the system, such as stealing valuable documents. After performing the malicious actions the virus will then transfer the control to the original program so that the normal work can be performed in such a way the user would not notice. The virus code can also perform other actions in order to avoid detection. For example, because the virus code is physically inserted into the original program file the file size of the original program obviously increases. And this can be a tell tale sign that a program has been infected. Therefore, in order to avoid detection, the virus code can compress the infected program so that the file size is the same as the size of the program before it is infected. Now let's look at the different types of viruses. The first is a parasitic virus. They typically scan programs on a system, for example on the hard drive, and then infect these programs. The second is the memory-resident virus. They're typically of an operating system, and when the system runs, the opening system is loaded into the memory. So as long as a system is running, the virus, resize the memory. Then it can infect any running program on the system. The third is the macro virus. They're typically embedded in a document. And when a document is opened, the virus also runs and spreads. The fourth, is the Boot sector virus. They reside in the boot sector of a hard drive, and whenever a system is booted the Boot sector virus will run and spread. A very important type of virus is called a Polymorphic virus. For a Polymorphic virus, each instance, or each infection, can look different because part of the virus program is encrypted by a randomly generated key at each infection. The purpose of using polymorphic virus is to avoid detection by easy signature matching. We will discuss this a little bit later. We should note that any of these type of viruses can by polymorphic. Now let's discuss boot sector virus in more details. First let's look at how boot sector works. A boot sector is a special sector on the hard drive of a system. When a system is booted the code in a boot sector will always run first. In a code, it's called bootstrap loader. The bootstrap loader is typically responsible for loading the operating system. For example, it may ask a user to choose a list of operating systems to boot from. For example, the bootstrap loader typically may ask a user to choose an operating system from a list and then loads that operating system. And this is how system boots from a hard drive. Again, it starts with code in the boot sector. And then when a bootstrap loader runs, it loads the operating system. When a boot-strapper virus infects the system, the virus code is inserted in the boot sector. And the reason is, boot sector again, is a special place in the hard drive and the code there will always be executed first when the system boots. So, by putting virus code there, whenever system boots the boot sector virus will run. Then, of course, the boot sector virus can perform a number of malicious functions such as infecting other programs on the system, spreading to other systems and stealing useful documents from the system. After the virus code runs, the boot sector virus should transfer the control to the original bootstrap loader so that the system can boot normally, at least appear to the user that the system boots normally. Now let's take a look at macro viruses. First, what is a macro? A macro is actually a program embedded in a document, such as a Microsoft Word Document. It typically contains instructions for some useful functions, such as opening a file or starting a new application. And because a macro is an executable program, it can be infected by viruses just like any other executable programs. What's unique about macro viruses is that users typically don't suspect that a document will contain a virus. Here's how a macro virus can typically spread. First, the attacker creates a macro that contains a virus and then attach it to a Word Document. And then this document can be sent around, for example, through e-mail attachment. And then, when an unsuspecting user clicks on the e-mail attachment and opens the document, the document is opened on the user's computer. When the document is opened, the macro executes and as a result, the macro virus also runs. The virus then copies itself to the global macro file. When the document opens, the macro executes and the macro virus also runs. When the macro virus runs, it can perform a number of malicious activities, such as sending the same Word Document to a number of users in the user's address book as an attachment. And the spreading itself. What's more interesting is that the macro virus can copy itself to the global macro file. As a result whenever the user opens a new document or creates a new document, the global macro will be copied into the document, and that's another way that the macro virus can spread. As we discussed, macro virus is embedded in a document. So it's not really at the operating system level. Boot sector virus, as we discussed, boot sector virus resides in the boot sector of the hard drive. And it runs before the operating system is loaded. So it's not really at the operating system level. Memory-resident virus, as we discussed it is embedded in the operating system, so that whenever a system runs, the virus stays in the memory and it can infect any running program. So therefore, memory-resident virus begins at the OS level. Now, lets discuss a special kind of memory resident virus called Rootkit. A Rootkit is embedded in an operating system. It typically modifies some of the code and even data structures of the operating system in order to perform some malicious activities. For example, a Rootkit can be used to hide a malware from he user. For example, when the user uses the hours command to list the contents of a directory, the Rootkit can change the output of the LS command so that the user will not see the malware file. Similarly, when the user uses the PS command to see what programs are running on a system, the Rootkit can modify the output of the PS command to hide the running of the malware. Let's study an example of how Rootkit can modify the operating system in order to perform malicious activities. For example, the rootkit is trying to hide the malware file from the user when he lists the contents of a directory. First, let's examine what happens when a user looks at the files in a directory. Suppose on Windows, the user use the command D-I-R, DIR, for looking at files in a directory. As we show here this command can be implemented by a loop that keeps looking at the next file in the directory. Now, let's look at how a Rootkit can hide a malware from the user when he looks at the files in a directory. We know that files and directories, they reside on hard drive, which is controlled by the operating system. Meaning that, any access to the hard drive has to go through the operating system. Therefore, in order to get informations about files in the directory we have to go through operating system functions in order to get such information. In other words, operating system functions are being called to look at informations about files and directories on hard drive and return the results back to the user. So here is what an operating system will typically return when the user looks at the files in a directory. That is, suppose the Rootkit is not embedded in the OS yet, then the OS will return all the files in the directory including the malware file, say mal_code.exe. And this is the file that the Rootkit would try to hide from the user. In order to hide the malware from the user, what the Rootkit can do is to intercept any function call to the operating system. And then reason whether the call will end up revealing the malware. And if no, it will just pass the call to the appropriate operating system function. And if yes, it's going to execute the call, but intercept the result so that it can filter out the result as necessary, in order to hide the malware. That is the Rootkit intercepts the function call to the operating system and knows that the operating system function call is looking at files in a directory, and then it knows that the return results may contain the malcode.exe. So the Rootkit will filter out the file name so that the user will not see this file name in the result. So here you go, when the Rootkit is embedded in the operating system, you can filter out the malware file. So when a user looks at the files in a directory, he will not be able to see the malware and this is how a root kit in the OS can hide a malware from the user. Again, the root kit is able to accomplish this by modifying the operating system. In particular, it intercepts a function called to the operating system, and future the result of the function calls. The question is, which operating systems can be affected by Rootkit? Is it Linux, iOS, Windows, Android, or all of them? The correct answer is that all of them can be affected by Rootkit. Again the reason is that a Rootkit is a piece of malcode that can be inserted into any operating system. Here I'm going to ask you to tell me which of the following statements are true and which are false. So the first statement, can only infect Microsoft Windows. This is clearly false. The second statement, can modify hidden and read-only files. This is true. You may ask, how can a malware find a hidden file or modify a read-only files? If you think about it, a malware can be inserted into the operating system, which has the highest privilege. It can find any hidden file on a system, and it can override any permissions, such as read-only permission. The third statement, spread only on disks or in email. This is clearly false. The fourth statement, cannot remain in memory after reboot. This is a somewhat tricky question. If this refers to the particular instance of the virus, then this statement is true. Because after reboot, the memory is wiped clean, and start over. On the other hand, although we say that the particular malware instance is gone, after the system reboots, we should know that if the virus is part of a operating system, then when the system reboots, the virus will be loaded into the memory again. So we really have to think about this statement in a context. The fifth statement, cannot infect hardware. This is true. But we have to note that by hardware, we mean, that hardware that does not have any software component, such as firmware. The last statement, can be malicious or benign. Although this may appear to be subjective, I will say the answer is false. The reason is that, any malware that gets on your system without your knowledge or authorization, already violates security policy, and therefore, it is already malicious. Now let's discuss malware that does not require host programs. The first type that we're going to discuss is Worms. Worms are independent malicious programs and they typically use network connections to spread from one system to the other. Worms represented a major advance in malware in the 1990s coinciding with the rapid expansion of the internet. Worms later evolved into botnets around 2005, which is still the dominant form of malware today. We will cover botnets in a later lesson. Now let's discuss the first major Internet worm, also called the Morris worm after its creator Robert Morris. Here's how the worm worked. When the worm runs on a system, it looks for other systems on the Internet that it can spread to. For example, these systems have some security vulnerabilities that the worm can exploit. And then by exploiting these vulnerabilities, the worm can infect these systems. In other words, it can lure itself to these systems and that's how it spread. Once the worm got on a system, it also employed a number of tricks in order to keep himself from detectable. According to each creator, the Internet worm was made as part of an experiment to measure the size of the internet. For example, by measuring how many computers are connected together. However, there was a programming error in the code. That is the worm would infect a computer regardless, whether the computer had been infected already one out of seven times. This proved to be too aggressive. And as a result, many computers get infected repeatedly. That is on these computers, there are many instances of the same worm running resulting in resource exhaustions. And that's really how the Internet worm was discovered, because the system admins find out that their servers were overloaded. As a response, many system admins disconnect their servers from the internet in order to stop the spread of the Internet worm. But since many servers were disconnected, the Internet was disrupted as well. When the Internet worm identified the next target to infect, it looked for several security flaws that it knew how to exploit. These include systems with guessable passwords, systems running the fingerd program that had a buffer overflow vulnerability. Systems running a sendmail program that has a trapdoor, which means that by supplying some special input commands, one can gain access to these systems. When the security flaws were exploited, the Internet worm can gain access to a target system. It then will load a small piece of code called the bootstrap loader on to a target machine and this loader will then fetch the rest of the worm code. It even used password based authentication to make sure that only the bootstrap loader of the worm can load the rest of the code of the worm. The Internet worm also employed a number of tricks to hide itself. For example, the worm code is loaded into the memory. It is encrypted and decrypted when necessary and the original file is removed from the hard drive, so that the user will not be able to see the worm program. The worm even periodically change its process name and process ID, so that even when a system admin looks at what programs are running on the server, he cannot easily discover the Internet worm. The Internet worm resulted in major deception to the Internet, because many servers were infected and had to be disconnected from the Internet. So what lessons did we learn? The first lesson we learned was that we need to perform security scanning and patching. The Internet worm was able to infect so many servers, because these servers had security flaws. Further, most of these flaws were not only well-known, but also had security patches or fixes available. Therefore, if we scan and patch computers on the Internet that have security flaws, then we can reduce the chances that they will be infected by malware. The second lesson is that we need to have a fast and coordinated response to a major security incident such as the Internet worm. And because of the Internet worm, the US government established the computer emergency response team or CERT for short. Nowadays, CERT is usually responsible for issuing alerts about security for flaws and recommendations about patches. Which of the following methods can be used to spread a worm? Here, we list a number of methods, email, instant messaging, downloading files, watch a video on Netflix, clicking on a popup, or using Facebook. All of these methods can be used to spread a worm. Some of these are obvious, for example email, instant messaging, and downloading files, and others may require some analysis, for example, watching a video on Netflix. A video may contain instructions or executables where the worm can be embedded in. Likewise, clicking a popup. A popup may contain malicious scripts that itself can be a worm. When you use Facebook there are a number of active contents, meaning those are executable programs or scripts that can be worm. Now, let's look at the counter measures. The first is prevention. For example, we can limit the content of a computer to the untrusted outside world. Meaning that it would not accept documents or programs, or any active contents from other computers. And of course, this will impose major inconvenience to a computer user. The second approach is detection. This means that we use a monitor to watch out for telltale signs of malware infection. The third approach is removal, meaning that once we detect that there's malware infection, we will remove the malware and perhaps we should also patch the system. Given that prevention severely hampers productivity, detections really the main counter measure that we can use, and there are four generations of antivirus software, or malware detection software. The first is malware scanners. These scanners use signatures or patterns of known viruses to scan program files to find matches. And if there's a match, that means that this program file has been infected by a known virus. One of the examples of signatures of viruses. A signature of a virus is typically the unique sequence of instructions of the virus code or the unique infection marker that the virus would use. These simple scanners are not effective against polymorphic viruses. And the reason is that for polymorphic virus each instance is encrypted with randomly generated key, such that there's no unique signature across all instances of the same virus. The second is the heuristic scanners, they are based on possible effects of infection. For example, if a program file has been infected with a virus code the checksum of the original file will have changed because new contents has been added to the file. However, this approach can be defeated if the malware deliberately makes sure that the checksum after infection remains the same. For example, the malware can include some additional bytes at the end of the file to make sure that the checksum remains the same as the file before its infection. The third is activity traps. These detectors look for particular kind of activities that malware will typically perform on a system. Such as modifying the Windows registry file, or reading the password file and sending it to the Internet, and so on. These detectors are based on our knowledge of malware activities, therefore, these detectors are not effective against malware that performs new kinds of malicious activities. The fourth is the so called, full feature analysis, which is the state of the art. It typically involves multiple approaches. For example, it typically includes host-based monitoring. That in turn includes activity traps and scanners. It also includes network-based monitors that analyze traffic to the Internet. For example, if there's a traffic that contains password file to Internet server, that can be a telltale sign that a malware has infected and host, and is attempting to steal the password file. Similarly, if there's a connection to a website that is well-known for a malware download, there's also a telltale sign that the end host has been infected. And the malware on that host is attempting to download an update. And you can also include a sandboxing-based analysis approach. A sandbox is typically used to run a piece of executable, for example, an attachment from an e-mail, to see whether this executable would exhibit any malicious activities. By executing this executable in a sandbox, we can make sure that there's no permanent damage to our system and network. And we can observe the behaviors of the executable from outside a sandbox. So that we can be certain whether this executable is a malware or not. Given what we've said about the limitation of signature scanners, why do we still use them? Is it because they are very efficient? Yes. They are efficient because signature matching or matching can be very efficient. Is it because they are effective against known malware? The answer is yes. Known malware means that a malware that matches a signature. And of course, signature based approach is effective against these malware that have signatures already. Is a good first line defense, yes, because with this approach at least we can detect all the known malware already. Therefore it is a good first line defense. Now let's do a fun quiz on worms. Which of the following has caused the greatest financial damage? Is it the ILOVEYOU worm? That as an email attachment gets sent around, and when it is triggered and executed, it will delete all the files on the host computer. Or CODE RED which is a worm that infects many Microsoft servers. Or the Morris Worm or the Internet worm that we discussed. The correct answer as I hinted, is the ILOVEYOU worm. The estimate cost is above $2 Billion. CODE RED is somewhere around $1 Billion and Morris Worm is around $100 Million. Some malware required host programs. These include trojan horses, trap doors, logic bombs, and viruses. Other malware can run as independent programs. For example, the Internet worms. In this lesson, we will first discuss the IP layer security protocol called IPSec. We will cover the main mechanisms of IPSec. And the Internet key exchange protocol that is used to set up the IPSec security parameters. We will then briefly discuss the transport layer security protocol. These protocols are based on the cryptographic operations and security protocols that we have covered in previous lessons. To understand the goals of IPSec, let's take a look at a critical weakness of our IPv4. In IPv4, there's no authentication of the source IP address. That is, if Alice receives a packet with Bob's social IP address, Alice cannot be sure that the packet is really from Bob. As a result, IP spoofing or forging the source IP address is a commonly used technique in cyber attacks. For example, bots in the botnet can send a DNS query to DNS servers asking the full TXT record of a domain. By spoofing, the source IP address of a victim website. As a result, the response from the DNS servers which can amount to a very large volume of data is sent to the victim website. And this would result in a denial-of-service of the victim website. IPSec provides security measures at the IP layer. This include authentication of source IP addresses, confidentiality and integrity protection of packet data. And authenticity of packet data, in particular preventing replay of packets. Of course, a network application or protocol can implement its own specific security mechanisms to achieve these goals. By having IPSec, that is implementing security at the IP layer, we can ensure secure networking not only for applications to have a security mechanisms, but also for many applications that are ignorant about security because all application run on top of the IP layer. Let's do a quiz on IP spoofing. Mark the answer or answers that are correct. The second statement is correct because IP spoofing only works for unidirectional communication. For bidirectional communication, the server will not reply to the attacker, but to the spoofed IP address, which will not respond appropriately. In IPSec, there are two operation modes. In transport mode, security protection is provided to traffic from one end host to another. That is, it is an end to end protection. In tunnel mode, security protection is typically provided to traffic from gateway of a network to the gateway of another network. This is how the so called virtual private network, or VPN, is implemented. Tunnel mode is the more commonly used operation mode. Suppose we have two end hosts, A and B, belonging to the same company but in two different local area networks over the Internet. If these IPs are tunneled between the gateways of the two local area networks, then traffic from A to B is automatically protected by the tunnel. That is, A can send unencrypted or unprotected packets. And before the packets leave the local area network, the gateway adds protection and sends the packets to the gateway to B's network which then unprocesses the packets. For example, decrypts the packets and sends them to B. The gateway of A's network actually encapsulates traffic from A to B by adding a new IP header that specifies the gateway as the source IP and B's gateway as its destination IP. To make sure that the protective packet is delivered to B's gateway first. It also includes the IP's header which contains information about the protection provided using Or ESP which we will discuss shortly. The original packet now becomes the data or payload of the new IP packet. Now let's do a quiz. Fill in the blank with the letter of the correct answer. IPSec can assure that. A, a router advertisement comes from an authorized router. B, a routing update is not forged. C, a redirect message comes from the router to which the initial packet was sent. D, all of the above. The answer's D. IPSec can authenticate a source IP address. It can also guarantee the integrity of packet data. In addition, it can also provide integrity protection of IP header fields such as destination IP address. Therefore, all of the above are correct. Let's discuss the architecture of IPSec. Security policy specifies more protection is needed at IP layer. The security mechanisms include a key exchange protocol for negotiating protection parameters, including cryptographic algorithms and keys, and two types of protections, ESP and. ESP stands for Encapsulated Security Payload. ESP can encrypt and authenticate packets. When ESP is applied, the packet data portion, or the payload, is encrypted for confidentiality protection. In addition, message authentication is applied to the encrypted payload and the IPSec header. Now let's do a quiz on ESP. Mark all answers that are correct. ESP can be securely used in encryption only mode, authentication only mode, encryption and authentication mode. All of these are correct. However, although ESP can be used in encryption only and authentication only modes, it is strongly discouraged, because only using the full encryption and authentication mode is secure. Here's a new packet layout when IPSec operates in transport mode and uses ESP. An IPSec header, in this case the ESP header, is inserted after the original IP header. The ESP header includes the security parameter index and a sequence number, and we will discuss these shortly. The ESP header also includes the IV for encryption. The ESP trailer has the padding information, and pointer to next header, such as the TCP or UDP header. The packet payload and the ESP trailer are both encrypted. But the ESP header is not, because it provides information, in particular, the security perimeter index that tells the receiving end how to decrypt the payload. For example, which algorithm and shared secret key to use. The ESP header, and the encrypted payload, are then hashed together with a secret key. And the hash value is stored as the message authentication code for the receiver to verify the authenticity and integrity of the message. If tunnel mode is used, then the ESP header is added after the new IP header. And a packet payload, which now contains the entire original packet plus the ESP trailer, is then encrypted. Therefore, even the original IP header data, including the original source and destination IP addresses, are encrypted. Similarly, the message authentication code is computed over the entire original packet plus the ESP header and trailer. Therefore even the header information of the original IP, for example, the source and destination IP addresses are authenticated. In ESP the IP header is not authenticated. So what if we want to authenticate the entire packet? We can use authentication header or. There are several fields in the IP header. For example, time to live or TTL, that may change in transmission. The values of these fields are not included, or zero out when the message authentication code is computed. Does not encrypt anything, but we can use ESP to encrypt the payload and then apply To authenticate the entire packet. If Is used with transport mode, the authentication code is in the Header, which is inserted after the origin or IP header. Other important IP set information in the Header includes the security parameter index and a sequence number. And again, we will discuss these shortly. If Is used with tunnel mode, the Header is inserted after the new IP header. Now let's do a quiz on ESP and. Label each statement T for true or F for false. First, ESP can provide both confidentiality and integrity protection. Second, if the authentication option of ESP is chosen, message integrity code is computed before encryption. Third, to protect the confidentiality and integrity of the whole original IP packet, we can use ESP with authentication option in tunnel mode. Fourth, in The integrity hash covers the IP header. First, ESP can provide both confidentiality and integrity protection. This is true. Second, if authentication option of ESP is chosen, message integrity code is computed before encryption. This is false, because the message integrity code is computed after encryption. Third, to protect the confidentiality and integrity of the whole original IP packet, we can ESP with authentication option in tunnel mode. This is true, because in tunnel mode the encryption will cover the whole original packet, and the authentication will also covers the original packet. Fourth, in The integrity hash covers the IP header. This is true. We have discussed that for two parties to communicate securely, they typically need to use a security protocol that performs mutual authentication and key exchange for two end hosts or two gateways to use IPsec for secure communications over the Internet. The security protocol is the internet key exchange protocol. This protocol allows the two parties to decide the security policies for the traffic between them. This protocol also allows the two parties to agree on a set of security parameters, for example, which algorithms to use for encryption or hashing. We will discuss shortly how security associations encapsulate these parameters. The protocol also establishes shared keys between the two parties. The security parameters for type of traffic. For example all HTTP connections from host A to B are described in a security association. Security association is asymmetric. For example for TCP connection from A to B, we need one SA for traffic from A to B, and another SA for traffic from B to A. An N host may need many SA's, and it uses an SA database to store them. Each SA has a unique index, and this is the Security Parameter Index or SPI. The SPI is included in an algorithm packet, so that the receiver can use it to look up the SA to unprocess, for example, to decrypt the packet. For example, when A and B agree on the security parameters, both sides will store the same SA to describe these parameters. And a unique index for B's copy of the SA is sent to A, so that A can store this SPI in its SA. Then when A process the packet, it uses the parameters defined in this SA, and also includes this SPI so that P can unprocess the packet correctly. The security perimeters define the security mechanisms, and the determined by the security policies, which are stored in a security policy database To illustrate, an SPD entry describes a security policy, which decides the security parameters which are stored in an SA in SADB. The unique index for the SA of the receiver is the SPI that is encoded in IP set packet header. Let's discuss an example of SPD and SADB. Recall that in transfer mode, the traffic is protected end to end, whereas internal mode, the traffic is protected between the gateway of one network to the gateway of another network. First, let's consider the end to end, or transfer mode policy from A to B. Suppose the policy says that for any traffic from A to B, the packets need to be authenticated. And further, the suggested algorithm is to use HMAC with MD5 as the embedded hash function. This policy is stored as an entry in SPD. The negotiated parameters, again A and B, are store in SA in both A and B's SADB. For A's SADB, it stores a secret key for HMAC and SPI for looking up the SA in B's SADB. Then when A sends out traffic to B it can include this SPI in the IPsec header so that B can use it to look up the SA and un-process the traffic. Now let's take a look at the tunnel mode traffic from the subnet that A belongs to, to the subnet that B belongs to. Suppose the policy says that for any traffic From A's subnet to B's subnet, the tunnel's destination is B's gateway which is D, and the data should be encrypted, therefore ESP should be used, and further, 3DES is requested. Since C is the gateway of the subnet that A belongs to, C's SPD stores this policy. And its SADB stores the SA that has a 3DES key. And SPI, for looking up SA in these SADB. Here's an illustration of the processing of outgoing IPsec traffic. First, the SPD is looked up to see if the traffic, for example http traffic from A to B, needs to be protected, that is whether the traffic should undergo IPsec processing. If there's an SPD entry then the SA is looked up in the SADB and the packet is processed accordingly and SPI is inserted in the IPSec header. For incoming traffic, the SPI in IPsec header is used to look up the SA in SADB. And the packet is unprocessed accordingly. Then the SPD is looked at to make sure that the packet had the proper security measures according to the policy. And only then, the packet is delivered to that player. The IPsec header has an IPsec sequence number designed to prevent replay. It is used only if Is used, or the authentication option in ESP is used. A sliding window of size n, which should be at least 32, is used. That is, although packets may arrive out of order, the sequence numbers should be within the window of size n. More specifically, when a packet arrives, if its sequence number is smaller than the smallest number of the window, it is rejected. If the number is greater than the largest sequence number of the sliding window, then the packet is accepted, and the window it advances to discover this number. If the sequence number falls in the window, it is checked to see if the sequence number have been seen before. If yes, it is also rejected. If not, it is accepted and the number is recorded as having been seen. That's how we reject duplicate packets. Label each statement T for true or F for false. First, the security association specifies a two-way security arrangements between the sender and receiver. Second, SPI is used to help receiver identify the SA to un-process the IPsec packet. Third, if the sequence number in the IPsec header is greater than the largest number of the current anti-replay window the packet is rejected. Fourth, if the sequence number in the IPSec header is smaller than the smallest number of the current anti-replay window the packet is rejected The security association specifies a two-way security arrangement between the sender and the receiver. This is false, because a security association only specifies a one-way arrangement between a sender and a receiver. Second, SPI is used to help receiver identify the SA to un-process the IPSec packet. This is true. Third, if the sequence number in the IPSec header is greater than the largest number of the current anti-replay window the packet is rejected. This is false, because in this case the packet is accepted and the window is at advantage to discover this new sequence number. Fourth, if the sequence number in the IPSec header is smaller than the smallest number of the current anti-replay window, the packet is rejected. This is true. Now let's discuss the internet key exchange protocol. When A and B require IPsec for traffic between them for the first time. We do not yet have an SA. In other words, they have not yet agreed upon the security parameters. Such as inclusion and authentication algorithms and keys. They need to negotiate these parameters and store them in an SA. The Internet Key Exchange Protocol is for this purpose. The protocol works in two phases. The first is to establish an IKE SA. Because the negotiation of an SA should itself be protected. Then this SA is used to protect the negotiations of multiple IPSec SAs. The ISA is bi-directional. That is, it protects the SA negotiation traffic from both sides. Now let's take a look at the Phase I of another protocol. The purpose of this phase is to establish a security association, so that it can be used to establish multiple IP security associations in Phase II. In Phase I, both sides negotiate the protection to be used. For example, Or ESP. And agree on the crypto algorithms to use. For example, AES and HMAC with SHA1. Then they establish a shared secret key. For example, they can use the Diffie-Hellman key exchange protocol to prevent man-in-the-middle attack, or generally to authenticate the shared secret. Both sides can use either a pre-shared key or digital signatures or public-key encryption to authenticate the key exchange. Here's an example of phase one. Both sides have a pre-shared secret key and they use Diffie-Hellman to establish a new shared key, and they use the pre-shared key to authenticate this newly established shared key. Here's how it works. First the initiator sends to the responder the crypto that it proposes to use, along with the cookie. The cookie can be easily computed by the initiator and can be easily verified by responder. For example, this cookie can be computed as a hash over the initiator's IP address and the current time stamp together. The cookie is used to prove that the initiator has done some computation, is serious about following through the protocol. In general, cookies are used to mitigate denial of service attacks where an initiator can send a lot of requests to a responder at no, or little, cost. The responder then sends back its choice of crypto algorithms and its own cookie to the initiator. Second, here YI and YR are the property components of the Diffie-Hellman key exchange, and NI and NR are the Nonce values of the initiator and the responder. Third, both the initiator and responder compute the same shared key according to Diffie-Hellman key exchange. And other keys for the IKSA, fourth they then exchange hash values to authenticate the newly established key using their pre-shared secret key. The hash is computed using the information that they have just exchanged along with the pre-shared key. We will explain this shortly. Let's do a quiz on Diffie-Hellman. The Diffie-Hellman key exchange is restricted to two party communication only. Is the above statement true or false? This statement is false because more than two parties can use the Diffie-Hellman key exchange to establish a shared secret key. Now let's discuss how both the initiator and responder can compute shared keys based on the information that they just exchanged. In our example, the initiator and responder have a pre-shared secret key, and based on the information exchanged between the initiator and responder, they can both compute the following keys using a pseudo-random function. The pseudo-random function can be built using HMAC and SHA-1 to generate a pseudo-random bitstream. Recall that HMAC's SHA-1 takes a message, say a block of data, and a key of length at least 160 bits, and produces a 160 bit hash value. SHA-1 has the property that the change of a single bit of the input produces a new hash value with no apparent connection to the preceeding hash value. This property is the basis for pseudo-random number generation. Both the initiator and responder computer root share secret. This is computed using the pre-shared key and the nonce values that they have exchanged. Next, they compute a key for IPSec SA, this key is computed as follows. They use the root secret as the key for the pseudo-random function and the info data block contains four values. The first one is K, which is a shared secret key computed using the Diffie-Hellman case change protocol. Second, the computer shared key used to derive keys for IPSec SAs. This key is computed using the pseudo-random function on the root secret and the input blog contains four values. The first one, K, is the shared secret key computed using the Diffie-Hellman key exchange protocol. The second and third values are the cookies exchanged between the initiator and the responder, and the fourth value is the number zero. Then in a similar fashion, both the initiator and responder compute the keys for IKE message authentication and encryption. Now let's take a look at how the initiator and responder and authenticate that key exchange. Both the initiator and responder compute a hash value using a pseudorandom function. And the input key is the real secret, which is based on their pre-shared key and the nonce values that they have exchanged. The input block data contains the information that they have exchanged, such as the public components the Diffie-Hellman key exchange, the cookies, the crypto algorithms that they offer, and the identity of the initiator and responder. Each party can verify the hash value computed by the other because the hash values are based on a pre-shared key, and the information they just exchanged. Therefore, these hash values can authenticate both parties identities and the data that they have just exchanged. Phase two induced with setting up IPsec SA which is a one way association. But multiple IPsec SA's can be negotiated with the protection of the same IKE SA established in phase one. The phase two protocol looks very similar to the phase one protocol. The difference is in how the IPSec keys are derived. If there's no perfect forward secrecy is required, then these keys can be derived from one of the shared keys, specifically the SKID-T computed in phase one. The weakness is that, if that key is somehow leaked, then all the IPSec SA keys are also leaked. Stronger security requires perfect forward secrecy. In this case, both sides exchange new nonce values and perform new Diffie-Hellman key exchange. With perfect forward secrecy, each time an IPSec SA is negotiated, its keys are created using the pre-shared key, and the new information that has been exchanged. Such as, the new nounce values and the new pubic components of the [INAUDIBLE] key change. Therefore, unless the pre-shared key which can be considered as the master share secure key is compromised. The keys for the current IPsec SA are secure, even if other keys previously computed have been compromised. To summarize, if host A and host B want to securely communicate, here is the typical IPSec workflow. Suppose this is the first time that A sends data to B, then according to policy requires protection. The router or gateway of A's network and the router of B's network then use the IKE protocol to first negotiate the IKE SA. And then use that IKE SA to negotiate the IPSec SAs. Then an IPSec tunnel can be created between the routers and the traffic from A to B is protected by the tunnel. For example, the packet data can be encrypted and optionally the head of information including the source IP address as well as the packet data can be authenticated. When A terminates the connection to B, the IPSec tunnel between the two routers also terminates. Label each statement T for True of F for False. First, an ISA needs to be established before IPSec SAs can be negotiated. Second, the identity of the responder and receiver and the messages they have exchanged need to be authenticated. Third, with perfect forward secrecy, the IPSec SA keys are based on the IKE shared secret established in Phase I. First, an IKE SA needs to be established before IPSec SAs can be negotiated. This is true, because the purpose of an ISA is to use it to negotiate IPSec SAs. Second, the identity of the responder and receiver and the messages they have exchanged need to be authenticated. This is true. This is the last step of the Phase One protocol. Third, with perfect forward secrecy, the IPsec SA keys are based on the IKE shared secret established in Phase One. This is false. With perfect forward secrecy, the IPsec SA keys are not based on the shared secret keys established in Phase One. So that if the phase one keys are compromised, the IPSec SA keys are not compromised. One of the most widely used security services is the secure socket layer or SSL and the follow on standard known as the transport layer security or TLS. TLS can be provided as part of the underlying protocol suite and therefore, all applications above the transport layer can benefit from the security services provided by TLS. Alternatively, TLS can be imbedded in specific application packages. For example, most browsers come equipped with SSL and most web servers have implemented the protocol. TLS is designed to make use of TCP to provide a reliable end to end secure service. TLS is not a single protocol but rather, two layers of protocols as illustrated in this figure. The record protocol provides basic security services to various higher layer protocols. For example, HTTP can operate on top of TLS. Three higher layer protocols are defined as part of TLS. The Handshake Protocol, the Change Cipher Spec Protocol, and the Alert Protocol. These TLS specific protocols are used in the management of TLS exchanges. Two important TLS concepts are the TLS sessions and the TLS connection. A TLS session is an association between a client and a server that is created by the Handshake Protocol. It defines a set of cryptographic parameters that are used by a set of connections within the session. So that we can avoid repeated expensive negotiation of new security parameters for each new connection. A TLS connection is a transport layer relationship between a client and a server. For example, a TLS connection can be an email connection between a client and a server, or it can be a set of such connections. A TLS connection is transient, for example if the client terminates the email connection, the TLS connection may terminate. Whereas TLS session is much longer term because it is created by a Handshake Protocol rather than a a transport layer service such as email, each TLS connection is part of a TLS session. Therefore, negotiation of new security parameters for each connection can be avoided. The SSL record protocol provides two services for SSL connections. For confidentiality, the handshake protocol defines a share seeker key that is used for symmetric encryption of SSL payloads. For message integrity, the handshake protocol also defines a share seeker key that is used to form a message authentication code or MAC. This figure shows the overall operation of the SSL record protocol. The first step is fragmentation. Each upper layer message is fermented into blocks. Next, conversion is applied. Then the next step is to compute the message authentication code over the compressed data. Next, the compressed message, plus the MAC are encrypted using symmetric encryption. The final step is to prepend a header, which includes the version and length fields. Know that there's no distinction that's made among the various applications that might use the SSL record protocol. The counting of the data created by these applications is opaque to the SSL record protocol. The record protocol transmits the data in a TCP segment. The receiving end decrypts the data, verifies it, decompress it, and reassemble the data and deliver it to the higher layer protocols. Let's take a look at the Handshake Protocol. As we have discussed, the Handshake Protocol establishes a TLS session. And it negotiates the security parameters between the client and the server. The phase one of the protocol, establishes the security capabilities, it is initiated by the client sending a client hello message to the server. The client hello message contains a number of parameters including version number, session ID, crypto suite, compression method, and the initial random numbers. After sending the client a hello message, the client waits for the server hello message, which contains the same kind of parameters as the client hello message. Therefore, at the interface one, both the client and the server know each other's security capabilities. The details of Phase 2 depend on the underlying public key encryption scheme that is being used. In some cases, the server passes a certificate to the client, and possibly, some additional key information and the request for a certificate from the client. The final message has to be server hello done which indicates the end of phase two. In phase three, the server should first verify the certificate of the server. For example if the client is connecting to a server say Georgia Tech's website then the client should be able to verify that the certificate contains the Georgia Tech's public key. That is, the public key certificate is Georgia Tech's public key signed by the public key of the certificate authority. And the client has the private key of the certificate authority to verify the certificate. Then the client can send key exchange information to the server. For example the client can generate a secret key and encrypt the secret key using the server's public key and send it to the server. Depending on the application requirements the client may send a certificate To the server in order to authenticate the client to a server. Usually, if a website is public facing, then the authentication is usually one way, that is, the client needs to authenticate the server. But the server does not require the client to authenticate his self. On the other hand, for internal or private web servers, mutual authentication may be required. In phase two, the client sends a change cipher spec message and copies the pending security parameters to the current cipher spec. It then signals the completion of the handshake protocol. In response, the server sends its own change_cipher_spec. Therefore, they now agree on the security parameters. And then the server sends its own message to signal the end of handshake. At this point, the handshake is complete and the client and server can begin to exchange application layer data protected using the agreed upon security parameters. Now let's do a quiz. Label each statement T for True or F for False. First, most browsers come equipped with SSL and most Web servers have implemented the protocol. Second, Since TLS is for the transport layer, it relies on IPsec which is for the IP layer. Third, In most applications of TLS or SSL, public keys are used for authentication and key exchange. First, most browsers come equipped with SSL and most web servers have implemented the protocol. This is true. Second, since TLS is for the transport layer, it relies on IPSec, which is for the IP layer. This is false. Although transport layer relies on IP layer, TLS does not rely on IPSec. Third, in most applications of TLS or SSL, public keys are used for authentication and key exchange. This is true. IPSec can operate in tunnel mode or transport mode. It uses ESP or To provide security protection. The one-way security association stores the IPSec security parameters. TLS has multiple protocols in two layers. The most important ones are the record protocol and the handshake protocol. Most users now use WiFi enabled devices, such as laptops, to connect to the network. In this lesson, we will first briefly discuss the WiFi security standards. A more recent trend is that more users now using smart phones for important tasks. Therefore, we will also cover iOS security and Android security. Let's first briefly review the WiFi technology. A typical use of WiFi is to allow personal computers or devices enabled with WiFi in a locale such as a home to access the Internet through the access point, or AP. The AP connects to these devices wirelessly. And connects to the Internet through physical wiring. Such as to a router provided by the Internet service provider. Devices in a locale can also connect to each other wirelessly through the AP. In wireless networking, data is not transmitted via physical wiring. Instead data is transmitted in air which is an open medium. In other words, there is no inherent physical protection of communications. Also since there's no hard wiring connecting two devices for direct communications, in wireless networking, communication between two devices is achieved by one device broadcasting and the other device listening to the broadcast. Even the characteristics of WiFi and wireless networking. Now let's do a quiz. Which of the following are security threats to WiFi? Select all that apply. First, eavesdropping. This means attacker listening to communications. Second, injecting bogus messages. Third, replaying previously recorded messages. Fourth, illegitimate access to the network & its services. Fifth, denial of service. Sixth, all the above. The answer is all the above. In fact, all of these threats are also threats to wired networking. And one is networking because there's no inherent physical protection and the nature of broadcast communication. These threats are even more serious. Now, let's discuss WiFi security. The earlier WiFi security standard is called Wired Equivalent Privacy, or WEP. WEP has been shown to be easily pickable even when it is correctly configured. Therefore, you should no longer use WEP. The new standard is 802.11i, and it is implemented as WPA2. WPA stands for WiFi Protected Access II. Now let's take a look at 802.11i. 802.11i enforces access control and the access control protocol is based on another standard called 802.1x. 802.1x is flexible, because it is based on the Extensible Authentication Protocol or EAP. EAP is designed as a carrier protocol and it's purpose is the transport the messages of the real authentication protocols such as TLS. In other words, you can implement a host of different authentication methods on top of EAP. In other words, 802.1x can accommodate a host of different authentication methods. The more and advanced EAP methods such as TLS provide mutual authentication. This means that it will limit man in the middle attacks. Because it authenticate both the server and the client to each other. Furthermore, this EAP method results in key material, which can be used to generate dynamic encryption keys. That means the encryption keys will change dynamically over time. In addition, 802.11i supports good or strong security practices. For example, it uses different keys for encryption versus integrity protection. And it also uses stronger encryption methods. In particular, it uses AES. Now let's do a quiz on WiFi security standards. Which security standard should be used for WiFi? Choose the best answer. Is it WEP? Or WPA2? The answer is WPA2, because without having shown to be easily breakable, even when it is correctly configured, and the new security standard is WPA2. Therefore, you should use WPA2 as the security standard for WiFi. Now let's discuss smartphone security. Here we plot out the sales figures of different devices over the years. It is clear that more people now are using smartphones and they're using them for more and more important tasks. Therefore, it is important for us to understand the security of smartphones. First let's take a look at the security of iOS. The iOS security architecture combines both software and hardware features to provide security to an iOS device such as an iPhone or iPad. It has built in crypto capabilities to support data protection, such as confidentiality and integrity. For example, notice that the crypto engine and the crypto keys are embedded in the hardware. And of course, beta protection is applied to not just the system files but also application data. The security architecture also provides various strong isolation mechanisms, for example, it uses App Sandbox to protect app security. This enables the apps to run securely. For example, apps cannot interfere with each other and also this ensure the integrity of the system. That is, even if an app is compromised, it's damage to the system is very limited. Now let's do a quiz on the vulnerabilities of operating systems. Select three operating systems with the most vulnerabilities in 2014. Is it Max OS X, iOS, Linux, Microsoft Windows Server, Microsoft Windows Vista, Microsoft Windows 7, or Microsoft Windows 8? So the top three operating systems with the most vulnerabilities in 2014 are Mac OS X, iOS, and Linux Kernel. These answers may be surprising because you might have thought that Apple platforms are inherently more secure. However, as Apple platforms, in particular iOS devices, gain market shares, they become a high target for the attackers. Therefore mobile abilities are being discovered and exploited. Now let's take a look at the hardware security support in iOS devices. Each iOS device has a dedicated AES-256 crypto engine built into the direct memory access path between the flash storage and the main system memory. This makes file encryption highly efficient. Recall that AES-256 means that the key length is 256-bit. The device's unique ID or UID, and the device group ID or GID, are AES-256 bit keys fused or compelled into the application processor, a secure enclave during manufacturing. This means that no software or firmware can read them directly. They can see only the results of encryption or decryption operations performed by the dedicated AES engines implemented in silicon using the UID or GID as a key. This is an important feature because the keys are stored securely in the hardware. The UIDs are unique to each device and are not recorded by Apple or its suppliers. The GIDs are common to all processors in a class of devices. For example, all devices using the Apple A8 processor. The GIDs are used for tasks such as delivering system installation and updates. Again, a unique feature of iOS devices is that the UID and GID keys are directly burned in the silicon and they can be only accessed by the Crypto Engine. And the Crypto Engine itself is part of the hardware. The security of a device should be established when the device is turned on initially. IOS uses trusted bootchain to achieve this goal. When an iOS is turned on, each application processor immediately executes code from boot only memory known as the bootrom. This immutable code known as the hardware root of trust, is laid down during chip fabrication and is implicitly trusted. In other words, this code is brunt into the hardware. The bootrom code contains the Apple root CA public key, which is used to verify that the low level loader, LLB is signed by Apple properly before allowing it to load. This is the first step in the chain of trust where each step ensures that the next is signed by Apple properly. When the LLB finishes its tasks, it verifies and runs the next stage below the iBoot. Which in turn verifies and runs the iOS kernel. This secure bootchain helps ensure that the lowest levels of software are not tampered with. And allows iOS to run only on validated Apple devices. In addition to the hardware crypto capabilities built into each IOS device, Apple uses a technology called data protection to further protect data stored in flash memory on the device. Data protection allows the device to respond to common events. Such as incoming phone calls but also enables a high level of encryption for user data. Critical system apps such as messages, mail, calendar, contacts, photos and health data values use data protection by default. And third party apps install on iOS 7 or later receive this protection automatically. Data protection is implemented by constructing and managing a hierarchy of keys. And views on the hardware encryption technologies going to each IOS device. Data protection is controlled on a profile basis by signing each file a class. And access to the file is determined by whether the class keys have been unlocked. Each time a file is created, data protection creates a new 256 bit profile key and gives it to the hardware AS engine which uses the key to encrypt a file as it is written to flash memory and the encryption is sent through ASCBC mode. The profile key is wrapped or encrypted with one or several class keys. And the wrapped per file key is stored in the files metadata. That is, the file key is used to encrypt the file contents and the key itself is encrypted using the class key and the encrypted or wrapped key is stored in the files metadata. The file metadata itself is encrypted using a file system key. Then when a file is opened or accessed this metadata is encrypted using the file system key. This will reveal the route to profile key then the profile key is unwrapped with the class key. The profile key then can be used to decrypt the file as it is read from the flash memory. The metadata of all files in the file system is encrypted using the same random key. We call it the file system key. This key is created when iOS is first installed, or when the device is wiped by the user. The class key is protected with the hardware UID, and for some classes, with the user's passcode. Mark all the answers that are true. First, all cryptographic keys are stored in flash memory. Second, trusted boot can verify the kernel before it is run. Third, all file of an app are encrypted using the same key. First all cryptographic keys are stored in flash memory. This is false because the device's UID and GID are fused into the processors, therefore they're not stored in the flash memory. Second, trusted boot can verify the kernel before it is run. This is true because that's purpose of trusted boot at each stage can verify the next Is signed properly by Apple. Therefore, the kernel can be verified before it is run. Third, all files of an app are encrypted using the same key. This is false because as we have discussed, each file has a profile key to encrypt its contents. Once the iOS kernel has started, it controls which user processes and apps can be run to ensure that all apps come from a known or approved source and have not been tampered with. iOS requires that all executable code be signed using an Apple issued certificate. More specifically, apps provided with the device, such as Mail or Safari, are already signed by Apple. Third party apps must also be validated and signed using an Apple-issued certificate. This mandatory co-signing extends the concept of chain of trust from the iOS to the apps, and prevents third party apps from loading unsigned code, or using self modifying code. A run time co-signature checks of all executable memory pages are being performed as the pages are loaded to ensure that our apps has not been modified since it was installed or last updated. This is done through a user-space daemon and is enforced by the kernel. In summary, iOS enforces mandatory code signing and identification to extend the chain of trust from iOS to the apps. As we have discussed, iOS requires military code signing. This means that, apps that come with the device, such as MAL or Safari, are signed by Apple. And the third party apps must be verified and signed, using an Apple-issued certificate. Now let's take a look at the process by which a third party app developer can obtain the Apple issued certificate. In order to develop apps on iOS devices, developers must register with Apple and join the iOS Developer Program. The real world identity of each developer, whether an individual or a business, is verified by Apple before their certificate is issued. This certificate enables developers to sign apps and submit them to the app store for distribution. As a result, all apps in the app store have been submitted by an identifiable person or organization, serving as a deterrent to the creation of malicious apps. Furthermore, all apps in the app store have been reviewed by Apple, to ensure that they operate as described, and don't obtain obvious bugs or other programs. IOS devices are only allowed to download apps from the official Apple app store. And because of this restricted app distribution model, plus co-signing, it is very difficult to upload malware to the app store and have devices download such malware. However, despite these measures there are still security holes. Let's do a quiz. Choose the best answer. In 2013, researchers were able to bypass Apple's app store security. What method did they use? First, uploaded malware disguised as an app without authorization, bypassing the review and check process. Second, uploaded an app that after it passed the review process morphed into malware. Third, uploaded an app that led users to a site that contained malware. First, upload the malware disguised as an app without authorization bypassing the review and check process. We know that's not possible given the restricted app distribution model by iOS. Second, uploading an app that after it passed the review process morphed into malware. This is possible and, indeed, this is the method that the researcher had used. In the spirit of full disclosure, I was one of the researchers. A link to an article about the research can be found in the instructor's notes. Third, uploading an app that led users to a site that contained malware. This is not the method that the researchers used. In effect, iOS would prevent malware from being downloaded from a site and run on the device. Once an app is verified to be from an approved source, iOS enforces security measures, designed to prevent it from compromising other apps, or the rest of the system. All third party apps are sandboxed, so that they are restricted from accessing files stored by other apps, or from making changes to the device. This prevents apps from gathering or modifying information stored by other apps. Each app has a unique home directory for its files, which is randomly assigned when the app is installed. If a third-party app, needs to access information other than its own, it does so only by using services explicitly provided by our iOS. System files and resources are also excluded from user's apps. The majority of iOS, as well as the third party apps, run as a non-privileged user mobile. The entire iOS petition is mounted as read only. Unnecessary tools, such as remote locking services, are not included in the system software. The iOS APIs, do not allow apps to escalate their own privileges to modify the apps or iOS itself. To summarize, by sandboxing a third party app, each app has its unique directory, and can access its own files, any access to other files will be restricted. iOS also has several other run time security measures. Address space layout randomization or ASLR protects against exploitation of memory corruption bugs. With ASLR, when an app is run, all memory regions are randomized. Randomly arranging the memory addresses of executable code system libraries, stacks, and heaps, and other related programming contracts, reduces the likelihood of many sophisticated exploits. For example, an attempted Lib C attack attempts to trick a device into executing malicious code, by manipulating memory addresses of the stack and system libraries. By randomizing the addresses of these system libraries and stacks, it will be hard for such an attack to succeed, because the addresses of these system libraries and stacks are very hard to guess. Again, any software can have security holes, therefore it is not surprising that despite all the efforts of security engineering, iOS still has security holes. What iOS security weaknesses were exploited by researches in the 2015? First, the malware was uploaded to the Apple App store. Second, the malware was able to bypass Sandbox security. Third, the malware was able to hijack browser extensions and collect passwords. Fourth, all the above. The correct answer is all of the above. The link in the instructor's notes point to a paper on this topic. The researchers were able to upload malware to the Apple Apps store, because the Apple Apps store review process is not bulletproof, which means that a malware can be disguised and pass the review process. And the Sandbox implementation has security bugs that can be exploited by the malware to bypass the Sandbox security. Similarly, browser is a piece of very complex software and also has security bugs. So again, the malware was able to hijack the browser and collect users' passwords. Another runtime security measure that iOS uses is Data Execution Prevention. iOS uses the ARM processor's eXecute Never feature, which marks memory pages as nonexecutable. More specifically, iOS marks pages that are writable in runtime as nonexecutable. For example, memory pages that contain stack will not be executable. On the flip side, memory pages that are executable are marked as nonwriteable. For example, the code pages can be executable, but they're not writeable. Data Execution Prevention is an implementation of the policy that makes writeable and executable mutually exclusive. If a page is writeable, meaning that a runtime, new data or code can be returned on that memory page, then this page is not executable. This prevents code-injection attacks because, to inject code, the attacker must write code into a memory page. But since we make writeable and executable mutually exclusive, after the code can be returned in the memory page, that page can not be executable. Therefore, even if the attacker can write code into memory, he cannot force execution on that page. Therefore, he cannot execute his malicious code. To prevent unauthorized use of a device, a user can use passcode or touch ID. Touch ID is the fingerprint sensing system that makes secure access to the device faster and easier. That is, it provides a degree of convenience. By setting up a device passcode, the user automatically enables data protection. iOS supports four digit and arbitrary length alphanumeric passcodes. To further discourage brute force passcode attacks, the iOS interface enforces escalating time delays after the entry of an invalid passcode at the last screen. Users can choose to have the device automatically waived if the passcode is entered incorrectly after ten consecutive attempts. Mark all answers there correct. First, each app runs in a sandbox and has its own home directory for its files. Second, all iOS apps must be reviewed and approved by Apple. Third, iOS apps can be self-signed by app developers First, each app runs in a sandbox and has its own home directory for its files. This is true. Second, all iOS apps must be reviewed and approved by Apple. This is true. Third, iOS apps can be self-signed by app developers. This is false. Self signing means that the app developers can issue its own certificate, and use the public key in a certificate to sign its own apps, and this is clearly false. Now let's take a look at Android security. Here's an overview of the architecture of Android which is based on Linux. Android is implemented in the form of a software stack architecture consisting of a Linux kernel, a runtime environment and corresponding libraries, an application framework and a set of applications. At the lowest level is the Linux Kernel. It provides a level of abstraction within device hardware and the upper layers of the Android software stack. Each application running on the Android device runs its own instance of the Dalvik virtual machine. After written in Java, and compiled to bytecode of the Java virtual machine, which is then translated to Dalvik bytecode. The Dalvik excutable format is very compact and it is designed for systems such as smart phones that are constrained in terms of memory and processor speed. By using Dalvik, Android can achieve performance and efficiency. The Android core libraries are Java based libraries for application development. For example, for web browsing, data access and database queries, graphics rendering, and so on. The Android call libraries do not actually perform much of the actual work. And are in fact, essentially Java wrappers around a set of C and C++ based libraries. These C and C++ libraries are included to fulfill a wide range of functions, including 2D or 3D graphics rendering, secure second layer, etc. The application framework is a set of services that collectively, from the environment, for Android apps to run. The application framework allows apps to be constructed using usable, interchangeable, and replaceable components. Furthermore, an app can publish its capabilities along with any corresponding data, so that they can be found and reused by other apps. At the top of the Android software stack are the apps. These include apps that come with the device such as home, contacts, and phone and browser. A lot of third party apps that the user has downloaded after he purchases the device. Running apps in virtual machines means that they're essentially sandboxed in runtime. This means that the apps cannot directly interfere with the operating system and other apps. Nor can they directly access the device hardware. Each app is granted a set of permissions at install time, and cannot perform operations that require permissions it does not have. More specifically, the Android platform takes advantage of the Linux user-based protection as a means of identifying and isolating application resources. The Android system assigns a unique user ID, or UID, to each Android app and runs it as that user in a separate process. The kernel enforces security between apps and the system at the process level through standard Linux facilities such as user and group IDs that are assigned to the apps. By default, apps cannot interact with each other and apps have limited access to the operating system. If app A tries to do something malicious like reading app B's data or downloading the form without permission, then the operating system protects against this, because app A does not have the appropriate user privileges. This sandbox model is simple, but on the other hand, it is based on decades of Unix style user separation of processes and file permissions. An app can announce the permissions that it needs, and user approval is required at install time. The permissions are typically, back up. The permissions are typically implemented by mapping them to Linux groups that have the necessary read-write access to relevant system resources, such as files and sockets. And therefore, they are ultimately enforced by the Linux kernel. The Android Sandbox and the iOS Sandbox have different implementations. But from a security point of view, one of the main differences is how they handle permissions. For Android, an app can announce the permissions they requires and the users can approve these permissions and install time. The apps can ask for very powerful permissions. For iOS, all the apps have the same set of basic permissions. If an app needs to access system resources or data, such as the users address book, user approval is required at the first time. In general, iOS apps have only limited permissions. Another major difference between Android security and iOS security is code signing. Android takes a very different approach from iOS. In particular, all apps are self-signed by developers. A developer can create his own power key, self sign it to create a certificate for its public key and then use that key to sign his apps. In other words, third party Android apps are not signed by a central authority. There's no vetting process for third party app developers. This means that anybody can become an Android app developer, self-sign its apps and upload it to Android Play Store. So, if code signing is not used to make sure that apps are from the vetted or verified developer, what is it used for? Code signing is used for making sure updates for an app are coming from the same developer. In addition, code signing is used to manage the trust relationship between apps, so that they can share code and data. Mark all the answers that are true. First, Android apps can be self signed. Second, Android apps can have more powerful permissions than iOS apps. First, Android apps can be self-signed. This is correct. Second, Android apps can have more powerful permissions than iOS apps. This is also true. For wi-fi security, we should use the WPA2 standard. iOS has these cryptographic keys and modules built into its device hardware. It uses mandatory code signing and very restricted app distribution model. It runs an app in a sandbox with run-time protection such as ASLR and DEP. Android is based on Linux and sandbox is based on Unix style user separation. These apps are self signed. The web is an extension of our computing environment, because most of our daily tasks involve interaction with the web. In this lesson, we'll first reveal how the web works and discuss its major threat vectors. We will then discuss several attacks, including Cross Site Scripting, Cross Site Request Forgery and SQL Injection. Let's first briefly review how the web works. The web browser and the web server communicate using the Hypertext Transfer Protocol or HTTP. The browser requests documents through a URL. The server responds with document in hypertext markup language, or HTML. Roots can include, not just the text, but also graphics, video, audio, Postscript, JavaScript, etc. The browser displays HTML documents and embedded graphics. It can run JavaScript and other helper applications. HTTP is a stateless protocol, each request is its own TCP connection. For example, if you log into your bank's website, each click on a URL generates a separate TCP connection. In order to carry information across multiple HTTP requests such as user authentication, cookies are used. A cookie is created by the web server when the user first logs into the site. It contains not only user identity information, but also security information such as access, expiration time, and if SSL is required. The user's browser stores the cookie and includes it in subsequent requests so that the server knows that these new requests are related. For example, they belong to the same user login session. Now let's do a quiz on cookies. Which of the following statements are true? First, cookies are created by ads that run on websites. Second, cookies are created by websites a user is visiting. Third, cookies are compiled pieces of code. Four, cookies can be used as a form of virus. Fourth, cookies can be used as a form of spyware. Sixth, all of the above. First, cookies are created by ads that run on websites. This is true, cookies are created by ads, widgets, and elements on the web page the user is visiting. Second, cookies are created by websites a user is visiting. This is true, as we have just explained. Third, cookies are compiled pieces of code. This is false, cookies are plain text. They're not compiled code. Fourth, cookies can be used as a form of virus. This is false, since cookies are not compiled code, they cannot be used as a virus. They cannot replicate themselves. And they cannot be executed and are not self-executing. Fifth, cookies can be used as a form of spyware. This is true, cookies store user preferences and browsing history, and therefore they can be used as spyware. Now let's look at the main web security issues. The browser accepts contents and often runs dynamic contents such as scripts from websites. The question is, can a browser trust these contents. In some cases, the browser can authenticate the website, but in many cases, authentication is not required. But even if a website is authenticated, the contents that it sends may not be trustworthy because the website may have security vulnerabilities that allow attackers to inject malicious contents that get passed to the browser. Or the website includes contents or links to other websites which may also have security vulnerabilities. On the server side a website runs applications that process requests from browsers and often interacts with back-end servers to produce content for users. These web applications, like any software, may have security vulnerabilities. Furthermore, many websites do not authenticate users. That is, attackers are not prevented from sending requests designed to exploit the security vulnerabilities in these web applications. Mark each statement as true or false. First, web browser can be attacked by any web site that it visits. Second, even if a browser is compromised, the rest of the computer is still secure. Third, web servers can be compromised because of exploits on web application. First, web server can be attacked by any web site that it visits. This is true. As we have discussed, we can not authenticate all websites, and even if a website is authenticated, it may still have vulnerabilities. Second, even if a browser is compromised, the rest of the computer is still secure. This is false, because if a browser is compromised, it can lead to malware installation on the computer. Third, web servers can be compromised because of exploits on web applications. This is true. The security vulnerabilities of web applications can lead to attacks that deface websites or the backend servers can be compromised as well. For example, credit card information can be stolen from the backend servers. Now let's look at several attacks on the web. The first is the cross-site scripting attack. Many websites allow user to input data and then simply display or echo data back. The website include the user input data in the HTML page to the user's browser, such websites include social networking sites, blogs, etc. But what if the users are attackers, and the user input is malicious? Let's use an example to illustrate. Suppose a website echoes the user-supplied data, such as name, back to the user. Suppose user Joe visits a website, and he supplies his name, Joe, to the website, then the website will send back a page saying, hello Joe. Now, suppose, instead of sending the user's name, Joe, to the website, the browser sends a script as his name. What's going to happen? The website will take this script as the user's name and echo it back to the web browser. That is the script will be included in the HTML page sent to the browser. Therefore, when the browser displays this webpage, the script will run, the webpage will display Hello World. Now, this is a benign case. Now what if the script is malicious? When the browser gets the HTML page from the web server, it would just execute the script. So if the script is malicious, then the malicious script will be executed by the browser. But why would the browser send a malicious script to the website without a user knowing about it? How can this happen? This can happen in a cross site scripting attack. Here's an example. The user has logged into a vulnerable site, naive.com. And his browser now stores the cookie from naive.com. The user is then phished and clicks a URL to visit evil.com. Evil.com returns a page that has a hidden frame that forces the browser to visit naive.com and invoke hello.cgi web application on naive.com with a malicious script as the name of the user. When the user's browser displays the HTML page from evil.com, he will be forced to visit naive.com and call hello.cgi with this malicious script as the user's name. Hello.cgi at naive.com then echoes the malicious script in the HTML page that is sent back to the user's browser. The user's browser displays the HTML page and executes the malicious script. The result is that the cookie to naive.com is stolen and sent to the attacker. You may ask, so what? If evil.com gets the cookie to naive.com? Because that cookie can include session authentication information for naive.com. Therefore, by stealing the cookie the attacker can now impersonate the user. Mark each statement as true or false. First, when a user's browser visits a compromised or malicious site, a malicious script is returned. Second, to prevent cross site scripting, any user input must be checked and preprocessed before it is used inside html. First, when a user's browser visits a compromised or malicious site, a malicious script is returned. This is true because this a required step in the cross-site scripting attack. Second, to prevent cross-site scripting, any user input must be checked and preprocesseed before it used inside HTML. This is true. For example, the website can check that the name of a user should not be a script. Cross-Site Request Forgery is another web-based attack. A user's browser may be running a script from a good site and also malicious script from a bad site. This can happen when the user has logged into the good site and kept the session alive. For example, the user has logged into Gmail and has not logged off. Meanwhile, the user maybe browsing other sites include the bad site that sends malicious script to the browser. The malicious script can then forge a request to the good side using the users cookie. The good side does not know that the request was not sent by the user, here's an illustration. The user logs in, and establishes a session with a good site, and keeps the session alive. Meanwhile, the user browses a bad site. For example, because he's phished. And the browser runs a malicious script from the bad site. The malicious script then sends forged requests to the good site. Here's a realistic example. A user logs into bank.com and forgets to sign off. The session cookie remains in browser. The user then visits a malicious website which sends an HTML page that contains a hidden I frame that includes this malicious content, that is when the user's browser displays the HTML page, actions will be performed on the bill payment form of bank.com as if users are entering these values. Because the I frame is invisible the user knows nothing about it. The browser will send a request on behalf of the user and without his consent or knowledge. And here is the illustration of this example. The user logs into bank.com and keeps a session alive. So the browser has the cookie to bank.com. Meanwhile, the user is phished to visit attacker.com. And in return, the web browser gets this malicious content. When this malicious page is displayed on the user's browser, the browser will make the request on the user's behalf to bank.com. And since the user is still logged into bank.com, the user's cookie is also sent to bank.com along with the request. And so the bank website believes that the request is from the user, and therefore, the payment request is fulfilled. Now let's compare cross-site request forgery and cross-site scripting. In cross-site scripting, the user's browser trusts a badly implemented website that does not check input content. That is, the attacker is able to inject a script into the trusted website and as a result, the user's browser executes the attacker's script. In cross-site request forgery a website trusts that requests are from the user. But in fact, the attacker forges user requests to the website. As a result, the website executes the attackers malicious actions. Both cross-site scripting and cross-site request forgery are the results of security weakness of websites. In particular, the lack of authenticating and validating user input. Which of the following methods can be used to prevent XSRF forgery? First, checking the http referer header to see if the requests comes from an authorized page. Second, use synchronized token pattern, where a token for each request is embedded by the web application in all HTML forms and verified on a server site. Third, log off immediately after using a web application. Fourth, do not allow browser to save username/password, and do not allow web sites to remember user login. Fifth, do not use the same browser to access sensitive web sites and to surf the web freely. Sixth, all of the above. The correct answer is all the above. For example, by checking the HTTP referer header, bank.com would notice that attacker.com is the referer, and then bank.com can stop the payment requests. Similarly, bank.com can generate a token and embed it with bill pay form that is sent to the user's browser, so that when it receives request from the browser it can verify that the request are from that bill pay form. In addition, these practices will prevent a malicious script from using the live session to good site. Before we discuss SQL injection attacks, let's quickly review SQL. SQL is the most widely used database query language. We can use it to retrieve database records, modify the database, for example by adding records to a table or modifying the specific values of a record. Nowadays, many databases have a web front end to allow users to create a database using the web. The website typically offers a web form for entering the query and runs a program to form the input into SQL query and send it to the backend database server. For example, the web application can be a PHP and it takes user input and forms a SQL query. The security threat here is that the input may be malicious that is the SQL to the database server is malicious and can lead to compromise of data confidentiality and integrity. Here's an example of a web form. A user enters his name and password. The password is sent in encrypted hash form to the web server, and the web server needs to retrieve the user's password hash from the database and compare and authenticate the user. The web server issues a SQL query to the database, so this is the example of normal usage. Suppose an attacker enters this malicious string as the user name. What's going to happen? The actual SQL query sent from the web server to the backup database server would look like this, and the result of running this query to database is to delete all user records. Now let's do a quiz on SQL injection attack. Which of the following is the better way to prevent SQL injection? Is it using blacklisting to filter out bad input or using whitelisting to allow only well-defined set of safe values? The correct answer is use whitelisting to allow only well-defined set of safe values. Blacklisting is very hard to implement, because there can be many, many possible ways to inject malicious strings. That is, it's very hard to have a complete blacklist. Both the web browsers and web service are vulnerable, because they need to execute or generate dynamic content based on user input, which can be malicious. In cross-site scripting, the attacker injects a script into a website, and the user's browser executes this script. In cross-site request forgery, the attacker tricks the user's browser into issuing a request. And the website executes this request. In SQL injection, the attacker injects malicious query actions. And a website's backend database server executes this malicious query. Modern day malware are tools for attackers to achieve financial and political gains. They have advanced capabilities. For example, they have robust command control infrastructures, and the abilities to evade analysis and detection. In this lesson, we're going to cover botnets and APTs and introduce malware analysis and detection techniques. Recall that malware is a malicious program. It can spread by various means from one computer to another. And it can perform a wide variety of malicious activities. Up until around early 2000, although some malware did some damage, such as defacing web pages, and some were launching large scale denial of service attacks, The majority of the malware were designed for experimenting or demonstrating some capabilities. For example, how fast or how large-scale a malware or worm can spread. Therefore, in the era, the malware was mostly for fun and fame. Since around 2005 malware has been used to control compromised computers and networks to perform malicious activities that often lead to elicit financial and even geopolitical gains. In other words, whereas computers in the past were merely targets of malware for fun, they are now resources that malware can take control of for profits and gains. Since modern malware are now used to do real work to achieve profits and gains, they tend to be technically sophisticated and utilize the latest technologies. For example, they have utilized popular peer to peer protocols and applications to set up communications. And they've used cloud computer servers to support their activities, and they have used the latest crypto algorithms to perform authentication and encryption to protect their communications from subversion and analysis. Botnet is the most prevalent modern-day malware, and most internet-based cyber attacks frauds are due to botnets. When we talk about botnet, actually we need to understand two aspects. One is bot, the other one is net. So let's first talk about bot. A bot is also called a zombie. It is essentially a compromised computer under the control of an attacker. When a computer is compromised, typically the bot code or malware is loaded on a computer and the bot code is responsible for communicating with the attacker's server. And carries out activities per the attacker's instruction. Then a botnet is a network of bots controlled by an attacker to perform coordinated malicious activities. That is, with a network of bots, the aggregated combination of power can be very large. In other words, the attacker now controls a very powerful computing platform. And with such a powerful platform, the attacker can launch a wide variety of malicious activities. And that's why botnets are now the key platform for most of the Internet-based attacks and frauds. I'm giving you three types of bot activities, as well as their definitions. Select the correct definition for each bot activity by typing the letter A, B, or C in the box next to each. So the first type of bot activity, Spamming. What is a spamming bot? Is it A, used by botmaster to fraudulently increase revenue from advertisers? Or used to gather valuable financial information? Or infect machines to send out unsolicited emails. It is obviously C. The second type of bot activities, Click Fraud. What is a Click Fraud bot? Is it A, used by botmasters to fraudulently increase revenue from advertisers? Yes, a Click Fraud bot can click on ads specified by the botmaster, as if the clicks were from the real human users. Therefore, this can fraudulently increase revenue from the advertisers. So this is A. The third one, Phishing, what is a Phishing bot? A phishing bot is typically used to gather valuable financial information from the user. For example, a bot net running a phishing campaign can trick a user to enter personal information or financial data into a web server that's controlled by the bot master because the users were tricked to believe that the server was a bank server. Regardless of the method used, modern botnets usually have one or two goals in mind, illegal profits or political activism. Here are some examples of attacks and frauds by botnets. Spam, almost all spams are sent by botnets. All Distributed Denial of Service Attacks or DDOS are now launched by botnets. Clickfrauds, phishing and pharming, key logging. Almost all bot malware is loaded via key logging capabilities. Password cracking, using the aggregated computational of power of a botnet to crack password. Use of botnet to run anonymization protocols and route traffic in order to hide communications by criminals and terrorists. Use the bots in a botnet to coordinate, in order to cheat in online games and polls. And the list goes on. Let's discuss a typical DDOS attack scenario by botnets. First, the attacker decides the victim and when the victim machine should be attacked. In order to launch a DDOS attack, the attacker must be able to commandeer a botnet. Here, we assume that the bots already have the bot code, and the bot code communicates with the attacker regularly. To start the attack, the attacker sends the command to all the bots in the botnet. For example, the command would tell the bots to send connection request to the victim at once. The result is that the victim gets connection requests from many, many bots at the same time. Therefore, the victim is being overloaded and the effect of denial of service is accomplished. A typical defense against DDoS, is for a company to buy more servers or bandwidth. However, DDoS attacks can be amplified to make such defense very, very expensive. Here's one example. On the internet, there are many so called open recursive DNS servers. These are DNS servers that any machine on the internet can query. A typical DNS query is to look up the IP address of a domain name. The DNS also allows query for the so called large TXT record of a domain that includes a lot of details of the domain. Even including organization note, description, information and so on. The point is that the size of the query response can be more than 1,500 bytes. So, here's how this amplified distributed reflective attack works. The attacker instructs the bots to query the open recursive DNS servers on the internet. And in the query, the source of the query is spoofed as victim's IP. Meaning that the response will be sent back to the victim instead of to the bot. And the query itself is for the Large TXT record. Because the query is for the Large TXT record, the response will be at least 1,500 bytes. And again, because the query is spoofed, as from the victim, the response will be sent to the victim. Because there are many bots, and each bot can query multiple recursive DNS servers. The victim as a result will get many many responses each with at least 1500 bytes. Therefore with just a couple thousand bots participating in the attack, the victim can get several gigabytes of traffic. Lets do a quiz on DDos. Put a check next to each statement if that's a true statement. The first statement. The attacker does not have to use his own computer in the attack. As we just illustrated, the attacker used botnets to launch such attack. Second statement. Since there are so many computers involved in the attack, it is difficult to distinguish legitimate from malicious traffic. This is true. A victim, say, for example, a web server, receives traffic from computers all over the internet, and it is difficult to distinguish legitimate connection requests from malicious ones. The third statement. The characteristics of DNS servers help mitigate the effect of DDOS attacks. And this is false. Because DNS servers can send a lot more traffic to a web server than an individual computer. Now let's discuss botnet command and control. This is the most important aspect of botnets. Recall that the definition of botnet is that a botnet is a network of compromised computers that the botmaster uses for malicious purposes. Here, by botmaster, we mean the attacker behind the botnet. Because the botmaster needs to make use of the network of bots, he needs to communicate with the bots. In other words, there needs to be command control, or C&C for short, from the botmaster to the bots. For example the bot should report to the botmaster its current status and the bot may be directed by the botmaster to another site to download the latest malware version. And the bot may receive instructions for spamming, phishing and DDoS, and so on, from the botmaster. In short, we can see that without C&C, a botnet is not a network, but just a collection of isolated infected machines. And as a result, the botmaster cannot utilize the aggregate competition of power of all the bots available. Let's study the botnet C&C problem. The key question here is that how can the bot master know which computers have been infected? In other words, how can the bot master contact the infected machines and use them? Let's think like a bot master for a moment. The easiest way to think of, is to have the victims or the compromised computers to contact us. So, okay, let's do this. The first step is to create the malware. We call it vx. So for example we can download some example vx code from the web, fiddle around it, compile it, okay? To make sure that it works. Then, we can use email to publicate it. For example, using social engineering to send it to some people that we know, and from there it will spread. That is, we create a malware, we spread it through email. Spreading is the easy part, but now what if we want to make use of these victim computers? As we said, our plan is to have the victims contact us. That is, after we spread a malware, each victim computer, or compromised computer, will somehow contact us. For example, we may hardcode in the malware some email address that we use or even our IP address and so on. The first problem with such approach of hardcoding our content information in the malware so that the victims can contact us, is that it is not stealthy. It is a safe bet, that eventually, security admins will find out that there are bots on their network and they may be able to obtain the botcode or the malware. And through malware analysis, they may be able to recover our hardcoded address. >From there, they can identify us, so this is not stealthy at all. The second problem is that, this is not robust at all, because there's only one single server for command control. That's the one that's hardcoded in the malware. For example, if we have hardcoded our email address in the malware, and the email address account has been banned, then there is no command control available for victims anymore. So this is not robust. It should be clear now that the first question to consider, in terms of, Botnet C&C Design is, how can the bots contact their master safely? In other words, in a stealthy and robust way. And as we have just discussed, a naive approach is not adequate. For example, if we have hard-coded the malware to include a single IP address of our command control server, then when this IP address has been identified by security admins. The network admins such as admins in the ISP can block all traffic to this server. Therefore, no command control traffic for the bots can be reached to the file master's server. And as a result, the whole botnet's intercepted. For example, if all the bots come to a single IP address that's hard-coded in the bot malware, then when this IP address is identified by secured admins, the network admins, for example those at ISP, can block all traffic to this IP. Therefore, blocking all command control traffic and disrupting the operations of the botnet. And because such naive approach is not stealthy nor robust, it is only used by script-kiddies and first-time malware authors. In addition to safety, utility is also very important to the Botmaster. In other words, a Botmaster wants efficient and reliable communications, so that he can reach and coordinate enough bots to perform a specific task. So to summarize the design considerations of Botnet C&C, the Botmaster would want efficient and reliable communication. That is the ability to reach a sizable set of bots within certain time limit. Stealthy communication, so that it is hard to detect C&C traffic. Resilient or robust communication, so that it is very hard to disable or block C&C traffic. Now let's do a quiz. Determine if each statement is true or false. The first statement, bots have more sophisticated communication capabilities, than worms and viruses. This statement is true. In fact, the ability to perform C&C, or command control, is how we distinguish bots from worms and viruses. The second statement, bots require direct communication with the C&C server, before beginning an attack. This actually is false, because bots may include conditions such as those based on a specific time to launch an attack, without the need to communicate with a C&C server. The third statement, a botnet will be less likely to be found if it uses custom communication protocols. This is false. A custom communication protocol means that it is a unique protocol and therefore it can stand out from normal traffic quite easily. That is, if a botnet uses a custom communication protocol, it is actually more likely to be detected. Many botnets use DNS for C&C. A key advantage is that DNS is used whenever a machine on the internet needs to talk to another machine, because DNS stores the mapping between domain name and IP address. That is DNS is always allowed in a network. And therefore, using DNS for C&C won't stand out easily. In this example here, the Botmaster releases malware and in the malware, the domain name of the C&C servers are coded. Have the malware spreads, then the question is, how do the bots communicate back to the Botmaster? In other words, how do they perform C&C with the C&C server? To perform C&C, each bot is instructed to communicate to the C&C server. As we said, the C&C server is identified by the domain name. So the first thing the bot will do is ask the DNS server, hey, what's the IP address for hacker.com? And the DNS server will tell the bot, here is the IP address. And then, the bots would actually communicate to the C&C server. And that's how Botnet C&C works through DNS. The kind of DNS service providers preferred by the bot masses are the dynamic DNS providers, because they allow the frequent changes of the mapping between domain name and IP address. That is the Botmaster can change to use another machine. User 10.0.0.1 for example, you can change it to 10.0.6.2 for example. It's a C&C server for the same botnet, and all he needs to do is log into the dynamic DNS provider and change the map ping, so that now hackers.com points to the new IP address. If we can detect that hackers.com is a useful botnet C&C, then we can detect that any machine that connects to it is a bot. But the question is, how do we know that hackers.com is useful botnet C&C? It turns out that the way that bots look up a domain would suggest that this domain is most likely used for botnet C&C. That is, the way that bots look up a domain would be different from a machine that looks up a web server because of legitimate use activities. Such as, when a user is looking up a news website. For example, if a domain such as hackers.com is being looked up by hundreds of thousands of machines all over the internet and yet this domain is unknown to Google Search. This is an anomaly. Therefore, we can use anomaly detection at the Dynamic DNS provider to examine queries to DNS domains in order to identify Botnet C&C domains. Once we identify that hackers.com is used for Botnet C&C, there are a number of responses available. One option is for a Dynamic DNS provider to send a mapping of hackers.com to the IP address of a sinkhole. That is, instead of giving the IP address of the C&C machine to the bots, the dynamic DNS provider would give the IP address of the sinkhole to the bots. Therefore, instead of connecting to the actual C&C server, all the bots will be connecting to a sinkhole machine. As a result, the Botmaster will not be able to command his bots. Because when he logs to the C&C server and looks through the logs, he will not be able to find the bots. Because all the bots are instead, communicating with the sinkhole machine. With the sinkhole, in addition to disabling the botnet operations, security researchers can learn where the bots are. For example, by looking at each connection from the bot to the sinkhole, security researchers would know the IP address of each bot, therefore it would know where the bots are throughout the Internet. Which of the following C&C schemes provide communication that is efficient, stealth and resilient? Here, we can assume that we can only observe network traffic, and cannot examine the botcode, or the malware yet. So the first game, a Gmail account is used for C&C, email address is hardcoded in botcode. This is not hard to detect. In particular, with system and send Gmail, or even your local network, they may be able to notice that an email account is receiving short emails for C&C from all over the internet. And it is easy for them to disable the Gmail account. The second scheme, a P2P protocol is used for C & C, and the query string is hardcoded in botcode. This is not hard to detect. In particular, in enterprise network where P2P traffic is typically not allowed. That is a C&C based on P2P protocol, can easily stand out in enterprise network traffic. In addition, if the hardcoded query string is identified, then it is easy to disrupt the P2P communication, as well. The third scheme, a news website has been set up for C&C. For example, commands can be parsed from news articles. The website and the parsing logic have to be hardcoded in the botcode. This can be quite hard to detect, because reading news would appear to be a normal traffic. On the other hand, if this is ever detected, then disrupting it will not be hard, because the website can be blocked. Therefore, in conclusion, none of these schemes can satisfy all of these requirements. The latest modern malware is APT, or advanced persistent threat. They tend to target specific organizations, while as botnets tend to have bots all over the internet. APT is advanced, which means that is uses special malware. Sometimes, the malware is specifically crafted for the targeted organization, but very often the malware's actually a special version of a common malware that has to be used in other botnets. By starting from a common malware, the attackers behind APT's want to have both convenience and deniability. That is, security admins would not suspect that the malware is APT. They would think that this is a new version of the common malware. Advance can also mean special operations. That is, APT is not used for common attacks and frauds, such as spamming and click fraud or phishing. Rather, it can be used for very high value operations, such as stealing the design of a new airplane. APT's persistent, meaning that once the malware gets into an organization, it's going to stay there for a long time. And it typically involves multiple steps. Each step is low and slow, to avoid detection. For example, one of them sending out the design of the airplane, which is a large volume of data to the internet at once. The APT can break the data into multiple chunks. And then it can sent each chunk to the internet, whenever there is a user connecting to the internet, that is, the transmission of each chunk of data will not raise any suspicion. APT is a very dangerous threat because, as we have discussed, APTs tend to target high value organizations and information. Let's take a look at the life cycle of APT. It starts by identifying the attack target. For example, the target can be a automobile or aerospace company. Then, the attacker will research about the target, in particular, obtaining information about a target organization, that can help the attack. For example, using network scanning and analysis, the attacker can learn the target organization's network, and therefore, the vulnerabilities of his network services. Such as, the possible security flaws of it's web servers. And by researching the web, the attacker can also learn the organization's C level executives and their email addresses. For example, many companies list their C level executives on their webpages. And once you know their names, it is not hard to find their email addresses by doing some web search. Then, the attacker code can develop a method to penetrate the organization's network. He can make sure that the attack will go through, without detection, by utilizing the knowledge he has learned about the organization. For example, the attacker can exploit a new vulnerability of the organization's web server. And since, the vulnerability is unknown to the security community, the network defenses at the organization won't be able to detect this exploit. Or, the attacker can send a so called deficit email to the chief marketing officer of the organization. It is called, spear fishing, because the attacker has spent time learning about the organization and the individual. And so, the email would appear to be legitimate. Once the the exploit succeeds, now the APT has a foothold in the organization. It can then communicate back to the attacker, for example, it can accept new malware updates, receive commands, et cetera. For example, it can now exfiltrate valuable data, such as, the design of a new airplane. The APT will try not to raise any suspicion, so that it can go undetected. For example, he wouldonly exfiltrate data from the server only, only when there are other legitimate network connections, from that server, to the Internet. And it does not operate, on any other server, if that server is not useful to the current task. That is, he would keep his footprint, as small as possible, whereas, the previous generations of malware, including bottom worms, would try to infect as many machines as possible. This cycle repeats and continues. For example, the attacker, based on information sent from the APT inside the organization, now learns that there is another part of the organization that also holds even more valuable data. The attacker now identifies that part of the organization as a new target, and this cycle continues. The most dangerous and advanced APTs are those that use a zero-day exploit or a specially crafted malware. A zero-day exploits a previously unknown vulnerability and therefore there's no patch or fix for the vulnerable system. And most likely there's no prevention or detection signature for the attack. That is, a zero-day exploit will succeed and go undetected. Likewise, a specially crafted malware is usually designed to defeat the signatures and behavior patterns in a detection system. That is, it also has a very high chance to succeed and go undetected. APTs are characterized by its ability to not only evade anti-malware and network intrusion detection systems, but also fool even the most sophisticated users. For example, an APT can be designed to first compromise an internal network router or server and all it does at that point is to learn about the valuable individuals. That is, the APT will first learn who's e-mailing to whom, on what topic and with what kind of attachments in the e-mail. With such knowledge, the APT can then forge email from a person to another. And this email can appear to be very legitimate. This is so-called, Spear Phishing because it is targeted to a specific individual. And if the internal network, router, or server has been compromised, the APT can play man-in-the-middle to make such social engineering attacks very successful and very convincing. For example, if the email recipient is not certain about the email attachment, and he sends an email to the sender and asks hey did you send this? The APT can pay the man in the middle to intercept inquiry and send answer back to the user to say yes I sent this. APTs are also designed to plan in with normal activities to avoid detection, even if that means it has to go low-and-slow. For example, if an APT is designed to forge a document, it will not do so, unless the user is also authoring another document. Similarly if the APT is designed to send our document it will not do so unless the users also sending our document. As another example if the APT is designed to change the setting off a controller in a nuclear plant, the APT does not make the changes all at once. Instead, it will make incremental small changes over time to accomplish the eventual attack goal. For example, the famous Stuxnet falls into this example, and you can search and learn more about it. And because APT activities are designed to blend in with normal activities and go low-and-slow, it is very hard for an anomaly detection system to catch an APT. APTs are often designed to stay in the compromised organization for long time. For example, always looking for more valuable data to steal. For example, an APT can include a number of steps, and at each step you only focus on a subset of the users, and only part of the network. And at each step, he may actually use a different version of the malware. And when it moves from one part of the network to another, it may clean itself from the O port. That is, at any given it's only present at the specific target. In other words, it's footprint remains to be very small. Here you match the APT activity with its correct description. Here we have five APT activities. Boy in the middle. Clickjacking. Man in the browser. Man in the middle. Keyloggers. And the descriptions are eavesdrops. Modifies web pages covertly. Covertly recording keystrokes. Covertly changes a computer's network routing. Web servers unknowingly click on something that is not as it is portrayed. So let's match the activity with the description. First one, boy in the middle. This means that covertly changing a computer's network routing. That's D. Clickjacking, it means that it fool the user into clicking something that is not as it is portrayed. So that's E. Man in the browser. It means that the APT can modify the web pages covertly. For example, when the user enters data in a web form, the APT can change the data without the user knowing about it. So that's B. Man in the middle. This means that the APT can intercept traffic and listen in on conversations, so that's eavesdrops, A. Keyloggers, this means that the APT can covertly record keystrokes by the user. So that's C. Although this example may appear to be abstract, it is actually quite realistic because it is similar to many real world APT instances. In this example, the CEO gets an email with a PDF attachment showing pie charts of sales activities. When he opens the PDF document inside browser, for example, so that he can use the Acrobat Reader plug in to view the document, the crafted attack data embedded in the PDF document would cause a zero exploit that broke out the plug in sand box in a browser and compromised the browser. In a detail of such attack can be for example a buffer overflow attack that lead to control flow hijacking and taking over the browser. Then the injected attack code will download a malicious browser extension. Record that a malicious browser extension is the malware embedded within the browser. >From this point on, each time the CEO send an email to his employees with an attachment, for example a PDF document, the malicious browser extension would infect the attachments. Eventually, the malware gets on the computer of a user who has admin privilege of the company's server, and therefore the malware can steal the credentials that will lead the malware into access into the companies servers. >From this point on, the malware can then steal the most valuable data from the company. This example captures a number of key characteristics of APTs. Fist, the users, including the system admins do not realize that their computers and their network have been compromised. Second, the APT activities blend in with normal user activity. For example, the APT does not just send email by itself, it only modifies emails sent by the CEO. Third, it takes its time in or to get to the key individual and steal his credentials. Now let's discuss defenses against malware. We will discuss intrusion detection in another lecture. Here, we want to focus on Malware Analysis. Malware analysis produces information about a malware that can be used for detection and response to malware. There are 2 typical approaches to malware analysis. The first one is Static Analysis. In Static Analysis, we look at the program or the insertion set of the malware in order to understand what the malware would do if it is executed. That is we want to understand what the malware can do without actually executing the malware. There are limitations for Static Analysis because some program behaviors, in particular those that depend on runtime or user input data, for example conditions, cannot be precisely identified by looking at the source code or binary code. In addition, malware code can be obfuscated. We will come back to this point later. Another approach is Dynamic Analysis. Here, we run the malware program and try to analyze its runtime behavior to understand what the malware is doing when it is executed. And we can perform dynamic analysis in different level of granularities. For example, we can analyze the malware execution instruction by instruction. That's very fine-grained. Or we can analyze the malware execution by looking at the system calls that the malware invokes. So that's the so called system called tracing, and that's coarse-grained. There are also limitations in dynamic analysis, because any particular runs of the malware only reveal behaviors of those runs. That is, dynamic analysis cannot provide the complete picture of malware behaviors. In addition, malware can try to resist analysis by delaying execution, input or trigger and so on. That is, in any particular run of the malware, the condition may not be right, for so then triggers in the malware, and therefore, their part of the behaviors may not be revealed. Even the limitations of both approaches typically a malware system will try to employ both approaches. To illustrate the need for malware analysis and the challenges associated with it, let's consider one of the common malware obfuscation techniques, packing. Packing refers to the process of compressing and encrypting part of an executable program. The result of packing is that part of the executable program now becomes data, instead of an instruction set, and, in order to execute that part of the program that has been packed in runtime, we must reverse it back from data to insertion set. That is, we now must include in the executable program some code that will reverse this packing process, and this is called the unpacking. To give an illustrated example, we have the original program, Program A here, and you see that the program contains instruction sets. With a packing tool, that includes compression and encryption and so on. The result of packing is that part of the program, meaning, part of the instruction set, now becomes data. We observed that the packing result looks random, meaning the data here looks random. This is because it is encrypted with a randomly generated key each time the packing program is run on a malware program. That is, even for the same malware program, each packed instance would look different, and, therefore, a signature-based approach is not effective in detecting the packed malware. Now you say this pack program will include code that try to unpack the packed part. Can we use the unpacking code as signature to detect malware? The answer's no. The reason is that legitimate programs may also include packing in order to hide certain logic and data, for example, for copyright protection. That is, some legitimate programs also have unpacking code. It will unpack some part of the program in runtime. Therefore we cannot use the unpacking code as signature to detect malware. Most modern malware use packing, and there are many packing tools we call packers available. There are many, many ways to obfuscate malware code. Furthermore, there are hundreds of thousands of malware samples released to the internet every day by attackers and, like I said, most of them are packed. This means we cannot manually analyze all the malware samples. Instead, we need automatic approach to unpack malware and analyze malware, and such approach has to be universally applicable to all the malware samples. An approach that we can take, is to combine static and dynamic analysis to do fine grain analysis. In particular, with static analysis we look at the packed malware code, and we get a set of instructions in the packed program. And then, we run the malware, and the dynamic analysis, we can identify instructions that are not in the statically identified set of instructions. Meaning that, these are new instructions that we did not see in the packed program. These must be instructions that have been unpacked just before execution. In other words, by identifying instructions that we can only see one time, we have identified part of the program that had been packed. Once we identify the unpacked code, we can then apply other anti-malware techniques, including signature scanning, to identify the malware logic. For example, even though after packing, each malware instance looks different, the malware code, meaning original instructions, once unpacked remain the same. Therefore, after unpacking, we can still use a signature to identify the malware. For example, the instructions that have been packed can be the logic that the malware uses to do key logging, and a signature base approach can identify this part of the malware. Record the APT example that we just discussed. The APT malware is a malicious browser extension. Which of the following approaches can be used to detect this APT malware? Check all answers that are applicable. The first approach, a network monitor that analyzes traffic to detect anomalies or known bad traffic. For example, traffic to known bad or suspicious domains. This can be useful, for example, if the APT malware, after stealing credentials from the sysadmin, now steals data from the company's server and sends it to attack a server on the Internet. A network monitor may be able to detect or at least log such traffic for further analysis. So this is useful. A host monitor that examines operating system activities such as access to files. Again, this is useful because, for example, if the APT malware is stealing data from the server, it needs to access files on the server. Therefore, if the monitor can log such access to files, it may be able to help detecting such APT malware. So this is useful as well. The third approach, a malware analysis system that identifies malicious logic, for example, by running the browser in a sandbox and tracing its detail execution. This is useful, for example, if you run the malware in a system, such as a sandbox, and trace its execution at the instruction level. Then we can identify that there is a part of the browser that modifies the PDF document. And this part of the browser is the malicious extension. So this is useful. This quiz illustrates that to detect a malware, in particular APT malware, we need a comprehensive approach such as including network monitor, host monitor, and malware analysis system. Botnets have several ways for command control. APTs can hide their tracks and lay low and slow. We need network monitoring and static and dynamic analysis in order to detect malware. Firewalls a part of the network defensive-in-depth mechanisms. If allow malicious packets, so that they can prevent intrusions to a network. In this lesson, we're going to cover the firewall filtering techniques, as well as Deployment Strategies. When it comes to defense against attacks, the most important principle is to employ defense-in-depth. In other words, we should deploy multiple layers of defense mechanisms. The first line is a prevention mechanism that stops attacks from getting in our networks and systems. Inevitably though, some attacks can defeat the prevention mechanisms. That is, there are holes or weaknesses in the prevention mechanisms that allow some attacks to get through. For example, an attack can be a JavaScript that can only be triggered by a specified time in a future, and so it'll be hard for the prevention system to stop this attack now because it does not know or see the attack behavior yet. The second line of defense is detection and response mechanisms that watch activities on our systems and networks to detect attacks and repair the damages. Again, there will be attacks that can go undetected, at least for a while. For example, attacks that blend in with normal activities such as the ATP malware that is a malicious browser plugin would be hard to detect initially, not until its effects such as data loss due to stolen credentials manifest sometime later. The third line of defense is attack resilient technologies. The enable the core elements or the most valuable systems on the network to survive attacks and continue to function. For example, a server is actually a collection of diversified systems. Varies with different implementations so that at least one of them will not be susceptible to the attack because an attack typically exploits specific vulnerabilities that only exist in some, but not all, implementations. To summarize, we need to deploy defensive in-depth mechanisms, or multiple layers of security mechanisms to protect our networks. Firewall is a widely deployed prevention technology. To motivate the need for firewalls, let's look at a typical enterprise network at a high level. An enterprise network is part of the Internet. It typically has an internal or trusted part, where only the company's employees can access to. For example, if this is a bank, the trusted part of the enterprise network has the internal email servers and systems that process financial transactions. And only the authorized staff can access such systems. The enterprise network can also have a public face, in part. For example, the bank has a web server for its customers to log in, or for the public to just learn about the bank. This public facing service or in the so-called demilitarized zone or DMZ, that while it is part of the enterprise network, it is separated from the trusted network. For example, while customers can interact with the web service in the DMZ to log in and submit transaction requests, they cannot directly access the servers in the trusted network that are authorizing and processing the transactions. When a company has multiple physical sites, for example a bank can have different branches in several cities, then each site can have its own local and trusted network, but the sites need to communicate with each other. For example, employees in one city or one branch, these are the trusted users, need to access the corporate network at the headquarters in another city. And such access or traffic is from across the Internet, which is not trusted. How do we get traffic to its destination correctly across the Internet? We need routers. Each local or enterprise network has at least one router at its perimeter. And there are core routers on the Internet that bond. Together, these routers transport packets from one local area network, to the Internet backbone, and on to the destination, local area network and to the specific host on the network. The routers can send traffic to the correct destination on the Internet, but as we have discussed, whether the network should allow such traffic, depends on security considerations. For example, traffic from another trusted network such as a branch office should be allowed to the trusted network of the headquarter. Another example, traffic from untrusted network should only be allowed to the web service in the DMZ and access from the DMZ to the trusted network is again restricted. In short, we need a device that can enforce these different security restrictions on traffic. A firewall is such a device. More precisely, a firewall is a device that provide secure connectivity between networks. For example, between internal trusted network to extended untrusted network. It is used to implement and enforce a security policy for communication between the networks. Although, virtually, all companies have firewalls in place, the number of security breaches continues to increase. The reason for this is that a firewall is not all things to all malware. There are some security breaches that cannot be stopped by a firewall. Let's do a quiz on firewalls. Look at this list of items, and check all those items that firewall can stop. The first one, hackers breaking into your system. The second one, Internet traffic that appears to be from a legitimate source. The third one, viruses and worms that spread through the Internet. The fourth one, spyware being put on your system. The fifth one, viruses and worms that are spread through email. Check all those items that a firewall can stop. The first one, hackers breaking into your system. This typically means that there is attack traffic coming from untrusted parts of the Internet or even from a known malicious site. So this can be stopped by the firewall. The second one, Internet traffic that appears to be from a legitimate source. Firewall cannot stop this. The reason is that the firewall is designed to allow traffic from a legitimate source, such as another trusted network. The third one, viruses and worms that spread through the Internet. This can be stopped by the firewalls. There are several ways to identify and block such traffic, such as the volume of such traffic or that said traffic has specially crafted packets that are known to target vulnerable surfaces, or the traffic is from some untrusted networks on the Internet. In short, the firewall can use such knowledge to block such virus and worm traffic. The fourth one, spyware being put on your system. It would be hard for a firewall to stop this because spyware typically steals information and sends it out to a site. The external site can appear to be legitimate, or at least not known to be malicious. And the volume of traffic can be very small. The fifth one, viruses and worms that are spread through email. A firewall typically cannot stop this because, while the firewall can track that, an email is from a legitimate source. It cannot track the contents of the email itself Or attacks can be considered a result of violation of security policies, therefore, as a prevention mechanism, a firewall should be designed to enforce security policies. So what is a policy? At a high level a security policy specifies whats allowed and whatever is not specified is by default not allowed. In other words, what is allowed means what is good or acceptable to the organization. Obviously, policy is organization specific, and we will come back to this point later. So specific to firewalls, a firewall is designed to enforce security policies on network traffic. That is, all traffic, inbound and outbound, meaning, from internal network to the internet or vice versa, must pass through the firewall. And the firewall will enforce policy on a traffic. In other words, only traffic authorized by the security policies is allowed to pass through the firewall. In addition to gravity enforcing the security policies, a firewall also must be dependable. This means that the firewall must not be easily to crash or disabled by an attack. The reason is obvious, because if the firewall is disabled by an attack, then all subsequent attacks can get into our network. A critical component in the planning and implementation of a firewall is specifying a suitable access policy. So what does the policy say? Simply put, the network access policy specifies what types of traffic can pass through the firewall. The types of traffic are typically defined by the address ranges, meaning, what are their machines, the protocols, the applications and the contents. We will give some examples of access policy, later. How do we decide on the policies? A policy should be developed through the security and risk assessment on the organization. The topic of risk assessment is discussed in another lecture. Essentially this risk assessment and policy exercise will tell us what types of traffic the organization must support. And what security risks are associated with these traffics. And therefore how the firewall should be implemented to mitigate such risks. Firewalls have limitations. That is, there are situations where a firewall provides no protection. If traffic does not pass through a firewall, then a firewall cannot examine said traffic and provide protection. For example, if traffic is routing around the firewall, meaning that the traffic does not pass through the firewall, then there is no protection. Or for traffic that's internal to the network meaning that the traffic does not go across the boundary between internal and external networks, then there's no protection by the firewall that sits between internal and external networks. If the firewall is misconfigured, the traffic that passes through, the firewall cannot examine it correctly. Firewalls also provide additional features. Firewalls can lock all traffic that passes through. And the law can be analyzed later to learn about the traffic such as the traffic volume to a specific part of the network. Firewall can also provide network address translation. This is useful when multiple machines in the internal network has to share an IP address to the external networks on the Internet. The firewall translates the source IP address of an internal host through this shared IPv4 address for outbound traffic. And for inbound traffic, the firewall translates the destination IP address to the IP address of an internal host. It can also provide encryption services. For example, when traffic goes out from one internal trusted network to another trusted network across the Internet, the firewall can automatically encrypt the traffic so that the untrusted networks on the Internet cannot learn the contents of such traffic. Let's do a quiz on firewall features. First, can malware disable a software firewall? Can a malware disable a hardware firewall? Likewise, can a malware disable an antivirus checker? Second one, can firewall stop pings, packet sniffing, outbound network traffic? First, can malware disable a software firewall? The answer is yes, because if the malware gets into the operating system, then it would have the privilege to disable software-based firewalls. Likewise, a malware can disable an antivirus checker. Because if the malware gets in the OS, it would have the privilege to disable the antivirus checker. Can a malware disable a hardware firewall? Typically, no, because malware typically cannot control hardware settings. Can firewalls stop pings? Pings are ICMP packets, and they can be stopped by firewalls because the firewall can look at the IP header and know that this packet is an ICMP packet. Second one, packet sniffing. Typically, this means that the attacker has a malware in the internal network that passively records traffic on the internal network. And so there's no activity that can be seen in the firewall unless the attacker is sending a huge amount of decoded traffic out to the Internet. But typically the attacker would have a malware performing the analysis of the decoded traffic in the internal network. And only sends the small amount of valuable data out to the Internet. So it would be hard for the firewall to stop this. Outbound network traffic. Yes, the firewall can control outbound traffic, because firewalls can inspect and control traffic both inbound and outbound. The main mechanism in firewalls is traffic filtering. Firewall filtering means that the traffic gets to the firewall and the firewall will decide whether to let the traffic through or not. In other words, each packet is stopped at the firewall and is checked against security policy. And then the firewall will decide whether to allow the packet or discard the packet. And both the inbound and outbound traffic is filtered or checked by the security policy of the firewall. There are two main types of filtering. The first type is filtering based on per packet. Essentially, the firewall policy is a set of access control lists based on the packet types. The second type of filtering is based on per session. In this type of filtering, a packet is examined based on its context within a session. And in order to do so, the firewall maintains information about a session or connection, and performs a so-called stateful inspection. In a packet filtering firewall, decisions are made on a per-packet basis meaning that decisions are made based exclusively on the current packet, and not on any other packet. Therefore, the firewall does not need to keep any state information about other packets. As we can see, packet filtering is the simplest and the most efficient, but it is not robust against attacks that span multiple packets, where each packet by itself is not indicative of an attack. A packet filtering firewall is relatively simple. It basically applies a list of rules to match the IP or TCP header of a packet, and based on the rules match, the firewall will then decide to forward or discard the packet. Here are examples of IP or TCP header information that a firewall can use to filter a packet. Source IP address, where the packet's from, destination IP address, where a packet's going to. Source and destination transport level address, this means the transport level port number, which defines applications such as SNMP or HTTP. Basically, this tells what application the packet belongs to. For example, is it for email or web traffic? IP protocol field. This defines the transport protocol, such as TCP, UDP, or ICMP. Interface, for firewall with three or more ports, which interface of the firewall the packet came from, or which interface of the firewall the packet is going to. This is useful when there are multiple ports in the interface network that are quite different security policies. A packet filtering firewall is typically set up as a list of rules based on matches to fields in the IP or TCP header. If there's a match to one of the rules, that rule is invoked to determine whether to forward or discard a packet. If there's no match to any rule, then a default action must be taken. There are two default policies. The default discard policy means that if there's no rule that matches the packet, then the packet will be discarded. This is a more secure or conservative policy because it provides more control about what traffic is allowed to the network. On the other hand, it can be a hindrance to users who see that some traffic are not allowed, and they have to tell the system admin to enable the traffic. The alternative is the default forward policy, which means that if there's no rule that matches the packet, the packet is allowed. Compared with the default discard policy, this policy is more user friendly, but it's less secure. The security admin must react to each new security threat add rules to the firewall. Now, let's have an exercise on firewall policies by considering user convenience and security. We're going to rate these policies on its user convenience and security. We use number one for the best and three for the worst. We have three policies here. The first one accepts only packets it knows are safe. The second one drops packets it knows are unsafe. Third one queries user about questionable packet. You can think of, the first one is the default discard policy. The second one, is similar to default allowed policy or default forward policy. The third one is in between the two. The first one accepts only packets it knows are safe. It is similar to default, discard policy. In terms of security, it is the best among the three. In terms of ease of use, it is the worst, because it may stop traffic that is actually safe and useful to user, but the firewall does not know it yet. The second one, drops packets it knows are unsafe. In terms of security, this is the worse because of our knowledge about packets or what traffic is unsafe is limited. Because attackers are constantly implementing new methods. But in terms of ease of use, it is the best. Because the users will have access to most of the traffic that they want. The third one, queries user about questionable packets. This is in between the two. Therefore, its security is in between a two, and ease of use is also in between a two. Let's discuss typical firewall configuration. First, let's provide some background. Most standard applications that run on top of TCP follow a kind and server model. For example, for a simple mail transfer protocol or SMTP, email is transmitted from a kind system to a server system. The client system generates new email messages, typically from user input. The server system accepts incoming email messages, and places them in appropriate user mailboxes. SMTP operates by setting up a TCP connection between client and server, in which the TCP server port number, which identifies the SMTP server, is port 25. The TCP port number for the SMTP client is a number between 1,024 and 65,535. >From this example, the port numbers less than 1,024 are so called well known port numbers and are assigned permanently to particular applications. Such as port 25 for server SMTP or port 80 for HTTP. The port numbers between 1,024 and 65,535 are generated dynamically and have temporary significance only for the duration of a TCP connection from a client to the server. Therefore, a packet filtering firewall must permit inbound network traffic on all these high number ports vorticity base connections. For the so-called, well-known ports that are below 1,024, there are protocols that use the entire range of ports. And in such case, the entire range must be allowed in order for the protocol to work. Let's go over an example of packet filtering. This is a simplified example of a rule set for SMTP traffic. The goal is to allow inbound and outbound e-mail traffic, but to block all other traffic. The rules apply top to bottom for each packet. That is, for each packet, the firewall is screened up high, each rule, one by one, from top to bottom, until there is a match. So let's explain the intent of each rule. As we can see, each rule here has a rule number, has a direction of the traffic, has a source, and a destination IP addresses of the packet. The protocol, the destination port, and the decision, whether it's permit or deny. The first rule is to allow inbound email traffic from an external host, therefore it says the direction is in, meaning inbound. The source IP address is an external IP address, because we are talking about inbound traffic. The destination IP address is an internal IP address. The protocol is TCP, the destination port is 25, which is for SMTP. Again, this permits inbound email traffic from an external source. Second rule, it's intended to allow a response to an inbound SMTP connection. Because here, in the first rule, we allow inbound email traffic, so we should allow outbound response to the email traffic. The third rule allows outbound email to an external source. That is, we allow outbound traffic to external email server, SMTP server, port 25 on an external destination IP address. And since we allowed outbound email to an external email server, rule number four allows the inbound response. That is rule number four is intended to allow an inbound email response. Rule number five, this is an explicit statement of the default policy, which is denied. This means that if a packet does not match any of the previous rules, then the packet will be discarded. There are several problems with this rule set. For example, let's look at rule number four. This rule allows inbound traffic to any destination port above 1023, whereas, the original intent is to allow an inbound traffic, that is part of the email connection. In other words, it is more permissive than its original intent. For security purposes, we want to make these rules more specific. Therefore we can add another condition to the rules. This condition is on the source port of the packet. For example for rule number four, our intent is to allow inbound traffic that is part of an email connection. Therefore the source port should be 25. We can make these rules even more precise. For example, because the intent of rule number four, is to allow inbound traffic, there's part of a established email connection. We will want to check the ackbit of the packet, and make sure that it is set. This is because in TCP, once a connection is set up, the TCP flag in the TCP header is set. So we can check this bit, to make sure that the inbound packet is part of an establish connection. The main advantage of packet filtering is simplicity. That is, it is very easy to implement packet filtering rules. Packet filtering is also very efficient, therefore it imposes minimal overhead. That is, the users typically would not notice any performance slowdown. In addition, these rules are typically very general because they apply to all packets, meaning that they are not specific to any application or uses, and therefore it is very transparent to the user experience. However, packet filtering also has weaknesses. Since packet filtering firewalls do not examine upper layer data, they cannot prevent attacks that employ application-specific vulnerabilities or functions. For example, a packet filtering firewall cannot block specific application commands or contents because, if the firewall allows a given application, then all functions, all commands, all contents within the application must be permitted. For example, once Web traffic is allowed, the firewall cannot block certain offensive page contents. The logging capabilities of packet filtering firewall is also limited. Again, this is because packet filtering firewall does not examine upper layer data. For example, the packet filtering firewall may allow FTP traffic but cannot log the actual FTP data, such as which files are being transmitted, and, since packet filtering firewall makes decisions on per packet basis, it cannot prevent attacks that span multiple packets. That is, it cannot see attacks that require multiple packets of a connection. Finally, as our example shows, packet filtering firewalls tend to have rules that have a small number of variables or conditions. That is, these rules may not be specific enough, and attacks can bypass the firewall. Let's discuss some attacks on packet filtering firewalls and the appropriate countermeasures. The first attack is source IP address spoofing. Here the attacker transmits packets from an outside host, but with a source IP address field containing the address of an internal host. That is, the attacker spoofs the packets source IP address as if it is from an internal host. The attacker hopes that the use of a spoofed internal source IP address will allow the firewall to let the packet pass. The firewall is typically configured to let traffic from one internal host to another to pass through. The countermeasure is to discard packets with an inside source IP address if the packet arrives on a external interface. In fact, this kind of measure is often implemented at the router, external to the firewall. That is, when the router receives the packet from the Internet, it would check whether the source IP address is correct. If the source IP address is an internal IP address, the router should know that this IP address is spoofed, because the router just received this packet from the Internet, meaning an external host. A related attack, is a source routing attacks. Here, the attacker specifies the route the attack should take as it crosses the internet. And the attacker hopes that this will bypass security measures and checks along the way. A countermeasure is for the firewall or the router to discard all packets that use this option. Another attack is a tiny fragment attack. Here, the attacker uses the IP fragmentation option to create extremely small fragments, and forces the TCP header information into separate packet fragments. This attack is designed to circumvent filtering rules that depend on TCP header information. Typically, a packet filter will make a filtering decision based on the first fragment of a packet. The attacker here hopes that the filtering firewall examines only the first fragment and that the remaining fragments are passed through. This attack can be defeated by enforcing that the first fragment of a packet must contain a predefined minimum amount of transport header information. If the first fragment is rejected, then all the subsequent fragments should also be rejected. Let's do a quiz. This quiz is on the rules on IP fragmentation. You can imagine that we can implement these rules in a packet filtering firewall to check valid IP fragments. Here, please mark all answers that are true. First, each fragment must not share a common fragment identification number. Second, each fragment must say what its place or offset it is in the original fragmented packet. Third, each fragment must tell the length of the data carried in the fragment. Fourth, the fragment does not need to know whether more fragments follow this one. Please mark all answers that are true. First, each fragment must not share a common fragment identification number. This is false because each fragment of the same IP packet must share the same identification number. Second, each fragment must say what it's place or offset it is in the original unfragmented packet. This is true because otherwise we cannot correctly reassemble the fragments into the original IP packet. Third, each fragment must tell the length of the data carried in the fragment. This is true. Again, this has the correct assembly of the fragments into the original packet. Fourth, the fragment does not need to know whether more fragments follow this one. This is false. Because each fragment must know whether there are more fragments to follow. Again, this information is necessary for correct reassembly of the fragments. Now, let's discuss Stateful Inspection Firewall. This is different from Packet Filtering Firewall. In a Stateful Inspection Firewall, a packet is analyzed and a decision is made based on the context of other related packets. Typically, this means the connection this packet belongs to. Therefor we need to record and maintain information about connections. And then decisions are made on each packet based on the current state of the connection and the context of the packet within this connection. More specifically as the packet arrives at the firewall, the firewall updates the information about a connection accordingly and then decides whether the packet is allowed in the context of the connection. For example, for inbound packet, there is part of an established email connection. The firewall can check that the inbound packet is a response to a previously outbound packet. In addition, the firewall can resemble multiple packets of the connection and inspect the connection data such as the exact FTP commands or which file the FTP is transmitting. Or the firewall can actually look into the page contents received from a web server. Here's an example of a connection table. As we can see each record contains the most basic information about connection. In particular the source IP address, source port, destination IP address, destination port, and most importantly the current connection state. For example whether it's established or not. Internally to the firewall, there could be another data structure that is linked to the connection table. For example, for web traffic, since a page can spend multiple packets, this internal data structure of the firewall can maintain the contents of a page that it has received so far. That is, this data structure which is linked to the connection table will allow the firewall to perform more specific analysis of the connection. The packet filtering or the state fill inspection firewalls on a two main or more traditional types of firewalls. There are other more modern firewalls. In particular, an application-level gateway, or sometimes called an application proxy, is an application-specific firewall. It essentially acts as a relay of application-level traffic, or a man or system in the middle. That is, to the external server, this gateway acts as the client, and to the internal client, this gateway acts as the external server. For example, many organizations have a web proxy, and that is a application-level gateway. Let's discuss a typical workflow of an application-level gateway. First, the user contacts the gateway using an application such as FTP or the browser. The gateway then asks information such as the name of the remote server, the user login information, and so on. The user then provides the valid authentication information to the gateway. The gateway then contacts the external server and provides the valid user authentication information. In other words, the gateway acts as the user to the external server. When the external server sends back a response, the gateway is going to analyze it, and if the traffic is allowed, it's going to forward it to the user, but in order for the gateway to be able to correctly analyze the server response, the gateway must implement the correct application logic. For example, if this is a web proxy, you must be able to process web traffic, just like a browser. As we can see, if you want to use application-level firewalls, we must implement proxy code for each application that we want to protect. The advantage is that we can then restrict certain application features. For example, a web proxy can prevent active scripts in web pages. This can be accomplished by the proxy removing the scripts in the pages returned by the remote server, and therefore application-level gateways tend to be more secure. On the other hand, application-level gateway does incur additional overhead. This is easy to see, because each connection from the user to the external server is actually spliced into two connections. One is from the user to the gateway, and the other one is from the gateway to the external server. And the gateway must examine and forward all traffic in both directions. To review the different firewall types, let's do a quiz. Here, please mark each statement as either T for true or F for False. The first statement. A packet filtering firewall is typically configured to filter packets going in both directions. The second statement. A prime disadvantage of an application-level gateway is the additional processing overhead on each connection. The third statement. A packet filtering firewall can decide if the current packet is allowed based on another packet that it has just examined. The fourth statement. A stateful inspection firewall needs to keep track of information of an active connection in order to decide the current packet. Please mark each statement as either T for true of F for false. The first statement, a packet filtering firewall is typically configured to filter packets going in both directions. This is true. In fact, all firewalls regardless of which type is configured to check traffic in both directions. The second statement, a prime disadvantage of an application-level gateway is the additional processing overhead on each connection. This is true. We just discussed this. The third statement, a packet filtering firewall can decide if the current packet is allowed based on another packet that it has just examined. This is false because a packet filtering firewall makes decisions based only on the current packet. The fourth statement, a stateful inspection firewall needs to keep track of information of an active connection in order to decide the current packet. This is true, because a stateful inspection firewall decides on a packet based on the context of the current connection. Now let's discuss firewall deployment strategy. That is, where in the network do we put a firewall? Typically, we put an application level gateway, such as a web proxy, to a dedicated machine and we call these machines the bastion hosts. These machines are made to be very secure. Here are some measures that we can take to make this bastion host very secure. First, these machines execute a secure version of the operating system. In addition, only the services that the admin consider essential are installed on the host. For example, a bastion host may only allow DNS and web traffic. A bastion host can also require that, even for traffic coming from an internal host, the user must authenticate himself to the bastion host. Each proxy running on the bastion host is configured to allow access only to specific host systems in the internal network. This is important because we don't want compromised proxy lead to attacks to the entire internal network. Each proxy module is a very small software package, designed with security in mind. That is, we want each proxy to be as small and simple program as possible so that we can check for security. For example, a typical Unix email application may contain over a couple hundred thousand lines of code, while an email proxy may contain fewer than one thousand lines of code. A proxy typically performs no disk access other than reading its initial configuration file. This makes it difficult for an attacker to install a Trojan horse on the bastion host and affect the proxy. Proxies are typically non-privileged and they're isolated from each other. The security benefit is that if a proxy is compromised by an attack, it cannot easily compromise the entire bastion host or affect other proxies. Firewalls are typically deployed in network perimeters. But, in addition to network firewalls, there are also host-based firewalls. A host-based firewall is a software module, used to secure an individual host, that is to enforce the host specific security policy. Similar to a network firewall, a host-based firewall can filter and restrict traffic to and from the host. Typically, a host-based firewall is installed on an internal, important server. There are several advantages of deploying host based firewalls. For example, specific security policies for service can be incremented with different filters for servers used for different applications. Since the firewall is not at the network parameter, both internal and external attacks can be stopped by the host based firewall. That is, while an attack coming from the internal network is missed by the firewall at the network perimeter, it can be stopped by the host based firewall. We typically deploy host based firewall in addition to network firewall, so therefore, host based firewalls provides an additional layer of protection. Personal firewall is similar to a host based firewall. It controls traffic between a personal computer and the rest of the network. Personal firewalls are also often deployed at home routers to protect home networks. These firewalls tend to be simpler because the home networks are less complex than the enterprise networks. Similar to firewalls at corporate networks, the main function of personal firewalls is to track traffic in both directions. For example, the personal firewall on the home router can stop attacks from reaching the internal home computers. It can also monitor outgoing traffic. For example, it can detect connection to an extended server as known for performing botnet command control and it can block such connection. Therefore, even when a home computer has been compromised, it cannot participate in botnet activities. As an example, these services are typically not allowed, but a personal firewall can selectively enable any of these services. In addition to disabling and enabling certain services, personal firewalls can also provide advanced features. For example, a personal firewall can be configured to hide the system from the Internet. This is accomplished by the firewall dropping unsolicited packets from the Internet. The firewall can also drop all UDP packets or ICMB packets and only allow TCB packets to certain ports. A personal firewall can also support logging. For example, recording all unwanted traffic and activities. It can also allow the user to specify only certain applications to subconnections from the Internet. For example, the user can specify that only the applications signed by public keys, issued by a valid certificate authority can connect to the Internet. Let's do a quiz. Suppose that a company already has a conventional network firewall in place. Which of the following situations would require an additional personal firewall? The first one, an employee uses a laptop on the company network and at home. The second one, an employee uses a desktop on the company network to access websites worldwide. The third case, a remote employee uses a desktop to create a VPN on the company's secure network. The last one, none of the above. Which of the following situations would require an additional personal firewall? The first one an employee uses a laptop on the company network and at home. In this case a personal firewall is required because when the employee takes the laptop to his home it needs protection. That is when the laptop is at home it is not protected by the conventional network firewall at a company, so it requires a personal firewall. The second one, an employee uses a desktop on the company network to access websites worldwide. In this case, it does not require a personal firewall because the desktop is within the company's network. The third case, a remote employee uses a desktop to create a VPN on the company's secure network. In this case, a personal firewall is required. In fact, a personal firewall on a desktop is typically used to create a VPN, so that the remote desktop can access the company's secure network. And obviously, this last statement is false. Let's look at how firewalls should be deployed to protect a network. Here's a figure that illustrates a common firewall configuration, that includes an additional network between the external and internal firewalls. An external firewall is placed at the edge of the local area network, just inside the boundary router. The boundary router connects the complete network to the Internet. One or more internal firewalls protect the bulk of the enterprise network. Between the external firewall and the internal firewall is typically the DMZ, systems that are externally accessible, but need some protections are usually located in the DMZ. These systems require or foster external connectivity, such as the public facing corporate web server, an email server, and a DNS server. The external firewall provides some basic or first line defense, but allows access to these public facing systems. The internal firewall provides additional protection. In particular, it protects traffic from a DMZ to the internal trusted network. Comparing with the external firewall, an internal firewall performs more stringent filtering. This is because the internal network would require more protection, than the public facing systems on a DMZ. The internal firewall protects the remainder of the enterprise network from attacks launched from the DMZ. For example, if the public facing web server in the DMZ is compromised, the internal firewall is to stop attacks from that compromised web server. In addition, the internal firewall also controls access to the DMZ from internal network. For example, only the authorized traffic from the internal network can reach to the DMZ to change the settings of a public facing web server. For example, such activities must be from a sys admin's work station. In addition, multiple internal firewalls can be used to protect different parts of the internal network from each other. That is, even if one part of the internal network has been compromised, the other parts are still being protected by their own internal firewalls. A distributed firewall configuration typically includes standalone network firewalls, host based firewalls, and personal firewalls. That is, the standalone network firewalls protect the internal network from attacks from the Internet. In addition, host based firewalls are placed at workstations and servers. These host based firewalls protect against internal attacks and provide protection tailored to specific machines and server applications. For example, with host based firewalls providing server specific protection, we can have one public facing, external web server and an internal facing web server. In addition, personal firewalls can be used to protect personal computers, regardless of where they are in the network. An important issue here is the need to coordinate the multiple firewalls for security monitoring. This typically involves aggregating the logs from multiple firewalls and perform correlation analysis. For example, if multiple host based firewalls see the same attack, this may suggest that there's a worm spreading inside the internal network. Let's do a quiz on firewall deployment. The question is, typically the systems in which of the following will require or foster external connectivity such as a corporate web site, an email server, or a DNS server? Are these systems in a DMZ, IP protocol field, boundary firewall, or VPN? The answer is DMZ. We typically put these public facing servers in a DMZ, but also protect the internal network from these servers. Here's another quiz on firewall deployment. Which of the following configurations involves stand-alone network firewall plus host-based firewalls working together? Is it packet filtering firewall, distributed firewall, boundary firewall or VPN? The answer is distributed firewall. Typically, a distributed firewall includes stand-alone network firewall, host-based firewalls, plus personal firewalls. We can now, summarize a spectrum of Firewall locations and Topologies. The first are the host-based firewalls that include, also, personal firewalls. These can be placed on servers or personal computers. We can place a network firewall on a router to protect the internal network from the external network. Or, a network firewall can also be placed on a dedicated device, such as a bastion host. A bastion host can also have a third interface through DMZ. We can also have an external firewall on a bastion host and a separate internal firewall on a different bastion host. We can then put the DMZ in between these two firewalls. All the external firewall bastion host can have a third interface to the DMZ. We can also use a distributed firewall configuration that includes network firewalls, and host based fire walls. Firewalls prevents intrusion by way of traffic filtering. The filtering techniques include packet filtering, session filtering, and application level gateway. Firewalls can be deployed at network perimeter, end hosts, servers, and they can be used to create a DMZ. Intrusion detection is also a part of the network defense-in-depth mechanism. In this lesson, we're going to discuss the system architecture, algorithms, deployment strategies, as well as performance metrics of intrusion detection systems. We're also going to discuss attacks on intrusion detection systems. We call that defense in depth principle. We need multiple layers of defense mechanisms. That is, we need detection mechanisms even after we have deployed prevention mechanisms to detect attacks that can be easily prevented. That is, there are always some attacks that we can not simply keep out of our networks and systems. We have discussed firewalls, which are prevention mechanisms. Today we will discuss detection mechanisms. Typically, these mechanisms are the intrusion detection systems. By intrusion, we mean any attack that aims to compromise the security ghost of an organization. For example, performing a remote root compromise, of an email server. Defacing a web server to display inappropriate content. Guessing and cracking stolen passwords. Stealing a database containing credit card numbers. Reading sensitive data without authorization. Running a packet sniffer on a workstation, to capture user names and passwords on a network. Using a permission error on an anonymous FTP server, to distribute pirated software and music files. Dialing into an unsecured modem, and gaining internal network access. Posing as a company executive, calling the help desk and resetting the executives email account. Using a workstation without permission. Select whether it is firewalls or intrusion detection system that best describes each of these systems. The first one tries to stop intrusion from happening. In other words, try to prevent attack from happening. This is obviously a firewall. The second one tries to evaluate an intrusion after it has happened. In other words, try to detect whether there's an intrusion happening. This is obviously an intrusion detection system. The third one, watches for intrusions that start within the system. Again, this means detection, so it is intrusion detection system. The fourth one, limits access between networks to prevent intrusion. Again, this tries to prevent attacks, so this is firewall. Intrusion Detection Systems, or IDS for short, are designed to counter these types of threats. They can be quite effective against known or less sophisticated attacks, such as large scale email phishing attacks. However, as attack techniques become more sophisticated, IDS will become less effective. For example, attackers can blend attack traffic with normal activities so that it is very hard to detect such attacks. And the most sophisticated attackers, in particular, the state-sponsored attackers, may use new or zero-day exploits to render IDS completely ineffective. Since IDS are not always effective, they need to be part of the defense-in-depth strategy for an organization. Typical defense-in-depth strategies should include encrypting sensitive information, provide detail of the trails of activities on systems and networks. Use strong authentication and access control. Actively manage the security of operating system and applications. The techniques and behavior patterns of intruders are constantly shifting in order to exploit newly discovered weaknesses, and to evade detection and counter measures. However, intruders typically use steps from a common attack methodology. Here are the common attack steps. The first step is target acquisition and information gathering. That is, the attacker identifies the target systems using publicly available information, both technical and non-technical and use network tools to analyze target resources. For example, the attackers can find out what network services are accessible from the Internet. And the attacker may find out some email accounts of high level company executives. The second step is initial access. This is accomplished by exploiting a remote network vulnerability. For example, a network service has a buffer overflow vulnerability that allows a remote attack to take over the service and gain access to a system. Or this can be accomplished by social engineering, whereby, for example, an attacker can send an email to a company executive with an attachment so that when the attachment is clicked, a malware is installed on the system. The third step is privilege escalation. This is taken after the initial access. And the attacker would try to use local exploit to escalate the privilege from, for example, the normal user to root privilege on the target system. The fourth step is information gathering or system exploit. That is, after an attacker has gained sufficient privilege on a system, he can then find out more about the network and the organization. And even move to another target system to further the exploit on the network. The fifth step is maintaining access. This is important because an attack may not be a one-time action. That is, the attacker may choose to come back from time to time, or continue the exploit for awhile. Therefore, the attacker may install backdoors or other malicious software on a target system so that he can continue to access this target system. Lastly, the attacked wants to cover his tracks. For example, the attacker can disable or even edit the system audit logs to remove evidence of attack activities. Or the attacker may install root kits to hide the installed malware. Decide whether each statement is true or false. The first statement, an intruder can also be referred to as a hacker or cracker. This is true. For example, we sometimes use hacker to refer to an intruder. Second, activists are either individuals or members of an organized crime group with a goal of financial reward. This is false. Instead of financial motives, activists typically have a social or political cause. Third, running a packet sniffer on a workstation to capture usernames and passwords is an example of intrusion. This is typically true, unless such packet sniffing is done with proper authorization. Fourth, those who hack into computers do so for the thrill of it or for status. This is false because it only describes some attackers. But there are many attackers who attack computers for other reasons, for example, for illicit financial gains. The last statement, intruders typically use steps from a common attack methodology. This is true. Choose the description that best describes the type of backdoors. First, compiler backdoors. Is it A, a backdoor that is hard to detect because it modifies machine code? Or B, this backdoor can only be used by the person who created it, even if it is discovered by others? C, this backdoor inserts backdoors into other programs during compilation. So it's obviously C. Second, object code backdoors. These are backdoors that are inserted into machine code. So that's A. Asymmetric backdoors. This backdoor can only be used by its creator, because it is protected or controlled by codographic schemes. Now, let's discuss some details of intrusion detection. First, what are the key design elements of an intrusion detection system. First, for intrusion detection to even be possible, we need to make some important assumptions. First, we can observe system and network and user activities. Second, from the activities that we can observe, normal and intrusion actions have distinct evidence, that is, for intrusion detection to even be possible, we must be able to find evidence of intrusions by observing systems, networks, and user activities. When it comes to designing an intrusion detection system, we must consider the following. >From the point of view of detection algorithm, we must consider the features, meaning how do we capture and represent intrusion or normal activity evidence? We must also decide the detection models. Meaning, how do we piece the evidence together, so that we can decide whether an activity is normal or intrusive. The system architecture typically includes several modules including audit data processor, a knowledge base, a detection and decision engine, and alarm generation and response mechanisms. Here's an illustration of the workflow of intrusion detection as well as the main components of an intrusion detection system. The input on an IDS is data that describes activities on systems and network. The data is processed by the data processor to extract activity records. They are important for security analysis this activity data needs to be analyzed by the detection engine. The detection engine uses detection models that have been already constructed for the ideas that is these models are stored with ideas. If a detection rule determines that there is an intrusion. The IDS produces an alert. The decision engine then decides the appropriate action according to decision table. For example, this can be a response that automatically blocks a network connection or report that is sent to the security admin. Again, for the IDS to work properly, we'll assume that system activities are observable and are captured in the input data to the ideas. And when detection models are applied to the activity data, normal and intrusive activities have distinct evidence. There are several ways to look at different intrusion detection approaches. >From the point of view of how data is analyzed, or how intrusion is detected, we have two approaches. One is misuse detection, also known as signature-based detection. The other one is anomaly detection. Misuse detection models all represents long intrusions. Anomaly detection models all represents normal activities. >From the point of view of deployment strategy, we can deploy an IDS on end host, or a network perimeter. In terms of how the detection models are viewed and maintained, we can use manual encoding of expert knowledge. All hand written rules, or we can apply machine running algorithms to data to automatically learn the detection rules. An IDS typically uses these approaches to analyze data and detect intrusions. Anomaly detection, by definition, it tries to detect what is not normal. Misuse or signature detection, you try to find a match of no intrusions. There are mainly two elements in anomaly detection. The first is training or profiling. The goal is to define or characterize the normal or expected behavior. This is accomplished by collecting data linking to normal activities, and applying data analysis algorithm to construct a model. The second, is to evaluate whether an observed behavior fits the normal profile. This is accomplished by analyzing the observed data to see whether it fits the established model. Misuse or signature detection, involves first encoding normal text into patterns or rules, and then comparing the current behavior with these rules or patterns. To see whether there's a match or not. Obviously, this approach can only detect known intrusions or known attacks. Check all statements that are true regarding anomaly detection systems. First. The longer the system is in use, the more it learns about network activity. This statement's true, because anomaly detection involves first learning or profiling what is normal. The longer the system is in use, the better it can learn what is normal. Second, if malicious activity looks like normal traffic to the system, it will not detect an attack. This is true, because anomaly detection, detects what looks not like normal. Therefore, if an attack managed to look like normal, then the anomaly detection system will not be able to detect this attack. The third statement, false positive can become a problem, normal usage can be mistaken for an attack. This is true. Because the definition of false positive is that, a normal activity is mistaken as an attack. At the minimum, false positives can waste systems time, because the system needs to investigate whether there's truly an intrusion or not. Check all statements that are true regarding misuse detection systems. The first statement, new threats can be detected immediately, this is false because a misuse detection of signature-based detection system can only detect attacks that match patterns or rules of known intrusions. Second, when a new virus is identified, it must be added to the signature database. This is true. Because a misused detection system detects attacks based on signatures of known intrusions, therefore when a new attack is discovered, its signature needs to be added to the signature database. Third, can only detect an intrusion attempt if it matches a pattern that is in the database, this is true. This is essentially the definition of a signature-based detection system. The anomaly detection approach involves first developing a model of normal or legitimate behaviors by collecting data from the normal operations of the monitored system and network. This is called a training phase. Once this model exists, the observed behavior is compared with this model in order to classify it as either legitimate or anomalous and this is the detection phase. A variety of approaches are used to construct such depression model. The statistical approach, process data into metrics or measures and then apply univariate, multivariate or time series analysis to view a model. The knowledge based approach uses expert knowledge to encode legitimate behavior into a set of rules as the detection model. A machine learning approach uses data mining and machine learning techniques to automatically learn a model from the training data. When we compare these approaches, we need to consider both efficiency and cost. Efficiency here means how fast we can learning a model from training data, and how fast we can apply the model to the observed data to determine whether it is anomalous or normal. The cost of detection here means, how much data do we need in order to view the model and apply the model, and how much completion of power it is required? For example, typically a machine learning based approach required a lot of data. Let's make sure we understand what we are looking for when we talk about anomalies with regards to intrusion detection. Which of the following is considered anomaly in a typical network? First, an IP address. Can this be an anomaly? If the IP address is not the one that normally accessed by users or is not well known, it can be anomaly. So this is anomaly. Second, a port address. Similar to the IP address, if the port address is not normally accessed, then this is an anomaly. How about packet length? Again, if the length is unusually long, for example, then this is an anomaly. How about flag setting on a packet? Again, if these flags are not normally seen under the same traffic conditions, then this is an anomaly. That is, all of these can be anomalies if they are not normally seen in normal operations of the network. Let's discuss the statistical approaches in more details. Statistical approaches use the captured sensor data as training data, to build up a model of normal behavior. The earliest approaches used univariate models, where each metric, or measure, such as a CPU utilization of a program, was treated as an independent, random variable. However, this was to crude to effectively detect intrusions. The later approaches use multivariate models to consider the correlations between the measures. For example, CPU utilization and memory size of the program are considered together. Some later approaches also use time series models to consider the order and time we can observe events. For example, user logging into a workstation and an email from the workstation are two events that consider in a time order. The main advantages of statistical approaches are that they are relatively simple, easy to compute and they don't have to make a lot of assumptions about the behavior. These approaches don't rely on assumptions about the expected or normal behaviors, and they're simple and efficient. On the other hand, the effectiveness of these approaches rely on selecting the right set of measures, which is a very difficult task. In addition, they have complex behaviors that are beyond the capabilities of these models. Knowledge based approaches rely on experts to develop a set of rules that describe the normal and legitimate behaviors observed during training. For example, these rules can classify activities into different classes. For example, a rule can say that a secretary typically only uses office productivity programs such as web browser, email, calendar, and Microsoft Office. These rules are quite robust and flexible because it's relatively easy to update and improve them. On the other hand, these approaches rely on manual efforts by the experts, and the experts must have very good knowledge of the data. Which of the following describes the statistical approach, versus the knowledge based approach? First, any action that does not fit the normal behavior profile is considered an attack. Since it talks about behavior profile, this is the statistical approach. Second, this means that any action that's not classified as one of the normal behaviors according to set of rules is considered to be an attack. So, this is the knowledge-based approach. Let's discuss the machine learning based approaches. Machine learning approaches can automatically build a model using the labeled normal training data. That is, a machine learning algorithm fixes input examples of normal data and outputs a model. That is then able to classify subsequently observed data as either normal or anormalous. Machine learning base models can automatically handle small changes or variations in the observed data. It can also capture the inter-dependencies or the deeper connections between the measures of the observed data. On the other hand for these approaches to be effective the normal training data must be representative of normal behaviors otherwise when we apply the model to detection there could be a lot of false positives. In addition in the training phase it required a lot of data and a lot of of power. However, once the model is produced subsequent analysis is typically, fairly efficient. A variety of machine learning algorithms can be used for intrusion detection. These include Bayesian networks. Bayesian networks encode the conditional probabilities among observed events. For example, how likely an email is sent by a user if the current time is 2:00 AM. If a low probability activity takes place it is an anomaly. Markov models, a Markov model is a set of states that are connected by transitional probabilities. For example, Markov models can be used to model legitimate website names because, the traditional probabilities from one letter to the next should be similar to that of real dictionary words because, users need to type the website names. On the other hand, a randomly spelled website name is anomalous and may be used by botnets for C and C, or command control, because bots don't have to type the website names. Neural networks. A neural network simulates how human brain perform reasoning and learning. They are one of the most powerful machine learning approaches. Clustering and outlier detection. Clustering groups the observed data into clusters based on some similarity or distance measure. And then identify subsequently observed data as either belonging to a cluster or as an outlier. For example, traffic from the internal network to the company's internal web server have common characteristics that can be grouped into clusters based on the webpages visited. On the other hand, an attack may access data on the web server that's rarely visited, which makes it an outlier. Let's take a moment to think about the machine learning base approaches to intrusion detection. Which of these statements best describes the machine learning base approach for intrusion detection? Is it, detecting new and novel attacks? Or, detecting attacks similar to past attacks? Acute limitation of Anomaly Detection is that it only uses normal or legitimate data in training. That is, there's no intrusion data involved. As a result, when an Anomaly Detection model detects an anomaly, it may not be limited to an intrusion. Here's an example of a very simple anomaly detection approach. First, it establishes the normal statistical runtime profile of a program. For example, in terms of CPU utilization, memory size, etc. This can be accomplished by running the program many times. And at each time, record the values of these measurements and then compute the means and variance. Once the profile or detection model is built, when the IDS observes that when the program is run, it's measures deviate from the means beyond the allowed thresholds or the variance. Meaning that the values are outside of their normal ranges. The ideas outputs an alert. Again, the main drawback of anomaly detection approach is that you can produce relatively high false positive rate because an anomaly can just be a new or observed normal activity. Now let's discuss misuse or signature detection. These techniques detect intrusions by looking at activity data, seeing if there's a match of known intrusion patterns, and if there's a match this activity is intrusion. Otherwise it is normal. One of the issues with anomaly detection is that a system must learn what is normal behavior. And in the training phase, when it is learning what's normal behavior, the network is vulnerable to attack. Meaning that before the IDS is deployed, the network is vulnerable. What can we do about this? This weakness can be mitigated by using a system that checks for normal attacks, such as a firewall. Many misused detection approaches are signature based. A signature is essentially a known pattern of malicious activity data. For example, this can be option settings of flags in a packet header, or it can be a particular string in the packet pay load. The set of signatures needs to be as large as possible in order to have a high detection rate and a low false-positive rate. Signature-based approaches are most widely used in anti-virus software and network intrusion detection systems. Signatures are typically very easy to understand. And symmetry matching is very efficient. Therefore, symmetry based approaches are widely used. On the other hand, every time a new malware or a new attack method appears, significant menu effort must be spent in order to create new signatures. In addition, these approaches can not detect zero-day attacks, because these attacks are new and they do not have known signatures yet. Perhaps, you do not think that it is important to detect zero day attacks. There are buyers of zero day attack information, can you guess some of the buyers? Is Apple a buyer, Google a buyer, Microsoft, or the US Government? The answer is, that they're all buyers of zero day attack information. For example, a zero day vulnerability in the Linux operating system was sold for $50,000. In addition to signature based approaches, a misused detection system can also use a more sophisticated Rule-Based Approach. A Rule-Based Approach uses rules to represent known intrusions. These rules, typically match multiple signatures or patterns in the activity data. These rules are not only specific to the known intrusions, they can also be specific to the target of protected network. Because the same intrusion may leave different evidence in a different network depending on the network set ups. SNORT is a widely known example of a Rule-Based Network Intrusion Detection System. Here's a simple example of misuse detection approach. The IDS matches the observer activities using a set of text signatures or patterns. If there's a match, the IDS outputs an alert. For example, here is the signature of the so called land attack. That is, the source IP, it's the same as destination IP and the source port is the same as destination port. A machine that received this packet may crash. Again, such a simple approach cannot detect new attacks because they don't have signatures. Let's check our understanding of intrusion detection. Here's a list of attacks that an intrusion detection system can detect. Match the definition of the attack with its name. First, an attacker sends various kinds of packets to probe a system or network for vulnerability that can be exploited. This is scanning the network in order to find weaknesses for attacks. Second, attempts to slow down or completely shut down a target, so is to disrupt the service for legitimate users. Disrupting the service is the same as Denial of Service. Third, an attacker gains an unauthorized control of a system. That is to say, the attacker has penetrated into the system. Now let's discuss some details of the systems and deployment aspects of intrusion detection. An IDS typically performs what we call passive monitoring. That is, it is recording and analyzing data about systems and network activities while these activities continue to take place. Only when the IDS outputs an alert and the response policy dictates a direct intervention, such as blocking a connection or terminating a program, are the activities affected. An IDS can be deployed at the perimeter of a network, or subnet, to monitor traffic going in and out of the network. Such an IDS is a network IDS. It typically uses a packet capturing tool, such as the libpcap tool to obtain network traffic data. The packet data contains the complete information about network connections. For example, if a user connects to a website using his browser, the packet data will contain all the TCP handshake information between the browser and the web server, and all the URL requests from the browser and the return page contents from the server. That is, by examining the packet data, the IDS has all the data sent and received by the user's browser. If the user's machine is infected by a bot malware, for example, any attempts to connect to a website for command and control, the network IDS will be able to see that the traffic looks like CNC activities and output an alert. An IDS can be deployed to monitor a network or subnet. It monitors traffic in real or close to real time so that it can react to intrusions in a timely manner. It can analyze traffic in multiple layers of the network stack. A network IDS can include a number of sensors. For example, each of them is deployed to monitor a subnet. In such case, it may include one or more serves in the backend that manages these censors and interface with a human analyst. Typically, a censor monitors traffic of a subnet and reports its finding to the backend server. Then, the backend management server can correlate evidence across multiple sensors to provide protection or detection of the whole network. An IDS can also be deployed in an end host. And such an IDS is a host-based IDS. It can use a variety of data on system activities. For example, most host-paced IDS use ptrace to obtain the system calls made by the program to monitor the behaviors of the program. System called data is very useful to security monitoring because whenever a program requests a resource such as memory allocation, access to the file system, networks, and IO devices, it needs to make a system call to the operating system because the operating system manages the system resources. That is, most of the interesting or useful activities by program are carried out through system calls. For example if the users browser receives a page with a malicious JavaScript that is able to break the protection in the browser and attempts to overwrite the Windows registry file, the IDS will observe a write system call to the registry file. And can decide that this is an anomaly. One issue, with network IDS, in particular, for large network, is that it may produce a large number of alerts of possible intrusions, and these alerts need to be examined by the systems administrator. How do we reduce the impact of excessive reporting on an administrator? One company used approach is to prioritize the alerts by adding a security level that is associated with each report so that the systems admin can focus on the top priority alerts first. One type of network IDS configuration, is inline sensors. The primary motivation for using inline sensors is to enable them to block an attack, when an attack is detected. That is, in such a case, an inline sensor performs both intrusion detection and intrusion prevention. And of course, for an inline sensor to be effective, it must be placed at a network point where traffic must pass through it. We can deploy an inline sensor as a combination of network ideas and a firewall, in a single piece of hardware. Or, we can deploy the inline sensor as a stand alone, inline network IDS. The more common deployment strategy for network IDS is to deploy them as passive sensors. A passive sensor only takes a copy of the traffic. That is, the traffic continues to reach it's destination without passing through the device. Therefore, from the point of view of network performance, a passive sensor does not add any overhead to network traffic. This illustrates a typical passive sensor configuration. The sensor connects to the network transmission medium, such as a ethernet cable, through a direct physical tap. The tap provides the sensor with a copy of all network traffic being carried by the medium. The network interface card for this tab usually does not have an IP address configured for it. Therefore, all traffic into this network interface card is simply collected with no protocol interaction with the network. The sensor has a second network interface card that connects to the network with an IP address so that it can communicate with a backhand management server. Let's compare firewalls with IDS. A network IDS performs passive monitoring. That is, while the IDS is copying and analyzing the network traffic, the traffic is continuing to reach its destination. Traffic analysis can take a lot of computing power, and therefore the IDS can be overloaded by large body morph traffic. When an IDS is overloaded, it cannot detect intrusion in a timely manner. That is, it fails to adequately protect a network. We call this situation fail open, meaning that when the IDS fails, the network is open to intrusions. On the other hand, a firewall performs active filtering. That is, all traffic must pass through the firewall and the firewall performs relatively simpler and more efficient analysis. However, it can still be overloaded by large volume of traffic. When this happens, it will simply not let the traffic go through. We call this fail close, meaning that when a firewall fails, the internal network is closed to the external network, and it is safe. Now let's do a quiz on IDS. Decide whether each statement is true or false. The first statement, intrusion detection is based on the assumption that the behavior of the intruder differs from that of a legitimate user in ways that can be quantified. This is true. This is the primary assumption of IDS. The second statement, the primary purpose of an IDS is to detect intrusions, log suspicious events and send alerts. This is true statement, because these are the basic functions of an IDS. The third statement, signature-based approaches attempt to define normal, or expected, behavior, whereas anomaly approaches attempt to define proper behavior. This is false, because a signature based approach is typically used to represent known intrusion patterns. The fourth statement, a network IDS sensor monitors a copy of network traffic. The actual traffic does not pass through the device. This is true because a network ID typically performs passive monitoring by copying the network traffic. The last statement. Network-based intrusion detection makes use of signature detection and anomaly detection. This is true. You can indeed use both approaches. Let's discuss more about network IDS deployment. Here's an example enterprise network configuration. The internal network has multiple subnets. And the enterprise has public-facing services, such as a public web server. Recall our lecture on firewalls. We typically want to place an external firewall to protect the entire enterprise network. In addition, we want to protect the internal network from the public-facing servers. These servers are put in what we call a DMZ and we use internal firewalls to monitor traffic between the internal subnet and a DMZ. The internal firewalls also monitor traffic between its subnets. So that's the deployment of firewalls, but what about IDS? A common location for an IDS sensor is just inside the external firewall. This position has a number of advantages. Obviously, you can see attacks from the outside world. By comparing the logs of the firewall and the IDS, we can also find out whether the firewall had missed an attack that it should have prevented. The IDS at this location can also detect attacks that are targeted at the public facing servers. In addition, because it can analyze all outgoing traffic of the entire enterprise network, it can also detect attacks from a compromised server, either from DMZ or the internal network. A network IDS can also be placed between the external network and the Internet. The main advantage of this location is that the network IDS can see all attempted attacks to the enterprise network, including those attacks that have been filtered by the firewalls. For example, if the firewall is overloaded, you would not only drop the incoming packet, it may not even have resource to log this packet. But an IDS at this location can see the packet and log it. Therefore, the IDS can see all attempted attacks. In addition to deploying network IDS at the perimeter, we can also deploy a network IDS to protect a subnet or set of servers. A network IDS at this location can perform more detailed analysis of traffic data. Because compared with a network IDS at the perimeter, it has smaller amount of traffic volume, due to the fact that it only monitors traffic to a subnet and the servers. In addition, it can also detect intrusions from inside the network. In addition to protecting the servers, a network IDS can also be placed to protect the workstations or networks of important personnels or departments. A network IDS at this location can focus on targeted attacks, for example, attacks that are targeted at financial transaction systems. Compared with an IDS at the network perimeter, which must examine traffic to the whole network, an IDS at this location can instead focus on traffic to these high-value systems. Let's do a quiz on network IDS deployment. Which of these are good practice? The first statement, set the IDS level to the highest sensitivity to detect every attack. This may appear to be a good idea, but in practice, this may lead to a large number of false alarms. Second, monitor both outbound and inbound traffic. This is a good idea. Because there will be a tech traffic in both directions. Third, use a shared network resource to gather NIDS data. This is not a good idea, because an attacker can disable the IDS or modify the alerts that sent. Fourth, NIDS sensors are not turnkey solutions. System admins must interpret alerts. This is true, because network IDS can produce false positives. Therefore, the system admins must interpret the alerts and take the appropriate actions. Let's discuss a network ideas example, SNORT. SNORT is open source, very easy to configure and very efficient. SNORT can be easily deployed on most nodes of a network, including end host, server, or even a router. It uses a small amount of memory and processor time. SNORT is very easy to learn, and very easily configured by sys admins. SNORT can perform real time packet captures, particle analysis, and content searching on the packet. SNORT can detect a variety of intrusions based on the rules that are configured by a sys admin. In fact, there's a community who maintains a very large set of SNORT rules that can be further configured by sys admin for his network. The SNORT system has four logical components. The packet decoder possesses each capture packet to identify the protocol headers at it's data link network transport and application layers. The detection engine does the actual work of intrusion detection. The detection engine checks each packet against a set of rules. That is, each packet is checked against all rules to determine if the packet matches the characters defined by a rule. A rule that matches the decoded packet would trigger the action specified by the rule. If no rule matches the packet, the detection engine will discard the packet. For each packet that matches a rule, the rule specifies what login or alerting options are to be taken. When the login selection is selected, the logger stores the detected packet in human readable format. This can then use the log files for later analysis. For each detected packet, an alert can be sent. An alert can be sent to a file, a database, or an email, etc. SNORT can be configured as in-line or passive. In the passive mode, it simply copies and monitors traffic, and the traffic does not pass through SNORT. That is, with the passive mode, SNORT is configured for intrusion detection. Snort uses a very simple and flexible rule definition language. Each rule consists of a rule header and a number of options. The options are the places where we can specify the intrusion detection logic. There are four main categories of rule options. The first is meta data. This does not actually affect intrusion detection because it contains information such as revision number to the rule. Payload, this is where we can specify the logic to examine the packet payload. Non-payload, this is where we specify the logic to examine the packet headers. Post-detection, this is where we can specify triggers, such as storing the packet information in a table so that we can correlate it with other packets. The most important field in a Snort rule header is the action field. It tells Snort what to do when it finds a packet that matches the rule. Here are the possible actions. The last three are only applicable when Snort is configured as an inline sensor. To summarize, a Snort rule has a header and multiple options. Each option consists of an option keyword which defines the option, followed by arguments which specify the details of the option. Here's an example of Snort rule. Typically the root user account is used only for specific privileged operations such as picking out file systems and setting up subnetworks. It is quite uncommon to send email using the root account. And such an event should trigger an alert. Here's an example of how Snort can capture this event. It looks for traffic to the SMTP port on any host in the /24 network, and checks if the content of the email contains, mail from: root, which indicates that a root user is sending email. It then sends an alert with the following message, root users attempt to send email. The content keyword here is one of the more important features of Snort. It allows the sys admin to set rules that search for specific content in the packet payload and trigger response based on that data. Let's do a quiz on SNORT. The question is, who can write rules for SNORT? Can users of SNORT write rules? Yes. Can the community who maintains a large set of SNORT rules? Of course. Can the security experts write rules? Of course. As an open source software, everyone can write rules for SNORT. The rules can then be submitted and improved by security experts, and shared with the community. Honeypots is another component of the intrusion detection technologies. Honeypots are decoy systems to attract the attackers away from the critical systems. By diverting attackers from valuable systems to honeypots, we can observe what the attackers are trying to do to our systems and networks. And based on that information, we can develop strategies to respond to the attacks. Typically a honeypot system is filled with fabricated information to make it appear to be a valuable system on the network. A honeypot system is instrumented with monitors and event loggers so that any access, or any activity on the honeypot system is logged. In order to attract attackers to a honeypot and keep them there, so that we can gather more information about attacks. Any attack against a honeypot is made to seem successful. Most importantly, a honeypot is not a real system used by any real user. Therefore any access to honeypot is not legitimate. Most likely, any inbound connection to honeypot is a network scan or direct attack. In addition, any outbound traffic from the honey pot means that they system is most likely compromised. Honeypot can be low or high interaction. A low interaction honeypot typically, emulates some network services, such as the web server. For example, you can speak the HTTP protocol. On the other hand, it is not a full version of the service. For example, the emulated web server does not have all the web content and server side programs. A low interaction honeypot is typically sufficient to detect network skin and probe and imminent attacks. On the other hand, a sophisticated attacker may realize that these services are not full version and probably are not real. A high interaction honeypot, essentially replicates what what a real server or work station has in terms of operating systems, services and applications. In other words, they look really realistic and they can be deployed alongside with the real servers and work stations. Since a high induction honeypot mimics a real server and workstation, an attacker may be attacking it for a long time without knowing it is a honeypot. Therefore, we can learn more about the attacks. On the other hand, it is also quite challenging to make a honeypot look like a real server and workstation. For example, we must emulate user activities and never traffic on honeypot and this requires a significant amount of programming effort as well as data storage. Honeypots can be deployed in a variety of locations on a network. A honeypot outside the external firewall is useful for tracking attempts to scan or attack the internal network. The main advantages of placing the honeypot at this location are that. First of all, it does not have any side effect. Second, since it attracts and traps attacks to the honey pot, it reduces the amount of traffic, in particular the attack traffic to the firewall. Therefore, it reduces the amount of alerts produced by the external firewall. On the other hand, honeypot at this location does not trap internal attackers. A honeypot can also be placed in a DMZ to trap attacks to the public facing service. On the other hand, a honeypot at this location may not be able to trap interesting attacks. This is because a DMZ is typically not fully accessible. That is, other than the well defined public facing services, no other services are supposed to be available in DMZ. That is, if an attacker is attempting to access the honeypot. And the service is not one of these well-defined, public facing services, the firewall is going to block the traffic. What if you say, let the firewall allow the traffic to the honeypot. But this would mean that we're opening up the firewall. And this is a security risk. We can also place the honeypot in the internal network alongside with servers and workstations. The main advantages here are that it can catch internal attacks. It can also detect a misconfigured firewall that forwards impermissible traffic from the internet to the internal network. On the other hand, unless we can completely trap the attacker within the honeypot. The attack may be able to reach other internal systems from the honeypot. In addition, in order to continue to attract and trap the attackers to the honeypot, we must allow his attack traffic from the internet to their honeypot. This means that we must open up the firewall to allow the attack traffic to come from the Internet to the internal network, and this carries a huge security risk. And now let's do quiz on Honeypot and network intrusion detection. Decide whether each statement is true or false. First, a common location for a network intrusion detection system sensor is just inside the external firewall. This is true. This is a very typical deployment strategy of network IDS. Second, a Honeypot can be a workstation that a user uses for work. This is false, because a Honeypot is not a real system used by any real user. Third, there's no benefit of deploying a network IDS or Honeypot outside of the external firewall. This is false. Using a network IDS or Honeypot outside of the external firewall will allow us to see what attacks are coming from Internet to the enterprise network. In the case of Honeypot, because attacks are trapped in the Honeypot, it reduces the amount of traffic that the firewall has to process. In other words, the firewall does not need to produce as many alerts. How do we evaluate an intrusion detection system? We typically use accuracy metrics to measure the detection algorithm. We use detection rate or true positive rate to measure how well an IDS can detect intrusions. That is, given that there is an intrusion, how likely would the IDS correctly output an alert? We can also use force negative rate. That is how many intrusions did we miss? Another aspect of detection accuracy is the false alarm rate. That is, given that there's no intrusion, how likely is the IDS is going to falsely output an alert? We can also use true negative rate. That is, how likely normal activities are currently classified as normal? And if you are a sysadmin, you may want to know about the Bayesian detection rate of an IDS. That is, given that the IDS has already produced an alert. How likely is it that an intrusion actually occurs in your network? We can more formally summarize these metrics. Here, we use A to represent alarm or positive. I to represent intrusion. Detection rate or true positive rate is the probability that given there's an intrusion, how likely the IDS will produce an alert. And false negative rate is 1 minus true positive rate. False alarm rate is the probability that, given that there's no intrusion. Not I, meaning it's normal. How likely an IDS would incorrectly output an alert? And true negative rate is 1 minus false alarm rate. Bayesian detection rate is the probability that given there's an IDS alert, how likely there's likely an intrusion. In addition to the detection algorithms, you can also evaluate IDS in terms of it's system architecture. We want the IDS to be scalable. Meaning that, it can function at high speed networks. We also want the IDS to be resilient to attacks. Meaning that, it is not easily disabled by attacks that target the IDS. Let's discuss more about the Bayesian Detection Rate. We can use the base theorem to expand this. And we got this formula here P(I) is the prior probability of attacks. Meaning this is the probability of intrusion evidences in the data. An intuitive example is that on a typical day, what's the percentage of packets that contain intrusion activities in our network. There's an interesting phenomenon about Bayesian detection rate called the base-rate fallacy. Even if the false alarm rate is very low, as long as it is not zero, then the Bayesian detection rate is still low even if the base rate is also very low. For example, using the formula in the previous slide, if we plug in these numbers, meaning the detachment rate is 100%, false alarm rate is 10 to the -5, and the base rate is 2 times 10 to the -5, then the Bayesian Detection Rate is only 66%. In other words, one-third of the time when the ideas produces an alert, there is no intrusion. If you look at these numbers more carefully, 100% detection rate is perfect. False alarm rate of 10 to the -5 is also great. This is not 0, but is very, very low. So you may ask, is this low base rate realistic? This is 2 x 10 to the -5. It depends on where do you measure the base rate. For example, if you measure base rate at the network packet level meaning that, the number of packets that contain intrusion activities which can be hundreds or thousands to a total number of packets in the network which can be tens and hundreds of millions. Then this base-rate can be quite realistic. So the base-rate falicy says that, as long as the false alarm rates' not zero, then when the IDS produces an alert the probability that an intrusion has actually occurred is also low. So how do we address this problem? We can reduce the false alarm rate to be zero, or as much as possible. In fact, that's what the vendors of IDS have been trying to do. Or we can deploy the IDS to the appropriate layer so that, at that layer, the base rate is sufficiently high. Modern day ideas use a heirarchal architecture to achieve this. We can also use multiple independent models. This is similar to medical diagnosis where multiple tests are used to reduce the overall false positive rate and increase the base indication rate. With a Bayesian detection rate and base rate fallacy in mind, lets discuss the system architecture of network IDS. First, typically the volume of packet data in the network can be huge. Which means the base rate at the packet level is typically low. For example, there can be tens of millions of packets per day in a network, but only a few involved the intrusion activities. Therefore, according to the base rate fallacy, if we apply detection algorithms at the packet level, this may result in very low Bayesian detection rate. Instead, we should apply detection models to data that has higher base rate. This can be accomplished using hierarchical architecture. First, we can apply filters to the packet data. For example, by inserting libpcap to capture only packets to certain services. Second, the event engine analyses the filtered packet data, and summarizes them into security related events such as failed log in's. Finally, detection models are applied to the security related event data. As we can see, the volume of data is decreased first by a packet filter and then the event engine. Therefore, as long as we can keep the intrusion evidence in the event data the base rate is going to be a lot higher than the original packet data. As a result the IDS model applied to the event data will yield a higher Bayesian detection rate. Was true in IDS should in order to improve detection performance. Check any statement that is true. First, reduce false alarm rate, while detecting as many intrusions as possible. This is true. Obviously, we want to detect as many intrusion as possible. We also want to reduce the false alarm rate so that we don't burden the systems with false alarms. Second, apply detection models at all unfiltered packet data directly. This is false, because most likely, the base rate at this level is very low. Therefore, the IDS will have a low Bayesian detection rate. Third, apply detection models at processed event data that has higher base rate. This is true, because as long as we can keep the intrusion evidence in the event data, the event data is gong to have a higher base rate and therefore, the ideas will have a higher Bayesian detection rate. Now let's look at how an attacker can defeat the IDS so that his attack can go undetected. We call that an IDS performs passive monitoring. That is, a network IDS takes a copy of the network traffic and analyzes it while the traffic is reaching the end host. Therefore, in order for the IDS to detect the intrusion that's happening at the end host, he must see the same traffic as the end host. However, this is not always the case. An attacker can exploit this in order to evade the IDS. The reason that the IDS and the end host are seeing different traffic is because they're using two different operating systems that process traffic in different ways. In particular, TCP/IP protocol specifications have ambiguities that lead to different implementations in different operating systems. As a result, if the IDS runs on Unix and the end host runs on Windows, they may not process certain packets exactly the same way. For example, options such as time to live or error conditions associated with fragments and checksums are handled in different ways in different operating systems. By exploiting these evidences, the attacker hopes that the IDS would not see the attack traffic, yet the end host will be affected by the attack traffic as intended by the attacker. For example, attacker can insert data into the packet stream, to cause the IDS to miss detecting the attack. For example, by including a packet with bad checksum value, the end host may reject this packet, and yet, the IDS may accept it. As a result, the end host gets the attack, and yet, the IDS misses detecting it. Here's an illustration of the Insertion Attack. For example, the attacker sends these packets. Although out of order, both of the IDS and the end host will assemble them according to the sequence numbers. One of the packets, X, has a bad checksum value. The IDS will accept it. Therefore, the IDS sees ATXTACK. On the other hand, the end host rejects this packet with a bad checksum value. In other words, the end host gets attacked by the traffic, yet, the IDS misses the attack. As another example, the attacker can also hide part of the attack and cause the IDS to miss detecting the attack. For example, by sending fragments that overlap, the IDS may discard a fragment that overlaps with a previous fragment. While the end host may accept both. Again, the result is that, the IDS will miss the attack. Here's an illustration of this evasion attack. Again, the attackers sends a packet although out of order, both the end host and the IDS will assemble them in order. However, the two As here are overlapping fragments. The IDS drops the second A, so therefore the IDS only sees A, T, T, C, K. On the other hand, the end-host accepts both fragments even though they overlap, therefore the end-host sees A, T, T, A, C, K. In other words, the end-host gets attacked by the traffic, yet the IDS misses the attack. Attackers can also use denial of service attacks to disrupt the network intrusion detection process. Similar to denial of service attacks on a network server such as a web server, an attacker can send a lot of traffic to the IDS to process. The result is resource exhaustions, for example, in CPU memory and network bandwidth. In other words, the network IDS may not be able to analyze traffic and such traffic may contain actual attacks. That is, the attacker can first denial service the IDS and then launch the real attack. Another attack approach is to abuse the reactive nature of intrusion detection. The intrusion detection process is reactive because when the IDS outputs an alert, the security admin must analyze the alert. Therefore, the attacker can send a lot of traffic that would trigger the alerts, for example, by crafting packets that on purpose contain signatures of attacks. The goal is to overwhelm the response system and the security admins. And then the attacker can send the real attack traffic that even if it triggers an alert, the alert will not be acting upon in time. That is because the security admins are so busy analyzing alerts of fake attack, the real attack is not analyzed until it's too late. Attackers can also use analysis of his attacks to disrupt the network intrusion detection process. In addition to intrusion detection systems, there's also intrusion prevention systems. Instead of simply sending alerts like and IDS. And IDS would try to block the attack when it detects malicious activities. For example, it can block network traffic that involves malicious activities. Similar to an IDS, an IPS can be deployed at the end host and the network perimeter or a combination of different locations. An IPS also uses the similar detection algorithms of an IDS. For example, it can use anomaly detection algorithm to detect abnormal behavior. And stop such abnormal behavior. And this is the main difference between an IPS and a firewall. A firewall typically only use simple signatures of attacks to stop traffic. Whereas, an IPS can use very sophisticated detection algorithms. How can an attacker defeat an IDS? Check any statement that's true. First, send a huge amount of traffic. This is true. This can cause denial of service of the IDS and cause it to not be able to analyze traffic that contains attacks. Second, embed attack in packets that cause non-uniform processing by different operating systems, for example, bad checksum and overlapping fragments. This is true because the result of this is that the IDS is seeing different traffic as the end host, and as a result, the end host may be attacked by the traffic, yet the IDS will miss it. Third, this is true because this will result in a lot of alerts that need to be analyzed by the sysadmins. And when the sysadmins are overwhelmed, then the attacker can send his attack that although the attack is detected and an alert is produced, the sysadmin will not have time to look at the alert until it's too late. Fourth, send a packet that would trigger a buffer-overflow in the IDS code. This is true because the buffer-overflow is a typical exploit method used to attack a program. For example, the attacker can inject his own code using buffer-overflow into a program. In other words, if the attacker can buffer-overlow an IDS, that means the attacker can now control the IDS. The main intrusion detection approaches include anomaly detection and misuse detection. The main department strategies include Network IDS, IPS, and honeypots. True positive, and false positive are the main performance metrics. In the effect of false positive is highlighted by the base rate policy. IDS can be bypassed by insertion and evasion attacks. And it can be disabled by the now service attacks. Cryptography is the foundation of security. In this lesson, we will first introduce the basics of encryption as well as attacks on encryption schemes. We will highlight several historical and simple encryption schemes. We will then introduce three types of modern-day cryptography and how they are used in security. Encryption is the most often used cryptographic operation. It is a process of converting data into a form that is unintelligible to the unintended or unauthorized party. The authorized party can reverse this process. That is, converting the data into the intelligible form. Record the readable data, the plaintext and the unintelligible data, the ciphertext. So encryption, then, is the process of converting plaintext data into ciphertext, and decryption is the reverse operation. That is converting ciphertext into plaintext. Notice that, in order to perform decryption, the authorized party needs to have the decryption key. There's a one-to-one mapping between a plaintext and a server text, so that the decryption process can always get back the original plaintext. Encryption protects data confidentiality, because only the authorized party with the proper secret, we call it a key, can decrypt and read the data. Encryption also provides other security services. These include integrity checking, that is to make sure that there is no tampering of data. It can also assure authenticity of message. That is it can verify the authorship of the message. It can also provide authentication. That is, to make sure that the party that you are communicating with is not an imposter. We often call an encryption scheme a cipher. You may not have realized this, but encryption has been used for thousands of years. For example, there is an evidence that Ancient Egyptians used some sort of ciphers. And then there is the famous Caesar's cipher, as well as other similar schemes. There are several types of ciphers. Symmetric ciphers, range from ancient schemes to present day algorithms. Asymmetric ciphers are relatively new, only invented in the late 70s. Most security protocols now use both types of schemes. Typically, they first use asymmetric ciphers to authenticate both parties involved in the communication. And then establish and exchange a encryption key for the communication. Then they use symmetric ciphers, and the established encryption key to encrypt data and traffic. They can also use asymmetric ciphers to digitally sign the data, so that they can ensure data authenticity. Given that encryption or cryptography in general, plays such an essential role in security, we can only expect that attackers will always try to break an encryption scheme. Typically, the goal of such an attack is to uncover the plaintext from the ciphertext or to discover the encryption key. For example, an attacker may try to recover the plaintext from the ciphertext that is transmitted on the Internet. And you may ask, how can the attacker obtain such ciphertext that's transmitted on the Internet? The answer is, actually this is very feasible because an attacker can use a packet capturing tool to capture the traffic, and then extract the ciphertext in the traffic. And then from there, the attacker can attempt to recover the plaintext from the captured ciphertext. So as a rule of thumb, we should always assume that the ciphertext that we transmit over the Internet can be captured by an attacker. Alternatively, the attacker may try to discover the encryption key, so that he can then decrypt all the data encrypted using this key. That are several attack methods. The first simplest and yet the most inefficient way is to use brute-force, or to search blindly. For example, an attacker can try all possible keys, one by one, until the one that can decrypt the ciphertext properly into plaintext. And you may ask, how does the attacker know that the decryption with a key has worked properly? Typically, the attacker knows what the plaintext should look like. For example, if the plaintext is an English sentence, then only the correct key can decrypt the ciphertext into data that can be read as English. This method is very inefficient because the number of possible keys can be huge, and so brute-force or search blindly can take a long time to succeed. Another approach is to use cryptanalysis. Here, a attacker has some knowledge of the encryption algorithm and perhaps the characteristics of data, such as, distribution of certain letters or words. With such knowledge, the attacker can do a lot better than using brute-force to search the entire key space. Attackers can also exploit implementation or system's issues. For example, it was shown that by using side channel analysis, for example, by observing the power consumption used by a crypto system, an attacker can deduce values of certain bits of key, and therefore the attacker can significantly decrease the key space that he needs to search. Finally, let's not forget that the weakest link in security is the naive users. And they can be exploited using social engineering tricks. For example, an attacker can pretend to be a sysadmin who has just forgotten the key, and he called an unsuspected user for the encryption key to a system. Now let's do a quiz. Suppose the attacker can only use brute force to attack a crypto system. Then which of the following ways can be used to counter such attacks? Should we use a longer key? The answer is yes. Because a longer key length means more keys, which means the attacker has to search a lot more keys. Should we use a shorter key length? Obviously not. Should we use a more complex algorithm? The answer is no. Because the attacker is to use brute-force to blindly try the keys one by one. So, his search is independent of the complicity of the encryption algorithm. Should we use a harder to guess key? The answers no. Because the attackers is going to use brute-force again to search blindly. That is, he's going to try to key one by one, he's not going to guess the keys. So, the correct answer is to use a longer key length. Again, this would mean that there are more keys for attacker to try. Let's review some simple ciphers. The most famous one is Caesar's cipher. It works by mapping a letter to another letter by always the same amount of shift. For example, if A is mapped to D, that means the shift is three, and so the letter B is then mapped to E. The shift amount is the secret. Or the key of this encryption scheme. So an attacker only needs to try 26 possible keys. Now let's decipher this message using Caesar's cipher. Here, A is meant to D and B is meant to E. Again, the fixed shift amount is three. The answer, meaning the plain text, is information security. As you can see, this cipher is not difficult to solve, because the Caesar cipher shifted letters by three. On the other hand, this code was secure for Caesar, because most of his enemies at the time, were either illiterate, or would make the assumption that it was in a different language, that they do not understand anyway. So in this case, social engineering worked to the advantage of information security. Caesar's cipher is also called shift cipher, because each letter is map to another letter by fixed amount of shift. So if we represent letters as numbers, meaning A is 1, B is 2 and Z is 26. Then this encryption scheme or cipher can be represented as the letter, meaning the integer plus n, the fixed amount marked 26. Here the shift amount n is a secret, or the key. And obviously, there are only 26 possible keys. Therefore, it is easy to break Caesar's cipher because you only need to try 26 possible keys. A generalization of this scheme is to allow arbitrary mapping of one letter to another. That is, we no longer require that a letter is map to another letter by always the fix amount of shift. Now of course, we need to avoid that two letters are being mapped to the same letter. Demapping, meaning how each letter is map to another, it is now the secret key. Can you tell, how many possible keys are out there? Meaning, how many possible ways to map one letter to another. The answer is 26 factorial, because there are 26 factorial number of ways to map one letter to another. Please note that n factorial is much larger than 2 to the n. For example, 26 factorial is approximately 2 to the 88th, which is much larger than 2 to 26th. In other words, this is very huge number. If the attacker attempts to use brute force, or just blindly search all the possible keys, it's going to take him long time, because there are 2 to the 88th number of possible keys. So, what should the attacker do instead? Instead of trying all of the possible keys, an attacker can analyze the statistical frequencies of letters to break the schemes. For example, in English, the most frequently used letter is E. And if in a cipher text, the letter X is the most frequent, then there's a high probability that E is mapped to X. For substitution ciphers rather than trying all possible keys, we can use the frequency of letters. For example, here is the frequency distribution of English letters. Now let's do an exercise together. With the frequency distribution of English letters, can we figure out the plain text for this following cipher text? To start we notice that the letter Q here in the cipher text is the most frequent. And we know that in plain text English, the letter E is the most frequent. So, there's a high probability that Q is in fact the plain text letter E. Then we can look at the three-letter words, for example, here and here. Now we know that Q is in fact E. So both end in E, these two three-letter words. So most likely one of them would be the, T-H-E. The other one would be are, A-R-E. So here we're going to start to use the knowledge of English language. In other words, what are the common words or what are the legitimate words in English to help us uncover the plain text? For example, we can see if we say H is A and R is T, then we have a two letter word here, HR. In this case, if H is A and R is T, that would be at, which is a commonly used two letter word in English. And also, if H is A, then we have this three letter word that has two letters the same. So if H is A, then this can be all. So we can continue with this process by using our knowledge of the English language to uncover the plain text. Here we use both the frequency distribution of the English letters and our knowledge of English words to help us. If you continue with this process, you should be able to uncover this following plain text. We will meet in the middle of the library at noon, all arrangements are made. In practice, in addition to the frequency distribution of English letters, we can also consider the frequency of letter pairs, or even triples. Now try your hand at deciphering this message. You can use the link in the instructor's notes to see a list of the most commonly used words in the english language. When we try to decipher this cipher text, other than trying all possible 26 factorial number of keys, we can leverage our knowledge of the English language. In particular, the most common word in English is the, T-H-E. There are two three letter words here, but the odds are good that the word the is not at the end of a sentence, so most likely this would be the T-H-E. In other words, T was not to W, H was not to A, and E was not to S. Therefore this four letter word start with T H. >From here we can use some guess work and use our knowledge of the English language to determine which guess is most likely to be correct. So if you follow this process of combining the knowledge of the English language, in particular the most commonly used word in English and plus some guess work, you should be able to get this answer, this is the end. Here's a real example of a poly alphabetic substitution cipher. It is called the Vigenere cipher. Vigenere cipher has a clever way of representing possible mappings from one letter to another as a matrix. Here we encrypt the plaintext by processing one letter at a time across the columns. We use the letters in a key to tell us which row to look at the mapping. Now suppose we have a plain text, and the first three letters are A-T-T, and we have a key. In effect, the key stream should be as long as the plaintext. So this will be lemon, lemon, lemon, and go on. Here, the first three letters of the key stream are L-E-M. Now let's see how we can encrypt the plaintext according to the key. We encrypt the plaintext by processing the letters one at a time across the columns. The first letter is A, so we look at column A. We use the letter in the key, L to look at the row to decide the mapping of A. Here the mapping says L, which means in server text, A has been mapped to L. The next plaintext letter is T. So we look at column T and we use the letter in the key to look at the row to decide the mapping. The letter is E in the key. And so we look at column T and row E. And the mapping is X. This means that the cipher text of T is X. The next plaintext letter is T again. So look at column T again, but here the letter in the key is M, so we look at column M, and the mapping of column T, row M, is F. This means that the cipher text of plain text T is F, therefore for plain text A-T-T and key L-E-M, the corresponding cipher text is L-X-F. For a long time the Vigenere Cipher was thought to be unbreakable, but later some attacks were discovered that can break this cipher. Can you see what weaknesses can be exploited to break the Vigenere Cipher? [BLANK AUDIO] First it uses repeating key letters. As we have shown in the example, the key stream is as long as the plain text and the key letters will be repeated in a key stream. For example, the key stream can be lemon, lemon, lemon, and so on. Because the key letters are repeated, think even a long plaintext message. The same letters may be mapped to the same cipher text, because they have the same repeated key letters. For example, if the plaintext letters A-T-T appear multiple times in the plaintext, and the key letters L-E-M are being repeated, then there high probability that the same cipher text, L-X-F will also appears multiple times in the cipher text. Then using knowledge of the English language or the context of the communication, the attacker can guess what other words that may be repeated multiple times. Therefore, by searching for the repeated cipher text words, the attacker can then compare the cipher text words and the plaintext words to uncover the key letters. So this is a weakness that can be exploited. Second, require security for the key, not the message. This is, in fact, a strength of the cipher. Third, the length of the key can be determined using frequency analysis. This is also a weakness that can be exploited. For example, using knowledge of the English language, the attacker can look at the cipher text and determine that the most frequently used three letter word is the, T-H-E, and then by looking at the distance between the two occurrences of there, the calculator can then guess the length of the key. Knowing the length of a key would help the attacker uncover the whole key instead of just a few key letters. So therefore, this is a weakness that can be exploited to break the Vigenere Cipher. We have discussed that in the encryption key should be a secret. What about the encryption algorithm itself? In general, we should keep the algorithm open so that it can be revealed and improved by the broader community. More importantly, we don't have to rely on the secrecy of the algorithm for security. In other words, we don't have to use obscurity for security. Therefore, in practice we should always use the widely known and deployed algorithms and standards. There are several types of cryptographic algorithms. The first is secret key cryptography. Here, the same key is used for encryption and decryption. In other words, the sender and the receiver of the confidential message, must use the same key. Another type of algorithm is public key cryptography. Here, there are two key components that are paired or linked together mathematically. The public component is used for encryption. And a private component is used for decryption. For example, Alice can use Bob's public key to encrypt a message, that only Bob can decrypt. Because only Bob has the corresponding private key that will decrypt the message properly. The private key component is also used for signing a message. And then, the public key component can be used to verify the signature. For example, Alice can send a message using her private key, and anyone knowing her public key can verify that. Only Alice can produce this proper signature. The third type of cryptographic algorithms is hash functions. Hash functions don't use keys. A hash function computes the hash, or the message digest of data of any size. The hash or the message digest is a fixed length output, typically, in the range of 128 to 512 bits. Hash functions are ,typically, useful authentication and protection of message authenticity and integrity. In order to provide these services, hash functions must satisfy these following properties. First, it should be relatively easy to compute a hash of a given message m. This would make hardware and software implementations practical. In other words, we want to be able to compute hash very efficiently. Second, for given hash value, it should be completely infeasible to find the original message m, such that, the hash of m is this given hash value. This so called one way property is very important. For example, suppose Alice wants to authenticate herself to Bob. Alice would hash a secret, that she shares with Bob, as with the current timestamp together. For example, she can concatenate the secret s, with the current timestamp together, and then, hash over the concatenated value, and send the hash value over to Bob. Bob would know that, this must be Alice, because he can check the current time stamp, and he has the share secret S. In other words, Bob can take the current time stamp concatenate with the shared secret s, and compute his own hash, and compare with the hash value with the received hash value, if they match, then he knows that he's been communicating with Alice. Now, because the hash value is transmitted over the Internet, and we must assume that an attacker can obtain any value that's transmitted over the Internet. That is, the attacker may be able to obtain this hash value. If the hash function is not one way, that means the attacker can reverse the hash function, and find out the input to the hash function, which includes the share secret s. If the attacker knows the share secret s, then the attacker can impersonate Alice to Bob, or Bob to Alice. So this is, an example, that demonstrates that the one way property of hash function is essential. Another property of hash function is that given a message m1, it should be computationally infeasible to find a different message m2, such that they have the exact same hash value. This is the weak collision resistant property. This is an essential property for message authenticity and integrity protection. Imagine the following situation. Alice wants to send a message, m, to Bob. And she wants to make sure that Bob knows that Alice is the real author of m. So Alice is going to send m, along with a sign hash of m, meaning that Alice would hash m and then, sign hash, using her private key. Now an attacker can look at the traffic on the Internet, and find that Alice is sending m, and assign hash to Bob. If the weak collision resistant property is not there, then the attacker may be able to find a different message m pi, such that the hash of m pi would be the same as the hash of m, therefore, although the attacker has changed the message from m to m pi, the signature will still match. And therefore, when Bob receives m pi, and the original signature of hash of m, Bob will not know that the message has been changed from m to m pi. This is a simple example to show that the weak collision of system property is essential to protect message authenticity and integrity. There's a stronger version of the collision resistant property. It says that it should be computationally infeasible to find different messages, m1 and m2, such that they have exactly the same hash value. This is a stronger property because it prevents the attacker from coming up with two different messages, that have the same hash value. Again, this property is essential for protecting message authenticity and integrity. For example, suppose Bob can write a IOU message, and send it to Alice to sign. Again, the way that Alice will sign it is to hash the message first, and then, sign the hash, using a private key. If Bob can find two different messages, that have the same hash value, one of which requires Alice to pay a small amount, and the other requires her to pay a very large amount. And the two different messages have exactly the same hash. Suppose Bob sends the message with a small amount for Alice to sign. After Bob receives Alice's signature on the message with the small amount, Bob can then go around and claim that Alice owes her a larger amount, because the two different messages, have the same hash. And the signatures would then be the same. It should be obvious that if a hash function satisfies the strong collision resistant property, it automatically satisfies the weak collision system property. The one way property of hash function is extremely useful for security. Recall that the one way property means that it is easy to compute a hash from the message, but it is impossible to find a message from the hash. You can think of it this way. You can make hashbrown from potatoes. But you can't make potatoes out of hashbrowns. Let's look at this example of using hash to verify password. Hash is particularly useful for password verification. We all know that when we authenticate ourself to a system, we need to supply a password. Now let's look under the hood. What happens? For example, when a user supplies the password "candy," the system would take the input "candy" and hash it, and then compare the hash value with the stored hash of a password on the system. If they match, then the access is allowed. Otherwise, access is denied. That is, the clear text password candy is never stored on the system. Only the hash of the password is stored on the system. And then whenever the user types in a password to authenticate him or herself to the system, the input will be hashed. And then the hash of the input password will be compared with the stored hash of the password. You may ask, why not just store the clear text password on the system? Think about it, if you do that then anyone who gains access to the system can look at the clear text password and then impersonate the user. This can include attackers or even other users on a system. Therefore, we never store the plain text of the passwords on system, only the has values. Of course, if an attacker gains access to system, he can steal the hash of the password. Then what the attacker can do is guess possible passwords one by one and hash each guess password and compare the hash value with the stolen hash of the password. If they match, then the attacker knows that he had correctly guessed the password. Therefore, it is important for us to use a password that's very hard to guess. For example, we should not use any word in the dictionary. Now let's do a quiz. Since hash function is used to protect passwords let's think about what we can do to improve password security. First, use a one-way hash function. This is obvious, because if the hash function does not have one-way property. Then from the hash value an attacker can easily reverse the hash function, and find the plain text password. So, we have to use a one-way hash function to protect password security. Second, should not use the avalanche effect. The avalanche effect means that a slight change of the input may cause a large change of the output. In fact, we want our hash function to have this effect, so that two similar passwords will have very different hash values. With this in fact, even when the attacker has correctly guessed a password, he still has the same amount of difficulty to guess another similar password. Third, should only check to see that the hash function output is the same as stored output. This obviously is yes. Because password authentication should only check that the hash function output is the same as the stored value. Let's discuss symmetric encryption. Recall that in symmetric encryption, the same key is used for both encryption and decryption. The input is a plain text message. The encryption algorithm takes the plain text input, along with the encryption key, and processes the plain text using substitution and permutation to produce ciphertext. The decryption algorithm performs a reverse of the encryption process. It takes as input, the ciphertext, and the key, which is the same that was used for encryption, and it produced the original plain text. The most commonly used symmetric encryption algorithms are the so called block ciphers. A block cipher takes as input a plaintext in fixed sized blocks, and it produces a block of ciphertext of equal size, for each plain text block. To process a longer plain text, we can first break the plain text into a series of fixed size blocks, and then, apply block ciphers to each block. The most important symmetric algorithms, all of which are block ciphers, are the data encryption standard, or DES, and the advanced encryption standard or AES. We will present more technical details of these algorithms in a later lecture. For now, we note that these two different algorithms have different block sizes and key length. We call that, a longer key length means that there are more keys. As a result, a brute force attack has to do more work to try all the keys. This table shows how much time is required for brute force attack for various key lengths. As we can see in the table, DES user key length of 56. And a number of possible keys, is 2 to the 56th. If a single personal computer is used to try all the possible keys, it takes about a year. If multiple PCs, for example, tens of thousands of PCs, work in parallel, then the time is drastically shortened. And today's super computers should be able to try all the possible keys, using DES, in an hour. Key sizes of 128 bits or greater are effectively unbreakable using, simply, brute force methods. For example, even with a super computer, it would take 10 to 17 years, to try all the possible keys using AES. Now let's do a quiz. There are various types of attacks on symmetric ciphers. Given the following attack methods, select the correct definition for each one. The first one, known-Plaintext attacks. The answer is d. In this attack, a specific known plaintext is compared to its ciphertext. Second, chosen-Plaintext attacks. The answer is c. In this attack method, the attacker can choose randomly some plaintext and obtain the corresponding ciphertext. Third, differential cryptanalysis. The answer is b. In this attack, the attacker controls a changes of the input, meaning the input plaintext. And then observe the changes of the output, meaning the output ciphertext. Four, linear cryptanalysis. The answer is a. In this attack method, the attacker models the relationship between plaintext, ciphertext, and the key, using linear equations. And then he uses the known ciphertext, plaintext pair to derive the key bits. Given a plaintext input, the encryption algorithm performs transformation to produce the corresponding ciphertext. The decryption algorithm reverses the encryption process. That is, it takes as input the ciphertext and produces the original plaintext. Instead of using a single key, as in the symmetric encryption here, asymmetric encryption use a pair of keys. One is used for encryption and the other is used for decryption. The two keys are paired mathematically together. That is, if you use a key for encryption, only the corresponding paired key can decrypt a message. Let's look at the essential steps in asymmetric encryption. First, each user generates a pair of keys that are paired together mathematically. One is the private key, the other is the public key. The public key can be published, for example, on a user's website or on a public repository. The purpose is for everybody to know your public key, but a companion private key has to be kept as a secret. That is, only the user should know his or her own private key. Since public keys are meant to be known by others, we can imagine that in practice, a user such as Bob would have a collection of public keys of his friends. For example, in this case, Bob has the public keys of Alice, Ted, Mike and Joy. Now suppose Bob wants to send a private message to Alice. Bob will use Alice's public key, which is in his collection to encrypt a plaintext using a asymmetric encryption algorithm such as RSA and then transmit the ciphertext to Alice. It is mathematically guaranteed that only Alice can decrypt this ciphertext properly into plaintext, because only Alice has the corresponding companion private key. That is, if a public key is used to encrypt a plaintext into the ciphertext, only the companion corresponding public key can decrypt the ciphertext properly into the plaintext. Therefore, in this case, only Alice can see the plaintext. Now let's do a quiz on asymmetric encryption. Check all tasks where asymmetric encryption is better than other algorithms. First provide confidentiality of a message. This is typically not a strength of asymmetric encryption, because asymmetric encryption is much slower than symmetric encryption. And therefore let me try to protect the confidentiality of a message, we typically use symmetric encryption instead of using asymmetric encryption. Therefore, this is not a task that asymmetric encryption is good at. Second, securely distribute a session key. A session key is used to encrypt all messages during a session. For example, a web browsing session. We can use a session key to encrypt and provide confidentiality of all the communications within that session. This is a task that asymmetric encryption is better at. For example, if Alice and Bob wants to establish a session key between them. It is easy for Bob for example, to create a session key and then encrypt the session key using Alice's public key and send over to Alice. It is guaranteed that only Alice can decrypt a message and extract the session key, because only Alice has the corresponding private key. Therefore, this is a task that asymmetric encryption is good at. Third, scalability. Imagine we have N users and they need to communicate with each other securely. For example to have confidentiality in their communication. In this case, we need N square number of keys. Whereas for asymmetric encryption, regardless of how many users are involved. We only require that each user has one public key and one private key. Therefore, asymmetric encryption is much more scalable. The point of public key encryption is that the public key component is really public. That is, any user can send his or her public key to any other user or just broadcast it to the world. Although this approach is very convenient, it has a major weakness. That is, anyone can forge such a public announcement. Some user could pretend to be Bob, and send a public key to another user such as Alice, and tell Alice that this is Bob's public key. The result is that when Alice sends a private message to Bob saying she encrypts it using Bob's public key. But remember this Bob's public key is actually forged by the attacker. Then the message can be intercepted by the attacker, and can be read by the attacker. Now, at some point hopefully, Bob can discover that there's a forgery going on and a fake public key of his was being used. But then what can Bob do? Bob can send Alice another message saying that, hey, this is my real public key. But how could Alice tell? That is, how could Alice tell that the previous key was a forgery and this key, that Bob just sent, is real. The solution to this problem of public key forgery is to use a public key certificate. In essence, a certificate consists of Bob's public key and Bob's information such as the user ID, let's say his name and address and so on. The certificate authority's information. And the whole blog is signed using the certificate authority's private key. The certificate can also include other information, such as the period of validity of this certificate, that is, for how long this certificate is valid for this public key, say, one year. Now let's see how certificate is created, and how it is verified, and how it is being used to distribute public key. Suppose Bob wants the certificate authority CA to create a certificate for his public key. Bob would contact the CA and provide authentication information such keys driver's license and so on, and then he will send his public key to CA. The CA will then put his ID, his public key and other information such as the period of validity together and then hash it. And then the CA will use his private key to sign the hash. So that creates the certificate of Bob's public key. Now Bob can send this public key certificate to anybody such as Alice. When Alice receives this public key certificate, she can first extract the key types of information of Bob's idea, public key, and all the information. And then she will hash this data, and then Alice will also use the certificate authorities public key to decrypt the signature or verify the signature and compare these two hash values. If they match, that means this public key has been properly signed by the CA. In other words, this public key of Bob's has been validated by the CA. So this is how public key certificate works. Now of course, the underlying assumption is that. The CA is a trusted party by everybody involved. In practice, the CA is a well-known company such as Verisign, Microsoft, Google, or Apple, and the public keys are already stored in, for example your web browser. That is with these public keys already configured on your system, they can automatically validate public key certificates signed by these entities. Recall we discussed that public key encryption is typically used to establish a symmetric key for encrypting messages for say between Alice and Bob. That is, before Alice and Bob can communicate securely they would use public key's encryption to exchange a sure and secret key. Of course, Alice and Bob can first establish this key before Alice can send Bob the first message encrypted using this key. But with Public Key Encryption we can do better, meaning that we can do it in a more efficient way with so called digital envelopes. Alice can send Bob a message encrypted using a share key that she just created, and this share secret key itself is encrypted using Bob's public key, so that only Bob can decrypt and extract the share secret key. This is similar to the situation where only the intended recipient can open the sealed envelope. To illustrate, Alice first creates a symmetric key that she wants to share with Bob. She can encrypt a message using this share key, then she also uses Bob's public key to encrypt this share key. And then she can put the encrypted message and the encrypted key in envelope. Then she can put the encrypted message and the encrypted key together and send them to Bob. On the receiving end, Bob can use his private key to decrypt the encrypted share key. Once he gets a shared key, he can now decrypt the encrypted message and get a plain text message. Now let's do a quiz. Decide if each of these statements is true or false. First. Symmetric encryption can only be used to provide confidentiality. This is false because symmetric encryption can be used for other security services. For example it can be used for authentication. Suppose Alice and Bob share a secret. Then Alice can use the shared secret as the key and encrypt message using symmetric encryption algorithm and send the message to Bob to prove that she's Alice. Second, public-key encryption can be used to create digital signatures. This is true. Given a message we can first hash the message and then encrypt the message using our public-key. The encrypted hash value becomes the digital signature of this message. Third, cryptanalysis attacks try every possible key on a piece of ciphertext until an intelligible translation into plaintext is obtained. This is false, because what's described here is actually the brute force attack, or the spy research all possible keys until the ciphertext is translated into a plain text. Well as cryptanalytic attacks would use knowledge of the algorithm or the plain text such as the frequency of letters in order to break a scheme. In other words a cryptanalytic attack typically does not need to try every possible key. Fourth, the secret key is input to the encryption algorithm. This is true. An encryption algorithm takes as its input, the plain text and a key. Encryption schemes and attacks on encryption schemes have been around for thousands of years. The main attack approaches are brute force and cryptanalysis. One other use of cryptography includes some combination of hash, secret key cryptography, and public key cryptography. Symmetric encryption algorithms are typically block ciphers that take thick size input. In this lesson, we will first discuss the basic primitives of block ciphers, and then we will study the DES and AES algorithms. We will also discuss how to encrypt a large message that has multiple blocks of input data, and how to protect message integrity. Most symmetric encryption schemes are called block ciphers because they take a fixed length plain text block as input. We will discuss later how these schemes can be used to encrypt input text arbitrary length. Here is a very high level view of block ciphers. A plain text block of fixed length, say n, is encrypted using secret key, k, to produce a cipher attacks block of same length, n. The same secret key, k, is used to decrypt the cipher attack's block into the plain text block. The goal of encryption is to transform a plaintext into an intelligible form. That is, given that the attacker can obtain the ciphertext, we don't want the attacker to be able to learn about the plaintext. In order to accomplish this, we apply two principles. The first is called confusion, it is a way to obscure the relationship between a key and a ciphertext. That is, although the attacker can obtain the ciphertext, the attacker will not be able to find out the key. This is typically achieved with substitution. For example, in the generalized substitution ciphers that we have discussed, each letter can be mapped to any other letter. Confusion or substitution alone is not sufficient. For example, even when a letter can be mapped to any other letter, the attacker can use statistical analysis of data frequencies to break the scheme. Therefore, the second principle is diffusion. This means that the influence of one plaintext bit is spread over many bits in a ciphertext and the goal is to hide the statistical properties of the plaintext. For example, in stead of mapping an English letter to another English letter, we can map a letter to parts of many a bit letters. And if you do that, then the frequency distribution of English letters will not be very useful in cryptanalysis. We can achieve diffusion with permutations. Further, we need this combination to effect every bit of the ciphertext. Therefore, typically, a block cipher has multiple rounds where each round combines substitution and permutation. That is, the first round affects some parts of the ciphertext and the next round further propagates these effects into other parts of the ciphertext. Eventually, all bits of ciphertext are affected. Now let's do a quiz on the high level concepts of block ciphers. Select all correct answers to complete this statement. A block cipher should first use substitution to achieve confusion. Second, a block cipher should use permutation to achieve diffusion. Third, a block cipher should use a few rounds, each with a combination of substitution and permutation. Fourth, a block cipher should keep the algorithm secret. First, a block cipher should use substitution to achieve confusion. This is correct. Second, a block cipher should use permutation to achieve diffusion. This is correct. Third, a block cipher should use a few rounds, each with a combination of substitution and permutation. This is correct. These are the three high level properties of block ciphers that we have just discussed. Fourth, a block cipher should keep the algorithm secret. This is false. Because we should never keep the algorithm secret. We should only keep the key secret. Many researchers have analyzed a public or standard algorithm and improved its security. Therefore, a public or standard algorithm can be more secure than a secret algorithm. Therefore, we should use a public algorithm. A widely used symmetric encryption scheme is based on the data encryption standard, or DES. It was published in 1977 and standardized in 1979. In DES the key is 64 bit, which is 8 bites but for each bite, there's one parity bit. Therefore, the actual value in the key is only 56-bit. And the oppo cipher text is again a 64-bit block. Here's a high level view of DES. There are 16 rounds of operations. >From the origin of 56-bit key, 16 subkeys are generated. One for each round. The process of decryption with DES, is essentially the same as the encryption process. It works as follows. Use the cipher text as input to this. But then, for sub keys, they use in reverse order. That is use key 16 at the first round of decryption. Use key 15in the second round of decryption. And so and so forth. That is for decryption, we run the same algorithm. But only the keys in reverse order. We call that diffusion is one of the principles in encryption and it is typically achieved through permutation. In DES, permutation works by changing the positions of the bits. Recall that DES has 16 rounds of operations. Each DES round has the same operations and uses a different per round key. Each DES round takes as input the server text produced by the previous round. And outputs text for the next round. The input is divided into the left half and the right half. The output left half is just the right half of the input. The right half of the output is a result of the left half of the input and the output of the function. The mangler function takes this input, the 32 bit input right half, expands it to 48 bit, then with the 48 bit program key. Then use the s-boxes to substitute the 48 bit value into a 32 bit value. We can also use algebra to represent your in a death round. That is, the left half of output is the right half of the input. And the right half of the input is the result of the left half of input. And the result of a mangler function, which takes as input the right half of the input and the [INAUDIBLE] key. The [INAUDIBLE] algorithm has the so-called five style structure. That is, encryption and decryption differ only in key schedule. That is in decryption, we apply the same operations as in encryption, but only with the key sequence in reverse. That is, round one of decryption uses the key of the last round of encryption. And so on and so forth. We can take a closer look to verify that decryption really works by applying the same operations as an encryption, and only the key sequence in reverse. In particular, we only need to look at the decryption operations in the last round. The input to each round, for example, the input to the first round of decryption is actually R and N in reverse order, meaning the right half is on the left, and the left half is on the right. This is due to the swap operation at the end of the encryption process. For example, the input to the first round of decryption would be R16 and L16, because encryption has 16 rounds. And here, because we are using the key sequence in reverse in decryption, then the key using the first round of decryption is the key using the 16th round of encryption. Record that we can use algebra to represent the desk operations. If you use those algebra expressions, you will be able to verify that given this input in the decryption process you should be able to produce this output. I'll leave these as a exercise for you to do at home. Therefore, after 16 rounds of decryption process, the output would be R and L in plaintext. In the final swap operation at the end of the process would produce the correct plain text result L and R in the correct order. And this is how decryption in this works. XOR is a basic operation in cartography. For example, in a DES round, the right half of the input is extended to 8-bit and then XOR with the parent key. So let's do a quiz to review the XOR operations. Here the key is FA F2 in hacks, and the plain text is Hi. We first convert the characters in the plain text into their ASCII codes. So H is 48, I is 69. So therefore in Hex, this is 0,1, 0, 0, 1, 0, 0, 0, that's 48. And 69 is 0, 1, 1, 0, that's 6, and 1, 0, 0, 1, that's 9. And FA F2 in Hex, and F is 1,1, 1,1, A is 1, 0, 0, 1. F is again, 1, 1, 1, 1, and 2 is 0, 0, 1, 0. So what's the result of XOR Hi with this key, FA F2? Now, recall that in XOR, 0 XOR 0 is 0, 0 XOR 1 is 1, 1 XOR 0 is 1, 1 XOR 1 is 0. Therefore, this is 1, 1, 0, 1, 1. This is 1 XOR with 1, and that's 0. 1 XOR 1, that's 0. And this is 1, 1, 0, 0, 0, 1, 1, 0, 1, and that is the encrypted version of the plain text. The Mangler Function, performs the bulk of processing in a DES round. It takes the right half of the input, expands the 32 bit data into 48 bit data, and xor it with a param key. The result is that a 48 bit value is being substitute to a 32 bit value, and then the result is permutated. That is, the Mangler Function performs both substitution and permutation. Now let's take a look at the S-Boxes. A S-Box substitutes a six bit value with a four bit value, using a predefined table. Therefore, we have eight S-Boxes in this. That is we have eight predefined tables. Given a six bit value, the outer two bits are used to index into the rows of the table. And the middle four bits index into the columns of the table, and the value in a table entry is the output of the four bit value. Again, these tables are pre-defined, and there are eight such tables in this. Now, let's do a quiz on S-Box. For the given input, what is the output? Again, the input is 6 bit and the output would be 4 bit. We use the outer 2 bits to index into the rows of the table. The outer 2 bits is 01, so it's this row, 01. And the middle 4 bits are used to index into the columns of the table. And the middle 4 bits here are 1101. So the 1101, and that's this column. The entry in a table is the output. In this case, the entry is 1001, therefore 1001 is the output for bit value. Let's discuss the security of DES. First, we observe that the actual key value is only 56 bits. That means there are only a total of 2 to the 56 possible keys. This key space is too small, because an attacker can use brute force to find the correct key relatively, easily using today's computers. Another issue with DES is that the design criteria for the S-boxes have been kept secret. On one hand, they have been shown to be resistant to attacks that were published years after DES was published. This means that DES is quite secure. On the other hand, because the design criteria have been kept a secret, one have to wonder what are the designer of this actually knew about these attacks years before they were published. One could argue that these design criteria should have been published, so the researchers can review them and improve upon them. The main shortcoming of DES, is that it uses a 56 bit key. Which means the keyspace is relatively small. To overcome this, we can run this multiple times, each using a different key. The standard is to run DES three times. And this is called a triple DES. The standard is to run the encryption process, then decryption process, then the encryption process again. And correspondingly for decryption, this will mean to run the decryption process first, then the encryption process, then the decryption process again. And record that with DES, the decryption process is actually the same as the encryption process, only that we apply the keys in the reverse order. The advantage of using this order of operations, is that it supports multiple key lengths. In particular If key 1 is the same as key 3, then the result is a 112-bit DES. If all three keys are different, then the result is a 168-bit DES. If we set key 2 the same as key 1, then the the triple DES has in effect become a single DES with key three. This is useful for compatibility. For example a triple DES device can be configured to communicate with a single DES device, by simply setting key 2, the same as key 1. Now let's do a quiz on DES. Check all the statements that are true. First, to decrypt using DES, the same algorithm is used, but with per-round keys used in the reversed order. Second, with triple DES the effective key length can be 56, 112 and 168. Third, each round of DES contains both substitution and permutation operations. Fourth, the logics behind the S-boxes are well-known and verified. The first statement, to decrypt using DES, the same algorithm is used but with per-round keys used in the reversed order. This is true. Second, with Triple DES the effective key length can be 56, 112, and 168. This is true. Third, each round of DES contains both substitution and permutation operations. This is true. In particular, the mangler function has the S-boxes that performs substitution. And the mangler function also performs permutation. Fourth, the logics behind the S-boxes are well-known and verified. This is false, because the design criteria of S-boxes have been kept a secret. Now let's discuss another symmetric encryption algorithm, the advanced encryption standard, or AES. Recall that a major shortcoming of DES is that the key length is only 56 bit, and that is considered to be short. In other words, the key space is relatively small, and an attacker can use brute force to find the key using the power of today's computers. Of course, we can use [INAUDIBLE] to increase the key length, but that would mean running DES three times, and that's not the most efficient way of doing encryption. Therefore, there was a need for a new encryption algorithm that can take longer key length, but also be efficient. So in 1997, NIST put out a public call for a new encryption standard to replace DES. After a few rounds of submissions and reviews, AES was finalized, and it became a new standard, replacing DES. Like DES, AES is also a block cipher, whereas in DES, the input plaintext block is 64 bit, in AES it is 128 bit. In DES the key length is only 56 bit. In AES it can be 128, 192, or 256 bits. These key lengths are considered long enough to defeat brute force attempt to search for a key. Here is a high level view of AES. The encryption process is as follows. Again, the plaintext block is represented as a square matrix. We call it a state array, and we first XOR with the key. Again, the key is also represented as a square matrix. Then the state arrays go through multiple rounds of encryption. At each round, it goes through several operations that represent substitution and permutation, and also the round key is XOR to this state array. The operations at each round include substitute bytes. This involves using a table referred to as a S box to perform byte to byte substitution of the block. Shift rows is a simple permutation that is performed row by row. Mixed columns is a substitution that alters each byte in a column as a function of all the bytes in the column. And then the result is XOR in a round key. The operations of the last round includes substitute bytes, shift rows and add round key, and the result is a cipher text. In AES, the decryption process runs the algorithm in the reverse direction. This means that each of these operations must be reversible. Let's take a look. Now, adding round key involves the XOR operation. An XOR operation by itself is reversible. The other operations, meaning substitute bytes, shift rows, and mix columns, an inverse function is used in a decryption algorithm. By using this inverse function, we can reverse the action of substitute bytes that was performed in the encryption. Likewise, we can reverse the effects of shift rows and mix columns in the decryption process. Therefore, each of the operations are reversible. As a result, when we run the algorithm in the reverse order, we can decrypt the cipher text back into the plaintext. Let's take a closer look at each round of AES. Again, in AES, data is represented as state array. The first operation is substitute bytes. This involves using S boxes to perform byte by byte substitution. And again, the result is stored in the state array. The second operation is shift rows. And this is permutation that's performed row by row. Again, the result is in the state array. MixColumns is a substitution that alters each byte in a column as a function of all the bytes in the column. Again, the result is in the state array. And then we XOR with the round key. The result is in the state array, and this feeds to the next round of AES. As you can see, each round of AES involves substitution, permutation, which represent confusion and diffusion, and also use the encryption key. Now let's do a quiz on AES. Check all the statements that are true. First, to decrypt using AES, just run the same algorithm in the same order of operations. Second, each operation or stage in AES is reversible, AES can support key length of 128, 192 and 256. AES is much more efficient than triple DES. The first statement, to decrypt using AES, just run the same algorithm in the same order of operations. This is false. Because in AES, to decrypt, we run the algorithm in the reverse order of operations. Second, each operation or stage in AES is reversible. This is true. As we have discussed, all the operations in AES are reversible. Third, AES can support key length of 128, 192, and 256. This is true. Fourth, AES is much more efficient than Triple DES. This is also true. In fact, these are the main motivations and requirements for developing AES in the first place. So far we have discussed block ciphers, which take as an input a fixed length data block, say in 64-bit 40 years, or 128-bit for 8 years. What if you want to encrypt a large message, that is, a message that's longer than 64-bit, or even 128-bit? The solution seems obvious. Why can't we just break a message into fixed-size blocks, apply a block cipher such as DES or AES on the blocks, then the collection of the ciphertext blocks is the ciphertext of the large message? Is that it? This seems to be very simple. Let's take a look at a few schemes that encrypt a large message. The simplest way is the so-called electronic code book method or ECB. Here, the original large message is broken down into fixed-sized blocks, say 64-bit, and we pad the last block so that it is also 64-bit. And then each plaintext block is encrypted using the same key independently of each other. And the collection of these ciphertext blocks is the ciphertext of the original large message. The term code book is used, because for given key, there is a unique ciphertext block for every plaintext block. Therefore, we can construct a gigantic code book in which there's an entry for every possible plaintext block, and the table entry shows the corresponding ciphertext block. Of course, we construct one code book for each key. In practice, only the plaintext blocks that are used in an application need to be included in the code book. That is, the code book does not need to include all possible plaintext blocks. Therefore, it does not have to be gigantic. So this seems to be a very convenient of encrypting a large message. There are major shortcomings with the electronic code-book method. First, we observe that for the same key, if two plain-text blocks are the same, then their corresponding server text blocks are also the same. This is true if the two plain-text blocks are in the same message, or across two different messages. As long as they use the same key. This is a major weakness that can be exploited by cryptoanalysis. For example, if it is known that the message always starts out with certain predefined fields, then cryptoanalysis can have a number of known plain-text, server-text pairs to work with. As another example, if the message has repetitive elements, then these elements can be identified by cryptoanalysis as well. In other words, whenever we see two identical cipher-text bocks, we know that the corresponding plain-text blocks are exactly the same. And if we have obtained the plain-text block off a cipher-text block, then if you see the same cipher-text block again, we know the corresponding plain-text block already. In other words, ECB does not provide a very strong confidentiality protection. Another problem with ECB is that the plain text blocks are encrypted into ciphertext blocks independently of each other. Therefore an attacker can use a previously captured ciphertext block to sub out a current ciphertext block. Or we arrange the order of the ciphertext blocks. The result is violation of message integrity. For example the attacker can fabricate specific information. The most widely used approach to encrypt a large message is CBC. It stands for cipher block chaining. In CBC, the input to the encryption algorithm is the result of xoring the previous ciphertext block and the current plaintext block. To encrypt the first plaintext block, we use a so-called IV, stands for initialization vector. And we XOR IV with the plaintext block and give the input to the encryption process to produce the first ciphertext block. And then the first ciphertext block is used to XOR with the second plaintext block as input to the encryption process to produce the second ciphertext block and so on so forth. Again in CBC, the current ciphertext block depends not only on the current plaintext block but also the previous ciphertext block. Which in turn depends on not only the previous plaintext block but also the ciphertext block prior to that. In other words, in analysis it is very hard to figure out the plaintext block just looking at the current ciphertext block. More importantly, if two plaintext blocks are exactly the same, meaning that they repeat in the same message or in two different messages, their ciphertext blocks are not likely to be the same. In addition, if an attacker attempts to swap in a different ciphertext block or rearrange the orders of the ciphertext blocks, the ciphertext is not going to decrypt properly into the plaintext. Here's how decryption in CBC works. A ciphertext block is decrypted, and then the result is XORed with the previous ciphertext block to produce the current plaintext block. For the first ciphertext block after decryption, the result is XORed with Iv to produce the first plaintext block. Therefore, the Iv must be known to both the sender and receiver. So far we are focused largely on message confidentiality. What if we want to protect message integrity. That is, we want to prevent or detect any unauthorized modification to the message. One standard approach is to send the last block of CBC, also called a CBC residue, along with a plain text. Now, if an attacker intercepts the message and modify the plain text, the attacker does not have the correct key, therefore he cannot compute a correct CBC residue with the modified plain text. So he's forced to resend the modified plain text with original CBC residue. The CBC residue that the receiver computes on the modified plaintext will not be the same as the CBC residue that he receives. Therefore, the receiver would know that the plaintext has been modified. In other words, the CBC residue provides a protection of message integrity. Now, what if we want both confidentiality and integrity? How about we encrypt the message and send all the CBC blocks, and that's the ciphertext blocks for protecting confidentiality. And then, we'll replicate the last block, which is a CBC residue, and that's for integrity. Would that work? Obviously this approach does not work because if we simply replicate the last block, we are not adding anything. That is, we are not doing anything in addition to protecting the confidentiality. Therefore there is no way this approach will work to provide both, confidentiality and integrity. We can instead use two different keys. One for producing CBC blocks for confidentiality, and the other for generating the residue for integrity, that is we encrypt twice using two different keys. Another approach is that we first compute a hash of the message. Appended it to the message and then encrypt the whole entity. Now let's do a quiz on CBC. Check the statements that are true. First, CBC is more secure than ECB. Second, we can have both confidentiality and integrity protection with CBC by using just one key. The first statement, CBC is more secure than ECB, this is true. This is because in CBC, the current ciphertext block depends not only on the current plaintext block, but also the previous ciphertext block. Such chaining provides better confidentiality and integrity protection than ECB. The second statement, we can have both confidentiality and integrity protection with CBC by using just one key. This is false. As we have discussed, we need to use two different keys and encrypt a message twice, one for protecting confidentiality and the other for protecting integrity. Block ciphers need to use both confusion and diffusion operations. AES uses longer keys and is more secure than DES. We should use CBC to encrypt a large message. We can also use the last CBC block as the message integrity code. In this lesson, we will first review the modular arithmetic using Public Key cryptography. We will then study RSA and Diffie-Hellman. Before we discuss the widely used public key algorithms, RSA and Diffie–Hellman, let's go over the background first. Both RSA and Diffie–Hellman are based on number theory. An ancient and yet active field in mathematics. These public key algorithms use modular arithmetic, including modular edition, modular multiplication and modular exponentiation. And they make use of results from number theory. Let's briefly introduce the background of modular arithmetic. First, let's explain modular addition. Given a modular M, x plus y, Mod m is the remainder of x plus y divided by M. For example, 2 plus 8 divided by 10 the remainder is 0. So therefore 2 plus 8 MOD 10 is 0 whereas, 3 plus 8 MOD 10 is 1, because 3 plus 8 is 11. And 11 divided by 10, the remainder is 1 therefore, 3 plus 8 MOD 10 is 1. The modular addition if we add a number and its inverse, then the result is 0. For example, If the modulus is 10 then for k equal to 2 is inverse is 8 because 2 plus 8 MOD 10 is 0. Having additive inverse means that modular addition is reversible. That is, by adding the inverse we can reverse the result of modular addition. This is very convenient for encryption and decryption because, as we have discussed, we want the encryption process to be reversible, meaning that ideally, the decryption process is just the reverse of the encryption process. For example, given plain text, p = 3 Suppose the key k is 2, and the way we include the plain text p is to add the key under modular addition to plain text p and the result is cipher text c. So the c equal to p plus k, which is 3 plus 2 MOD 10. The result is 5. There is the cipher text c of plain text p equal to 3 is 5. Now how do we decrypt? To decrypt we use the same process but instead of using the key we use the key's inverse. We know that the inverse of 2 is 8. So therefore c plus k inverse is 5 plus 8 MOD 10 that is, 13 MOD 10. The result is 3, which is exactly the plain text. And so this is how we can do decryption by using the exact same process as the encryption, and instead of using the key, we use its inverse in decryption. And we take advantage the fact that under modular addition, each number has a inverse. Therefore each key has a inverse and we just use the inverse of the key in decryption. Now let's do a quiz. What is the additive inverse of 8 MOD 20? You can write the answer in this box. The inverse of 8 is a number that when we add to 8 MOD 20 will result in 0. So obviously the answer is 12, because 8 + 12 MOD 20 is 0, because 8 plus 12 is 20, 20 MOD 20 is 0. Therefore, the additive inverse of 8 is 12. For modular multiplication, the story is a bit more complicated. First, similar to modular addition,in modular multiplication the result of x times y not m is the remainder of x times y divided by m. In modular multiplication, the result of a number times its inverse, MOD M, is one. For example, if the modular is 10, then three and seven are inverse of each other because three times seven is 21 and 21 MOD 10 is one. Because, the remainder of 21 divided by 10 is one. It turns out, that, for a given modular, only some numbers have inverse. For example, if the modular is 10, then the numbers two, five, six, eight do not have inverse. For example, for the number two, can you find its inverse, say x, such that two times x, MOD 10 is one? There's no such x. The great mathematician Euclid invented the method to find multiplicative inverse. That is, for the modular n given X, Euclid had an algorithm to find Y. Such that X times Y mod N equal to one. Euclid also proved that only the numbers relatively prime to N has MOD N multiplicative inverse. Now, let's do a quiz. What is the multiplicative inverse of 3 MOD 17? Enter your answer here. That is, you need to find a number, let's say X, such that 3 times X MOD 17, equal to 1. The answer is six because three times six equals to 18 equal to one mod 17. Eucher's theory says that only the numbers that are relatively prime to n have multiplicative inverse not n. So, what does relatively prime mean? If x is relatively prime to n, that means there is no common factor between x and n other than 1. For example, three is relatively prime to 10 whereas two is not relatively prime to 10. For given n how many integers are relatively prime to n? This is measured as a totient function of n. How do we compute totient n? If n is a prime number, then totient n is n-1, because every number that's smaller than n is also relatively primed to n because n is prime number itself. If n equal to p times q and p and q are prime numbers, then totient n is p minus 1 times q minus 1. You can verify this easily on your own. More generally, if n equals to p times q and p and q are relatively prime to each other, then totient n is totient p times totient q. Let's do a quiz. If n is 21, what is totient(n)? Write your answer in this box. We know that 21 equals to 3 times 7. And 3 and 7 are prime numbers. Therefore, totient 21 should be 2 times 6. And the result is then 12. So 12 is the answer. In modular exponentiation there is a very nice property of quotient function from number theory. That is, if you raise x to the power of y mod n, the result is the same as x raised to the power y mod quotient n. Given this property, if y equal to one mod quotient n then if you raise x to power y mod n, the result is the same as x mod n. Now let's do a quiz. Use the totient technique to compute the result of raise 7 to the power of 27 mod 30. Write your result in this box. Using totient technique, we know that this is the same as 7 raised to the power of 27, mod, totient 30, mod 30. So what is 27, mod, totient 30? First we need to compute what is totient 30? Totient 30 is computed as follows. 30 is 3 times 10, and 3 and 10 are relatively prime to each other. So, therefore, totient 30 is totient 3 times totient 10. Totient 3 is 2, because 3 is a prime number. Totient 10 is 4 because 10 equal to 2 times 5 and they're both prime numbers. Therefore, totient 30 is 2 times 4, equal to 8. So then what is 27 mod 8? The result is 3. Therefore, this is same as 7 raised to the power of 3 mod 30. And the result is 13. With a math background we can now describe some of the most widely used public key algorithms. The first is RSA. Named after its inventors. R backup RSA is the most widely used public key algorithm and one of the very first. RSA supports both public key encryption and digital signature. The security strength of RSA is based on the hypothesis that, factoring a very large number into two primes is a very hard problem. This is, given a large number it is computationally very hard to factor it into two primes. RSA can support variable key lengths. In practice, most people use a key length of 1,024 bits, or 2,048 bits, or even 4,096 bits. The plaintext length can also be variable. In RSA, every data is treated as an integer because we can interpret any data with bits of one and zero as an integer. And the requirement here is that the plaintext input has to be smaller in integer value than the key. The size of ciphertext is the same as the key length. This summarizes the RSA algorithm. The first step is key generation. First we select two prime numbers, p and q. Say each of them are at least 512 bits long. And then we compute p times q. The result's n. And while we have p and q we also compute totient n. Which is p minus one times q minus one because both p and q are prime numbers. Then we select an integer e that is relatively prime to totient n. After we have selected e, we find the multiplicative inverse of e marked totient n. Having computed n, e and d, now we can forget about p and q and forget about totient n. The public key is e and n. The private key, which we have to keep to ourself and secure it, is d and n. Now for encryption, suppose Alice has published her public key. And Bob wishes to send a message, M, to Alice. And he wants only Alice to be able to read M. So Bob obtains Alice's public key and to encrypt message M, Bob computes M raised to the power of e (mod n). For decryption on receipt of this ciphertext, C, Alice will use her own private key, D, and compute C raised to the power of d (mod n). And this would result in the original plaintext M. The property of RSA guarantees that only Alice Can decrypt this message because only she has the private key that's paired with the public key that was used to encrypt the message. Again, every user can publish his or her own public key and keep his or her own private key securely to himself or herself. To encrypt message to a user say, Alice, Bob would obtain Alice's public key and compute the ciphertext by raising the plaintext to the power of e mod n. To decrypt, Alice would use her own private key to raise the ciphertext to the power of d and then mod n. What about creating digital signature? Alice will use her own private key to raise the message m to a power of d mod n. To verify, Bob would get Alice's public key, raise the signature to a power of e and mod n, and this will produce the original message m and verify the signature. Again here the property of RSA guarantees that Bob will know that the signature was created by Alice, because only she has the private key that is paired with the public key that Bob was able to use to verify the signature. Here's the math on why RSA works. We know that the public key is e and n and the private key is d,n. We also know that n equals to p times q, and p and q are prime numbers. Therefore, quotient n is p minus 1 times q minus 1. The public key e and the public key d are multiplicative inverse of each other, mod potion n. That's why we say the public key and the private key are paired together. And therefore when we compute X raised the power of e times d it's the same as X raised to the power of e times d mod potion n which happens to be 1. That is, let me compute X raised to the power of e times d is the same as X mod n. So that is a quick recap of the properties of the RSA public and private keys. Now, to encrypt a message m, we raise m to the power of e mod n. To decrypt, we take the ciphertext, raise it to the power of d mod n. This is the same as raising m to the power of e times d mod n, which is the same as m mod n according to this property. Since we require that the plain text input is smaller than the key, that means m mod n equal to m and that's the original plaintext. So this shows that decryption indeed works. As an exercise, you can similarly verify that digital signature under RSA also works. Here's an example of RSA, the keys are generated as follows. We select two prime numbers. p equal to 17, q equal to 11 so n is p times q, which is 187 and totient n is p minus 1 times q minus 1 because p and q are prime numbers. So in this case, totient 187 is 16 times 10 which is 160. For public key we select a number that's relatively prime to totient n. In this case, we select 7, because 7 is relatively prime to 160. The private key d, is the multiplicative inverse of e Mod potion n. So the multiplicative inverse of 7 mod 160 is 23, therefore the private key is 23. To summarize the key generation process, we have the public key 7, 187, public key 23, 187. Suppose we use a public key to encrypt the plaintext message 88. We raised 88 to a power of 7 mod 187, the result is 11. To decrypt we raise 11 to power of 23 and then mod 187. The result is 88, the same as the plaintext. Now let's do a quiz. This quiz has two parts. The first part is about key generation. Please fill in the answers in text boxes. Given two prime numbers, p and q, we need to compute n, total n, and then we need to select a public key, and from there, we need to compute a private key. We know that n = p x q. Therefore, n is 33, and since p and q are prime numbers, total n is (p-1) x (q-1). Therefore, total n is 2 x 10 = 20. We selected e public key 7, which is a prime number. Obviously, because it is a prime number, it's relatively prime to total n. It will have a multiplicative inverse. What is the value of t such that e times t mark total n is equal to 1? Given that e is 7, it's obvious that t is 3 because 3 x 7 is 21 and 21 minus 20 is 1. So what's the public key? The public key is the pair e,n. Therefore, it is 7, 33. The private key is d,n. Therefore it is 3, 33. In the second part of the quiz, given that we have generated the public key and the private key, then given the Message m, what is the encryption of m? And how do we decrypt to get m? You can use ** to denote an exponent. So what's the encryption of m? To encrypt, we raised the plain text which is two to the power of public key e, and then Mod n. And the result is 29. To decrypt the cipher text which is 29, raise it to the power of the private key, which is three, and mark n, which is 33. And the result is two, which is the plain text. Why is RSA secure? The security of RSA is based on the hypothesis that factoring a very large number is very, very hard. When we say a very large number, we mean a integer that is at least 512 bit, but often in the range of 1024 bits to 4096 bits. That is, even such a large integer is going to take a long time to factor it into two primes. On the other hand, if someone finds a very efficient way to factor a large number into primes, then the security of RSA can be broken. Here's why. The public key is given, so we know n and we know e. If we can efficiently factor n into p x q, where p and q are two primes, then we can computer totient(n) as (p- 1)(q- 1). And if we can compute totient(n) knowing the public key e, we can then find its multiplicative inverse, mod totient(n), and this inverse is the public key d. Therefore if we can efficiently factor a large integer into two primes that would mean that from the public key we can derive the private key. And that would break the security of RSA because if you know the private key then you can decrypt confidential messages or you can also forge digital signatures. We need to consider a few issues when we use RSA in practice. First, for given key the same plain text message will always be encrypted into same ciphertext. A related issue is that for some special-case plaintexts, such as zero, one or minus one, the ciphertexts are always zero, one or minus one regardless of the keys. These shortcomings allow the attacker to know the plaintext even though he doesn't know the key. Another issue is that RSA is manageable. That is if the attacker transforms a ciphertext into another, it would produce a predictable transformation of the decrypted plaintext. Suppose Bob sends Alice a encrypted message using Alice's public key. The attacker intercepts the ciphertext. Then the attacker changes the ciphertext. Then when Bob receives this modified ciphertext, he's going to decrypt it as s times m instead of m. That is, the attacker has the ability to change the ciphertext in such a way that it would produce predictable effect on the decrypted plaintext. In this case, the plain text is increased by a factor of S. In practice, the standard uses some sort of padding. That is, add some random bytes as prefix to m, and this addresses the issues that we just discussed. Now let's do a quiz. Select the best answer. When implementing RSA is it best to use your own custom software to ensure a secure system, or should you use the standard libraries for RSA? The answer is, you should use the standard libraries. The reason is that the standard libraries have been reviewed and tested by the security committee and therefore are more likely to be more secure. Now let's look into the details of the Diffie-Hellman key exchange algorithm. There are two publicly known numbers, a prime number Q and an integer alpha that is a primitive root of Q. Now suppose users A and B wish to exchange a key using the Diffie-Hellman key exchange algorithm. User A selects a random integer XA that's less than q, and computes you that's raised alpha to the power of XA and then mod q. Likewise, User B independently selects a random integer XB that's less than q, and then computes YB, that is, alpha raised to the power of XB and then mod q. Each side keeps the X value private and then sends the Y value to the other side. Because the Y value is transmitted, in effect it is public. User A upon receiving YB would compute a key as YB raised to the power of XA mod q. This is actually the same as alpha raised to power of XB times XA then mod q. Likewise, User B upon receiving you computes a key that is you raised to the power of XB mod q, which is the same as alpha raised to the power of XA times XB mod q. That is, both sides have, in effect, computed the same key. In other words, now User A and User B have a shared secure key. Furthermore, XA and XB are private, and an adversary such as User C only knows Q and alpha, which are public, and you and YB, which is transmitted, and they are also public. If the attacker wants to compute the shared key, he must first compute the X value. For example, the attacker must compute the discrete log of YB in order to get XB. If the attacker can compute a discrete log and obtain XB, he can then compute the shared secret key by using the you value. The security of the Diffie-Hellman key exchange algorithm lies in the fact that while it's relatively easy to calculate the exponents modulo a prime, it is very difficult to compute a discrete logarithm. For large primes such as Q at at least 300 digit long, this task is considered infeasible. Now let's illustrate Diffie-Hellman key exchange algorithm using an example. The prime number q is 353, and the alpha, which is the primitive root of q, is 3. User A selects a random number, which is a secret to herself, and this value is 97. So as you compute you, which is 3 raised to the power of 97 mod 353, and the result is 40. User B selects a random number, which is a secret to himself and this value is 233. And you compute YB, which is 3 raised to the power of 233 Mod 353, and the result is 248. And they exchange these values, that is user A sends 40 to user B. And user B sends 248 to user A. Then user A computes the following, Y8 rates to the power of X8. In this case, will be 248 Raise to the power of 97 mod 353. And the result is 160. Likewise, User B] computes you raised to the power of XB, mod q. In this case, it is 40 raised to the power 233 mod 353. And the result is 160. So as you can see, by exchanging you and YB, both users A and B compute the same secret value, which is 160. And this would become the shared secret key between users A and B. We assume that an attacker would be able to obtain these publicly exchanged values which encode in public keys, and they also know the publicly known numbers Q and alpha. In this case, because the integers involved are very small. It is feasible that the attacker will be able to figure out the secret value Xa and Xb, which is 97 and 333. However if very large number is involved, such as when the prime number Q is at least 200 bits long, then a task of finding Xa, the secret value, from 1a the public value is not feasible Now let's do a quiz. Suppose Alice and Bob agree to use prime q = 23 and primitive root alpha = 5. And these numbers can be made public. And Alice chooses a secret, a = 6. Likewise Bob chooses a secret, b = 15. The question is, what number does Alice send to Bob? And that is the public number XA. Likewise, what number does Bob send to Alice? And that is the public number YB. We know that according to the Diffie-Hellman key exchange algorithm, you is alpha which is 5 raised to the power of xa which is 6 mod q which is 23. And the result is 8. Likewise, yb is alpha which is 5 raised to the power of xb which is 15 mod q which is 23. And the result is 19 Now let's analyze the security of the Diffie-Hellman key exchange algorithm. First, we observe that the shared key, meaning the shared secret, that is computed by both users A and B, itself is never transmitted. Further, the local secret of user A or B itself is also not transmitted. That is, the local secret, XA, is kept to user A, and the local secret, XB, is kept to user B. In summary, no secret value is being transmitted. On the other hand, the values you and YB are transmitted. Then the question is from you which is alpha raised to the power of XA mod q. And given that alpha and q and you are known can an emissary compute xa? The security assumption here, is that this grid algorithm when large number is involved is very hard. That is given y, alpha, and q, it is extremely hard to compute the value of x, or it is not practical to compute x. Because q is a very large prime. For example, q is a least 300 bit long. On the other hand, if this conjecture is not true, that means an adversary knowing you and knowing alpha and q, can easily compute xa which is the local secret to user a. This would mean that the adversary can compute secret key as they're shared between user A and B, just like user A. Because now, the adversary knows the local secret of user A. There are a few shortcomings in Diffie-Hellman key exchange algorithm. First, suppose that Alice tells Bob to use Diffie-Helman. The first thing Bob has to do is to compute his YB. And this computation involves exponentiation, which is very expensive. Therefore, if Alice is a malicious attacker and Bob is a server, then Alice can request multiple Diffie-Hellman sessions with Bob at the same time. And this would cause Bob to spend a lot of CPU cycles on exponentiations, and this can then result in denial of service. Second, Diffie-Hellman is only for key exchange. It itself does not offer encryption. And the Diffie-Hellman algorithm does not authenticate either Alice or Bob. That is, there's no authentication and it cannot provide digital signatures like RSA. The biggest threat to Diffie-Hellman key exchange is the so-called bucket brigade attack. It is a kind of man-in-the-middle attack. That is Trudy, the adversary, can intercept the message you that Alice sends to Bob. And instead, Trudy sends her own yx to Bob. And fooling Bob to accept this as you. Likewise, Trudy intercepts Yb that Bob sends to Alice, and instead sends her own Yx to Alice. Fooling Alice to believe that Yx is actually Yb. And the result is that the shared key that Alice computes, is actually the shared key between Alice and Trudy. And likewise, the shared key that Bob computes, is actually the shared key between Trudy and Bob. In other words, Trudy Plays Bob to Alice, and Alice to Bob. This man in the middle attack is possible because the Diffie–Hellman key exchange protocol does not authenticate Alice or Bob. For example, there's no way for Alice to know that the message that she receives is really from Bob. There are a number of ways to fix this. For example, everyone, for example, Alice and Bob, can publish her or his public key, you or Yx. That is, instead of having to send it and risk interception and forgery, just publish you or Yb at a public trusted site. Or, if Alice has already published her RSA public key, she can signed you when she sends it to Bob, so that Bob would know that you is really from Alice. Because Bob can verify that uses Alice's RSA public key. In addition to RSA and Diffie-Hellman which are the most widely used algorithms. There are other public key algorithms. First, the National Institute of Standards and Technology on list, has published the digital signature standard It is based on the secure hash function SHA1. Note that this algorithm is only for signature. Is not for encryption or key exchange. RSA is the most widely used public key cartography algorithm for encryption and digital signatures. Recently a competing system called elliptic curve cryptography has begun to challenge RSA. The main advantage of ECC over RSA is that if offers the same security with a far smaller bit size. That is, for the same security strength, it can be much more efficient than RSA. On the other hand, RSA has been subject to a lot of cryptanalysis work over the years. whereas for ECC, the cryptanalysis work is still at a beginning, therefore we are not as confident in the ECC as we are in RSA. ECC is more difficult explained in the form that medical description is beyond the scope of this course. But briefly, the technique is based on the use of a mathematical construct known as the elliptic curve. Roughly speaking, two points, p and t on the elliptic curve are known or made public. And p and t have the following relations, t times p equal t, under the so called point multiplication and the fact that d is the private key. Mathematically, even p and t, which are public, it is very hard to find t, which is private. Now, let's do a quiz on RSA and Diffie-Hellman. Check the statements that are true. First statement. RSA is a block cipher in which the plaintext and ciphertext are integers between zero and n-1 for some n. Second. If someone invents a very efficient method to factor large integers, then RSA becomes insecure. Third, the Diffie-Hellman algorithm depends, for its effectiveness, on the difficulty of computing discrete logarithms. Fourth, the Diffie-Hellman key exchange protocol is vulnerable to a man-in-the-middle attack, because it does not authenticate the participants. Fifth. RSA and Diffie-Hellman are the only public-key algorithms. First, RSA is a block cipher in which the plaintext and ciphertext are integers between 0 and n- 1 for some n. This is true because for RSA we require that the plaintext input is smaller than n and the server text has the same length as the plaintext. Therefore the servertext is also smaller than n. Second, if someone invents a very efficient method to factor large integers then RSA becomes insecure. This is true because if you can factor efficiently that means for public key n, you can factor n equal to p times q where p and q are prime numbers. And knowing p and q, you can compute [INAUDIBLE] n. And given the public key e, then you can compute a private key d, which is the multiplicative inverse of e, marked n. In short, if you can factor efficiently, that means you can recover the RSA private key. Third, The Diffie-Hellman algorithm depends for its effectiveness on the difficulty of computing discrete logarithms. This is true, because if it is easy to compute discrete logarithms, then given the publicly exchanged value cya, an adversary can compute the local secret xa. And knowing the xa, the adversary can then compute a [INAUDIBLE] s. Four, the Diffie-Hellman key exchange protocol is vulnerable to man-in-the-middle attack because it does not authenticate the participants. This is true. Because the protocol does not authenticate. When Alice receives a message, she has no way of knowing that the message is really from Bob. That is, Alice has no way of knowing that the value of y b, which she uses to compute a shared secret s, is really from Bob. Which means Trudy can send y x to Alice and fool Alice to believe that it is y b. Fifth, RSA and Diffie-Hellman are the only public-key algorithms. This is false. For example, there are other public key algorithms such as the digital signature standard and the elliptic curve cryptography. RSA can be used for encryption and signature. Its security is based on the assumption that factoring a large number is very hard. Diffie-Hellman is used for key exchange. Its security is based on the assumption that the discrete log on large numbers is very hard. In this lesson, we will first introduce the birthday paradox and apply it to decide the length of hash, in order to make hash functions resistant to collision attacks. We will then cover the secure hash function and H max scheme. A hash function can be applied to a block of data of any size. A hash function produces an output of fixed size. Typically in the range of 128 bits to 512 bits. It should be very efficient to compute a hash of a input. For given hash value it should be computationally infeasible to find the input so that the hash matches the given hash value. That is a hash function should be a one way function. Given the input data say m1, it should be computationally infeasible to find another input value, say m2 that is not equal to m1. Such that they have the same hash value. This is the weak collision resistant property. The strong collision resistant property is such that it should be computationally infeasible to five different inputs, m1 not equal to m2 such that they have the same hash value. The first three properties make hash functions practical for security applications. In particular, it can handle data of any size, and it's very efficient to compute hash. The one way property says that it's quite easy to compute a hash value given a message, but it is virtually impossible to find or generate input message. Given a hash value. This property is very important for message authentication. For example, we can authenticate a message by hashing a secret value together with the message. The secret is not sent, the hash and the message is sent. If the hash function is not one-way, then the attacker being able to intercept the hash value that's being transmitted. He can then find an input that computes the hash value. And the input would include the secret value, that is the attacker would be able to obtain the secret value. Therefore, this one way property is extremely important. The weak collision resistant property says that for given message, it is not possible to find a different message, such that it will have the same hash value as the given message. This is a very important property to ensure message integrity. Now let's take a closer look at this weak collision resistant property. Suppose when a sender sends a message to the receiver, he wants to ensure the integrity of the message. So the sender sends a message along with an encrypted hash code of the message. Now an attacker can intercept the message, and the encrypted hash code that is being transmitted. Given a message, the attacker can also compute the hash code. Of course this hash code is not encrypted, because the attacker does not have the key. Then if the weak collision resistant poverty is not true. The attacker will be able to find another message, such that each hash value is the same as the hash value of the original message. Then the hacker simply forwards to the receiver the forged message, along with the original encrypted hash code. The receiver will not be able to tell that the message has been modified by the attacker. Because the forged message has exactly the same hash value as the original message. The strong collision resistant property, says that it is not possible to find any pair of two different messages, so that they have the same hash value. It should be obvious that the strong collision resistant property implies the weak collision resistant property. The weak collision resistant property means that the hash function is collision resistant. Only to the specific given input messages. Whereas the strong collision resistant property, means that the hash function is collision resistant to any pair of different messages. Therefore, this is stronger property and it implies the weaker property. The strong collision resistant property, provides strong message authenticity and integrity protection. Let's take a look at an example. Suppose Bob did some work for Alice, and he wants Alice to pay him later. So he can draft a IOU message and ask Alice to sign and then agree to pay for it. To sign the message, Alice will hash the IOU message. And then sign the hash using her private key. This will prove that Alice authorized the IOU message. In other words, Alice would agree to pay this amount later. In a later day, Bob can present this IOU message, along with the signature, to Alice or Alice's bank to get the money. Now suppose the strong collision resistant property is not true. That would mean that Bob can find two different messages, one with a smaller dollar amount. For example, this small amount can be just one installment of Alice's payment to Bob. And the larger amount would be several times the amount of money that Alice owes Bob. And if these two messages have the same hash value, then Bob can present the message with a small amount to Alice, and have Alice sign it. Bob can present a signature with a different message. That is, the message with a larger amount, and ask Alice to pay for it. And because these two different messages have the same hash value, Alice can not deny that she has signed it. That means that Alice can not deny that she has agreed to pay this larger amount. To understand the constraints or potential weaknesses of hash functions, there are two concepts we need to discuss. The first is the pigeonhole principle and the second is the birthday paradox. First, let's take a look at the pigeonhole principle. Imagine we have nine pigeonholes, if we have nine pigeons then one pigeon can be placed in each hole. That is suppose we have n number of pigeons and m number of holes. If n equaled m that means there's exactly one pigeon per hole. Now if we add another pigeon then there will be one hole with two pigeons. That is if n, the number of pigeons is greater than m the number of holes then there's at least one hole that must have more than one pigeon. Now let's apply the pigeon hole principle to another problem. How many people do we need to have in a room such that there's a good chance that two of them will have the same birthday. There are 365 birthdays and we can think of these birthdays as the pigeon holes. How do we calculate the probability that two people in the room have the same birthday. Obviously, if you want the probability to be 100%, we need 366 people and that's the pigeonhole principle. Because we have 365 birthdays or 365 pigeonholes. So we need 366 pigeons, or people, in order to make sure that two of them will have the same birthday. Now suppose, we only need a good chance, say only 50% chance, that two people in the room would have the same birthday. How do we calculate how many people do we need in a room? We can solve this problem using the following procedure. We can model this problem of having n people choosing from k days as their birthdays. Of course k is 365. If we allow everybody to choose any of the k days then there are k to the n that many scenarios. Because for each of the n people, they can choose any of the k days. So there are k to the n, that many possible scenarios. On the other hand, if we insist that no two people should have the same birthday, then the number of scenarios is choosing n out of k without replacement. And that is k(k-1) times k- 2, so on and so forth, and (k- n +1). Because for the first person, he can choose any of the k days. And once he makes his choice, then the next person can only have k- 1 choices. Because we don't want the second person to have the same birthday as the first person. And once the first two people have chosen their birthdays, then a third person can only have k-2 choices. And so and so forth. So this is the number of possible scenarios when we insist that no two people should have the same birthday. Then we can use this formula to compute a probability where there's a scenario where two people share the same birthday. And this can be approximated to n(n-1)/2k. Therefore if you want this probability to be 0.5 meaning that there's 50% chance that two people would share the same birthday then n should be the square root of k. Again this is just an approximation. If k=365 then its square root of k=19. That means as an approximation if we have 19 people in a room, there is a good chance that two of them share the same birthday. I've tried this in my class every year with the students in my classroom. And it always works out as the math tells us. Once we understand the pigeonhole principle and birthday paradox, we may realize that some of the properties of hash function seem to contradict each other. In particular, a hash function can take as input data of any size and the output is always a fixed size. Since the size of hash value is fixed, that means that there are fixed number of possible hash values. On the other hand since the input to hash functions can be of any size that would mean that there are many many more possible inputs to the number of possible outputs. In other words, there are many more pigeons than the pigeonholes. Then applying the pigeonhole principle, many inputs were mapped to the same output hash value. In other words, many different input messages will have the same hash value, but this violates the property of collision resistance. However, if we take a closer look at hash function properties, in particular, the collision system properties. They only say that it should be compositionally in feasible to find two different messages that have the same hash value. It did not say that it should be mathematically impossible to find such collision. In other words, although it is mathematically possible to find the collision, the hash function property says that we want to make it invisible, or impractical, to find such collision. So how do we accomplish this? Obviously, the larger the number of possible output hash values, the harder it is to find the collision. In other words, the longer the length of the output hash value, the better. In short, to avoid collision, we should use longer hash values So, we know that, in order to avoid collision, we should have a longer hash value. Now, the question is, how many bits should we have in a hash value so that it is not feasible to find two different messages that have the same hash value? Suppose the hash value has L bits. Then, there are 2 to the L, that many possible hash values. According to the birthday paradox we can think about we have 2 to the L, that many possible birthdays, therefore if we have a square root of 2 to the L, that is two to the L divided by two, that many messages. Then there's 50% chance that two of them will have the same hash value. In other words, the attacker only needs to search 2 to the power L divided by 2, that the many messages, in order to find collision. Therefore, if L is 64, which means the hash value has 64 bits. Then there are 2 to 64, that many possible different hash values, This means that the attacker only needs to search 2 to the 32, that many different messages, in order to find a collision, and this is quite feasible with today's computing power. Therefore, we need the hash value to be longer than 64 bit. Therefore, in hash functions, the hash value is at least 128 bits. Now let's do a quiz. If the length of hash is 128 bits, then how many messages does an attacker need to search in order to find two that share the same hash value? Is it 128, 2 to the 127, 2 to the 128, or 2 to 64? Because the length of hash is 128 bits. That means there are 128 possible hash values. Using the birthday paradox, the attacker needs to search a square root of 2 to the 128th, that many possible messages in order to find two that share the same hash. Therefore, the answer is 2 to 64. Now let's discuss the Secure Hash Algorithm or SHA. The Secure Hash Algorithm was developed by NIST. The original algorithm is SHA-1. SHA-1 produces a hash value of 160 bits. Later, NIST revised the SHA standard and specify the SHA-2 algorithms. This algorithms have key lengths of 256, 384 and 512 bits, and their operations are similar to SHA-1. This table compares the SHA parameters. The message digest size refers to the length of the hash value, so for SHA-1 it is 160 bits, and for SHA-512 the hash length is 512 bits. Message size is a size limit on the input. You can consider these size limits are not having an effect in practice, because most if not all messages will be smaller. For example, even with SHA-1, the message size is limited to 2 to 64. This means that you can hash a message so big that it occupies the entire address space of a 64 bit computer. You can also think about it in another way. 2 to the 43 bits is already a terabyte. As we can see, the algorithms from left to right produce hash values with more bits. Hence, they are more secure as we move from left to right. For example, with SHA-1 because the hash value has 160 bits then the search space for the attacker to find a collision is 2 to 80. Whereas for SHA-512 the search space is 2 to 256. In this lecture we will provide a description of SHA-512, the other versions are quite similar. For SHA-512 it takes as an input a message that's smaller than 2 to the 128. Again, this is not a limit that will have any impact in practice. The output is a hash value that is in 512 bits. The input is processed block by block and each block has 1024 bits. This figure shows the overall processing of a message to produce a hash value. The first step is padding so that the message is a multiple of 1,024 bits because the message is going to be processed block by block. And each block has 1,024 bits. The padding bits are appended to the last block of the message. First we leave the last 128 bits to store the length of the original message, that is the length before the padding. Then for the space between the original message and the last 128 bits we add one and a number of zeros necessary to fill up the space. And that's how padding works. After padding, then the message is processed one block at a time. Again, each block has 1,024 bits. The logic of per block processing is described in the next slide. The result of processing the current block is the input to the processing of the next block. That is when processing the second block, the input includes not only the second message block but also the output of the processing of the first block. And for the first block, the input includes the Iv. The Iv is a 512 bit value, hard coded in the algorithm. Again the message is processed one block at a time. At each block the input incurs the output of the processing of the previous block. And the result of the processing of the last block is the hash of the entire message. Now let's take a look at the processing of a message block. The processing involves 80 rounds. The input includes not only the current message block but also the result of the processing of the previous block. And the result of the processing of the current block will be the input to the processing of the next block. The result from the processing of the previous block is a 512 bit value, and it is divided into 8 64 bit values. And they are a, b, c, d, e, f, g, and h. Again, the processing involves 80 rounds and each round the input includes the result from the previous round, some constant k, and some words derived from the current message block. The constants here provide randomized values and the purpose is to eliminate any logravities with the input data. The operations at each round include circular shifts and primitive pulling functions based on and, or, not and x, or. The output of the last round is added to the input to the first round. And the result will be used in the processing of the next message block. Now let's discuss hash based approach to message authentication. Using hash values for message authentication has several advantages. For example, hash functions are very efficient to compute. And libraries of hash functions are widely available. On the other hand, a hash function such as SHA cannot be used directly and by itself for message authentication because it does not rely on a secret. There have been a number of proposals to incorporate a secret key into an existing hash function. The approach that has received the most support is HMAC, and it has been adopted to other protocols such as IPsec and TLS. Here's an illustration of how HMAC works. HMAC invokes a hash function and also secret key K. The message M consists of multiple blocks of b bits, for example, for SHA 512. Each block is 1,024 bits, so b would be 1,024. So the key K would be first padded to b bits. This is accomplished by simply appending zeros to the end of K. Then the padded key is XORed with ipad, which is a constant designed to eliminate any irregularities of the key. And the result is a b bit S1. S1 is prepended to the original message. Then the entire message there is S1, and the original message is hashed to produce a n bit hash value. For example, if the hash function is SHA 512, then n will be 512. Then the n bit hash value again will be padded to b bits. Then the padded key, K+, is XORed with opad. Again, opad is another constant designed to eliminate irregularities in the key. The result is a b bit value, S0. The padded hash is then appended to S0, and the entire message is hashed. And the n bit result is HMAC of the message with the key, K. And this is how HMAC works. To summarize, HMAC uses an existing hash function and includes a secret key, K, in the processing. Let's discuss the security of HMAC. First of all, the security of HMAC depends on the cryptographic strength of the underlying hash function. That is, the underlying hash function used in HMAC must satisfy the basic properties of one wayness and collision resistant. Further, compared with the cryptographic hash function, it is much harder to launch a successful collision attack on HMAC. The main reason is that a secret key is hashed together with the message content. As a result, without knowing the secret key, an attacker cannot compute the correct HMAC. For example, supposed the attacker is able to obtain the HMAC of message M1, and he wants to find another message that have a collision with M1. That is, a different message M2 that's not the same as M1, but had the same HMAC value as M1. But without a secret key, the attacker cannot compute the correct HMAC value for M2. That is, the attacker does not even know whether M1 or M2 will have collision in HMAC. In summary, because of the use of the secret key, HMAC is much more secure than a cryptography hash function. Now let's do a quiz on hash function. Check the statements that are true. First, the one-way hash function is important not only in message authentication, but also in digital signatures. Second, SHA processes the input one block at a time, but each block goes through the same processing. Third, HMAC is secure, provided that the embedded hash function has good cryptographic strengths, such as one-way and collision resistant. The first statement, the one-way hash function is important not only in message authentication but also in digital signatures. The one-way property of hash function says that it is very efficient to compute a hash value of a given input message. On the other hand, given a hash value, it is computationally infeasible to find the original input message. Hash function can be used in message authentication. For example, if Alice and Bob share a secret key, then Alice can authenticate herself by sending a hello message and hash the hello message along with a share secret key. And when Bob received this hash value, along with a hello message in plain text, Bob can hash the hello message along with a shared key and see whether that hash value matches with the hash value that he just received. And if they match, then Bob knows that this message is from Alice. In other words, Alice is authenticated. Now if the one-way property of hash function does not hold. That means the attacker intercepting the hello message and the hash value can then reverse the hash function and find that the secret key that was used to hash along with the hello message. Therefore, the attacker would able to obtain the share key. Therefore, the one way property of hash function is important for message authentication. Now what about digital signatures? Here's typically how digital signature works. Alice can create a digital signature for message, say m. She first hashes m, and then sign the hash value of m using a product key. Then she sends the message m in plain text along with a signature, therefore the one way property of hash function is not important here because the input plain text message is sent anyway. To recap, this statement is not true because the one way property is important for message authentication but not in digital signatures. Second statement, SHA processes the input one block at a time but each block goes through the same processing. This is true because this is how SHA works. Third, HMAC is secure provided that the embedded hash function has good cryptographic strengths, such as one-way, and collision resistant. This is true, that is, if the underlying has function is not secure, the HMAC would not be secure. On the other hand, as we have discussed, HMAC is more secure than a cryptographic hash function because of the use of a secret key. In order to resist collision attacks, the length of hash should be at least 128 bits. Secure hash function processes a message block by block and produces an intermediate hash value. HMAC hashes a message with a secret key. It is a standard way to provide message authentication. Security protocols are the foundation of secure network applications. In this lesson, we will discuss authentication protocols, key change protocols, and the Kerberos systems. A network protocol defines the rules and conventions for communications between two parties. A security protocol defines the rules and conventions for secure work communications between two parties. So what are these rules and conventions for secure communications? That is, suppose Alice and Bob want to communicate securely over the internet. What should they do? They need to authenticate each other so that Alice knows that she's communicating with Bob and Bob knows that he's communicating with Alice. Further, if they want to communicate securely, most likely they want to encrypt their messages. Therefore, they need to establish and exchange keys, and agree on what cryptographic operations and algorithms to use. The basic tools, or the building blocks of these security protocols, are the public-key and secret-key algorithms and hash functions that we have discussed. Let's take a look at authentication. That is, Alice needs to prove to Bob that she's really Alice, and vice versa. Meaning that Bob needs to prove to Alice that he is Bob. Suppose that Alice and Bob share a secret key, KAB. That is, only Bob and Alice know this key, KAB. Here's an authentication protocol using symmetricartography. First Alice says to Bob, hey I'm Alice. Second, Bob said to Alice, really? Prove it. And he sends a random value R1, which we will call a challenge. Third, Alice then encrypts R1 using the shared key KAB. And sends Bob the cybertext, which we'll call it the response to the challenge. When Bob receives the response, he decrypts it and see if it matches the plain text, R1 that he just sent to Alice. If it matches, then he knows that the party that he's communicating with must be Alice, because R then, himself, only Alice knows the shared key KAB. And without KAB, R1 cannot be encrypted properly. That is the server text that he receives won't be decrypted properly to R1. Fourth, now it is Alice's turn to authenticate Bob. So, similarly, she sends Bob a random challenge, R2. Fifth, Bob encrypts R2 with the shared key KAB and sends a cyber text to Alice. Upon receiving the response from Bob, Alice decrypts the cyber text and if the result matches the plain text R2 that she just sent to Bob, then she knows that the party that she's communicating with must be Bob. Note that if only one-way authentication is required, for example, Alice is a client and needs to authenticate to Bob, which is a server, but Bob does not need to authenticate to Alice, then only the first three steps are necessary. And if Alice is a human user, then typically the key KAB can be derived from a password hash, which is known to Bob. We just discussed how challenge and response are used in authentication. It is very important for the challenges, say R1 and R2, to be not easily repeatable or predictable. Otherwise an intruder such as Trudy can record the challenge and response between Alice and Bob and replay them to impersonate Alice or Bob. For example, an intruder called Trudy wants to impersonate either Alice or Bob. Suppose Trudy has spent time recording the authentication messages between Alice and Bob across multiple sessions. Now, if when Trudy tries to impersonate Alice and Bob happens to send R1, a challenge that has been used previously and recorded by Trudy, then Trudy can just simply send the response that she has recorded from Alice. That is, Trudy is able to send the cyber text of R1 to Bob and Bob will be tricked to believe that she is Alice. As another example, suppose the challenges always increase in values. Then Trudy can first impersonate Bob and send a large challenge, say R1 equal to 1000, and record the response from Alice. Meanwhile, the real Bob is using a smaller R1, say R1 equal to 950, then Trudy can impersonate Alice some time in the future when the real Bob finally sends R1 equal to 1,000. To recap, first of all, the messages that were sent over the Internet can be captured by intruder Trudy. And our job is to prevent these messages being replayed so that Trudy can't impersonate Alice and Bob. And in order to do that, the random challengers R1 and R2 should really be not easily repeatable or predictable. Again, the most important precaution of using this authentication protocol is that R1 and R2 should not be easily repeatable or predictable. Otherwise an intruder such as Trudy can impersonate Alice and Bob by simply doing record and replay. Therefore, R1 and R2 should be large, random values. Another security precaution is that the shared secret key, KAB, needs to be protected. Because if Trudy can steal a copy of the key, let's say from one of the complication end points either on Alice or Bob, then she can impersonate either Alice or Bob. In other words, the security of the end points is as important as the security of the link between the two end points. Now let's do a quiz. Mark each statement as true or false. Use T for true and F for false. First, the challenge values used in an authentication protocol can be repeatedly used in multiple sessions. Second, the authentication messages can be captured and replayed by an adversary. Third, authentication can be one-way, for example, only authenticating Alice to Bob. First, the challenge values used in an authentication protocol can be repeatedly used in multiple sessions. This is false. If the challenge values are repeated then there's a chance that the attacker can capture and replay the same challenge and response and impersonate Alice or Bob. Second, the authentication messages can be captured and replayed by an adversary. This is true. We should always assume that the messages that are sent over the Internet can be captured by the adversary. An adversary can attempt to replay them in order to impersonate Alice or Bob. Third, authentication can be one way, for example only authenticating Alice to Bob. This is true, as we have discussed the first three steps of the protocol authenticate Alice to Bob. Mutual authentication protocol takes five steps. Can we simplify it? That is, can we use fewer steps? It seems that the following would work. First, Alice says hi to Bob and sends along a challenge R2. Second, in response, Bob sends Alice the ciphertext of R2, and his own challenge R1. Upon receiving this response, Alice decrypts the ciphertext to see if it matches the plain text, R2, that she has sent to Bob. If it matches, then she knows that she's communicating with Bob. Third, Alice then sends Bob the ciphertext of R1. And Bob decrypts it and matches it with the plain text, R1, to authenticate Alice. Is there anything wrong with this simplified protocol? It turns out that there is a reflection attack, a kind of man in the middle attacks. Here's how it works. First, Trudy impersonates Alice. By following the protocol, Trudy will be stuck at step three because she can not encrypt the challenge, R1, sent from Bob. She does not have the key KAB. Then Trudy simply opens another connection with Bob and, again, impersonating Alice. This time Trudy sends Bob the challenge, R1, that Bob has sent her in the first connection, the one that she stuck in. According to the protocol, Bob responds with a ciphertext of R1 and another challenge, R3. Notice that Bob has send the ciphertext of R1 to Alice. So what Trudy can do is to take this ciphertext of R1. Then Trudy can now go back to the step three of the first connection and send it back to Bob to complete the first connection. That is, at this point, the first connection successfully concludes, and Trudy has successfully impersonated Alice. This is called a reflection attack because Trudy simply sends back to Bob what Bob had just sent her from another connection. So how can we prevent reflection attacks? One way is to use two different share keys. One is for the initiator of the connection, say Alice, and the other is for the responder, say Bob. The result is that the ciphertext that Trudy gets from Bob can not be presented as a ciphertext from Alice because these two ciphertexts should be using two different keys. That is, there are two share keys being used. Say Alice used KAB1 and Bob used KAB2. Therefore the ciphertext from Bob is encrypted using KAB2, and Bob is going to expect a ciphertext using KAB1. Therefore, even though Trudy can accept the ciphertext, but this Cipher text is produced using KAB2. She cannot simply send it back to Bob because the ciphertext sent to Bob must be encrypted using KAB1. Another way to prevent the reflection attack is to use different challenges for the initiator and the responder. For example, we can use an even number challenge for Alice and odd number for Bob. Therefore, when Trudy receives a challenge, R1, from Bob the challenge is an odd number, and Trudy cannot use. This is a new challenge for Bob because Bob is expecting an even number. We can also use public key cryptography for mutual authentication. Suppose Alice and Bob have each other's public key. In this protocol, first, Alice sends Bob a challenge R2 and R2 is encrypted using Bob's public key. Second, upon receiving this challenge Bob decrypts the server text using his own public key, and sends back the plain text challenge R2, along with his own challenge R1. And R1 is encrypted using Alice's public key. Third, when Alice gets the response from Bob, the plain text R2 in Bob's response, tells Alice that she's communicating with Bob because only Bob has the private key that is paired with the public key that encrypted R2. Alice also decrypts the server text R1 using her own private key and sends a plain text back to Bob in step three. Bob will then know that he is communicating with Alice because only Alice has the private key that is paired with the public key used to encrypt R1. As an exercise, you can modify this protocol to use signing with private keys instead of encryption with public keys. Now let's do a quiz. For each statement, decide whether it is true or false, using T for true and F for false. First, a reflection attack is a form of man-in-the-middle attack. Second, to defeat a reflection attack, we can use an odd number as challenge from the initiator and even number from the responder. Third, we can use signing with public keys to achieve mutual authentication. First, a reflection attack is a form of man-in-the-middle attack. This is true, because in reflection attack, Trudy is the man in the middle. In particular, in order to impersonate Alice, Trudy intercepts the message Bob sends to Alice, and repay it back to Bob. Second, to defeat a reflection attack, we can use an odd number as a challenge from the initiator and even number from the responder. This is true. By doing so, Trudy cannot repay the challenge that she received from Bob, which is an even number, back to Bob, because Bob is expecting an odd number. Third, we can use signing with public keys to achieve mutual authentication. This is true. We have discussed a mutual authentication protocol using encryption with public keys, but you can easily modify the protocol using signing with public keys. After authentication in order to protect message security, Alice and Bob will need to establish a shared secret key for each session. This can be accomplished in several ways. Supposed Alice and Bob share a master secret key. Then Alice can use this master secret to encrypt a new key and send the encrypted new key to Bob. Or Alice and Bob can also use public keys to encrypt a new key. We say that Alice and Bob should establish a shared key for each new session. This is true even if Alice and Bob already has a shared secret key. Typically, Alice and Bob share a long term secret key. And we call it the master key. For example, the master key can be derived from a password. For each session, Alice and Bob would use the master secret key to authenticate and establish a new key for the session. Then all messages in the session are protected using the session key. The main benefit of using session key is that if the key is leaked or broken, the impact is limited only to the current session. Intuitively, the more a secret is used, the higher a chance that the secret can be leaked. Therefore, we should limit the use of the long term master secret. And only use it at the beginning of a session for authentication and establishing the session key. Here's an example. Suppose Alice and Bob already share a master key, Kab. The first three steps are just for Alice to authenticate Bob, say Bob is a server. Then, both Bob and Alice computer save session key that is based on the sheer master key and something about the current session. So that the session key is both a secret and unique to the session. So for example, they can add 100 to the master key and use the result as the key to encrypt all which is a challenge used in this session to authenticate Alice to Bob, and the result of encrypting R using a modified master key is the shared session key. Alice and Bob can also use their public keys to exchange a shared session key. For example, Alice can send to Bob a key, encrypt it using Bob's public key so that only Bob can decrypt and get the key. And then signs the result using Alice's private key. So Bob knows that the key is sent by Alice. Or Alice and Bob can use their private keys to sign the public messages that they exchange in the Diffie-Hellman key exchange protocol. And this can prevent the man in the middle attack, that is when Alice sends Bob the public message, she signs it. Likewise when Bob sends Alice the public message, he signs it with his private key. A major short coming of using pairwise key exchange based on a shared secret, is that it cannot scale. That is, suppose we use a shared master keys as a way to establish and exchange a new session key. This scheme does not scale easily. That is Alice needs to share a master key with Bob, and then another master key with Carol, and so on and so forth. Using a Key Distribution Center, or KDC, can solve this scalability problem. Each party has his or her own master key shared with the KDC. That is, the KDC has many master keys, one for each party. But each party only keeps one master key that is shared with KDC. So, for example, Alice has KA that is shared with KDC. And Bob has KB that is shared with KDC. Now, suppose Alice and Bob wants to have a secure session, therefore, they need a session key KS. First, Alice sends a request to KDC saying that, I need a key to talk to Bob along with a nonce and 1. A nonce is a random value. Then, the KDC sends a message back to Alice that's encrypted using the master key KA that is shared between Alice and KDC. This message contains the session key KS that the KDC just created for Alice and Bob to share. The message also contains the same request that Alice sent to KDC along with the same nonce value, N1 and a message record ticket. The ticket is encrypted using the master key KP that is shared between Bob and the KDC. And it contains session key KS and the ID of Alice. When Alice gets back the message from the KDC, she can decrypt it because she has the master key, KA. And so, she can extract the session key KS. And she knows that the message is from the KDC, and it's fresh, that is, it is not a replay, because only the KDC can use KA to equip properly a message that contains the original request and the nonce that she just set. Alice then sends the ticket to Bob. Note that only Bob can decrypt the ticket, because it is encrypted using KB, the master key shared between Bob and the KDC. In fact, when Bob decrypts the ticket, he knows that the ticket is created by the KDC because only the KDC can encrypt the ID of Alice properly. And he knows that the session key, KS was created by KDC and is for communication with Alice. Then Bob sends a message that contains a nonce N2, which is a random value, and it's ID encrypted, using the session key KS to Alice. When Alice receives this message, she knows that she is communicating with Bob, because only he can decrypt the ticket and get a session key, Ks, and encrypt the ID properly. Alice then performs an agreed upon transformation on N2. Say, add 100 to N2 and encrypt the result using KS, and sends it back to Bob. This proves to Bob that he is communicating with Alice because only she has the session key KS. The distribution of public keys is often done through a certificate authority or CA. Alice sends a public key to the CA. The CA verifies Alice's identification, and then creates a certificate of Alice's public key and sends it to Alice. The certificate contains time of creation, validity period, and the ID of Alice, and the public key. And, it is signed using a CA's public key. Alice can then send this certificate to any user, say Bob. Or, Alice can publish it so that any user can get it. It is assumed that all users have the CAs public key, and therefore a user, say Bob, can use the CAs public key to verify the certificate and obtain Alice's public key. Likewise, Bob can get his certificate and send it to Alice so that Alice has Bob's public key. Now let's have a quiz on session keys. Decide whether each statement is true or false. Use T for true, and F for false. First, a session key should be a secret and unique to the session. Second, authentication should be accomplished before key exchange. Third, a key benefit of using KDC is for scalability. Fourth, in order for Bob to verify Alice's public key, the certificate authority must be online. Fifth, signing the message exchanges in Diffie-Hellman eliminates the man-in-the-middle attack. First, a session key should be a secret, and unique to the session. This is true. Second, authentication should be accomplished before key exchange. This is true. Third, a key benefit of using KDC is for scalability. This is true. Fourth, in order for Bob to verify Alice's public key, the certificate authority must be online. This is false, because as long as the users have the CA's public key, they can verify the certificate. Fifth, signing the message exchanges in Diffie-Hellman eliminates the man-in-the-middle attack. This is true. Now let's look at how the principles of authentication and key exchange that we have discussed are used in actual systems. Kerberos is a standard protocol used to provide authentication and access control in a network environment, typically an enterprise network. Every entity in a network, that is, all users and network resources, such as workstations and printers, have a master key that it shares with the Kerberos servers. And the Kerberos servers perform both authentication and key distribution. So for convenience, we use KDC to refer to a Kerberos server. For a human user, the master key is derived from his or her password. For a network device, the key is configured in. All the keys are stored securely at the KDC. Now let's go over a few typical Kerberos scenarios. When user Bob logs in, his workstation contacts the KDC with an authentication service request message. The KDC generates a per day session key, SB. And the so-called ticket-granting ticket that contains SB and the ID of Bob. And the ticket is encrypted using the KDC's own key. The KDC then sends back a message to Bob's workstation, and this message is the authentication service response. The message contains the per day session key and the ticket-granting ticket. And the message is encrypted using the shared master key between Bob and KDC. And because KB is the master key shared between Bob and KDC, only Bob's local workstation can decrypt this message. And then it can store the private key and the ticket granting ticket. Bob's local workstation will then use SB for subsequent messages with the KDC, and it would include the ticket granting ticket to remind and convince the KDC to use SB. That is, any new request to the KDC will include the TGT in the request message. And the new ticket from the KDC will be encrypted using SB. There are several benefits with this scheme. First, the localhost does not need to store Bob's password or password hash once Bob has logged in and once he has obtained SB from the KDC. Second, the master key KB, that Bob shares with the KDC, is only used once everyday when Bob logs in initially. That is, the exposure of the master key, which is derived from Bob's password and is subject to password guessing attacks is very limited. Now suppose Bob wants to access the printer HP1. That is, Bob wants to send a print job to the printer HP1. His local host sends a ticket granting service request to the KDC. The request contains the ticket granting ticket and a authenticator. Which is the current time stamp encrypted. Using Bob's per day session key, SB. When the KDC gets the request, it can decrypt the TGT. And it knows that it is a valid TGT. Because only the KDC has the key to properly encrypt the ID of Bob contained in the TGT. It then uses the key, SB, contained in the TGT to verify the authenticator by decrypting it. And checking the time stamp is current. This proves that the sender is Bob, because only Bob has the key SB. That can encrypt the current time stamp properly. The KDC then generates a ticket for Bob to communicate with the printer. This ticket contains a session key KBP and Bob's ID and its encrypted using the printer's master key. Note that a network device such as a printer has a long and random master key configured in. And it's typically hard to guess or crack. Therefore, the tickets for these devices are encrypted using their master keys. The KDC sends a ticket granting service response to Bob's local host. It contains a session key KBP. The ID of Bob and the ticket to the printer and it is encrypted using Bob's private key Sb. Therefore, only Bob's local station can decrypt this, and verify that it is from the KDC. Because only the KDC can decrypt the ID of Bob correctly with the key SP. Because the key sp is embedded in TGT and only the KDC can decrypt the TGT. Then Bob's local host can authenticate itself to the printer. It can send a message to the printer containing the ticket. The authentication request contains the ticket and an authenticator. The authenticator is the server text of the current timestamp encrypted. Using a share key between Bob's local host and the printer. When the printer gets the ticket, it can decrypt the ticket using his master key shared with the KDC. Because the ticket was created by the KDC. And it was encrypted using the share key between the printer and the KDC. That is, the printer can verify that the KBP was created by the KDC, to communicate with Bob's Local Host. The print server can then use KBP to verify the authenticator by decrypting the server text and verifying that the result matches with the current time stamp. It then sends a response to authenticate itself by say, adding one to the current timestamp and encrypt it. Using the share key between Bob's Local Host and the printer. And after this, authentication steps, then Bob's Local Host can send a print drop to the printer. Now let's do a quiz on Kerberos. Decide if each statement is true or false, using T for True and F for False. First, Kerberos provides authentication and access control. Second, Kerberos also distributes session keys. Third to avoid over exposure of a user's master key, Kerberos uses a per-day key and a ticket granting ticket. Four, the authenticators use the new quest to KDC and application servers can be omitted. Fifth, access to any network resource requires a ticket issued by the KDC. First, Kerberos provides authentication and access control. This is true. Second, Kerberos also distributes session keys. This is true. Third, to avoid over-exposure of a user's master key, Kerberos uses a per-day key and a ticket-granting-ticket. This is true. Fourth, the authenticators used in requests to KDC and application servers can be omitted. This is false. These authenticators are used to authenticate the senders. And authentication is an important goal of Kerberos. Therefore, these authenticators must not be omitted. Fifth, access to any network resource requires a ticket issued by the KDC. This is true. Authentication can be accomplished using random challenge and response. And a pre-shared secret key or public keys. Session keys can be established using a pre-shared secret key or public keys and can involve KDC and CA. The Kerberos system provides authentication and access control in a enterprise network environment So far, we've been focusing on the technology aspects of information security. However, the organization note as assigned to aspects are also very important. In the next few lectures Dr. Mustaque Ahamad is going to cover several of these topics. We're going to start with managing cyber security in the context of an organization. So what is it cyber risk? How can it reduce that risk with using technical solutions that we discussed, and what are the cost benefit trade offs for example? Once we do that, we're going to move on to how can we have consequences for the bad guys. Well laws, or cyber laws in particular, are one way to do that. So we'll discuss some of the US cyber laws and then also some ethical considerations. Then we're going to wrap up with online privacy which is a topic that is of great deal of interest to many of us. So far we have discussed a number of technical solutions to deal with cyber threats. But these solutions have to be considered in a context of an organization. For example, what are my cyber assets? What kind of risks do they face? Do the technical solutions really reduce this risk significantly? Are there people and process issues? These topics come under what we call managing cyber security, and cyber security management is going to be the focus of this lesson. So far we have been talking about technical controls, whether it has to do with authentication. Or in the context of network security, we talked about firewalls and [INAUDIBLE] detection and so on. And these are used to secure systems that we have in an organization. What are the assets that need to be secured? Who do they need to be secured from? So let's talk a little bit more about this organizational context. We said, there's something of value that is under threat, so we need to worry about securing it. But what other reasons may be there for us to worry about this problem? While there may be legal and compliance reasons, financial data and health data, for example, HIPAA mandates how you can share online health data or distill health data and things like that. One solution that we've been talking about is that of course there are various kind of technical controls. For me to use these technical controls, I have to understand what kind of risk that I'm facing. What is the threat source? What is that threat landscape for me? Of course the technical solution or control that I have is going to have an associated cost. I have to worry about what that cost is, and, what is the benefit of actually deploying this particular solution that we're talking about? So what are the challenges that we're going to face when we have this task of managing cyber security for the organization that we're talking about? We said we have to know what are the assets that are of value and are they under risk? Also, sort of understand the threats. And, how serious are those threats? So when we talk about risk, actually we're going to look at this a little bit more, but risk is really the likelihood of an attack, okay? So probability of an attack. It's not the worst case but the worst can happen. It's sort of trying to compute the likely case in some sense. So continuing with the challenges, even if I sort of identify my assets and threat source, and perhaps the likelihood of attacks, then I have to worry about, well what can I do about it? What sort of solutions are out there or controls that exist? Deciding that of course is a challenge. We did talk about cost-benefit and the trade off that are there when you think about the deploying various solutions. So, of course we want to do into a cost effective manner. It sounds simple, but doing that, again, is a serious challenge. Obviously we have to argue that the cost is less than the reduction in risk we're going to have. Finally, we know that when you look at sort of the threat landscape, and I think in the context of database security, we did the question where we said, what are the threats? External hackers or insiders, and we say in one of the surveys, insiders and unauthorized access was ranked higher than external hackers. So we have to of course, understand the people and the process that we have. The question's asking if you think that it should be part of this policy, Georgia Tech's policy. I mean, if you are responsible for managing security at a University like Georgia Tech would the staff, student's, and faculty, would they need to adhere by the kind of requirements these options present? If you agree, then you check that. If you don't agree, then of course, you leave that. By requiring that the passwords are changed periodically we are sort of ensuring that it's going to be less likely that someone who is not the right user is going to be able to gain access to an account. So, this is actually part of the policy we're going to see later on. And if there is a compromise, if a computer is hacked, if a password is stolen, then we have to report it to somebody who's responsible for cyber security at Georgia Tech, maybe in your unit or could be the university wide, but yes our policy does, so this option is also there. The third one says Georgia Tech computers cannot be used to download illegal content, example of that here is child pornography. It's absolutely a part of that Our next question is about a botnet operator. So let's say it compromises a number of computers in an organization. These computers are running malware [INAUDIBLE] by the bots, and they're sending lots of spam email, but they don't look at any sensitive data. They're all just using the computational resources and the network resources to pump out lots of spam. So, sensitive data, they don't do that, or they don't even interfere with any legitimate activities that may be going on on those machines. So, if this happens, tons of spam email sent by computers on your network, what should you do? I guess this is one of those situations, again, where compromises happened, and when something goes wrong, how do we respond to it? That's what we're thinking about here. The abuse of resources by this unauthorized parties. We have to detect and act on it, so the first option is the right one. You really won't want to recommend the second option because although sensitive data is not sent out, as I said, you may get blacklisted eventually, you wouldn't be able to communicate, so of course this legitimate activities will be impacted So let's talk about sort of planning for security. When we talk about security here, obviously we're talking about cyber security. So of course, the first thing you're going to ask is, you talk about what is of value what are the assets? What needs to be secured? You've been asked to come in and plan. The next thing you have to say is if something has to be secured, whose responsibility is it to do that? So who is responsible for it? Okay once we do that, the next thing then we're going to say well security is going to require some controls okay, so once you have that [INAUDIBLE] we know who is responsible for it. We know what kind of things they're doing or what controls they're putting in place, but are people really supported to do what the need to do? Do they have the budgets? Do they have the authority? And one thing to keep in mind is that no matter how well we do all these things that we are talking about. The risk is now going to be zero. So the chance, or likelihood that something might go wrong, well it's going to be there. And if something does go wrong, fortunately if that were to happen, how do we respond to that? How do we recover from that adverse event that has happened because some control either didn't exist or didn't function as it should have. And of course if we had people who are responsible for doing certain things in our security planning that we did and something bad happens, there has to be accountability. So in the planning process we always have to keep in mind that we're never going to get 100% security. Okay. The risk is never going to be zero so we have to worry about, we sort of focus on prevention, detection but then of course we have to focus on response and recovery as well. And the planning process has to cover all those aspects. The first part of security planning is to start doing inventory of our assets. What is that we have which is of value? And something that is of value of course, has to have some risk to it. So there has to be a threat. That is the source of that risk that is faced by this particular asset. So the service is the software, the hardware, these are the assets that we have. And we have to concern ourselves about securing them. So, when you think about cyber security and planning for it, of course, you have to worry about all these assets, or the list of what needs to be secured has to include all these. When you talk about the software, so this is sort of the hardware. When it comes to software, we're running operating systems on these servers, or laptops, or mobile devices. The databases that store large amounts of data, perhaps, the services that are talking about the applications actually we have on the devices. Again when it comes to software and services we have to worry about that. And of course the databases or the file systems, they're storing lots of data, either structured data or in files. Some of the data is going to be sensitive. Some of it could be highly sensitive. Could be your Lexile property, you could in employed accords and things like that, so of course we have to say where this data resides. What kind database, which server, and are we securing the server, the database, and the data that's stored in it. And whenever you talk about assets and securing them, securing them from whom? Are we concerned about external attackers or hackers? They could be cyber criminals. They could be motivated for some other reason. Activists, for example, activists perhaps don't like you or your business or whatever it is. So is it remote hackers that concern us, when it comes to securing all these assets that we are talking about? Or it's insiders, it's employees within the organization So in this question, you're actually asked to sort of think about what are some of the challenges that potentially can sort of help explain why the VA did not do well on this cyber security audit that was done by the Inspector General. So there are three options, and check all the ones that you think are possible reasons that may have contributed to the sorry state of cyber security that existed in the VA and resulted in a failing grade. So the first option here is saying the VA needs to manage, or the CIO who's explaining why they got the poor grade. It's talking about a huge number of devices. So this is how the complexity gets you reason saying no we have such a huge problem, okay? And that's why we not able to fully address it. And actually this is one of the explanations he gave. The next option here says lack of sense of urgency in fixing vulnerabilities. And the CIO actually did not say he did say that you know they take vulnerabilities seriously, so this wasn't that reason. The last option here is choosing to support key functions even when this could introduce vulnerabilities. This is kind of interesting. We say sometimes security gets in the way. These organizations have to get certain things done. And when it you have to choose between you know not being able to do something and having security of course you're going to say no, this needs to get done. And even if there's some vulnerability that exists, I have to take the risk. Because the risk of not being able to do it is, is greater. So the CIO actually did say that there are situations where they run into that they must support certain functions knowing very well that support those functions are critical. They need to be supported, but supporting them is going to introduce certain vulnerabilities We talk about people who are responsible for cyber security. Well, the Chief Information Security Officer, or CISO sometimes also called CSO, Chief Security Officer, is the executive who is responsible for information security in a company. If you think Target had a CISO when the leaks happened, you check yes. Otherwise you check no. The answer to this question is the second one. They did not have a CISO, or CISO, at the time. They did have a CIO, and she was fired, who was victim of one of this breach. And we're going to talk about later on who's responsible, who sort of entirely focuses on cybersecurity. The CISO's job is to do that. I should say that post-breach, they did actually end up hiring a Chief Information Security Officer who came from General Electric, where he was responsible cybersecurity and risk. So security planning, of course, has to address this question of what sort of controls make the most sense given our assets and given our threat, posture, and so on. So what are some of these controls? So first thing that you have to worry about is identity and access management. This should come as no surprise because you've been talking with authentication. That tells us who is making a request for any kind of resource we have in the system, and then we have access management. So Identity Access Management is a buzzword you hear all the time. Access management basically says if somebody is making a request for a resource, do they have permission to access that resource? So, credentialing is essentially deciding that the person is, we're not going to worry about what you need to show where to show who you are. And based on that they're going to create an account for you. And they're going to decide what kind of access should be granted to you. Using passwords, then, for example, you may have password policies. How long the password has to be. How frequently it has to be changed. We talked about the Georgia Tech policy for example. Some places may have a multi factor authentication for example, they may require that you have a token in relation to a password and things like that. Then we have our assets we are talking about, we have networks and we have hosts, servers and desktops and so on. So we have to have defenses for those as well. So we may have firewalls who control what comes into our network or our that leaves our network. We may have Intrusion Detection and Prevention Systems to look at the traffic with the network level then look at network traffic or host activity, and so on, and decide if there is something suspicious that may be going on. Another controller solution we might acquire is that people have anti-virus systems running on their machines. People talk about their effectiveness and question how good they are, but there's not going to be one solution that takes care of everything or one control that takes care of everything. They all help you increase the level of assurance, as we said, that you are secure. So we may require this. If people access our system, the network's from outside, we may require that we have a VPN solution, a Virtual Private Network solution. If they bring your own devices, that might necessitate something else. We know that the software, we are talking about systems and controls, so the software we run on our systems may, vulnerabilities may be discovered and patches may become available. And when we talk about controls I think we're talking technological controls. These examples of course, password management or identity management, firewalls and so on. So it looked like those, but not a control is how do we educate our users? So this could be, for example, some sort of periodic thing you run. We've seen many companies, for example, to educate their employees so they don't fall for phishing attacks. Monthly or they have these things where they send you an email and if you fall for it, this is not a real attack. This is sort of used to train and educate you that you shouldn't be falling for those kind of things. So we talked about security planing sort of, consist of many different parts, the last one was we talked about controls. Next, we talk what we call a security policy that you have. Who talking daily about enterprise security policy. You could have policies at national level or something like that. Federal government may have one across its different agencies, and each agency may have its own and things like that, but if you think about this in the context of an enterprise, security planning requires that you have some sort of a security policy, what exactly is a security policy. A security policy really sort of at the high level articulate, what are the security objectives are? What is that want to? What goals do we have? We want to maintain confidentiality. We want to make sure that sensitive is not disclosed to parties not authorized to see it. We want our systems to be available. We don't want our data to be corrupted. We don't want sensitive data to get into the hands. We want to be legally compliant or whatever. Cause you can think of all the kind of security objectives somebody might have. Which actually motivate what a policy is. So often this high level articulation requires it would include some sort of a legal, business and regulatory rationale for why the policy has what it does. While legal and regulatory you don't really have lot of choice. Business, reasons tell you why it's good for the enterprise or for the company. And what's good for the company, of course, is good for its employees. So that's the articulation of the outlined reasons why we want to have the kind of policy that we do. So the policy is really what you should do and what should not be done. So we talked about passwords, for example. What kind of password you should choose, what the size and length of your password, things like that. We may have web and email policy, for example, can use surf the web while you're at work. The policy actually might say, if something were to go wrong there is some security event that occurs, what sort of response is that we're going to have. Do you need, if your machine gets compromised. Do you had informed somebody for example. The security policy or the dos and don'ts, and we saw some examples of those, of course they may have to do with prevention, so bad things don't happen. They may have to do with detection, so compromise how you find out that there was a breach may have to do with how we respond to that sort of a thing and how remediation occurs, and of course it must address the concerns and impact and that could any of these things have. So rationale, articulation of why things need to be the way the policy says they should be and if it doesn't sort of address the concerns and needs and impact it may have on users of course the policy may not be well accepted. So we talked about organizations as part of their security planning need to have a security policy. While Georgia Tech has a computer and network use policy, we talked about do's and don'ts. What is okay to do, and what you should not do. So let's quickly sort of look at some things that are there in the Georgia Tech computer and network use policy. It does articulate, it does talk about what some of the guiding principles are. Why does the policy look the way it does? First of all, we want the policy guiding principle is that we want in order to protect important IT resources that Georgia Tech has. So when you protect, you're talking about protecting the data, the services, and things like that which are enabled by those resources. Another guiding principle is that we don't let anything illegal happen that involves our computer systems, our IT systems. So these are the guiding principles, but you should read the policy. And here, we're just going to talk about sort of couple of quick highlights, things that you might think jump out and say, why do we have this? So it talks about copyright and intellectual property, actually. So why is it talking about? Of course, as a research university, Georgia Tech creates intellectual property and so on, and that intellectual property resides on our computers. But at the same time, state campuses, they used to have a problem where people would download music. In particular, they will download illegal music, and universities got into trouble because of that. So we're obviously addressing that explicitly in the policy. Remember, the policy is about dos and don'ts, and if it is copyrighted material or there's intellectual property, how that should be handled. Of course, it talks about that. Interestingly, it also talks about export controls. While universities have people from all over the world, and people work with counterparts in other countries and things like that, when there's exchange of things across national boundaries, of course, export control becomes an issue. And that's why it's interesting that it sort makes its way into this policy. But again, this is sort of the guiding principle here, is that there are any legal requirements we are okay with them. Georgia Tech's policy actually also, remember one of the things we said, we have to make sure that the people who responsible for it can be held accountable and things like that. So it does address responsibility for securing the resources or the assets that we have, and it's kind of a distributed responsibility at Georgia Tech. When you look at sort of the larger network Office of Information Technology which is a central organization for the entire university, well they're responsible for it. But if you look at individual devices, laptops, servers, or desktops, all that is the responsibility of the unit or the individual whose machine it is. A unit for example, School of Computer Science or the College of Computing, for example, would be responsible for the computers that are used by their staff, faculty, and students. But protecting the Georgia Tech network that is the responsibility of the central office. As I said, you should read the policy and all the different aspects that are covered by it. The question is, should the policy address personal use of university resources? Remember, this is sort of dos and don'ts, so in some sense, you should say, well, yeah, it should. And if it addresses it, does it say blanket, no personal use of university resources or are there some exceptions? Actually, if you read the policy, and we said the policy sort of describes the dos and don'ts and where they come from, [ersonal use is allowed to some degree. They say it's incidental personal use is okay. So it's all right if you send a message to your friend or you family member. Again, sort of need to use your judgement when incidental personal use ends and personal use that is inconsistent with this part of the policy starts. So, yet, it's not blanket prohibition, but of course, I can't be using Georgia Tech computers to support a business that I haven't assigned. So, next question actually is about data, about students, their grades for example, what is that motivated by, okay? So, the two options, regulatory reasons or the data is sensitive and it's the right thing to do, okay? So, choose either one or both as you wish. Actually student data has to be protected, a regulatory reason there is FERPA. We talked about HIPAA when health data where FERPA is in the educational context. Student grades and their performance is of course their personal information and you can't release that or let somebody hack into your system Fairly recently, there was a large scale breach. Anthem, actually, I was personally affected by it, because I do have insurance in Blue Cross Blue Shield, and so Anthem has all these health insurance that they offer these companies in different parts of the country. And they had a breach, and Close to 80 million customers data was perhaps stolen by whoever breached their systems. So this question is about, actually, what happened after that breach, okay. How did Anthem respond to it? Did they respond well, or they did not? Okay, that's the question. So when it said somebody didn't respond well to an adverse thing that happened. Reasonable people of course can have different answers and agree to disagree. They did a number of things that were kind of right. They actually discovered it themselves. It wasn't in the newspapers [INAUDIBLE]. They fairly quickly actually reached out to law enforcement, also customers and so on. So a number of people feel that they actually did a good job, there were some things that they didn't do as well according to others. One was that since they responded fairly quickly, they didn't know all the details of the response was kind of weird. Others said, well, they didn't inform other key stake holders, or people that they had business with, and things like that. But overall, I think tone was positive, so I'm going to pick a yes here. If you read more about it and you have a different opinion and you have a good justification for it, here it's okay to choose a no. So we said we were going to have controls in place, that we're going to have people responsible, we're going to have policy to inform people of our do's and don'ts and things like that, but that's not going to reduce our risk to zero. There's still the possibility, because there are non-vulnerabilities, where people became victims of a phishing attack, we hear about those on a regular basis. So the risk is not going to go down to zero, something could still go wrong, despite having the best planning and best management for cyber security. So if there is a risk, how do we sort of get a sense of how much risk is that we still have? What kind of risk are we dealing with? Assessing such risk is important because we talked about cost benefit. So if you're going to make investment in cyber security, those investment decisions have to be based on risk and its reduction. So you're going to pay for a certain control. How much is the risk being reduced? And they say risk is going to be reduced because it's never going to be zero. The only way to make it zero is you disconnect from the rest of the world. But then you can't do anything useful. So some risk is going to remain, and if the risk remains, well, how do you quantify it if your investments are going to be based? Investments passed a certain control, what are you going to pay for it? Certainly, there's a number. So if you're assessing risk or quantifying it, how exactly do you? It'll be great to say, this is my risk and this is what I can do to deal with it, but how do you assess risk? And one of the things we're going to find out is that quantifying it is actually not easy. There are various frameworks to sort of see what you do or don't do that may pose some degree of risk, but actually quantifying it is what is hard, so that's what we're talking about. So if we talk about quantifications, is there a formula for risk or the risk exposure we have? Risk is really, we said, how likely, the probability of an adverse security event happening. So the first thing you have to say, well there is a threat out there, they may come after me. I have some controls in place, but they may be able to get around them. And actually, I might get compromised, may experience a breach or whatever it is. So what is the likelihood of that? So you have to have the probability, then you have to multiple by the impact of that adverse event. It's going to cost me $10 million to deal with that. Well, the probability is half. That's way too much. But if it is half and it's going to cost you $10 million, your risk is $5 million. We said quantifying, of course we had to quantify both of these quantities. The probability, as well as the impact or the loss that you're going to suffer as a result of the adverse event that we're talking about. So there's something called risk leverage, because we like to reduce our risk exposure. And one way we can do that is by having some sort of a control in place. A control we know comes with a cost. So risk leverage is for a given control. Talking about quantifying it and then sort of, if it's too high, you don't like it, we can try to reduce it by putting in one or more controls. So when you put a control you can ask what the risk level of that control is. So, the way to sort of compute that is that you say, what was my risk exposure before or without the control that is under consideration? That's without deploying that. Minus the risk exposure after this control is deployed. This difference really is nothing but the amount by which the risk is reduced because of this control that we're going to put into our enterprise. So, that's the decrees in risk divided by the cost of that control that we're talking about. We've been saying, we have to have cost benefit analysis. This is how much risk. This is what happens to risk if we do something about unacceptable level of risk. So this is the cost of that control, that we have in the denominator here. So, if you divide the reduction in risk by the cost, that gives you the risk leverage. For any control that you have, risk leverage should be greater than 1. It makes no sense for it to be less than 1 because, in that case you're saying, your risk reduction actually is smaller than the cost. It's not very smart that you pay more and, overall, you're not in a better place than you were before in terms of the cost and whatever risk that you have. So, this is how we compute the leverage, and whenever you talk about a particular control, if you do this and the value is not greater than 1, then of course that control should not be considered. We said risk assessment is challenging, we don't understand all the threats, we don't know what our vulnerabilities are of course, so it's two playing two. Likelihood of a successful attack of course is going to depend on what sort of vulnerabilities you might have and who's trying to target you. But we said, well if you can come up with a probability and what happens as a result of an attack, then you can compute the risk, that's how you quantify or assess the risk. And you can try to reduce it, but that's really a question of managing that, how do we manage? What options do we have for managing the risk that our business is exposed to on the cyber front. So we talked about assessing it, but if it's too high, then of course, we'd like to reduce it. So how do you reduce cyber risk? As we said before we get into reduction, assessing required the likelihood as well as the impact. So how do you assess the impact? It's the expected loss we're talking about, so it's expected losses, really loss multiply the probability, so we do have get to the loss, and when you talk about loss it could be reputation loss, target of core suffered including degrees and sales. It's the cost of calling somebody in. So, for example, if you're Sony and you call FireEye or Mandiant to come figure out what happened, I'm sure they paid for that Mandiant or FireEye got compensated really well. Your response, it may be legal costs, you may have to buy identity theft protection for your customers and things like that. It's real dollars, reputational. What you do in the aftermath. All that has to add up to sort of the cost or the loss that you would attribute to a certain attack. And expected I said was that value multiplied by the probability. So that's really risk. Risk was probability times impact, so maybe the impact is a loss. So let's just talk about loss here. So this one would still be talking about you're assessing risk. Now let's talk about how do we manage risk. So the only three things you can do once you come up with some sort of a risk estimate. You can say well, I can live with it. In that case, you're accepting the risk. You may want to transfer it. So the risk is transferred to another party. So if something bad happens to you, the consequences or the cost is going to be borne by somebody else. That somebody else is going to do it only if they are paid to be in that business. And that business is the insurance business. So you can buy insurance. And by buying insurance, you're transferring it to whoever insures you. The other option is you can reduce it. How do you reduce it? Well, you're going to reduce it by deploying new technology solutions, and maybe a more expensive, more effective firewall. You can reduce by educating your people or having training and things like that. More security awareness, of course, is going to ensure that it's less likely that they make a mistake, so that's going to reduce the likelihood of the probability of some adverse event. Which we know a factor, the way we compute our risks. So, managing once you have an estimate of the risk, there are a couple of options on how you can deal with that risk. As we said this transfer thing happens in a lot of other domains. You buy insurance for your car, for your health and things like that. Now that we talked about risk, and we said risk is actually probably multiplied by impact. Impact is the loss you incur or suffer as a result of the breach. So, this questions is about, how do you figure out that loss? So, the company, we talked with Anthem for example, or Target, stores sensitive data, customer data. Data that could be used by identity theft for example, or credit card numbers and things like that. So stores sensitive customer data. Impact of a breach of such data. And remember the risk was probability times impact. So this is saying, what are we going to include in the impact? First on is cost of purchasing identity theft protection for your customers. So Anthem actually offered that to me. Some banks offered when they had a breach to their customers and so on. So this is a cost that is definitely because of this breach that we have had. Loss of business due to reduced customer confidence. Well, that is an impact of this breach, as well. It is a loss and it impacts you adversely. So, that should figure into the impact of this event. Compensation for new cyber security personnel the company hires to better manage cyber security in the future. Well, this is, if you include this or not, actually happened as a result. Maybe you wouldn't have done it if you didn't if you didn't have this breach, but this is really sort of investment in the future. So, in the future, we don't have such a breach, or don't get attacked. Act the way we did this case. So this is really part of the response. It's how you get to a more secure state. This impact is probably not something that should be included, the loss that occurs because of this particular incident, or this breach. This is something that probably should have been there and if it was there then this situation wouldn't have happened. There are two options. One is we look at cost and risk reduction. So one is $100K. That is reducing the risk by 150. The other one is 250 and reducing the risk by 500. So, the question is, which solution would you recommend? So how can we answer? Of course, one is cheaper but the risk reduction is also lot smaller, it's 150 only. And it's the cheaper because it's 100K, but the risk is only reduced by 150 here. Other one is two and a half times as more expensive, but it's reducing the risks lot more, 500K. So, we said one way we can think about it is risk leverage. So the risk leverage was reduction in risk, so the first case, it's 150, divided by the cost, and that's 100. So the risk leverage is 1.5 for the first solution, or the cheaper solution. So this is 1.5. For the more expensive solution it's 500, the amount, this is how much the risk is reduced by, divided by 250 which is the cost. So risk leverage is higher for the more expensive solution, and based on that reasoning you would recommend this one. So cyber insurance was one way to, remember managing risk accepted, transfer it. So cyber insurance is how you transfer it. Is not very popular. Based on a 2014 survey, what percentage of customer's major insurance brokers were interested in buying cyber insurance? There is more to read about it, but car insurance is mandated, you have to have it. So when it comes to cyber, the question is how many people actually are interested in buying cyber insurance. Small, which is really, the exact number is not important. Is small or significant? These are the two buckets you're asked to choose from. So, what do you think? Which answer is the right one? Think and mark, and then we'll talk about the solution. I guess it gave a very, when it says it's still not very popular, of course, a lot of people not interested in buying it. There are a variety of reasons actually. People who are selling cyber insurance have too may exclusions. It's not really worth the cost. You pay for it and when you need it they say this this is excluded and things like that. So people are interested in insurance when they think it's a good value for them, which means there are not too many exclusions and the cost is low, and those things don't hold when it comes to cyber insurance. So not very popular not a lot of people are actually asking for it and buying it actually. So that's what this link has more information about it. You should read it. So the whole idea of security planning and security management is to be better prepared when it comes to cyber security. So, if your enterprise, your company, if your ask how well prepared you are, how good is the state of cyber security? Really, that's what the cyber security posture is. So, cyber security posture,how do we address our security, how do we prepare ourselves for cyber security, how do we handle when something goes wrong. The posture can either be reactive, unfortunately, in many cases, this is what it is. What does reactive mean? Well, we worry about it, because we're sort of forced to do it. Because there's a regulation compliance requirements they say you must do so. And we react to that regulation that comes down our throats. All it's customer's demands, customer's saying I will not do business with you unless you have that. Lot of companies these days, especially business to business, b to b, they don't want to business with someone who has lax cyber security. So again, we react to this demand that comes from somebody we want to do business with. It could be in response to something bad that happens to us, okay. So breach for example, the target, Home Depot examples, everybody talks about. So, we react to the adverse event that we suffered, so that's another example of what reactive security posture is. And, finally, it may be in response to events that occur, our competition may have suffered. One bank, for example, suffers a bridge and other banks may react to that by making sure that the vulnerabilities that were exploited don't exist in their IT systems. So all these are examples of, so this is a reactive posture, I said, unfortunately security is event driven when something something bad happens. People say, we need it and so we need to do something about. But this is what is called a reactive posture to cyber security. Of course, it's better to have what is called a pro active approach to security. You plan for it, that's what security planning is talking about. You work hard to assess your risk, you do things to reduce it, look at the cost and do that before either you're forced because of regulation or forced because something bad happens to you and you respond to it reactively. So what are some examples of proactive things that companies should be doing? Well first of all, it should be somebody's job to worry about cyber securities and assets, how they protect it, how people are educated, cyber security insurance, having a good policy. Making sure people are held responsible and empowered to do things, and all the things we talked about when it comes to security planning and management. So having a champion whose job it is to worry about it, a champion should have influence. If is doesn't, he or she doesn't have influence of course you're not going to get anything meaningful done. So one thing people talk about, for example, is well, is this proactively addressed at the highest level in an organization? Talk about the board level conversation. The board addresses various kind of risks an enterprise has to deal with. Is cyber security risk one of those, or is there a conversation about it? Is there a conversation about what needs to be done to reduce that risk? Is there an investment made in cyber security controlled in solutions and personnel and things like that? If that's done before bad things happen, that would be a proactive approach. It's again unfortunately not aware this is happening in every company. In fact, one of the things that happened we said target hired a CSO, who does the CSO report to? Does the CSO report to CIO? Well when it's not talking about the highest level it doesn't report to the CEO. The CEO maybe doesn't bring it up at the board level and things like that. So proactive means somebody, the champion, looks ahead, has the influence that's necessary and has support at the highest levels and that would be a proactive posture for cyber security. If you are the person who has to champion cyber security and get the company to a proactive cyber security posture, how do you make your case? You're champion, it's your job to be as well prepared as possible given the cost benefits and risk tolerance and so on. What are the kind of arguments you can make? The easiest argument to make is the bottom line argument, or the economic argument that it's going to save us money. Of course security doesn't make money. Security we would say it's a cost. It doesn't bring money in, but we could lose money because of that. The economic argument we can make is, the return on investment argument, or the ROI argument. Saying, if you don't do this, we're going to lose $10 million. If you do this, it's only going to cost us 1 million and we save 5 million and the losses we're going to suffer are going to be reduced by half. Then of course it's an investment you make, you're putting out one million but you're saving five million. So that is a pretty good return on investment that we have. So the economic argument of course makes sense when you say that by coming up with a budget to put a control in place. The cost of that we're going to have Is going to result in savings that are much greater than the cost. Significant savings greater than the cost, so there is in a way a return on the investment. This is money we're going to lose that we're not going to lose now. And that's how our bottom line is going to be in a better state. Of course this does require that you estimate the cost benefits that we're talking about, and unfortunately, it's pretty tricky. Cost and benefits, either you can exactly quantify them, so that would be data driven, or its perception. So with perception it's sort of what they perceived to be the risk, and what they perceived the risk reduction, maybe if we do A or B, or something like that. Versus sort of actually knowing those probabilities and those impact numbers that you are talking about. So you're going to make an argument. This is all fine and good, but a lot of times it comes to perception, and then people would say, well, this is their opinion. How do you know for sure? Do you have some hard numbers? And unfortunately, making this argument is difficult because a lot of times those hard numbers are not easy to come by. So we've been talking about security planning and management. There are many sort of different pieces that make up this whole process of planning and then managing the security. How do we bring it all together? If you're going to go explain this to someone in a few minutes. What are the things you should be talking about. Well first of all you're going to say that there are things that are of value which are at risk. Maybe your reputation, maybe your intellectual property, there are assets that are of value which are at risk. So then you're going to talk about the risk comes from certain threats sources. Not only there are bad guys out there. Of course, we have certain vulnerabilities, those are the attack vectors, that's why those threat sources would be able to do us harm. That's why we need to worry about, the risk comes from the fact, not just because we have something of value, but that thing could be harmed or could be breached. So the threats and attack vectors, really talking about threats and vulnerabilities. Together they come together, result in an attack. So an attack then leads to a breach. And that's where we have the adverse consequences. So if we have that, we have to talk about things that are of value. We have to talk about threats. These two are the reasons that help us explain that there is something at risk. And the amount or level of risk that we may have. And the way to manage that risk is to plan for security, implement whatever controls that are meaningful, and then sort of manage that in terms of controls, people, processes and things like that. And all that has to be done, again, this control that we're talking about, of course we're going to look at the cost. The benefits they offer? What impact they have? How user friendly they are? Things like that. So we have to sort of look into the controls that we have, what their effectiveness is. Then we have to identify people. It's their job to worry about security. We have to empower them to do it. They have to have the budget, they have to be able to have policies that can be enforced and things like that. And people have to be held responsible and accountable. Because we know that something was wrong. They should be called to answer what exactly happened. And no matter how well you do that, we have to plan for something going wrong. And when something goes wrong, a response is going to be needed. Remediation has to happen. There shouldn't be any surprises. We should be prepared and have a plan in place saying if something bad were to happen, this is how we're going to respond to it. Of course, people as we said, need to be aware of the importance of security. The guiding principals, the do's and don'ts, all the things we were talking about under security policy so we need to think about that. And finally we need to understand and people have to recognize that a proactive option is what gets you ahead of the threats in some sense. Threats change continuously. If you're just going to be in reactive mode something new is going to come and hit you and you're going to suffer the same sort of thing that you did before. So proactively addressing the risk and the threats and the vulnerabilities and having a champion to do that is another part of this security planning and management that we're talking about. Overall understand what is of value, why there's risk to it which is the threats and the vulnerabilities we're talking about. And then plan, implement, manage, and do that all proactively. That's sort of bringing it all together when it comes to security planning and management According to this report, this yes/no question saying, are we budgeting, making larger allocations for cyber security, as the threats become more sophisticated? According to this report that I'm talking about, and this is fairly recent, the answer is no. In fact, they report a slight dip in 2014. So people are not investing. Either they don't have the money to invest, they don't think they're getting a good return on it, maybe they don't understand the risk very precisely. If it's vague then of course it's hard to convince someone to put out the dollars to do that. So now cyber security budgets or investments in cyber security are not dramatically going up. We talked about sort of reactive and proactive security measures. So this is saying, we have two options here. Which is you would say is an example of proactive security? Remember reactive is sort of post something bad happening, it's the cleaning up and doing something in response. Proactive is sort of getting ahead of that, so which one of these two you would say is proactive measure? The first one is not really proactive. The regulation is sort of forcing you to do something and when you comply, you essentially, you're reacting to the regulation and doing what they demand that you do. But the other one is Chief Risk Officer of the company addressing cyber risk regularly at the highest level. And risk and investigation discussed, I would say that is proactive. Because you do this on a regular basis. And highest level means you do something to address them as well. So this is proactive versus reactive that we had discussed We saw that managing cyber security is a complex undertaking because it has technical, people, and policy dimensions. We studied the tradeoffs that exist when we consider various security controls in the context of an organization. Security management covered how we can explore these tradeoffs. And pick the right solutions to manage the cyber risk that is faced by an organization. So far we've been focusing on defending ourselves, but we know that the attacks come from malicious actors. Are there ways that allow us to go after these malicious actors and have consequences for them? Well, one way to do that is to have laws that govern illegal online activity. In this lesson, we're going to talk about cyber laws that address these and some of the issues that are related to intellectual property and its protection. And then we're going to talk about, what is considered ethical behavior when it comes to our online activities? In the second half of the lesson, we're going to focus on online privacy. In particular, we're going to address, how information about our online activities can be captured by others? How can they use it? Who can they share it with? And, what kind of options do we have in controlling each one of these? So laws related to online activities in particular are malicious online activities. So, things like theft and extortion. Of course are illegal in the physical world. We talked about data theft. Someone hacking into your computer, walking away or siphoning off your sensitive data, using that to take on your identity is identity theft. Gaining access to your financial accounts or getting credit cards and things like that, it could even be extortion, from locking up your computer to doing a denial of service attack, saying your website won't be up unless you pay up. You would think those kind of criminal things that happen in the cyber world, we should have laws related to those. And we going to see that actually is the case in the US. The other set of laws have to deal with copying and distribution of digital objects. So these digital objects are essentially, your intellectual property. It could be software. It could be other kind of content like music, videos and so on. And if it's your intellectual property, we know that in the known distal world we do have intellectual property. And we have ways to protect those. And the way we do that is through copyrights or patents and trade secrets. So question is, do these also apply when it comes to digital objects that we access and manipulate in the online world? And if there are, we do need to have these kind of laws, how exactly do we apply them? There's something different about the online world compared to the physical world. So how exactly, for example, copying something could be extremely easy. That may not be the case in the physical world. So, if you do have we allow someone to copyright a digital object or protect it. Well how exactly the law will be applied is a question that you might have. The other thing that, we actually not going to talk a great deal about it, is the idea of privacy. We do all of things online. So our activities online what exactly we do, what websites we visit, what services we access, what do we search for. It's very easy to collect all the kind of information about these activities. These are my activities and somebody else can collect information about what I do. Are there any laws that protect me, in terms of what kind of information could be controlled, how could it be used? There are actually, I said we'll get through some of these, but not all. But the idea of privacy here is that the internet allows, it makes it possible at a scale of magnitude that we have never seen before. So, the idea of Big Brother for example, somebody knowing everything that you do, is there something in the laws to protect me from that? So in this question, what we're asking is, having laws, and sanctions against criminal acts. Well, does that help reduce the magnitude of the treats? Or do you think that's not quite the case, based on what we have seen? So, there are two options, choose the one you think is the best one. So I'm actually going to go ahead and check the first one as the correct answer. Clearly whether it is to do with theft of data, identify theft, or theft of intellectual property, somebody invading your privacy, we need laws against those. Those people shouldn't be able to do it without any sort of a consequence. So criminal sanctions against those kind of activities, in particular those cyber-crime laws, are necessary and we have them. If you're going to pick the second option, we will actually talk about a number of challenges that you have to deal with when you have laws. And you perhaps can take the position that no really laws, this notion of what is right or wrong, we can debate that. Sometimes, while they're overarching and things like that, but this is more about having legal deterrents, that their consequences intend on doing harm to others, their consequence. I'm going to go over this one So our next question actually is about how bad is cybercrime, in particular the cost of cybercrime. People say it's a big problem. And people actually try to estimate it. So there's a pointer to a report that talks about cybercrime in the US and other countries. So this question is sort of your intuitively what do you think how big this problem is? If you look at both options, it's in billions of dollars, but is it tens or is over hundred? Those are the two options Okay, so cyber crime actually is big. In fact, the estimates are that for the US it is .6% of the US GDP, which I think the most recent numbers are sort of approximately $17 trillion. So if you take .6% of $17 trillion, it's going to get you over hundred billion dollars in cyber losses due to cyber crime. So, we're going to talk about a couple of the U.S. laws related to cyber crime and illegal online activities. The first one is the U.S. Computer Fraud and Abuse Act, that's CFAA, actually goes back a while. But the goal of this law or this act, was to define what is criminal, when it comes to things that you do online. So, various types of online abuse. And then define criminal sanctions against those kind of actions or activities. That constitute abuse or fraud. So, it really covers unauthorized access to computers. So, you're not allowed to be some place in the physical world, we have laws like that. Here we're sort of trespassing somebody's computer that is connected to the internet and we can reach out to it. So computer obviously has things of value that you are after. If you happen to the the cyber criminal, that's who is being targeted by this law. So I said it goes back a while, initially it was rather net-over it was trying to protect. It's sort of multuation came from national security, national defense. So, it's saying if you gain unauthorized access to a computer that has data related to national defense. That is illegal. This was then extended to financial data, because people do care about their money. And criminals, cyber criminals actually after money. They want to steal your online account or steal credit card information and so on. If you access either defense related or financial data and the access is unauthorized. CFAA says that that's an illegal act. So, what exactly is unauthorized access? Unauthorized access essentially, is getting hold of this data, gaining access to it. So remember, when it comes to data we had the confidentiality, integrity and availability requirements. That we had the CIA requirements, that we had to find when you talk about securing data. So, gaining access to it, you're able to see the data that you shouldn't be able to see. Use it, for example, of financial gain modification is affecting its integrity, corrupting it. Destroying is exactly that, disclosure is to anybody who's not authorized to see it and so on. If all this is done when the system under consideration happens to be a government system. Well that's how CFAA started, so this basically is saying the law covers access to information. Modification, we're not allowed to do so and using that information in any shape and form. Destroying it, affecting availability, that's what you would do with this, is illegal. So, this one initially was something that is operated by or on behalf, so it could be a contractor or the US government. But this, as a history this was broadened to include any computer. The language is any protected computer, basically a protected computer is any computer that's connected the Internet. So, a protected computer, if you access it without permission, and gain access to the data, or destroy it. Or corrupt it while you're committing an illegal act. It then actually keeps getting more and more specific. Transmitting code or malware, malicious software that can damage a computer, well that's illegal as well. And finally, it specifically addresses passwords that I'd use to gain access to computers. So, if you steal a truckload of passwords and want to sell them on the black market, the underground market. That's trafficking in computer passwords and if you did that, that would be illegal as well. So, accessing data that is not yours in various forms. Infecting computers with malicious software, trafficking in passwords that allow others to gain access to computers that are protected. All those things are covered under the Computer Fraud and Abuse Act that we have in the United States. So the other law that we're going to talk about is the Digital Millennium Copyright Act or popularly known as the DMCA. And this has to do with, we discussed intellectual property. So content, digital content software, and the problem it helps to address is piracy. Illegal copying for example or theft of these kind of digital objects. That's what DMCA covers. So first of all, it says that digital objects, the kind I was talking about, actually can be copyrighted. So you have protections that you get when you're able to obtain a copyright on something. Well, the same thing now applies to, that was the non-digital law, the same thing now applies to digital objects. Well, DMCA says it is a crime for you to circumvent and disable any measures that are there that have to do with copy protection and anti-piracy. So it is built into the object, it sort of and undo that, and that act is considered illegal if you do that. It also not only can't do that, it makes it criminal for you to actually, whatever idea that you had to produce something based on that, manufacture it. And then sell and distribute it so somebody else can defeat the copy protection or the anti-piracy measures that are in place. So essentially what we're saying is that we need to protect those and it's illegal to undo those protections and sort of indulge in commerce that allows you to let other people do the same. But the DMCA does allow some exclusions. So, there are exclusions for research and educational purposes. So, if you're doing research in sort of understanding how strong, secure some anti-piracy techniques are, hopefully you're not doing anything illegal. Similarly, as I said, libraries can make some limited number of copies and that's not viewed as illegal either. So that's one of the exclusions. However, I should say that, one of the criticisms people have for these laws is that they're kind of vague and broad. And people sort of have different interpretations of those depending on who they are. So although we sort of say there are exclusions, of course, how far those exclusions go, there have been disagreements about that. So RIAA, which is the Recording Industry Association of America, they'll actually file lawsuits against P2P sharing music. So these are on one side, the Electronic Frontier Foundation or EFF, sort of what is about a too broad a scope of these laws and potential abuse of the laws themselves So what are some of the challenges when it comes to the CFAA or the DMCA? The laws that we just talked about related to fraud abuse and protection of distal objects. First of all, enforcement is actually quite challenging. It's difficult for a number of reason first of all, attribution is hard. How exactly do you know that this is the person who was responsible for something fraudulent that was done? We know that a request or data may have flowed to a certain computer, but there may be malware sitting on that computer. It may not be any fault of the person who owns that computer, it could be some bot that's under the control of somebody else who's actually doing that. So how exactly do you attribute a certain illegal activity, or action to a person? And what makes this really even harder is the fact that the internet has no national boundaries. So that international nature or the transnational nature of the internet, of course presents another set of challenges. Finally, I think we had a quiz question talking about have the various laws related to computer fraud done any good? Have they reduced cyber crime or has cyber crime gone up? Well one of the challenges that we, making laws is a slow process. And cyber criminals can basically stay ahead of it. The criminal ecosystem can keep evolving. They can work around the legal safeguards we put in place. Though and that can happen very rapidly, much faster than our ability to put laws in place and find ways to enforce them, get people in front of judges and courts and things like that. So there have been cases we're going to look at a few, but obviously one of the challenges. How do you stay current with what is going on in the cyber criminal world? Now that we have talked about a couple of the laws, U.S. laws, let's see how they have been applied. Have they been used, the cases where they were used to go after someone? The first question actually is asking you about The Melissa virus. It was a while ago, I think in the early 2002, or 2003, or something like that. The CFAA was used in this case. The first use actually was, of the CFAA, was for the first large scale Internet worm, was the Morris Worm, but this came later on. And the goal of this question is, what exactly did the author of this virus do because of which he was sentenced to a prison term and also fined? What was the abuse that was illegal, that was perpetrated by the author of this virus? So we talked about the CFAA. We know that to destruction of data on a protected computer is illegal and carries criminal sanctions. Similarly, denial of service, becomes unavailable to you, interruption is illegal as well Okay, so the Melissa virus actually did the following, it sent email. So once it infected a computer, it'll find your address book and send an email to everybody else. And as these emails got to people and more computers got infected, of course, more and more emails got sent. As a result essentially servers and various networks were saturated with the traffic that we generated because of this and they basically became unusable. So it was abuse where it wasn't destruction of data, that wasn't what this virus was doing, but it was consuming resources and sending all these irrelevant email messages. Because of it we authorized people who were not able to use the systems and the network. So the fact that you did something such that, that was unauthorized obviously what you did, the fact that you made these computers unusable. That was the reason this person was actually sentenced to something like several years in jail and fined. Again it's not destructions of data but it's essentially exhaustion of resources through this denial-of-service attack which makes the system essentially unavailable to everybody else. This is a case involving two companies. One company had some, lets say, proprietary data that its customers could download and see. Except that you can get a trial subscription, and try it before you buy it kind of a thing, and gain access to that data. So, in this case, the person who got a trial subscription was employee of another company that competed with this company whose data we are talking about. This other company's employee got a trial subscription, downloaded data, and the data was available to anyone who would provide information for a trial subscription. But the company whose data it was, actually went to court. And its attorneys said, the access by the competition of the data, it was unauthorized and covered under CFAA, so they should be a penalties for this company. And so the question here is, do you agree with the attorney of the company who wanted to sue this competitor for its employee having a trial subscription to download this data? Or do you think this is authorized because anyone could actually try the trial subscription and gain access anyways? So the judge actually ruled that this was not covered under the unauthorized access provisions, that we have in the CFAA, the Computer Fraud and Abuse Act. Because the data actually, essentially, was publicly available. Okay, we're going to do one more question, quiz question. This is related to the Digital Millennium Copyright Act, or DMCA, and we said digital objects can be copyrighted, but there are some exclusions for research education purposes. The question is, you know, if you're talking with DMCA, which one of these, the DMCA would cover? The first one to do a anti watermark removal, or the second one to do a assessment of security, or looking for security vulnerabilities in the way you pay fare for the transit system? So choose one of them. Then, we'll talk about it. Well, since we're talking about the DMCA with you know it's about protecting digital objects. And circumvention of techniques that are used to, to protect them. First option was the case Professor Felten at Princeton actually was threatened. He was doing research. So this was sort of a, a, a competition where you would be given an audio sample that had a watermark in it and you had to remove the watermark without affecting the quality of the audio. That was protected by this watermark. So Professor Felten's group actually was able to do that and there were more details to this particular case but they wanted to publish a paper on how this is this can be done. What sort of techniques that were used and they were threatened with a lawsuit. And became sort of a really big deal thing. Eventually they were able to, to publish it, but the idea is that circumvention of watermark which is used to protect this music object that we're talking about. That is covered by the DMCA. The second option I would not choose here because this is actually in a, as we said, finding vulnerabilities and based on those vulnerabilities you can say defrauding the Transit Authority because somebody who learns those, finds out about those vulnerabilities doesn't have to pay the transit fare. And if that's obviously not good in that case the Transit Authority is defrauded in this case. So the second one actually was covered by, or the, the students were threatened with that they'll be taken to court under the CFAA, the Computer Fraud and Reviews Act provisions. This also idea is that it's research that you want to publish it and this is comes again and again. While if you do that won't bad guys be able to then use that to, to successfully craft attacks? Now who knows if the bad guys going to do it just because of this? They may already know it and things like that. So again this was somewhat controversial and, and I think the students were able to talk about what they did or their methodology and what they found at some conference. So the second one clearly is sort of a CFAA type and the first one is the DMCA. So how this could be applied is, is what we wanted to to explore with this question. Okay so we just have one lesson that covers law ethics and privacy so we not talking about any of these in great detail. As I said, I only talked about two of the laws, I didn't get into privacy. But we do want to very briefly talk about some of ethics related to. Online activities. So what's the difference between laws and ethics? We know that ethics have to do with sort of, an individual's sense of what is right and wrong. On the other hand, laws are sort of societal, we as a society decide state, local, or federal, whatever it is laws that say what is wrong. If you did that, you would have certain kind of sanctions. Laws may not cover certain things, but your sense of right and wrong might say well I should not be doing this. And we talking about that. In the context of what we do online or what we do in cyber space. Before we sort of go further, it's worthwhile pointing out that when it comes to your ethical sort of your sense of right and wrong, of course there is no external arbiter or enforcement agency like law where we have police and the courts and things like that. So what's sort of the example of an ethical code that you could think about? So what happens if you discover a vulnerability in a commercial product that's used by lots and lots of people? What is ethical disclosure of this vulnerability? So here is the idea and the dilemma. If you disclose it, it's disclosed to the bad guys. The bad guys can then exploit it. And then can go and attack the system in which you have discovered a vulnerability, okay? So that said, maybe disclosing it is not a great thing, the other way to think about this is that if you don't disclose it that doesn't mean that the bad guys actually you want to find out about it. And if you don't disclose it, then, there's no pressure on the vendor who actually produced this commercial system or product to actually do something about it, okay? So, by disclosing, you sort of fix in with problem or vulnerability we have, hopefully in a timely fashion. But at the same time, you're also making this information available to bad guys. So that's where do you draw the line? What's the ethical thing to do here? So of course, we have our personal notion of right and wrong. So, it could be sort of your personal thing or it could be a code of conduct defined by some professional society that you may be a member of. So ACM or I triple E, or even Georgia Tech. So ethical standards are, in a court of conduct, doing the right thing. Knowing what's right and what is wrong of course, laws and ethics go together to help us understand what is that we can do without doing anything illegal or doing anything wrong versus what is that should not be done because that's the wrong thing to do. Guesses. Further explore this idea of ethics and laws that we covered before. Let's look at this question. And this perhaps has happened to you, and happened to others. Let's say due to an error or mistake, somebody sends you sensitive data in an email. They just put the wrong attachment. They were trying to send you one thing and end up sending something else. Well this is sensitive information that is arrived at your desk, or your desktop, should you read it. First of all, if you were to read it, you obviously didn't go into the computer, and didn't do unauthorized access that's been sent to you. So, the two options we have here, should you decide to read, or not read it. And if you decide not to read it, what's the reason why you think you should make that choice? So pick the right option, and we'll talk about it. Okay let's look at the two options, so the first one says professional code of ethics requires you. So if you read the IEEE or ACM code of conduct that I was talking about, actually does ask you. And personally that may be something If I don't want others to violate my privacy I have to give the same to others and not do things that violate their privacy. So my ethical standards or professional code of conduct says even though the file has come to your computer, the information it has is none of your business, you shouldn't be looking at it. So, that's what I would pick in this case. You shouldn't be liable under CFA because you didn't break into that computer, okay? You didn't use unauthorized access to pull this data out, so because of that I will not pick the second option. We're going to do one more quiz question before we move on. And this has to do with, we did talk about responsible disclosure and the dilemma you face if you do or if you don't. So in this case US-CERT, this is the computer emergency response entity that we have in the US. And there are similar CERTs in other places, other countries. So it follows what they say a responsible disclosure process for vulnerabilities. If you find a vulnerability you can report it to US-CERT. And US-CERT has, we talked early on about a database of vulnerabilities. There's the National Vulnerability Database and so on. So, you can report it and then it'll inform everybody by adding it to one of those databases. So, you find a vulnerability. You report it to US-CERT. What does US-CERT do, or what should it do? What is responsible disclosure in this case? Make the vulnerability information available to everyone. It doesn't know who are affected, so basically that amounts to make it public immediately is the first option. The second option is provide a certain amount of time. An argument for that is that if you inform the vendor and provide a certain amount of time, maybe they have a patch to fix the vulnerability. So before it's public including the bad guys come to learn about it, well, we at least have a way deal with it. So that's the second option. So think about what you would consider responsible disclosure. Actually, US-CERT uses one of these, I'm asking you which one you think is the right one. Okay. So, US_CERT actually doesn't think it's a responsible disclosure to announce it immediately without giving the vendor a chance to come up with some way to address it. So they actually have a time window before they make it public. I think it's 45 days. And the reason for that is, the vendor hopefully has a patch. So by the time it's made public, people not just know that they have this vulnerability, but they have some way to deal with it. And so the second option is what you should have checked here. So we're going to keep going. Started with law. Spent a little bit of time talking on ethics. But then going to move on to privacy. And privacy's something that we all have some idea of what it is and the fact that we want it. It actually goes all the way back to Justice Brandeis. Famous thing about the right to be left alone, but we want to examine it in the context of, again, the online world. So how do we define privacy in this context? Privacy is obviously related to information about us. In this context it'll be information related to what we do online, and data that we have in the visible form. So definition basically is a user's ability to control. So that's the key thing here, control. How data pertaining to us or our activities Can be collected. Who can actually collect such data? You do your things, you visit a website, you send an email, you make a phone call, whatever it takes, but who can collect information about these activities? How can this information be used, and can they, the entity that collects it, share it with somebody else? Here are the company, agency, whatever. So it really the focus is on our ability to control all of these, and that's what privacy is. It's our ability to control information about us and in this case we're going to focus on information that is collected, shared, used in the online world. Privacy actually is not a new problem. We have had it forever. Now people have always worried about what others know. Others could be your friends, could be your family, could be your adversaries, your competition. It could be government agencies when we talk about Big Brother, when we talk about the government. So what do they know about us, in particular our activities and what we do, or who we are? So we always have worried about this. So what's new in the online context? Well the scale and magnitude at which information about what we do can be collected, okay? Our activities online and someone's ability to collect information about what we do, and then be able to share it and use it, and even sold, sold for and monetize that information. The scale at which this can be done now is unprecedented. So we said we're going to focus on what we do online. So let's dig deeper into it. What that we do online should be considered private? Okay, so what is private? And I think we're really talking about either data that is going to be stored in digital form in computers, or some activity that we do and record of that activity is what we're talking about. So financial statements, we do online banking, credit card statements, all these financial records, we would say, that should be private, for two reasons. It's no one's business how I spend my money or how much of it I have, but if they gain access to these, of course, they can do identity theft. Health medical conditions, health information is obviously highly sensitive and we don't want that to be made public. Legal matters they're attorney client privileges, and things like that. Well, most people would view that as something that is private information. If it's online in email messages or certain files that are stored in a computer, of course, you would view them as private information. Biometrics, fingerprints for example, which could be used for gaining access to certain information. You would think your fingerprints shouldn't be arbitrarily collected, shared, be made public and things like that. So you would say this is private as well. Your political beliefs, what party you belong to. Well, you can always find out from what button someone's wearing or what party they contribute to but it's no one's business if I don't want to make it public. Remember, it's your ability to control, and when you making a contribution or when you are putting a button of supports on a certain candidate, to do that. So the ability to control is what is privacy is all about. So my political belief, unless I want to disclose it, there's no reason why someone should find out about it. Although we didn't talk about privacy related laws, but in the US there are laws that cover some of these. Health, for example, HIPAA. Financial, there is Sarbanes-Oxley and things like that. So, the privacy laws, not only this information is private, considered private, but those who thwarted privacy, there could be sanctions against them. Well, if we go on school records, your employment record is private. Most of us would want to keep our annual evaluations, won't want them out in the public. School records, how you do your grades are actually covered again as regulation, and things like that. And more interestingly, our web browsing habits. What do we search? What websites do we visit? What kind of websites? If somebody knew, then maybe they'd know your political beliefs, for example. So what we do online, you would think that's private. Communication that's not public. Emails that you send or phone calls that you make and so on, you would think that is private. Our [INAUDIBLE] and we've been talking a lot about metadata and stuff like that. And a lot of people are upset and surprised and some maybe not because we did think phone calls, we had control in a sense that we knew that the government wouldn't collect information about those. So communication is certainly a thing that you would consider private. And if you sort of keep going down this line, what we do in cyberspace, what about stuff that's about us that can be seen by others online. Going back to whenever. What about past history? So the interesting are the right to be forgotten this thing in the EU now that we have, and we'll explore it a bit more. It says if there's information about me that's irrelevant I don't want Google to actually return that as a result of a search that somebody performs. So if there's information out there that was collected and made public or made available to somebody, is that forever or do I have the right to remove it at some point. We talked about a whole bunch of things that we think should be private. We should be able to exercise control over who can learn about them and who can use that information. But there are other things that are clearly not private. For example, where I live, my address. You buy a house, there's some that's recorded, the deed is recorded, and by your local government or whoever. And anyone can go see that book where that's done. Am I registered to vote in the US? This information is public. You don't have the right to say, it shouldn't be collected or shared with anyone else. And some more surprising things may not be public. I, as a professor at Georgia Tech, which is a public university, my salary is public because the state of Georgia said so. Actually, this is a good point. The context is extremely important as well. So, we talked about communication, emails. Well, as the employees of the state of Georgia, we're subject to, Georgia Tech is subject to, it's a public university, subject to open records requests. The other side of privacy we're going to see is your ability to access information, so privacy limits it. So, there is certainly sort of trade-off, and what you and I view as private. Some things are clearly private versus non-private, but obviously in other cases, we may have different opinions and those opinions may have been formed by who we are, our cultural backgrounds, and things like that. So I do want to mention before we move on to something else that privacy I've said in a individual's ability to control. But it doesn't just have to apply to individuals. There may be reasons for universities, hospitals, charities, or even political action committees sometimes don't need to tell the rest of us who's contributing to their cause and whatever. So when you talk about privacy you focus a lot on the individual and their ability but it also applies to organizations and some of those examples we have here either because they want to protect date about people who are affiliated with those organizations as their employees or are their employees and things like that. So, although privacy we largely talked about individual, but we should keep in mind that it also applies to organizations. So the question is, we always talk about privacy hawks, who are really concerned, sort of paranoid about their privacy. Then, sort of the people who don't care about privacy. And in between there may be the privacy pragmatists. So, this is not a question, didn't say do you care about privacy? But it did care about how important do you think is that you'd be able to control information about you, that you would consider private? So, what percentage of the people who responded to this question do you think thought that this is quite important, that they be able to control information about them? Actually, this was a very large percentage. It was over 90%, the people who felt that it is important that they be able to control information that is private and about them, and they should have the ability to control that. So based on this survey, obviously, what we're saying here is that internet privacy is an important thing for people. How hard they work to protect it, that's a different question. But if you were to go and sort of ask someone, should you be able to control it, it is not a surprising result, of course, of this survey. But yeah, people would like to be able to control who can get information about their online things that they do. Our next quiz question is about the right to be forgotten. So last year, the European Court of Justice ruled that EU citizens have the right to be forgotten in the Internet world. I think the court ruled that Google must not return links to information that can be shown to be inaccurate, inadequate, irrelevant, or excessive. This kind of information about me, I should have the right that it be forgotten in the Internet world, so people can't get to it. The question is, the two examples that we have here, which one of the following is an example of kind of information that Google decided not to return as a search result? Okay, because of this ruling that we're talking about. Remember, this information should not be returned if it's inaccurate, inadequate, irrelevant, or excessive. If it meets these criteria, then Google shouldn't return it. So in these two cases, the first story is one where Google decided because of the EU, or the European Court of Justice ruling, decided they would not return a link to it. The second one, public still should be able to know some history about this doctor if they want to use his services, then we reasonably can agree that this stuff should not be forgotten. So I think you know Google, these kind of requests have to be reviewed and Google is going to decide whether they're going to comply with it or not. And this is only for EU. If you live in the US you can't ask you don't have the right to be forgotten. So they will return what they can return. Obviously they don't want to have or all the head of having to review these and so on Okay, so now we have some idea of what is privacy in the online context. What kind of information is private and what may not be private. Let's talk about some threats to privacy. Well on the internet, we talk about a source of a request that goes to a certain service or a destination, and so we communicate between two endpoints. Analysis of who's talking to who. Obviously, think that who I communicate with should be private. Of course this traffic analysis is a threat to that. It can violate, or disclose that information. Other threat to privacy is, of course, large scale surveillance. And recently, in fact this month, this letter from this encryption experts was published about the law enforcement agency's claim that if we allow encryption without, sort of, some way to undo it, or backdoors and things like that, then it's like going dark. Their ability to see what bad guys are doing completely going away. So, the other people who are arguing against that by saying, this is sort of the golden age of surveillance. The amount of information they can collect is unprecedented. So, in some sense, everything you do online potentially can be tracked and collected, information about your activities can be collected. So if somebody is doing that, of course, that's a massive threat to privacy. Finally, with big data, data mining analytics that we have, we can do all kind of inferences and link different pieces of information, okay. When we talked about database of privacy, we did discuss inference attacks and anonymization and things like that. Well, those could be defeated and we have, by linking different pieces of information, very simple online advertising. They're sort of making a guess that you visit certain websites or click on certain kind of links, you'll be interested in certain things. There are plenty of other sort of threats. Social media, whatever you choose to make public, people can learn a lot about you, including who your friends are and things like that. Tracking of web browsing, what sites you visit, actually it's usable. The sites I choose to visit obviously know about I visiting those websites. But the third parties that actually can record or track the fact that you visited the New York Times for example, or you visited Facebook, or whatever it is. There are a lot of location aware applications that know where you are all the time. Again, the example of someone seeing that you're in a certain location sort of randomly is very different than being able to do it constantly. And sometimes we are the enablers for this sort of tracking that could happen, so we are the willing party. For example, if you use loyalty cards for a little discount in your grocery store, of course they're able to track you. They know that you visited them, they know what kind of things you bought, because they're able to tie your multiple register use of this loyalty card that we're talking about. So let's sort of focus on privacy threats and the context of tracking information or online information. Somebody tracks your activities that you do online. Okay, so tracking, essentially, it's collection of information about the activities that you do online. So, a threat is collection of such information, the act of tracking what you do. If they do it with your consent that's one thing. But doing it without your consent, obviously, that many of us, we'll view as invasion of our privacy. Okay, so one threat is without consent, doing this without consent. If they have collected this information how can they use? How did I agree to this specific use that they have in mind? If I had not agreed to it, or this was not sort of revealed to me, well, that's a threat to my privacy. How long can they keep some information that I agreed they can collect about me? We're talking tracking information. You went through a certain website, you searched for results. How long do they keep this information? Who do they share this information with? Maybe this information has some value associated because it describes me, what I do, what my preferences may be and things like that, so they can actually sell it to somebody else. And if they're selling to somebody else, did I agree to it? Are these parties that I had authorized or I had given consent that sharing is okay with me? Or do they do it without my knowledge? If they do it without my knowledge, of course, that's another privacy threat, because my information then gets to somebody else that I had no idea that they would be able to gain access to it. So, I may actually specify what they can collect, and who they can share it with and how long they can retain. So that's what the constitutes we're going to see and examples that are privacy policy that we agree to. But what if the policy changes actually, so information collected by me, actually they change it to a more lax policy, then they don't ask me to agree to it. Then they will be able to do things that they were not able to do before, and they're not able to do those things before because they hadn't agreed to it, to them. So, now by changing the policy they could basically can get around that requirement that I give explicit consent and that's a threat if all privacy policies can be changed to anytime without informing me and getting my consent, then that's a threat to my privacy. And finally, sort of information security, this is what we're talking about, they collect all this information. What if they have a breach? The bad guy walks away with all this information. They didn't explicitly share it with the bad guy, they were trying to sort of respect my privacy and only share it with whoever I had agreed to, but a bad guys, they don't care about consent and agreement. The reason they were able to reach and gain access to this information that's private is may be my information security was not what it should be. We hadn't have good identity access management, didn't have good monitoring and place and various other threats. So, if they are going to collect this information and in good faith going to try to use it the way you agree that they be able to use it. Well, we still have to protect it from the bad guys. And if they don't, that is a privacy threat. Okay, so we talked about information that we think is private, also something we thought perhaps is not private. And then we talked about threats, how that information can potentially be abused while letting out privacy. So one thing that companies do, companies that we interact with in the online world, is that they have privacy policies. So they say, if you're going to track what you do, or we're going to keep it from collecting information that we think you might see as private. Well, how exactly what we can do with that, we're going to define, that in a privacy policy. Most websites have these privacy policies. How many of us actually looked at them carefully? That's a different question. So we going to look at one example. Who else to go but Google that knows a lot about you. Has collects that information, builds a profile for you. So let's sort of look into what is collected, why is it collected, how can be shared. This is all captured in this policy that Google has. So let's see some details of that policy. So let's start with what information is collected about us by Google. Well, all kind of personal information. We agreed to give it to Google, actually. Name, email address, credit card number, telephone number, G-mail now does stronger authentication, using your telephone number. This is information that we provide to Google when we set up an account, when we create an account. This is our personal information we're giving it to Google. Essentially it's private information, so what can be done with it? Of course we want to address that. With visit certain websites or use Google services, of course, Google will know about it, and they can track and record it. And Google's business model is that they use it for advertising. So, we know why they're doing that. They actually have information about your device, from where are you're coming, to Google. So what kind of hardware it is, what operating system it's running, its IP address, network information, and so on. So they would have that. Interestingly, in the case of Google, they would also know your search queries, what kind of things you look for on the internet, what kind of information you're interested in. If you use Google Voice, they would know who you call, how long you talk. And cookies are sort of more general mechanism by which websites keep track of various types of information, in particular when we register them before and things like that. So cookies, at some state they keep on our machine, about what we have done with respect to these websites in the past. Google actually does set cookies that they're going to use to track your activities that you do. They also collect your location information, what kind of applications you use. All this is what is collected, okay. So privacy policy should say this is the information we collect about you. Well this is an example of what Google collects about you Well, the next question that the privacy policy should address is, you tell me what you collect about me, so how do you use this collected information? What do you do about it or what do you do with this information? Google says they're collecting it to really improve your user experience, personalization search results that they returned are the most relevant for you, okay? And you and I maybe have very different kind of interests. So even if you type the same search query terms, what the return could be different because of this personalization. So, the idea is the more we know about you, the more information that we have better is the experience that you will have when you use their service. And they make their money by selling advertisements. So the other thing that they use your information for is for serving targeted advertisements. Although you can set preferences and things like that. But this information that we're talking about, what they collect, what you search for, what sites you visit, things like that. Well, that's all used to try to figure out what kind of advertisements you will be most interested in as that's how they make their money. So their information used essentially to keep you hooked, which is the good user experience, personalized user experience. And then be able to show you ads that hopefully are of interest to you. Now that we know what Google collects and what do they do with this information, the next question to ask, again in a privacy policy, is do they share this information with somebody else? And if they do, now who are they? So if you opt-in, you agree to it, then they can share it with the companies, Individuals, organizations, whoever outside of Google. Google collects it, Google uses it, and then they can share it with somebody else if you choose to opt-in. They can share information about people who provide support to your organization, so domain administrators and resellers and things like that. So information that you give to Google can be shared with them. They can share it with what's called affiliates and other trusted businesses or persons. Of course they will say with appropriate confidentiality and security, but this is really addressing the sharing part, okay? So somebody is a trusted business or an affiliate, they can share the information with them. And finally they can share it with the government for legal reasons. Google has been, actually more recently, been good about transparency of government requests for legal reasons and having to share what kind of information with the government or law enforcement. But this sort of defines how the private information about you is collected, used by Google, and then shared by Google. And we said there are a couple of other aspects to a privacy policy other than collection, use, and sharing, which is how hard do they work to address the security of that information? Many of it's services actually use extra TPS or use encryption. I mentioned that they do stronger authentication, they do two factor authentication now, password, and then they send you, on your phone, they send you a code that you use. And they use other safeguards, we talked about a lot of their technical safeguards who are protecting information or securing information. So they do all these things so the private information that they have about you doesn't get into the hands of malicious actors. They do have something about changing the privacy, all these things we were talking about. If they want to change any aspect of these, they will not change them and change in a waywhere that reduces your rights without your consent. So the idea is the privacy policy says, well, if we ever want to go to a more lax privacy policy, okay, which is what reducing your rights is, we would ask you, okay? And only if you consent to that, we will change the policy. And so it's a way that you can view it as more lax than what they had before, okay? So repeating the policy says what we collect, how we use it, how we share it, how we secure it, and if anything changes about the policy what would it take to do that. Okay, so that's sort of a quick, brief overview of what a privacy policy should look like. We use this example, Google, but you can go study somebody else's privacy policy and see what aspects of the things that we discussed here are covered in that policy and what is not covered. Well so, let's look at some quiz questions. The first one is the Electronic Frontier Foundation is concerned about online privacy. And one thing it does, it ranks websites with privacy scores, and the scores are really based on how do they deal with some of the things we discussed should go in a privacy policy. So if we look at, there is a link that you can go look at, at the 2015 scores for major U.S. companies. And one of them is AT&T, who gets just one out of five stars. So that's a really low score. So this lowest score is explained by a number of things that its privacy policy doesn't address. So I have three options here. Check all that you think could be the reasons for AT&T's score being so low. So AT&T collects data about you, and it doesn't tell you how long they're going to keep it. So it doesn't disclose its retention policy. Actually we talked with Google, it didn't do that either but we'll get to that. So they get a lower score because they don't disclose their retention policy. They do use industry best practices to protect the data. So this is the information security, the data they collect, how do they secure it from attacks that can come from bad guys? So they do best-practices. But other reason they lose points is because they're not transparent about what data government demands. Okay, so law enforcement department comes to them to get information they have collected. And being transparent means you tell what kind of requests they get from the government and what kind of data they supply in response to those requests. And this transparency is not there, and because of that, they lose another star, and that's how they end up with very low scores. So, both of these, if you don't do these things, should count against you in a privacy score, yes? That's what, and this is not true in this particular case that we're talking about. Okay next question is about actually Google privacy policy we examined. So did the policy say anything about data retention? Does it the policy disclose anything about how long they retain the data they collect? We did talk about collection, use, sharing, and protection information security. And changing the policy we considered all those five things, did we consider it our retention this is sort of think about if the Google policy or go and take a look at it. Well, the EFF report actually gives a lower score to Google. I think Google got three, if I remember it right. But one of the reasons they have a lower score, or they lose a star, is because they don't disclose it. So data retention policy is not disclosed. The answer to that is no. And the Google policy, we didn't talk about it, and in fact the policy doesn't. Not only did we not talk about it, the policy does not address data retention. So one more question. And we're talking about legal deterrents having CFAA or BMC and those kind of laws. We didn't talk about privacy laws. I did say that they exist. But let's go back to sort of connect the privacy discussion we had with some of the stuff we talked about was. So we said poor privacy, what does poor privacy mean? Somebody collects data about you, they don't tell you what they're collecting, they don't protect it, they share it with whoever, and things like that. So that would be poor privacy. Good privacy is, you know what they collect, you know what they use it for, you know who they share it with, and they protect it, work hard to protect it. So poor privacy would be one of those things not being done. So poor privacy is good for bad guys, because they can use information about you. Poor privacy means they can gain access to information that you consider private. They can get hold of it. So why is that useful to them? That's why we need those legal deterrents against them. They can target you in phishing attacks that are highly sort of, we call it spear fishing. Which is highly targeted, the information about you that'll come with the phishing. They try to look like a bank. They know something about the last transaction you may have done or something like that. The way you did it or things like that. So, if they can get hold of this information, then they know about you, and that information that they know about you can be used to craft a targeted or spear phishing attack, which makes poor privacy good for them, because it enables them to get access to information about you that can be used in these kind of phishing attacks. They know about where you went to high school, or where you live, certain age, things like that. Maybe they can come up with correct questions about, correct answers for security questions. And that information, again, can be used to gain access to online accounts. So, poor privacy is basically something that you know, or you are what you do, or something you know, or something you do. That information leaks out to the bad guys, and of course, that information can be abused by them for doing these kind of things. So poor privacy is good for bad guys and, of course, it is bad for us. Okay, so companies do have privacy policies that address the various things that we discussed. Well, how well do they do on this privacy policy? We haven't actually talked about scoring the policy, which we did, but if they have these policies in place, how well do they follow them? Or do they adhere to these policies that they declare they're following and operate according to the policy that you had agreed to. Now you want to do business with them, you want to have a Facebook account, Facebook said you can only do that if you agree to the privacy policy. You gave consent. Are they going to sort of stay within the boundaries of that policy, or they are not? And there are times when companies actually don't do that. Facebook, in the example that we are talking about, not only didn't do it, the U.S. Federal Trade Commissioner FTC actually had to go after it for a while in the people's privacy policy in the sense that they were doing things that the users have not given consent to. So what did they do that was bad which resulted in the FTC action? So either they did something they shouldn't have done, or they didn't do something that was expected by users that they would do. Well they did a couple of things. They made information users had designated as private particular with friend's list, they made it public without user consent. They also made personal information available to applications of friends. So the idea here is, friend then they can access something but this is personal information that you had not agreed per the policy. The policy you had agree to that was made available to these applications. They shared information with advertisers that it had promised not to share with them. The members who collect use and the next thing was share and I should know who they're sharing with. In this case, they shared it with people. When they'd promised or told me that they were not going to share. And they said certain apps were verified which were really not verified so that was misleading of course. So these are the kind of things that they did which caught the attention of the Federal Trade Commission and The FTC then went after them saying, you are violating your user's privacy. So Facebook then did things that were inconsistent with the privacy policy they had advertise. So we said the FTC took action and some of the sanctions example to that, the consequences of this violation for Facebook are the number of those that the FTC actually mandated. One is that they had to have a privacy audit, and that had to be done periodically, biannually, and this is going to be done for the next 20 years. So someone from outside has to come and do a privacy audit, you're not going to take their word for it. Their Facebook is prohibited from, told not to do it, misrepresenting privacy and security settings that are provided to consumers. Okay? If you say something is not public, it should not be public. You shouldn't make it public without the user being informed about it and agreeing to it, and they also required Facebook to get consent before sharing any information in a way that exceeds their privacy settings. Okay? So, the consent thing is big when it comes to privacy. If you give consent then they can do whatever you consented to. But the idea here is that consent, affirmative express consent because some default should not be that we can share without checking with them. It should be that they should agree to it expressly, or you can share it if this is in a way that is not revealed by their privacy settings. So these are some examples of the sanctions that the FTC, when they dealt with the Facebook case they had, they came up with. And that's what was slapped on Facebook. So privacy's a good thing, so do we have technologies that can help enhance privacy or include privacy? So here we're going to talk about a few of those. So one of the threats we talked about was traffic analysis, okay? You know who's talking to who, so we said communication, who I want to talk to. Or if Alice is talking to Bob, somebody else shouldn't be able to figure that out and they could do that with traffic analysis. What machine is talking to what other machine for source and destination information. So what if we don't want somebody to be able to infer that? So this we can do with this technology called TOR, and we'll see what exactly it does. But it's basically sort of anonymous communication is what we're trying to get to here. Let's say what's happening here is, but Alice is, maybe interacting with the website, but it doesn't want the website to know that it's really Alice. Or, doesn't even want to know, this website to know, that, so it's Bob's site is, the website is operated by Bob. So Alice doesn't want Bob's server to know that the request is coming from her, or even the IP address from which it's coming. So one way you can do that is, Alice has a client, a TOR client. So that fix a random path the bunch of TOR nodes that you need to find some set of those. And it constructs this path in such a way that any node here, sort of knows about this predecessor and its immediate successor, but doesn't know who this request has come through in the past, or except its immediate successor, who is the next node that's going to get it. So this guy is told saying, well just send it to the next node, and the rest of the information, the node after that, that information is encrypted with this node's key, so this guy would have no idea that once it sends the message here, it's going to go to this one. Once it comes here, then it decrypts that, the next destination, finds out. And it doesn't know who it's going to go after that, so it doesn't have the full picture either. So basically we create this anonymous communication, or implement anonymous communication as TOR does, by using this kind of a scheme where instead of nodes actually forward the message repeatedly and make use of encryption to sort of know who the message is coming from, and where it's going, and that's all they know about. They don't know the full path, and as a result, traffic analysis, no one is going to be able to figure out that Alice is visiting Bob's website, unless they have control of all these nodes, which the likelihood of that would be less. So just a little bit more detail of how TOR works. So it's called onion routing, is what I think the example we had was sort of doing something like that. It uses onion routing. So to start, the client we are talking about uses a director service to get a set of TOR nodes. These nodes would then help us route the message from Alice to Bob's website. You pick a random set and order it, so the message is going to go from one to the next in the order that we come up with. Alice is going to prepare her message and creates these onion layers, so in the end it's going to go to Bob's website, but then we know it's going to be sent to Bob's website by the last node. And its predecessor is going to send it to that and things like that, so the onion layers that we're talking about essentially, when you peel one, you know where it goes next, okay, but that's all. You don't see any further down through the layers beyond that. So we peel one layer. We know who to send it to next in this order that we're talking about. And that node is going to peel another layer, and send it to Its successor, and we're going to continue like that. And this peeling of the layers actually is done through encryption using keys, you encrypted with someone's public key. So, only they can decrypt it because they have the corresponding private key, and that's how this thing works. So that's anonymous communication. Well, you can, Alice doesn't have to say that it's Alice if Bob's website demands some sort of identity for her. You can create sort of fictional or pseudo identities, you can have multiple one of those. So somebody when you switch your identity they can't say that Alice is doing A, B, and C, if you switched after A, then whoever is tracking would think B is being done by somebody other than whoever had done A. So, psuedo-anonymity is not a way by which we tried to sort of defeat the tracking and making the kind of inferences that we're talking about. And of course, we talked about aggregation and privacy enhancing transformations like anonymization of data, and diversity in data, so someone is not able to inference back to you. This must be some information about you and things like that, so the variety of this technologies and techniques that one can use to improve privacy to if not defeat, at least reduce that effectiveness of tracking, and that happens in collection of information that can be tied to a particular identity. This technology sort of helps us disrupt that. Hey, so, one particular thing we need to be concerned about is all the tracking that happens when we visit websites, they know where we're going. So if you go to a website of course, they will know that you're accessing that website, but what about others? So you'll be surprised how many other websites are able to figure out that you withered for example www.NewYorkTimes.com. These are tens of them, I think at one point I read 50 other sites learn about the fact that you are visiting www.NewYorkTimes.com. These third parties are able to do it because they're either advertisements embedded in the New York Times phase that you get back, or they're the likes, the Facebook, Twitter, and whatever other things. And some of these things we may not even see or pay attention to, but essentially, we pull some bit of the content that makes up the page from all these websites that allow them to track the fact that we are visiting New York Times. So you can do what is called third party cookie blocking. So browsers actually, they allow you to do a variety of things. You can block cookies. If you block cookies altogether, then maybe you can't do anything useful with the NewYorkTimes.com either, but third party cookies blocking, of course, protects you. Blocking third party cookies of course, protects you. You can tell your browser that you don't like to be tracked, and in this case, the browser when it sends some information to a website would actually tell the website that the user doesn't want to be tracked. Whether the website honors that or not is a different question. Cookies are kept on your computer to track your history and so on. We can clear those and when the next time you visit the website, the website won't have this knowledge of what you had done previously. You can block pop-ups. You can, there are other private browsing sort of things that you can do, browser modes that and clear out any state that may be built while you're using a tab incognito mode or even more sort of aggressive privacy things that you can do to browsers. So the idea is that tracking is progressive. There's some things we can do about it. We don't know how effective those things are but at least there are things that you can do by going into your privacy setting of your browser that you use. And they're slightly different across different browsers. So So I'm not talking a great deal about how exactly you do, but I would highly encourage you to explore these. So now we're going to do couple of quiz questions. The first one is the online movie ticket purchasing company, Fandango. They were not so nice to people who did business with them, in particular, protecting the user privacy. And the reason for that is what we're exploring, there's sort of two possibilities here. You pick the one you think is right. So in this case actually, they violated privacy because they didn't protect users' data. Didn't use encryption in certain situation where they should have used it and things like that. So remember one of the information security of information you collect, the security of information you collect from your users, you need to address that. And they were not doing a good job with people's information. Next question is, you talked about some privacy enhancing technologies. It says a company tracks your activities based on your machine's IP address. What's our defense, technical defense, might you have that can help defeat this? So we have two options. Disable cookies, you can do that but you do talk to this company's website, they know where your request is coming from, they know your IP address. So they can still use that IP address. If you really want to hide your IP address, of course you have to anonymize the communications so they don't know where it's coming from and for that use Tor to do that. We explored some of the US laws that have been developed for dealing with cyber crime and theft of proprietary online information. Now these laws could be different across different countries. But one thing we saw is that enforcing them has some serious challenges. We also discussed ethical behavior when it comes to our online activities that impact cyber security. In the end, we explored online privacy. In particular, who can collect information about what we do online? What can they do with such information? Who can they share it with? And what options do we have in the use and sharing of information about our online activities?