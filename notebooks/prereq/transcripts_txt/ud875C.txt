Playing audio in the car is another key feature of Android Auto. You can use either the touchscreen to select songs based on a variety of criteria, or you can use your voice to speak the name of a song or artist using the voice button on the steering wheel. This is great because the user is driving a car meaning they need minimal distractions and they need to keep their eyes on the road. So let's take a quick look at what it looks like to play audio with Android Auto. And then I'll show you how to implement this. So here's our universal media player sample app, which is installed and running on my phone here. Android Auto is able to connect to the app and then browse through the media collection. The user can perform a search using voice commands. Play music jazz. [SOUND] [MUSIC] You can also touch the screen to browse as well. So I'll pick something in the rock genre. [MUSIC] And there you go, it's that easy to use, and in this lesson, I'm going to show you how we implement all of this. We're going to explain how to use the Android media browsing framework. We're going to walk you through a short sample that implements basic functionality, and then, how to implement a full featured media playing application that works on Android Auto as well as lots of other platforms as well. Android 5.0 introduced a new way of writing media playback applications. Based on media session, media controller, and media browser service classes. One of the nice features of this API is that apps can be abstracted into two components. One part stores and plays back the media and the other part provides a user interface to allow the user to select the item to play, and a pause, next track, etc. An interesting side effect of this design is that your app can export the media to other apps running on the device. So the Android Auto app can query the available music from your app and show it using a user interface appropriate for the car. Android Wear is also able to do the same and show user interface appropriate for the wearable device. This all works without you having to right user interfaces for these other platforms. If you implement the API correctly, it all comes for free. As a final bonus, Android Wear also includes media browser support. If you allow Android Wear to call your own GitRoot method, it will allow you to browse and play back music from a wearable device. You don't need to write any code for Android Wear, it just works for free. You can try the Sound player yourself to see this in action. So let's talk about how we implement a media playback app for Android Auto. This diagram shows three components. Your app on the right, which is all in an APK file, the Android Auto Companion app, this app is responsible for drawing the user interface and sending the pixels to the vehicle display, and this here is the vehicle display itself. It doesn't do much except draw the pixels from the Android Auto Companion, play sound, and caption's voice input. Inside your app, you implement a media browser service. This service maintains a true structure of all of your media, such as a collection of music. This could be either streamed or stored locally. Your app also implements a media session, which tracks the current state of what your app is playing. You implement a MediaSession.Callback, to get notified of when the user selects a new song or when the playback controls are pressed. The Android Auto App connects to your media browser service and queries it to get the tree structure of your app's music library. The driver of the car then gets to select a song using either the touch screen like this or using voice search in the car. Then Android Auto we use the media session to run your call backs. So you never have to worry about all the details of making a user interface that is safe for driving, because Android Auto takes care of all that for you. Let's go through how to create a minimal Android Auto compatible app that implements all of this functionality. We will be able to put all of the code into a single file to give you a nice concise walk-through. Let's create a new project from scratch. So go to File, New Project and give it a name of something like this, MinimalAutoMedia, and create it in an appropriate location. Next. Make sure just the Phone and Tablet option is checked and everything else should be off. And for the API level, select API 21 or later. This is when the media API was introduced. Click Next. Select a default blank activity. Next again. You can leave the activity and layout names at the defaults. Click Finish and Android Studio will create the project. We need to declare an XML file that will contain Android auto information. So let's open up the Android manifesto XML and lets add something inside the application tag but not inside the activity. You can see here we're defining something for com.google.android.gms.car.application and we're specifying an external place to find it. Now we need to create the referenced file. So we go to RES, and you won't have an XML directory. So create that by right-clicking, new, directory, type the word XLM, and then hit OK. You'll notice that I had to collapse and then expand the RES directory to make the XML folder visible. Right click on that directory and select new, file, and type in this name, automotive_app_desk.xml. And hit okay. Inside this file you need to declare support for media with some XML like this. And if we go back to our Android manifest, you can see that this is now green. Right now this application will not do anything on order yet, since it's just a standard Android app. So we need to write a service that implements media browsing. So go into Java, and in the package here, right click New > Service > Service. And let's call it TestMusicBrowserService. Leave all the defaults for exporting and enabled selected. Hit Enter to create the service. This will add an entry to your Android manifest.XML shown here. We now need to modify the service definition to mention that it is to be used as a media browser service. So we add this intent folder for Android.media.browse.mediabrowserserv- ice. We're going to download some music from the internet so we need to request permission for this in manifest file as well. Now, let's open up the test music browser service definition and let's replace it with the following code. It's a bit long but we'll go through it line by line so don't worry. I also have to hit Alt+Enter a bunch of times to import necessary packages. Let's walk through the code and discuss what it does. Let's minimize the project window here so we can see all the code. Here is where we create MediaMeta.objects that represent each of the songs we want to play. We can provide URLs and they'll be fetched automatically for us. You can set all kinds of information such as song title, artist, and duration which will be shown on the display. This creates the media player which actually decodes and plays back the MP3 file. Here we create a media session. Auto will use the media session object to send playback commands requested by the user to your app. Here is where we'll create the callbacks that'll execute when the user makes a request. This method searches for a song in the music library by ID and starts playing it. This method is called when the user presses the Play button. And if we go in here, if there's no song currently selected, we'd pick the first one and then start playing it. If there's a song already active but paused, then we could start playing it. Down here, we also update the UI to indicate we're in Play mode and no longer paused. If the paused button is pressed we need to do the opposite and stop the song from playing. Now, we activate the media session. And finally, we call the set session token from the media browser service class we inherit from. This gives it the token from the media session. This token can be used later to create a media controller to interact with this session. So, that's basically it. Now, we have some helper methods to make the above code a bit easier to read. The buildState method builds up a playback state object to tell the UI what icons to show based on whether we're stopped, paused and so forth. We can also set the current position in the song, but we'll just hide code that to keep things simple. Here's some generic code that we used many times above it. It just starts playing a song using the media player interface. So we tell the media session about the current playback state which will update the UI buttons. And here's where we copy over the meta data for the song title, artist and background art etc, and give it to the media session to update the UI. The reset method stops and resets the media player. Then we tell the media player the URL of the song we want to play. Now we schedule a call back to run when the music has been buffered. And is ready to be played. And once that's ready the onPrepared method is called and we call the start method to begin the playback of the music. Once we have the onPrepared listener created, we then tell media player to schedule the playback, which starts the buffering of the URL. Then the onPrepared call back above. And then start the playback with mp.style. The onGetRoot method is the first method that Android auto calls into. This method is responsible for returning the root node of your media hierarchy. It can also implement security checks to help prevent other apps from accruing your music. In this case, we're doing something super simple, so this does nothing except returning a simple root node automatically. If you want to reject a request, you'd return null here. Once the remote app is coded onGetRoot, it then uses that to return a list of children. This method needs to return that using a result list of media item objects. You can see here it is doing a full loop over the array created at the side of this code and building them up into a list of media item objects. You can implement tree structures, but in this simple example we're just doing two songs. So that's everything the code is now ready to run. The next thing we need to do is get an emulator ready to run the code within. Go to Tools > Android and start the AVD manager. Now select Create a Virtual Device and you can select a phone model to use here. I'm going to pick a Galaxy Nexus since it's an older device with a smaller display and smaller memory footprint that will work on my small laptop display easily. Click next and then select Lollipop API 21. Although, any newer APIs will also work as well. Make sure you pick one with Google APIs since that is required for Android auto support. If you pick the X86 simulator, and have the HAXM driver installed, the emulator will also run a lot faster. Click next. In the scale section, you may need to play with the settings to make sure the emulator fits on your screen. Otherwise, leave the defaults as they are and click finish to to create the AVD. Now, you have a list of emulators available. So select the one we just created and click the play button to start it up. Once the emulator is ready, you can unlock it. Now we need to install the Audio Simulator. This app uses the same API's as the Real Android Audio Head Unit so we can see how our app will work without having to sit outside in a car. You can also do this on a hand held device if you'd like. It's actually downloaded as part of the SDK manager just like we did it earlier for messaging. So let's go to Tools > Android SDK Manager. If we scroll down to the bottom in the Extras category, you should see that it's installed here from when we used it previously for Android Auto Messaging. So let's close the SDK Manager. Once you've got that package installed, now we need to find the media simulator APK in the Android SDK directory. In this case my SDK's in ~/Library/Android/sdk and now, we need to change some into the extras/google/simulators directory. And in here you can see the media-browser-simulator.apk file. So let's install that to out emulator with adb install. And that's now ready to go in our emulator. Now, we need to build and install our code to the running emulator. So click on the green icon here, which does it all for us. Now, we select the emulator and hit OK. Now, it's installing to the emulator. Once it's done, your app will start running on the emulator. Note that it's just running the default activity included with the sample. But you can't see a music playback interface yet. This is because we don't provide a UI, just service. Let's go to the home screen on the emulator and find the media simulator. Let's go to the list of apps and you can see here there's a media simulator, so click the icon to launch it. You'll notice that the app needs to run in landscape mode. So rotate the emulator. Typically, you can press Ctrl + F11 or Ctrl + Fn + F11 to make this happen like so. Now, you can select your media app from the list. There may be others installed on the phone as well. Here's their app, click on it, and as you can see here, the songs we had coded into the sample are available here and we can select one to start playing it back. Voila, music! We now have built a super simple Android media application, congratulations! Let's talk a bit more about the flow of who calls what with Android Auto media playback. You can see that Android Auto initiates the process by calling onGetRoot of your application. This returns back a route note ID, and also a session token. Next, Android Auto calls onLoadChildren to get the complete hierarchy of the media available from your application. This allows browsing, but also voice search overall with songs. When the user is ready, they get to pick a song, which is then cast as an argument to the media session dot call back. Your app then takes action, such as selecting a song, play, pause, next track, et cetera. After taking action, your app communicates changes in the UI by changing the metadata or the playback state When building the onGetRoot method, it's important that you think about the security of your content. By default, any application running on your phone has that ability to call this, and interact with your music library. You might not want this, and only want to allow official Android apps to access your media. To do this, we need to verify the origin of the package accessing your onGetRoot method. Here, you can see we're calling a PackageValidator class that takes this information in the isCallerAllowed method, and checks it. If the package is not allowed, we return null and this will not allow the media browser operation to continue. To get yourself a copy of the PackageValidator, you should use one of the Android Auto samples that I'll introduce later, to see what the PackageValidator class looks like. You should probably just use this code. Instead of trying to write your own. Also at the bottom here, you'll see that we're doing a test to see if we're being queried by Android Auto. This is useful if your app needs to adapt itself differently for ads, music libraries, or anything else when it's running in the car. For the onLoadChildren method shown here, you get to build up a hierarchy of media ID's. Each call returns a list of media items, which we show here, and an item can be playable or browsable. So what does this mean? Let's look at a diagram. Browsable items imply that there is another level underneath, and further calls to onLoadChildren will be made. OnLoadchildren is call recursively, until we reach only playable nodes. At the top level, we have a root node and then we have browsable categories of genre, artist and album. Genre is broken-down into classic and rock. And then, classic contains a reference to Song1, while rock contains a reference to Song2. It's totally fine for the same music item to appear multiple times, under different categories. So there's also flexibility here. If you have a large music hierarchy that will take some time to build, like if you have to fetch it from the network, then you need to do this in the background and not block the user interface. To do this, you should spawn off an AsyncTask and then call this detach method shown here. When the result is ready, the AsyncTask can then return back the results, using another method called sendResult. Using an AsyncTask ensures that the user interface is responsive, and is not blocked while your code is querying your media library. Since the user will be in a car, it's important that you present items in a way that minimizes user interaction. You should put frequently used items like playlists, channels, or popular content right at the top. You should also avoid showing unbounded broad categories like all artists, because these lists will be too hard for users to navigate through. So definitely don't do that. Instead, you should try to create bounded content that is going to be what the user wants. Android Auto has a limit of six touch interactions to reach any point in your media hierarchy, to ensure the user does not spend too much time distracted while driving. If you try to exceed this limit, the UI will prevent it, and you'll need to present your media hierarchy differently. After going through some simple examples, now we're ready to show you how to build a real media app for Android Auto. We've built a very complete sample that is a great reference point for you. So, let's load it up. Go to File, Import Sample and search for Universal Music Player. Select it. Click next, and then finish with all the defaults. Let's now run the sample and see what it looks like. Click the green play icon here, and we're going to use a simple debug to build and install it to the emulator just like we did before. We select the emulator, press OK. So now, it's running on the emulator. You can see that we can browse by genre. We can pick one, and then pick a zone. And once again, we have music playing. It's pretty easy, hey? So, how does this work? In the previous simple example we walked through, there was no UI on the Android side. We needed to run the media simulator. In this case there is an Android activity. Let's go over to the code. The UMP sample implements it here. Go to Java, Package, UI and Music Player Activity. This activity implements a media browser and media controller that connects to the media browser's service to retrieve the music hierarchy and then start the music play back. The activity implements exactly the same API, as what the Android Auto simulator did. Let's go back to the home screen, start the media simulator. And if we go to the top, we can now see that the Universal Music Player is also available, and we can browse in the same way that we did last time. So, the same media is visible here, too, but with an Android Auto experience. The UMP sample is nice because it implements user interfaces for many different platforms as well. It also provides an activity that Android TV can start, showing the same music library suitable for presentation in the Leanback UI on Android TV. If you're implementing a large media application, UMP is a great starting point to see best practices in implementing this, and how to make a professional, polished looking app for many platforms. So that's everything you need to know to get started with playing audio on Android Auto. The Android Media Framework is used not just in Android Auto, but in Android in general, as well as Android Wear. So by investing your time into this, you get other platforms for free. By exposing the media in your app to Android Auto, the user is able search and browse through the available content using their voice or the touch screen. Android Auto takes care of all the work of presenting the interface to the user, saving you time in building user interface for cars. So I'm looking forward to seeing what kinds of cool music apps you build for Android Auto. I'll see you on the road. [SOUND] As you've seen, Android Auto provides an integration between your phone and your car. This allows you to make your phone perform tasks without having to touch the phone display. This is great because the user is driving, meaning they need minimal distractions and they need to keep their eyes on the road. One of the key features of Android Auto is messaging support. Let's have a quick look at what this might like. [SOUND] Here's the message. Are you coming back to the studio for filming? [SOUND] Reply. [SOUND] What's the message? [SOUND] Yes, in five minutes. [SOUND] Got it. Here's your message. Yes, in five minutes. Do you want to send it? [SOUND] Yes. [SOUND] Sending message. As you can see, this is a very simple workflow. In this lesson, I'm going to show you the following. How to get an existing sample for notifications, how it works, and then how to get it running. Then, I'm going to show you how to add extensions for Android Auto Messaging, and then how to test that messaging functionality on the Android Auto Simulator. Let's head back to the studio and get started. Regular Android applications use the notification API to show important information to the user of phones and tablets. Android Auto Support for messaging uses the same notification API to the underlying implementation. The Android Auto app that runs on the phone looks at the notification screen being generated, and it notices those that contain the car extender, like these two red ones here. Let's look inside one of these objects to see what it contains. The Unread Conversation object contains the text to speak out to the driver, and this is handled automatically for you. Speech Recognition is then used to get the reply from the driver. And Remote Input is used to help capture this for you and give you a string of the spoken audio via this intent here. Remember, the phone is handling the text to speech and speech recognition, and the car is just displaying images and capturing audio. Let's get started with some code to understand how notifications work and how to add these extensions. We'll use the basic notification sample, which is the same starting point that we use when we discuss notifications on Android Wear. So, let's find a sample in Android Studio that demonstrates what we need. So here, we go to File, Import Sample, and we're going to look for a sample called Basic Notifications. See it there, we hit Next, and we can leave everything with the default values. So now we've got a new project on our machine with the basic notification sample. You've had this dialogue box here, we just hit yes to that because we just want to reload it after some small changes that have been made, and now we have our sample that we can get started. The next thing we need to do is get an emulator ready to start the sample within. Go to Tools, Android, and start the AVD manager. Now what we do here is we create a virtual device, and you need to select a phone model to use here. I'm going to pick a Galaxy Nexus, since it's an older device with a small display and a smaller memory footprint that will work on my small laptop display easily. So, now we hit Next. And it's important here that we select Lollipop API 21, although any newer API will work as well. And we need to make sure we pick the Google APIs, since that's required for the Android Auto Support. If you pick the x86 emulator, and have the HAXM driver installed, the emulator will also run a lot faster. So we hit Next. Now here on this scale, you may need to select 2DP on device is 1px on screen to make sure the emulator fits on your screen. Otherwise, leave all the defaults as they are, and then click finish to create the ADV. Now you have a list of emulators available, so select it and click on the play button to get it started. Let's step through the sample now to refresh your memory on how notifications work. So let's go to the code for the sample, which is in Applications, Java, in this package, Main Activity. So this variable here is the first item of interest notification ID. This is the unique identifier for the notification. Each notification we generate has a unique value, and if we issue two notifications with the same ID, we'll override the first one. The interesting code is down here, in the sendNotification method. This here creates an intent that is fired when the notification is clicked by the user. We need to create a PendingIntent, so that the notification service can run the intent later on for our app. We need to set up a builder NotificationCompact objects. We will call methods on this to set it up. This sets up the icon we show for the notification. This tells the notification what pending intent to use which we created earlier. This here makes the notification automatically disappear when it's clicked on. This is a different kind of icon used on the left of the notification. These three lines configure the main title, the main text, and also some smaller subtext, all which is shown on the notification. This grabs a reference to the NotificationManager, and this takes the builder and generates a NotificationCompat object using the build method, and passes it to the NotificationManager using the NOTIFICATION_ID value we created earlier. As we said before, the NOTIFICATION_ID value must be unique, amongst the other notifications that you might want to create. Now let's [INAUDIBLE] run the sample. So let's click the green play button, which runs the assemble debug task to compile it. It then asks us if we want to run it. So we select the emulator and we hit okay. This then starts the installation process to the emulator, so the samples now running, so we click Do It to generate the notification, which calls send notification that we looked at earlier. You can drop the notification shade down and see the contents. Exactly as we described during the code walk-through. When you click on it, you'll see it launch a web browser and open up a URL for the notification documentation which is what the intend is configured to do. So now that we've seen how regular notifications work, how do we go about extending this application to work with Android Auto? These are the same steps you would have to take, if you had an existing messaging app that you wanted to modify. So this would be very useful for you to understand. When a user connects their phone to an Android Auto head unit, a companion app runs on the phone which implements the Android Auto experience. Shown here. This Auto Companion app looks for apps on the device, such as these here, that declare Android Auto support, and so our messaging app needs to declare this in the AndroidManifest.xml file. So let's open that file up. So we go to manifests, AndroidManfest.xml, and here it is. We need to declare an XML file that will contain the Android Auto information. So let's add something like this inside of the application tag. Now we need to create this reference file. So if we go over here and select res, you won't have an XML directory so we need to create that. So right click New, Directory, type the word XML and hit OK. Right click on the XML directory, select New File, and type in automotive_app_desc.xml, and hit OK. Inside this new file, you then need to declare support for notifications, like this. The next step is to create broadcast receivers that will be called, whenever an Android auto message has been heard by the user or replied to. So we can use Android Studio to create these receivers very easily. And it'll up the AndroidManfest.xml file for us as well. So to do this, click on File, New, Other, Broadcast Receiver, and for the name, we're going to type my message heard receiver and we'll leave all the defaults as they are, and hit finish. You can see now, we have sub class created for us. And you can see the AndroidManifest.xml changes here, as well. You need to add an intent filter, so that this receiver will only respond to specific actions. Note that we include the package name here. You should change this package name to whatever the name is for your application. Ours is defined here, at the top of the AndroidManifest.xml and so we're going to use that here, too. We also need to create a receiver for message replies as well, so let's do the same thing File, New, Other, Broadcast Receiver, and this time we're going to use MyMessageReplyReceiver. Once again with the same default and then finish. If we go back to the AndroidManifest.xml file, we need to make sure we add a similar intent-filter as before. And so we put that in like this. You'll see here that the action is a reply, instead of heard like we used before. We also need to double check that we're including the Android support library as well. This is defined in the build.gradle file for the application module. We can find that here. Here, and then we load it up. And if we scroll here, you can see that we're already including the support-v4 library here, so we're good to go. Now it's time to add Android auto support. So let's open up the mainactivity.java file and let's go to the start of the send notification method here. The first step is to create an intent and pending intent to be triggered when a message is heard and when a message has been replied to. We already created broadcast receivers earlier, and now we're going to use them. So let's created a message heard intent, and note that we use addFlags to indicate that we should be able to trigger packages that are stopped. Our messaging app might not be running at the time when a message is heard by a driver. We use set action to specify a unique action name so that the broadcast receiver we want is triggered. Make sure the package name here matches your package name in the Android manifest.xml. Also, you putExtra to specify the conversationId which is some kind of identifier that your app might need in the broadcast receiver to identify which conversation notification is referring to. We've used 42 here, but you would normally have some kind of index into a data structure that your messaging app has. Now we create the PendingIntent which is a wrapper that lets the notification trigger this intent on behalf of our application. We pass the same 42 value as used for conversation ID earlier. We parse in the intent and we allow this notification to be an update on a previous one if needed. We need to now create the intent and pending intent to be triggered when the driver replies to a message. The code for this is almost identical to the previous code so we're going to cut and paste that in now. The code for this is almost identical, except we used a different action string. You can see here we're using MY_ACTION_MESSAGE_REPLY instead. You'll see this is the same ConversationId from earlier since this is the same conversation. Now we need to implement the remote input object, which will handle the speech recognition for you and deliver it to your broadcast receiver as a string. The voice reply key here will be the key used to extract out the reply later on in our broadcast receiver. An interesting thing to note is that Android wear implements a similar remote input extension as well, and the set label call is used there. You'll notice here that Android Studio has highlighted remote input as not being valid. You can press Alt+Enter and it will import the package for you. Make sure you pick the one from the support library. Let's now go and show you how to receive the results in the two broadcast receivers. Open up my message heard receiver, the onReceive method is called when a message has been heard by the driver. You can retrieve the ConversationId key value by using the following code to get it out of the intent. We should remove the code that generates this exception here, and let's put some logging here instead. Once again, we do Alt+Enter to add this dependency. Now, let's look at the MyMessageReply receiver. This unreceived method is called when a message has been replied to by the driver. You can retrieve the same ConversationId here as well just like before and now we can retrieve the text of the reply. Once again, we hit Alt+Enter a few times to resolve the dependencies and now we can retrieve the text of the reply. Note that we use voice reply key that we supplied earlier to the remote input and we can retrieve it using getCharSequence. You can do whatever you need with this stream of characters for your app. In this case here we're logging it so we can see it in the ADB logcat output. And just like before, we need to remove the code that generates the exception here as well. Let's go back to the MainActivity.java file. The next step here is to create an unread conversation builder. We will use this to organize a group of messages that are From a particular sender. So, use the code. You'll notice here we have conversation name which is the person who these messages are from. I've had coded my name in here but you typically get this information from the messages in your application. Also, you can see we've references the two pending intents from earlier. So that auto knows which intents to run when your conversation is read or replied to. Now we need to actually add the messages from the conversation. So we've hard coded an example here that you would typically loop through an array and add each message separately. The add message method here is used to add the message string through this conversation. Set latest time stamp is used to indicate when the message actually arrived. I've used the current time stamp here since we're just simulating things. So this is all the new code that we need to add, now we need to edit the existing notification to use the objects we just created. The way we extend a notification if using car extender. Once again, use alt enter to add the package for this. At this time, we call the build method of the unread conversation builder from before and store it into the car extender. The car extender is then stored into the notification builder using this extend method. Finally, we need to issue the notification and the code is already here for that. But there is one gotcha here that you need to watch out for. When creating notifications with extensions, such as for auto aware, you need to use notification manager compact. If you use the standard system notification manager, the car extender will be lost, so why is this? Well, if you're building against an auto framework, such as API10, the system notification manager does not know about these extensions and so will not pass them on. So to be safe, you should use the compact versions of both notification and NotificationManager to ensure that everything works properly and you don't have any problems. First, we work the NotificationManager code here like so. So we grab the NotificationManagerCompat like this, use old answers to resolve dependencies, and we're done. So now, let's test this by compiling and installing it to our phone. Click the green triangle, select the emulator Hit okay, switch over to the emulator. If you followed the previous steps, it should run on your phone, although you won't be able to see any difference just yet, since we're not actually using this inside an android enabled vehicle. Go ahead and try to replicate all the steps that I just did. You can refer to the Android auto messaging training guide for a detailed rundown of how to do what we just discussed. The URL is at the top, right here, it's developer.android.com/training/auto/mes- saging. Once you've completed the task, check the box to continue. Quiz question number two. If you intend for your code to run on an API 18 Device, which notification API should you use when adding Android Auto Support to your code? Do you use notification? Or do you use Notification Compat? You should use Notification Compat since the support library provides the latest features, such as Android Auto, which were not available back in API 18. To test your application, we provide a messaging simulator that looks very similar to the Car Experience from Android Auto. It's actually downloaded as part of the SDK Manager, so let's get that going so we can download it. So we go to Tools. Android SDK Manager. And if you scroll right to the bottom, you'll see it here as Android Auto API Simulators. So check that. Hit Install. Agree to the License. And it's downloading it now. Close the SDK Manager. Once you've downloaded the APK, we now need to use the terminal to install it to the device. The Messaging Simulator is located in the SDK's extras/google/simulators/ directory. You need to ADB install the Messaging Simulator to your Emulator. So let's now go back to the Emulator and run it. You'll notice that the application needs to run in Landscape Mode. So, let's rotate the Emulator. Typically, you can press Control F11 or Control Fn F11 to make this happen like so. Let's just move the window up a little bit. Next, it'll ask for permissions to be able to get access to your notifications. Click continue, check on the box here, and hit OK to agree to it. To allow it to see your messages. Now press back to get back to the messaging simulator again. The simulator interfaces to the Android notification system, and implements the car extender extensions. So you can see here how the car will interpret your messages. This is great for testing at your desk during development, when you don't want to be sitting in a car for testing. So now that we've got the simulator working, let's start up our app and see how it works. So let's build it and install it to the Emulator. OK. Go to the Emulator. Can you see here our sample is running? So press the DO IT button and generate the notification. You then slide it down, you can see the notification is there. So now let's switch over to the simulator to see what it's showing. So, we click on the square icon which shows all the apps running, and we click on the messaging simulator. And so, if you look here you can see that we've got our message, so that worked great. So, let's click on the message now to hear the audio. New message from Lane P,, received. Hello there. How are you? You'll also notice in the logout output that you've seen acknowledgment that conversation ID number 42 has been read. Let's click the Reply button now. Note that we can't actually talk to the simulator here. That's actually not supported by the simulator. But you can't see the string, This is a reply, was passed to our broadcast receiver, which handles replies for conversation ID number 42. So that's everything. You now know how to make a fully Android Auto-enabled messaging app. Test and install your application, as well as the simulator, and verify that you can hear a message and also reply to it. Check all the boxes once you've completed each part. Now that you've seen how a basic notification app can be turned into an Android Auto app, you're now ready to explore a more complicated example. We have a sample called Messaging Service that is a much more complete example, that you should really have a look at. So let's create a new project that uses this. We can find the sample by going to find, import sample. So we type messaging service, we click on it, hit Next, and then we can hit Finish with the defaults. Now that the sample's loaded, let's expand the list of Java files that are included. So you can see here, we have a much more detailed example. Let's run this on the simulator and see what happens as well. So with the sample running, click these buttons to generate different kinds of conversations. So click this button here, you can see we have some debug output which shows you the kind of message that was generated. Let's go to the messaging simulator and see what it looks like. New message from Jane Doe. Did you finish the messaging app yet? So, now that you've seen it work in the simulator, you can play around with the sample and get more familiar with it. So what you should do now is you should review the messaging service sample, and try it out for yourself. Build and install it to the emulator, and then try it with the messaging simulator. This is a great starting point that shows all the capabilities of Android Auto-Messaging. Check off each box, once you've completed each task. Let's do a quick review of all the steps needed to add Android Auto Messaging to an app. The first is to edit AndroidManifest.xml. Then create automotive_app_desc.xml. Add the support library to your build.gradle file. Add read and replied intent filters. Add PendingIntents for read and reply actions. Add an unread conversation. Add a CarExtender. Make sure you use NotificationManagerCompat and not the framework notification manager. And finally test it all using the Messaging Simulator. And that's it, we're done. So that's the end of this section about messaging on Android Auto. As you can see, the fundamentals of your application remain the same, but you added some simple extensions to your notification objects to get Android Auto features. This concept of extending notifications is very common on Android and is similar to how the extensions work for Android Wear as well. Android Auto provided all the speech recognition for you. So, you just need to do something with the text string. And it takes care of speaking to the user as well. We also covered some of the important things to know about extending existing messaging apps. So if you really have an app you should be ready to extend it for Auto. In the next lesson, we're going to talk about how to add Android Auto Support to your existing music app. So let's go try that. I'll see you then. [NOISE] Commuters in the US spend an average of one hour a day in their car. And a person checks their smart phone an average of 125 times per day. Our cars get us where we're going while our phones keep us connected. But using your phone while driving can be distracting and dangerous. But what if you could stay connected using your phone but in a safer and more convenient way that's integrated in the car? And better yet, what if developing for this new platform was easy and familiar? Welcome to Android Auto, bringing Android to the car. It's the same platform you already use for phones, tablets, televisions, watches, and more! In fact, all these experiences will often be in the same APK. But now your app can also extend to the car in a way that's safer and more efficient for the driver. So they can stay connected, but with their hands on the wheel and eyes on the road. Using Android Auto is easy. Users go to the Play Store and download apps onto their phone that support Android Auto. They then connect their phone to their car, the phone goes in the car mode, and casts the Android Auto experience to the car's screen. This means that although all the apps and services are actually running on the phone, they're displayed in the car's dash and you interact with them using the vehicle's controls such as a built-in touch screen and microphone. Now because of this, you don't need to write a new app. Instead, you can just extend your existing Android apps to work in the car. Now there are two great use cases for this today: audio and messaging. Audio apps such as music, radio, and audiobook players, which are awesome way to stay entertained while you're driving, can now be extended into the car. The APIs are available to expose content, receive playback control, and customize the UI. Messaging applications, which are a great way to stay connected while you're driving, can now be extended to the car as well. APIs are available to show notifications, play back the message, and even reply with speech. Now that you have an overview of the Android Auto platform, let's take this road trip further with a couple of lessons from Wayne on extending your messaging and audio apps to the car. Okay Google, send a message to Wayne. I'll be at our building in 20 minutes, can you get Josh and meet me there? Okay Google, start a run. [SOUND] Here's the message. I'll be at our building in 20 minutes, can you get Josh and meet me there? Reply. How about we make it five minutes? [SOUND] Got it. Here's your message. How about we make it five minutes? Do you want to send it? Cat videos. [MUSIC] I'll see you in 20 minutes, Timothy. [MUSIC] I'll see you in five minutes, Wayne. [LAUGH] Okay. One, two. Welcome to Ubiquitous Computing. I'm Timothy Jordan, a Developer Advocate at Google. And I'm Wayne Pakowski, also a Developer Advocate at Google. And I'm Josh Gordon, and you guessed it, also a Developer Advocate at Google. As you'll learn in this course, Ubiquitous Computing is simply a fancy phrase for technology that's accessible to the user, whenever and wherever they want or need it. Perhaps, they're outside going for a run. Or having some friends over to watch cat videos and play games. Or maybe, it's time for a road trip. Your users can be in any of those situations, and you don't want them to have to think about what device they need to get to your service. No, you want an experience that presents itself to the user wherever it makes sense. By building an app that runs on whatever devices they happen to have nearby. In this course, we'll teach you about the existing platforms and tools that can make your app available on a variety of form factors. But before we get started, I'll explain just a little bit more about what it means for your app to be ubiquitous. The traditional approach to supporting different devices is to make an app for each device and platform as they become available and adopted by users. This has typically meant a new project for each and every device developed and supported independently. For most of us, we actually think of the experience on any of these devices as part of a single whole experience for the user with our service. Approaching each as a separate project is well, it's just the wrong way to go about it. Instead you're going to want to think about having a view into your service from every device that makes sense. Now this is really the definition of ubiquitous computing where your service is available anywhere and everywhere. Each view into your service will differ slightly depending on the features available, and what the user needs in that context, but taken as a whole these views lead to a single seamless experience for the user across their life. We'll be teaching you how to use Google's platforms and tools to build these kinds of experiences for your users, while staying relevant and unobtrusive. Now keep in mind this is not an introductory course. We're going to assume that you're already familiar with the Java programming language, and that you already know the Android platform. If you're unsure if you're ready for this course, we recommend you start with Developing Android Apps. Also make sure to check out the instructor notes for other resources that will introduce you to Android and Java. Let's get started! One of the best parts of being a developer advocate is the opportunity to talk to developers and designers around the world in building amazing things. And we talk about their existing integrations, ways to make them more performant, how to take advantage of new services and increasingly, how to extend them to new form factors and more parts of users lives. And in these conversations, I hear a lot of great ideas. Some of these ideas can be characterized like this. Incredibly cool [LAUGH] while also not useful at all. And herein lies a central issue with building for new form factors. Often our imaginations run so far to the science fiction of our art, that we forget to make sure we're solving actual user problems and then test to make sure our solution is a good one. That's why we're going to take a few moments right now to discuss the big picture of design for new form factors as a baseline for our more detailed design discussions later in the course. Now let's start here, with the desktop computer many of us have spent countless hours working with to build and use software. It looks conventional and familiar only because we've each spent so much time with it. But something you'll notice about all these devices when you look at them without anything else in the picture. It's hard to see the human. Now one of my favorite thought experiments is by the VR pioneer, Jaron Lanier. You may have heard this before, so just bear with me for a moment. What if aliens came to Earth and people were nowhere to be found, and what if these aliens tried to figure out what we were like based on our computers. What would they think? DI mean do we have a hundred and two fingers and one big eye? Now if they look to the hammer, he'd make more sense. You can almost see the arm that would wield this tool and hammer in nails. But this computer? I mean, just look at that keyboard, it's not really built for us, in fact, it's not even built for us to type fast. Back when this keyboard layout was designed, it was intended to keep us from typing so fast, that these tight bars would get caught together and screw everything up. But this, is what evolved into the modern keyboard. Now this is a very physical difference between a person and a machine. Let's look at a less physical difference that's just as dangerous. Now what's wrong with this picture? In many ways, this is our relationship to technology today, it gives us a lot of value. However, we're adapting our lifestyle to the technology that assists us, and there's a heavy cost, distraction. We pay that cost frequently. But what if we didn't have to? These are some ways that you can use the technology we discussed in this course. And what I love about each of these examples, is that you can hardly see the technology. Some people think that the future of computing is keeping you immersed, and keeping you away from you life. But as it turns out, with ubiquitous computing, less is more. It's where we, as developers and designers, put the user before the technology. As we remove the abstract barriers, such as what functionality resides on what device. Our users can focus more on what they're doing, and less on how to start doing it. Now, this simple core philosophy to ubiquitous computing is what you see over and over again in terms like seamless, simple, microinteraction, and glanceable. And it's not new. It's the idea that success isn't measured by how long the user is engaged with the interface, but how quickly we can get them to what they need. Now there's a website that's been around for just over 15 years, that was built with that philosophy in mind. Google. This kind of user interaction is in our DNA. Back at a time when the search engine philosophy was to keep the user on the page as long as possible, Google went the other direction. And as it turns out, that's exactly what users wanted. And that's why it made sense for us to recognize that as a core philosophy across all these new form factors.