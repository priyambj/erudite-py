So far, in our discussion of algorithms, we've restricted our attention to problems that we can solve in polynomial time. These aren't the only interesting problems, however. As we saw in our discussion of NP-completeness. There are many practical and important problems for which we don't think there are polynomial algorithms. In some situations, the exponential algorithms we have are good enough. But we can't obtain any guarantees of polynomial efficiency for these problems unless P is equal to NP. Besides resorting to exponential algorithms, we also have the option of approximation. For instance, we might not be able to find the minimum vertex cover in polynomial time, but we can find one that is at most twice as big as it needs to be. And many other NP-complete problems admit efficient approximations as well. This idea of approximation, will be the subject of this lesson. We'll start our discussion with a very simple approximation algorithm, one for the minimum vertex cover problem. As input, we're given a graph, and we want to find the smallest set of vertices such that every edge has at least one end in this set. Recall that this problem is NP-complete. We reduced a maximum independent set to it earlier on in the course. The approximation algorithm goes like this. We start with the empty set, and then, while there's still an edge we haven't covered yet, we choose one of the edges from the graph arbitrarily. Say these two here, and then we add both of these vertices to the set like so, and then we go and repeat the process. Picking an arbitrary edge from the remaining part of the graph, let's say these two here, and then removing all the edges incident on these two vertices, like so. And so on, picking this edge here, and so forth, removing all edges incident on those two. And we keep doing this until there aren't any edges left. This set C, which we've chosen, must be a cover. Looking back at the original graph it's not too hard to see that this cover of six vertices is not a minimum one. Here is an optimal cover. It only has three vertices. So this approx VC algorithm returned a cover twice as large as the optimal cover. Fortunately this is as bad as it gets. Given a graph G having a minimum vertex cover, say C*, the algorithm ApproxVC returns a cover C such that the ratio between the cover that it returns and the optimum one is at most two. To prove this, it's useful to consider the set of edges chosen in this line here. We'll call this set M. We use M here because this set is a maximum matching. It pairs off vertices in such a way that no vertex is part of more than one pair. In other words, this set of edges must be vertex disjoint. That means that in order to cover just this set of vertices, any vertex cover must include at least one vertex from each of these. Therefore, the size of this set M is a lower bound on the size of the minimum vertex cover, C*. Since C* is a minimum cover, the set C, chosen by the algorithm, can only be larger. IT is of size twice M, which is at most, C star, as we already argued. Dividing through by C star here, then gives the desired result. Given this theorem, let's explore the relationship between the size of the optimum cover and the one returned by our ApproxVC algorithm with a quick question. Suppose that G is a graph with a minimum vertex cover C*, and suppose that our algorithm returns a set C. Fill in these blanks so as to make these statements as strong as possible. The answer for this first one, is that c could be as small as 32, the optimum, but of course it can't be any smaller. On the other hand, it could be as big as 64, twice 32, but it can't be any greater by the guarantee provided by the theorem we just proved. Next, taking it as a given, that the algorithm returns a vertex cover of size 50. Then it might be with the case that the optimum cover is half of that size, 25. You can't be any smaller, because of the guarantee that the algorithm provides. On the other hand, it might be as large as 50, since it's possible the algorithm actually found the optimum cover. Even though the algorithm we just discussed for a minimum vertex cover was very simple, it illustrates some key ideas found in many approximation schemes. Consider this figure here, illustrating the possible sizes of the minimum set cover. We have the size of the set returned by our algorithm here, and the size of the optimum one here. And we would like to be able to find some guarantee about the relationship between the two. The trouble, of course, is that we don't know the optimum value. Actually, finding the optimum value is NP complete, and that's why we're searching for an approximation algorithm anyway. We resolve this dilemma by finding a lower bound on the size of the optimal cover. That's the role played by the maximal matching that the algorithm finds. And then we find an upper bound on our approximation then, in terms of this lower bound. The cover that we returned has sized twice that of the maximal matching. Note that the approximation does not tell us enough about the optimum value to allow us to solve the decision version of the problem. Does the graph have a vertex cover of some particular size? Our algorithm might be twice the optimum value, as in these two graphs here, or it might find an exact solution, as in this case here. So you can't tell what situation we're in. We can't tell where the actual minimum vertex falls. In our discussion of computability and complexity, we focused on deciding languages. As we began our discussion of algorithms, however, we began to talk about optimization problems instead without ever formally defining them. This was fine then, but as we discuss approximation algorithms, we're going to circle back to some of the ideas we encountered in complexity. And we need a formal way to connect them. Therefore we're going to define an optimization problem using the min vertex cover as an example to illustrate. And this will allow us to give a formal definition of an approximation algorithm. The first thing we need to define an optimization problem is a set of valid instances. For the minimum vertex cover problem this is just a set of undirected graphs. For each instance, there's a set of feasible solutions. For Min Vertex Cover, this is the set of vertex covers for the graph. And we need an objective function, the thing we're trying to optimize. For min vertex cover, this is just the size of the cover. And of course we need to say whether we're minimizing or maximizing this objective function. Naturally for min vertex cover, we're minimizing. Relating this back to our treatment of complexity, we say that an MP optimization problem is one where these first three criteria are computable in polynomial time. That is to say, there's a polynomial algorithm that tells whether the input instance is valid. There's one that can check whether a solution is feasible for the given instance, and there's one that can evaluate the objective function. All these things being done in polynomial time. Now every optimization problem has a decision version of a form is the optimum value at most sum value for the min, and a least sum value for the max. For minimum vertex cover, we ask is there a cover of size less than some threshold T? With this in mind, we say that an optimization problem is NP-hard if it's decision version is. Our problem is NP-hard, by the way, if an NP complete problem can be reduced to it. In our example, min vertex cover is NP-hard because the decision version is. Remember that we reduced maximum independent set to vertex cover. So that's how optimization relates to complexity. Next, I want to use this definition of an optimization problem to define an approximation algorithm. An algorithm is a factor delta approximation algorithm if on any instance I it produces a solution S. Such that the objective value of the solution S on instance I, is that most delta times the optimum for the instance I. Note that it's okay for delta to be a function of the instant size. Also, when we're working with maximization problems instead of minimization ones, this inequality gets reversed. Our previous results for the min vertex cover problem then can be stated in terms of this definition by saying that min vertex cover has a polynomial time factor 2 approximation algorithm. Or approximation scheme As we saw earlier on in the course, the complement of a minimum vertex cover is a maximum independent set. It's natural to ask then, can we turn our factor two algorithm for a minimum vertex cover into an algorithm for maximum independent set. Such an approach yields what about the maximum independent set problem? Choose the best substitute for this blank here. The answer is that it yields nothing. The actual maximum independent set will have size of V minus size C star vertices in it. While the compliment of the vertex cover that we find will have size of V minus size of C vertices in it. But, if C star happens to be the number of vertices over 2, well then the size of C could be equal to V, and then this ratio becomes entirely meaningless. We've now seen the potential trade-off between time and accuracy in the context of the vertex cover problem. There is an exact exponential algorithm, and there's a polynomial factor to approximation. What if a factor to approximation isn't what I really want however? What if I'd be happy with a factor ten approximation if it were fast enough? What if I really need to be within 1% of the optimum, and I'm willing to wait for an answer as long as it takes? It would be very convenient if there was some way to easily trade off speed versus accuracy in this way. For vertex cover it turns out that this is highly unlikely but for other problems there are schemes that allow us to trade off speed versus accuracy with precise guarantees. We'll illustrate this possibility with the example. The subset sum problem emits a fully polynomial time approximation scheme, or FPTAS for short. Recall that the decision version for subset sum was whether, given a set of numbers, there was any subset summed up to a particular value. The optimization version of this is to maximize the sum of sub subsetted numbers without exceeding some threshold t. This problem emits a very important class of approximations. For any epsilon greater than 0 there is an order n squared log t by epsilon time. Factor 1 over epsilon approximation algorithm for the subset sum problem. The smaller the epsilon, the better the approximation, but the slower their running time. It may be intuitive, that one should be able to trade-off spending more time for a better approximation guarantee, but it isn't always the case that we get to do so arbitrarily as in this theorem. Because this isn't a particular algorithm, but rather a kind of recipe for producing an algorithm with the right properties, we call this a Polynomial Time Approximation Scheme. Or PTAS, for short. For every epsilon you choose, there's an algorithm that can approximate that well. This approximation scheme is extra special, because the running time is polynomial n 1 over epsilon, as well as polynomial in a size of the input. Therefore we say that this is a fully polynomial time approximation scheme. The alternative would be for the epsilon to appear in one of the exponents perhaps and then it would just be a polynomial time approximation scheme. The scheme itself and it's analysis are a little bit messy, but they're still well worth looking up on your own. After hearing about these approximation schemes, the optimist may be saying, hey, maybe every NP complete problem admits a fully polynomial time approximation scheme. Unfortunately, this isn't true unless P is equal to NP. There are some problems where approximating the optimum within certain factors, would lead to a polynomial algorithm for solving every NP complete problem. This phenomenon is known as the hardness of approximation, and occupies an important place in the study of complexity. We'll illustrate this idea by showing that the traveling salesman problem is hard to approximate to within any constant factor. In case you haven't seen the traveling salesman problem before, it can be stated like this, we're given a graph G and usually all the edges are present and with each one of them is associated some cost or a distance. We'll assume that all edges are present, so we won't draw them in our example graphs like this one here. The goal is to find the minimum cost Hamiltonian cycle. That is to say, we want to visit each of the vertices without ever visiting the same one twice, and to do so in a way that minimizes the sum of the costs associated with each of the edges that we chose. This problem is NP-complete in general. And even a constant factor approximation is impossible unless P = NP, as we will prove next. Being a little more formal, we say that if P is not equal to NP, then for any constant alpha at least one, there is no polynomial time factor approximation for the traveling salesman problem. For the proof we reduce from Hamiltonian Cycle, where we're given a graph, not necessarily complete this time, and we want to know if there's a cycle that visits every vertex exactly once. Here then is how we set up the traveling salesman problem. We assign a cost of one to every edge in the original graph G. And we assign a cost of alpha times V + 1 for every edge not in the original graph. Clearly then, if G has a Hamiltonian Xycle, then the optimum for the traveling salesman problem has a cost of V. One unit of cost for every edge that it follows. Then a factor alpha approximation for the traveling salesman problem, will return a Hamiltonian Cycle H, whose cost is at most alpha times this number of vertices. On the other hand, if the original graph has no Hamiltonian Cycle, then the cost of the Hamiltonian Cycle returned by our traveling salesman approximation has to be greater than the optimum. But that must include at least one of these edges not in the original graph. Note that this term comes from following edges in the original graph. This one from following edges not in the original graph. There has to be at least one of them. Simplifying gives the lower bound of 1 plus alpha times V. Therefore, to decide the Hamiltonian cycle problem, we just run the alpha approximation on the graph with these costs here. Then we compare the resulting cost to alpha times V. If it's larger, then there can't be a Hamiltonian Cycle on the graph. Any Hamiltonian Cycle would have cost V, and our approximation algorithm couldn't have returned such a large cost. An the other hand, if it's the same as alpha V or smaller, then we can't have used one of the edges not in the original, so there must be a Hamiltonian Cycle. Thus, if there were a polynomial time constant factor approximation for the general traveling salesman problem, it would yield a polynomial algorithm for the Hamiltonian Cycle, which is NP complete. Unless P is equal to NP, no such approximation algorithm can exist. So far, we've seen three different kinds of results, related to approximation algorithms. We've seen a factor two approximation, in the example of vertex cover. We talked about fully polynomial time approximation schemes, in the context of subset sum. And we've seen a hardness of approximation result, in showing that the general travelling salesman problem cannot be approximated to within any constant fact. That's a good sample of the types of results, one sees in the study of approximation algorithms. Before ending this lesson, however, I want to talk about one more classic result. Perhaps the traveling salesman problem can't be approximated in general. But if we insist that the cost function obey the triangle inequality, as it does in many practical applications, then we can find an approximation. The triangle inequality, by the way, just says that it's never faster to go from one vertex to another via a third vertex. Here's the approximation algorithm. We start by building a minimum spanning tree. The usual approach here is to use one of the greedy algorIthms, either of Kruskal or Prim that are typically taught in an undergraduate class. In Kruskal's algorithm, the idea is to simply take the cheapest edge between two unconnected vertices and add that to the graph until a tree is formed. Thus we might start by adding this edge here, then this edge here, and so on and so forth. Here I connected these two components. And we just keep going until we've formed a tree. Next, we run a depth-first search on the tree, keeping track of the order in which the vertices are discovered. For this example, let's label the vertices with the letters of the alphabet, and start from this vertex c over here. Then the discovery order would go something like this. >From c we go to b and then to e and then to h and then to i and we would record that like so. At that point we'd turn back to h but since we've already discovered it, we don't write it down. Instead we write down the next vertex we visit j. And this process continues. We skip over h and e on the way to d, and then we'd go to g and f, and then skip over g and d on our way up to a. And then we skip over all these vertices, which we've already visited, to get back to our original vertex c. Finally, the algorithm returns the list of vertices, in this discovery order, as the Hamiltonian cycle. Note that this cycle follows along the tree from c to b to e to h to i, but instead of going back to h, it goes directly to j, then it goes directly to d, and so on. This cycle always seems to be taking short cuts, compared to the traversal that the depth for search performed. For the general traveling salesman problem, we can't be sure that these are in fact shortcuts, because we can't assume the triangle inequality. Where we do have the triangle inequality, however, all of these will be shortcuts, and as we'll see, that will be the key to the analysis. The algorithm just described, which we call approximate metric TSP, is a polynomial-time factor 2-approximation algorithm for the metric TSP problem. The process for building the minimum spanning tree is order v squared for dense graphs and the depth for a search process takes time proportional to the number of edges in the tree, which is the same as being proportional to the number of vertices. That takes care of the efficiency of the algorithm. Now for the two factor part. Consider this example here and let H star be a minimum cost Hamiltonian cycle. This is what an exact solution to the traveling salesman problem might look like. Well, the cost of the minimum spanning tree that the algorithm finds must be less than the total cost of the edges in this cycle. Otherwise, just removing an edge from the cycle would create a lower cost minimum spanning tree. Remember that all the cost here must be non negative. Now let's draw a minimum cost spanning tree. The cost of the depth for a search traversal, found by the algorithm, is twice the cost of the sum of the edges in the tree. This traversal starts and ends at the same vertex. The trouble, however, is that it's not Hamiltonian. It might visit the same vertex more than once. It's simple enough, however, to simply count only the first time a vertex is visited. This is what ordering vertices by their discovery time achieves. By the triangle inequality, skipping over intermediate vertices can only make the path shorter. So the cost of the cycle returned by the algorithm is at most twice the cost of the sum of the edges in the minimum spanning tree that we found. This recall, however, is at most the cost of the optimum cycle, giving us a two-factor approximation, that's the proof of the theorem. It may be useful to view the argument on a scale like this one, with zero at the bottom and maybe the cost of the most expensive cycle at the top and the minimum cost cycle somewhere in the middle. As we argued, the cost of a minimum spanning tree must be less than the cost of the optimum cycle. We can just delete one edge from the cycle and get a spanning tree. A depth-first traversal of this spanning tree uses every edge twice, and is therefore twice the cost of the tree. Shortcutting all but the first visit to a vertex in this traversal gives a Hamiltonian cycle, which must have lower cost than the depth-first traversal because of the triangle inequality. In response to any approximation result, it's natural to ask, is the analysis tight, or does the algorithm actually perform better, even in the worst case than the theorem says. Let's address that question for our metric TSP algorithm. Here's an example graph. Let the blue edges have cost 1, and the red ones have cost 2. Enter a minimum cause solution in the box here There are many possible solutions here, but none of them follow a red edge. I chose this path here. We have to follow a blue edge to get in and out of the center, and we follow blue edges all around the outside as well. Thus, we obtain a cost of just six. Now we consider what our approximation algorithm might have returned. Recall that the optimum value for a Hamiltonian cycle is 6. The approximation algorithm begins by building a minimum spanning tree for the graph. Perhaps it chooses a star, like so. Then, preferring lowest index vertices, a depth first traversal would produce this cycle. First, we go from 1 to 2. Then the next step would take us back to 1. So we skip that and go to 3. The next step would take us back to 1. So we skip that and go to 4. And so on and so forth until the cycle is completed. Notice that every edge followed in this cycle is a red one, except the first and the last. Hence, the cost is two time six minus two for a total of ten. The ration then is ten over six, but there wasn't anything special about the fact that we were using six vertices here. We can form an analogous graph for any n, but in the lighter edges be the union of a star in a cycle around the non-center vertices, all other edges can be heavy. My question to you then is how bad does the approximation algorithm get for 100 vertices? Give your answer in this box. Remember that the lighter edges are the ones that go from the center out to the perimeter plus the cycle of edges around the perimeter. The answer is 198. And in general, it's 2 times n- 2. We could go in the wrong order among all the non-centered vertices. So it's possible that every edge costs 2 except for the ones leading in and out of the center vertex. Of course, the optimum involves just using the blue edges which only have cost 1. So in this case, the optimum has cost 100. By choosing n large enough, we therefore can get this ratio of 2n- 2 over n, as closest to 2 as we want. So our analysis that this gives a factor of 2 approximation is optimal. The example just given shows that our analysis of that particular approximation algorithm for metric TSP is tight. There are some graphs at least that really could return a tour that has twice the cost of the optimum 1. So we can't improve on the theorem just by better analysis. It turns out that for the metric traveling salesman problem, there are algorithms out there that give better approximations. The most famous being a 3 over 2 approximation called the Christofides algorithm. If you're familiar with matching with general graphs, and Eulerian tours, then understanding it and the approximation guarantee isn't too much more work than what we've done already. So I encourage you to look it up on your own. In addition, there are many more results for when various alternative assumptions are made about the costive edges. All of them giving different approximation factors and running time guarantees. If you ever find yourself needing to solve a traveling salesman problem, think carefully about what assumptions you can make and look up the best approximation result for your case. Chances are, you won't be the first person to encounter that version of the problem. In this lesson, we've just scratched the surface of the vast literature on approximation algorithms. One could create a whole course on this subject, consisting of results not much more complicated than the ones we've seen here. And of course, there are many more advanced results besides. The main takeaway message then, is that when you encounter a problem for finding an optimal solution seems intractable. Ask yourself, is an approximate solution good enough? You may find that relaxing the optimality constraint makes the problem tractable. Hello and welcome back. In this lesson, we'll discuss the problem of finding a maximum matching and a bipartite graph. A problem intimately related to finding a maximum flow in a network. Indeed, our initial algorithm will come form a reduction to maximum flow. And at first maximum bipartite matching might just seem like a special case. As we examine the problem more carefully however, we'll see some special structure. And eventually, we'll use this insight to create a better algorithm. The plan for the lesson is this. We'll start by defining bipartite graphs, the concept of matchings and some other needed ideas. Then we'll show how the problem of finding a maximum matching can be reduced to the problem of finding a maximum flow. And this will give us an E times V algorithm. Next, we'll show that the Vertex Cover problem is related to the max matching problem. Much in the same way that the minimum cut problem was related to the Max-Flow problem. And finally, we'll give the Hopcroft-Karp algorithm and show that it finds a maximum matching in time order E times the square root of V. There are many good references on the topic of maximum matchings and bipartite graphs. This lesson will follow most closely Kleinberg and Tardos' Algorithm Design for the first three parts. And Dexter Kozen's, The Design and Analysis of Algorithms, for the Hopcroft-Karp algorithm. We begin our discussion by defining the notion of a Bipartite Graph. An undirected graph is bipartite if there exists partition into L and R, think left and right. Such that every edge has one vertex in L and one in R. For example, this graph here is bipartite. I can label the green vertices as L, and the orange ones as R. And then every edge has one vertex, that's green and one that's orange. A few observations are in order. First, saying that a graph is bipartite is equivalent to saying that's 2-colorable, for those of you who are familiar with colorings. Next, let's take the same graph and add this edge here to make it non-bipartite. Note that I've introduced an odd-length cycle. And indeed, saying that a graph is bipartite is equivalent to saying that it has no odd-length cycles. For graphs that aren't connected, it's possible that there will be some ambiguity in the partition of the vertices. So sometimes the partition is included in the definition. And we also often draw the graphs with all the vertices in one partition on the left. And all the vertices in the other partition on the right. Here is a quick exercise on bipartite graphs. I've drawn a graph here that is not bipartite, and I want you to select one or two edges for deletion so that it becomes bipartite. I chose to delete this edge here. I started by coloring this vertex green, and then reasoning about what colors the other vertices might be. Assuming that I don't delete this edge, this vertex here has to be orange. And assuming that I don't delete this edge here, this one has to be green. And assuming I don't delete this edge here, this one has to be orange again. And that's okay, because this edge here has one green and one orange vertex. This vertex here must be green, since both of its neighbors are orange. This last vertex here has both green and orange neighbors, so I need to delete one of its edges. I picked this edge to delete, and I colored it orange. Our next important concept is the matching. Given a graph, a subset of the edges is a matching if no two edges share a vertex. Note that the graph doesn't have to be bipartite. Take for example, this graph here. These two edges here constitute a matching. By the way, we'll refer to an edge in a matching as a matched edge so these two edges are matched. And we'll refer to a vertex in a matched edge as a matched vertex. So these four vertices here are matched vertices. A maximum matching or max matching, for short is a matching of maximum cardinality. Note that a maximal matching is not a maximum matching. For instance, this matching here is maximal because I can't add any more edges and still have it be a matching. On the other hand, it's not maximum because there's a matching of greater cardinality. For instance, this one here. This happens in bipartite graphs too. Here is a maximal matching but it's not maximum because there is a greater matching. Now that we know what bipartite graphs and matching are, let's consider where that problem of finding a maximum matching in a bipartite graph comes up in real world applications. Actually, we'll make this a quiz, where I give you some problem descriptions, and you tell me which can be cast as a maximum matching problems, in bipartite graphs naturally. Don't worry about whether or not there's some complicated reduction. First, consider the compatible roommate problem. Here some set of individuals give a list of habits and preferences, and for each pair we decide if they're compatible. Then we want to match roommates so as to get a maximum number of compatible roommates. Next, consider the problem of assigning taxis to customers, so as to minimize the total pick up time. Imagine that you're a taxi dispatcher service. Another application is assigning professors to classes that they are qualified to teach. Obviously, we hope to offer all the classes that we want. And lastly, consider matching organ donors to patients, who are likely to be able to accept the transplant. Of course we want to be able to do as many transplants as possible. The answer is the last two only. For the roommate problem, we make a vertex for each individual. And we add an edge between two vertices if the two individuals are compatible. We find a max matching and matched edges become roommates. This is a great application of maximum matching. But, more likely than not there will be an odd cycle of compatibility, meaning that the graph is not bipartite. This changes the nature of the problem significantly. The taxi problem on the other hand is clearly bipartite. We model the taxis with one set of vertices and the customers with the others. And we want to find the minimum total pick up time. We can imagine having edges between the taxis and customers finding and matching here, and adding up all of these times. The problem here is the existence of the costs, which demand a different solution from just finding a maximum matching. Assigning professors to classes is a bipartite matching problem. One side of the partition is the professors, and the other is the classes, and we include an edge if the professor is qualified to teach class. A maximum matching solution then will allow us to teach as many classes as possible. Hopefully, this will be all of them. The situation with organ donors and patients is analogous. We add an edge if the patient will accept the organ, and we match as many organs to the patient as we can. At this point, we've defined a bipartite graph and the notion of a matching. Now we're going to see the connection with maximum flow. Intuitively, a maximum matching problem should feel like we're trying to push as much stuff from one side of the partition to the other. It should be no surprise, then, that there turns out to be an easy reduction to the maximum flow problem, which we've already studied. We build a flow network that has the same set of vertices, plus two more which serve as the source and the sink. We then add edges from the source to one-half the partition, and from the other half the partition to the sink. Internal edges are given a direction from the source side to the sink side. All capacities are set to 1. Setting the capacities of the new edges to 1 is important to insure that flow from or to any vertex isn't more than 1. We only want to match each of these vertices once. Having constructed the graph, we run Ford-Fulkerson on it. And we return the edges with positive flows as the matching. Actually, all edges will have flows zero or one, as we'll see. The time analysis here is rather simple. Building the network is order E, or maybe order V depending on the representation used. In any case, this is small, however, compared to the cost of running Ford-Fulkerson, which is E times V. Note that V is a bound on the total capacity of the flow, and hence a bound on the total number of iterations. In this particular case, this bound gives a better one than we would obtain either by the scaling algorithm or by Edmonds-Karp. Lastly, of course, we have the step of simply returning the edges with positive flows, and that's just a matter of writing the answer out, so that's order V. Clearly, Ford-Fulkerson is the dominant part, and we end up with an algorithm that is order E times V. We assert the correctness of the reduction with the following theorem. Consider a bipartite graph G and the associated flow network F, which we just constructed. The edges in G which have flows of 1 in a maximum flow obtained by Ford-Fulkerson on F constitute a maximum matching in the original graph G. To prove this theorem, we start by observing that the flow across any edge is either 0 or 1. Any augmenting path will have a bottleneck capacity of 1. Remember all capacities are 1. So we'll always augment flows by 1. But 1 is also the maximum flow that can go across any edge. The conservation of flow then implies that each vertex is in at most one edge that we return. On the left hand side, we only have one unit of capacity going in, so we can't have more than one unit of capacity going out. So each vertex on the left hand side will be included in at most one edge. And similarly, for the right hand side, we have, at most, one unit of flow going out. So we can have, at most, one unit of flow going in, which implies that every vertex on the right will be involved in, at most, one matched edge or output. This means that the set of edges in the original graph that have flow 1 in the flow network constitute a matching. Is it a maximum matching? Well, if there were a larger matching, it would be trivial to construct a larger flow. Just give those edges flow 1 in here, and connect them with out flow values of 1 to the source, and the one on the right with flow values of 1 to the sync. So yes, it must be a maximum matching. At this point it's tempting to be satisfied with the result. After all we've found a relatively low order polynomial solution for our problem. On the other hand the sort of flow networks we're dealing with here are very specialized. They have all unit capacities and this bipartite structure. Our algorithm doesn't exploit this at all. In the remainder of the lecture, we'll look at the special structure more closely, gain some additional insights about bipartite matchings. And finally these will lead us to a faster algorithm. We start the search for a deeper understanding, with the idea of the augmenting path. Recall that in our original treatment of the max flow problem, we were given a flow over some graph, and we defined the residual network, which captured the ways in which we were allowed to modify this flow. This included adding backwards edges that went in the opposite direction from edges in the original. Augmenting paths then were paths over this residual network that went from the source to the sync, and augmented the flow. In a network that arises from a bipartite matching problem, there are few special phenomena that are worth noting. First observe that all intermediate flows found by Ford-Fulkerson correspond to matchings as well. If there is flow across an internal edge, then it belongs in the matching. Also because flows along edges are either zero or one there are no anti-parallel edges in the residual network. That is to say either the original edge is there or the reverse edge is there, never both. Moreover the matched edges are the ones that have their edges reversed. Also, only unmatched vertices have an edge from the source to them, or from them to the sync. Matched vertices have their edges reversed. The result of all this is that any augmenting path must start at an unmatched vertex, and then follow an unmatched edge, followed by a matched edge, followed by an unmatched edge, until eventually it reaches an unmatched vertex on the right hand side, and then it can follow that edge to the sync. This realization allows us to strip away much of the complexity of flow networks and define an augmenting path for bipartite matching in more natural terms. We start by defining the more general concept of an alternating path. Given a matching M, an alternating path is a path where the edges are alternately in M and not in M. And an augmenting path is an alternating path where the first and the last vertex are unmatched. For example, in this graph here, going from this unmatched vertex in L to this matched vertex in R back to this matched one in L and then to this unmatched one in R constitutes and augmenting path. We use it to augment the matching by making the unmatched edges matched and the matched edges unmatched. Like so. This always increases the size of the matching because before we flipped, there was one more unmatched edge than matched edge in the path. So when we reversed the matched and unmatched edges, we increased the size of the matching by one. In fact, we can restate the Ford-Fulkerson method purely in these terms. We initialize the matching to the empty set, and then while there's an augmenting path, we update the matching to be the symmetric difference between the current matching and the path that we found. The symmetric difference includes an element if it's in exactly one of these sets, or alternatively, you can think about it as the union of the two sets minus the things that are in both. The things that are in both here correspond to matched edges in the path. And remember that we flipped them to being unmatched. And finally, once there are no more augmenting paths, we simply return the matching. Now we turn to the concept of a vertex cover, which will play a role analogous to the one played by the concept of minimum cut in our discussion of maximum flows. Given a graph, G, we say that S is a vertex cover if every edge is incident on a vertex in S. Thus we say that S covers all the edges. Take this graph for example here. If we include this vertex, then we cover these three edges. Including this vertex covers these three more, and this vertex includes these more edges. And lastly, we need to cover this edge here so we'll ad this vertex here as well. One pretty easy observation to make about a vertex cover is that its size serves as an upper bound for the size of a matching in a graph as stated in the proposition here. The proof is simple. Clearly for every edge int he matching at least vertex must be in the cover, and of course all these vertices must be distinct because no vertex is in two matched edges. Let's get a little practice with vertex covers. Find a minimum vertex cover, i.e. one of minimum cardinality, for the given graph below. There are a few solutions, I happened to choose this one here. Note that although the minimum vertex cover is three, the maximum matching is only two for this graph. The size of the vertex cover serves as an upper bound, but it is not tight for general graphs. As we'll see next, however, it is tight for bipartite graphs. Now we're ready for the matching equivalent of the max flow min cut theorem. The Max-MAtching/ Min Vertex Cover Theorem. Consider a bipartite graph G and a matching M. Then the following are equivalent. M is a maximum matching. M admits no augmenting paths and there's a vertex cover That has the same size as the size of the matching. The proof will be very similar to the max flow min cut theorem. First we'll show that if M is a max-matching, then it can't admit any augmenting paths. Well suppose not. Then there is some augmenting path, and if we augment M by this path, we get a larger matching, meaning that M was not a maximum as we had supposed. Next we argue that if M admits no augmenting paths, then there exists a vertex cover that has the same size as M. This is the most interesting part of the proof. We'll let H be the set of vertices reachable via an alternating path from unmatched vertices in L. We can visualize this definition by starting with some unmatched vertices in L, then following its outgoing edges to some elements in R, and then following all the matched edges back to matched elements of L, and back and forth and so forth, as many times as we can. Note that because the matching doesn't admit any augmenting paths, all of these paths must terminate in some matched vertex on the L side. Let's draw the rest of the graph here as well. We have some matched vertices in L, the vertices of R that they're matched to, And possibly some unmatched vertices in R as well. Note that H and the compliment of H correspond to the minimum cut that we used when discussing the max-flow min-cut theorem. To get a min vertex cover, we select the part of H that is in R and the part of L that is not, and we call this set S. S contains exactly one vertex of each edge in M. If the edge in M happens to belong to H, then we've got its right side. And if it doesn't belong to H, then we got its left side. So the size of S and the size of M must be the same. Next, we need to convince ourselves that S is really a vertex cover. Remember that there are no edges internal to R or internal to L, so we don't need to worry about those. The part of S that is in R covers all the edges internal to our H set of vertices, and these over here in L, they cover all edges in the not H part of the graph. So what we really need to worry about are just edges that go across the cut. Well, from here to here, both vertices in the edge are covered. So we really only need to worry about the part of L that is in H being one vertex, and the part of R that is not in H being the other. But if there were such an edge, crossing here. It would have been in H, because we would have been going from a vertex in H via an unmatched edge to R. We conclude there are no such edges, and hence S is a vertex cover. Finally we have to prove that the existence of a vertex cover that is the same size as a matching, implies that the matching is a maximum. And this follows immediately from our discussion that a vertex cover is an upper bound on the size of a matching. So if it wasn't a maximum matching, then the whole idea of vertex cover wouldn't be an upper bound, as we argued already. Before we turn to finding a faster algorithm for finding a maximum matching, there's a classic theorem related to matchings that we should talk about. This is the Frobenius-Hall theorem. For a subset of vertices, say x, we'll use n of x to indicate the union of the neighbors of the individual vertices in x. If we consider this graph here then the neighbors of the orange vertices will be the green ones. Note that the fact that if we call this x, then the size of the neighborhood of x is larger than x. That this bodes well for the possibility of finding a matching for all the vertices on the left hand side here. At least, there's a chance that we will be able to find a match for all these vertices. When this is not the case however, as seen for these three vertices here, then it is hopeless. Regardless of how we matched the first two, there will be no remaining candidates for the third vertex. We can make this intuition precise with the Frobenius-Hall theorem, which follows from the max matching min vertex cover argument. Given a bipartite graph G partitioned into L and R, there exists a matching of size L if and only if for every X with is a subset of L, the size of the neighborhood of X is at least the size of X. The forward direction is the simpler one. Let M be a matching of size L, and we'll let x be a subset of the left hand side. We let y be the vertices on the right hand side that x got matched to. Clearly, y is the same size as x, but then y is also a subset of the neighborhood of x, and hence, the size of the neighborhood of x Is at least the size of X. The other direction is a little more challenging. Suppose not. We'll let M be a max matching, with cardinality strictly less than the cardinality of the left-hand side. We let H be the set of vertices reachable from an alternating path from the unmatched vertices in L. This is the same picture used in the max, matching, min vertex cover argument. There's at least one unmatched vertex here by our assumption. Well then the neighborhood of the left hand side of H is equal to the right hand side of H. But this left hand side must be strictly greater because there's at least one unmatched vertex. If you like, you can think about the matched vertices on either side canceling each other out. And we're left with some non-0 number of unmatched vertices on the left hand side. Our treatment of matchings wouldn't be complete, without talking about the notion of a perfect matching. Consider a bipartite graph, G, where the size of the left and the right hand side are equal. Then we say that a matching is perfect, if it has the same cardinality as the two sides. Another way of saying this is to say that all vertices are matched. To review some of the key concepts from the lesson so far we'll do a short exercise. Assume the left and right hand sides are of the same size and then the question is which of the following imply that there is a perfect matching. The fact that for all subsets of the right hand side. The neighborhood of that set is at least the size of the set itself. How about the fact that there is a vertex cover of size L? Or the fact that there is a matching M that admits an alternating Hamiltonian cycle. The answer is the first and the third. The first is just the Frobenius-Hall theorem, and in fact this condition is equivalent to there being a perfect matching. A vertex cover of size L always exists, just pick L. Remember that the vertex cover is just an upper bound, so this does not imply that there's a perfect matching. Lastly, if there is a matching that admits an alternating Hamiltonian cycle, then there have to be half as many matched edges as there are vertices, which is the size of L. Note that the implication does not work the other way. There are plenty of perfect matchings that don't admit alternating Hamiltonian cycles. Now that we have a deeper understanding of the relationship between maximum matching and max flow problems, we're ready to understand a more sophisticated algorithm. Back in the maximum flow lecture, we considered two ways to improve over the naive Ford-Fulkerson. One was to prefer heavier augmenting paths. Ones that pushed more flow from the source to the sync. In the max matching context, this doesn't make much sense, because all augmenting pats have the same effect. They increase the matching by one. The other idea was to prefer the shortest augmenting path, and there was Dinic's further insight that the breadth first search need only be done when the shortest path length changes, not once for every augmentation. Pursuing these ideas gives us the Hopcroft-Karp algorithm. The Hopcroft-Karp algorithm goes like this. We first initialize the matching to the empty set, then we repeat the following. First, we build an alternating level graph rooted at the unmatched vertices on the left part of the partition using breadth-first search. Let's pause for a moment here and see how this works in an example. The first iteration is trivial, so let's start with a later iteration where we have some existing matching marked by the orange edges here that we're trying to augment. Level 0 consists of the unmatched vertices on the left side and to go to the next level, we follow the unmatched edges. Then we follow matched edges to get back to the left hand side. And then we follow along unmatched vertices which lead us to an unmatched vertex on the right hand side. So that's a level we can stop at. And we end up with this here for our total level graph. Having built this level graph, we use it to augment the current matching with a maximal set of vertex disjoint shortest augmenting paths. We accomplish this by starting at one of the unmatched vertices in R and tracing our way back. Let's say that I used this path here. Well then because I'm looking for a set of vertex disjoint paths, I can go ahead and delete these vertices. And with these vertices deleted, I can also go and delete all the other vertices that got orphaned in the process. And at this point we note that I had to delete the other unmatched vertex, nr, and that tells me that I found a maximal set of vertex disjoint shortest-length paths. Once we've applied all these augmenting paths, we go back and rebuild the level graph, and keeping doing this until no more augmenting paths exist, and then we can return the matching that we found. >From the description, it's clear that each iteration of this loop, what we'll call a phase from now on takes on the order E time, building a level graph is done by breadth first search. And so that only takes order E time and the second part can also be thought of as a single traversal of the graph essentially. As we're picking a path from one of the unmatched vertices in R to one of the unmatched in L we can follow any edge here. We don't have to be careful at all, because in effect, all roads lead to an unmatched vertex in L. The key insight is that only about the square root of V phases are needed in order for us to get to a point where there are no more augmenting paths. Overall then, we seek to prove the theorem stating that given a bipartite graph, the Hopcroft-Karp algorithm finds a maximum matching in time order E times the square root of V. Our first step in the proof is to understand the difference between the maximum matching that the algorithm finds eventually and the intermediate matchings that the algorithm produces along the way. Although actually, we'll state the key claim in terms of two arbitrary matchings, M and M prime. And actually, I want you to help me out. Think about the graph containing edges that are in M but not in M prime or vice versa and tell me which of the following statements are true. The answer is that all of them are true. Lets consider these two matchings, one blue and one orange in this example here. When I take the symmetric difference, these edges that are in both matchings will get deleted. Every vertex can have degree at most two. Because it can only get one edge from M and one edge from M prime. And indeed, every path is alternating with respect to M and M prime for essentially the same reason. The third point follows almost immediately from the first. Since in a graph where the degree is at most two disjoint paths and cycles are the only things that can happen. This next demo will characterize the effect of choosing to augment by a shortest path. We'll let M be a matching and we'll let P be a shortest augmenting path for M and then we'll let Q be an augmenting path for M augmented by P. Well, then the size of Q is at least the size of P plus twice the overlap between P and Q in terms of the number of edges. For the proof, we first observe that the size M augmented by P and augmented by Q minus the size of the original matching M is equal to 2. So the symmetric difference between the augmented version and the original, which is also equal to the symmetric difference between P and Q must contain two vertex-disjoint augmenting paths of length at most P. They couldn't be shorter than P. Otherwise, P wouldn't be as short as path. Therefore, the total number of edges in the symmetric difference between P and Q is at least twice the size of P. Now the size of Q plus the size of P is the size of what is in exactly one of the two plus twice the size of what is in both. You can think about the overlap being counted once in P and once in Q, but not at all in the symmetric difference. Well then, applying our bound on the size of the symmetric difference between P and Q. We then have that this is at most, twice the size of P + 2 times the overlap of P and Q and subtracting off the size of P from both sides gives us the theorem. A corollary of this lemma is that if P and Q are the same size, then they have to be vertex disjoint. The fact that they are edge disjoint follows directly from this equation to argue that they have to be vertex-disjoint, we'll argue that sharing a vertex implies sharing an edge. So let's take this theorem and suppose not, we'll let v be a shared vertex. Well then, v has to be matched in M augmented by P, so Q must share the matched edge with P. We can visualize this argument by drawing P with the original matching M. Then once we've augmented, P becomes like so. And regardless of what vertex the path Q happened to share with it, that vertex is now matched and augmented by P. So Q has to follow that edge, but then Q and P share an edge and Q is greater than P by at least two and that then leads to a contradiction. Our next lemma states the key property of a phase of the HopcroftKarp algorithm. Each phase increases the length of the shortest path by at least two. For the proof, we'll let Q be a shortest augmenting path after a phase that augmented with paths of length k. It's impossible for Q to be less than k, by a previous lemma which showed that augmenting by a shortest augmenting path never decreased the minimum augmenting path length. On the other hand, Q being equal to k implies that Q is vertex disjoint from all paths found in the previous phase, by the lemma we just showed. But then it would have been part of the phase, so that doesn't make any sense either. Thus, Q is greater than k and it also has to be odd, so the size of Q must be equal to k plus 2. k itself was, of course, odd as well. And that completes the lemma. Now that we know each phase must increase the length of the shortest augmenting path by at least two. We are now ready to bound the number of phases. Specifically, the number of phases used by Hopcroft- karp is Order square root of V. Note that the trivial bound saying that there can be only one phase per possible augmenting path link isn't good enough. That would still leave us with an order V bound. In reality, we will have a phase for length 1, for length 3, probably for length 5, maybe not for length 7, and so forth. But as we consider greater and greater possible phase lengths, the ones for which for which we will actually have augmenting phases gets sparser and sparser. The key argument will be that after roughly square root of V phases, the path length will be at least square root of V. And that then will imply that we only have square root of V phases left. To fill out the argument, we'll let M prime be the matching found by Hopcroft- karp. And we'll let M be the matching after the ceiling of the square root of the V phases. Because each phase increased the shortest augmenting path by 2, the length of the shortest augmenting path in M is at least twice the square root of V plus 1. And hence, no path in the symmetric difference between M and M prime that is augmenting for M can have shorter length. This implies that they're at most the size of V divided by this shortest path length, augmenting paths for M in this symmetric difference here. We just run out of vertices, there aren't enough left to fill out any more paths than this. And if there can be only so many augmenting paths in the difference, then M can't be too far away from M prime. Certainly, at most the square root of V. Hence, M will only be augmented square root of V more times, meaning that there can't be more than the square root of V more phases. We have square root of V phases to make all the augmenting paths long enough. So that there can be only square root of V possible more augmentations, and that completes the theorem. Overall then, we've just shown that Hopcroft- karp finds a maximum matching in time Order E times the square root of V. We only had square root of V phases. And each phase, because of the clever way we were able to reuse the level graph, only ends up using Order E time. And this result is actually the best known. That concludes our lessons max matching in bipartite graphs. If the topic of matchings is interesting to you. I suggest exploring matchings in general graph instead of just the bipartite ones we studied here. And also taking a look at minimum cost bipartite matchings in the Hungarian algorithm. Check out the references in the instructor notes. Our last lesson covered the basics of Turing machines. At the beginning of last lecture, I said that the Turing machine precisely captures an ocean of computation. In this lecture we will give strong evidence for this Church-Turing thesis, the statement that everything computable is computable by a Turing machine. The Church-Turing thesis is named for Alan Turing and Alonzo Church. Church had an alternative model of computation known as the Lambda calculus, which turns out to be exactly as powerful as a Turing machine. We call the Church-Turing Thesis, a thesis because it isn't a statement we could prove or disprove. In this lecture, we'll give a strong argument that our simple Turing machine can do anything today's computers can do. Or anything a computer could ever do in the future. To convince you of the Church-Turing Thesis, I'll start from the basic Turing machine and then branch out, showing that it is equivalent to machines as powerful as the most advanced machines today, or in any conceivable future. We'll begin by looking at multi-tape Turing machines, which in many cases are much easier to work with. And we'll show that anything a multi-tape Turing machine can do a regular Turing machine can do, too. Then we'll consider the random access model. A model capturing all the important capabilities of a modern computer. And we will show that it is equivalent to a multi-tape Turing machine, and therefore must also be equivalent to a regular Turing machine. This means a simple Turing machine can compute anything your Intel i7 can or whatever chip you may happen to have in your computer. Hopefully, by the end of the lesson you'll have understood all these connections and you'll be convinced that the Church-Turing Thesis really is true. Formally, we can state it like this, a language or a function for that matter is computable, and I've used quotes here to indicate that we mean this in the common meaning of the word, not in the technical sense. If and only if it can be decided, or in the case of a function, computed, by a Turing machine. Before going into how single tape machines can simulate multi-tape ones, I want to warm up with a very simple example to illustrate what we mean when I say that one machine can simulate another. So let's consider what I'll call stay-put machines. These have the capability of not moving their heads in a computation step. Which turing machines as we defined them are not allowed to do. So the transition function now includes a S for the direction, which makes the head stay put. Now this doesn't add any computational capability to the machine. Because I can accomplish the same things with a normal turing machine. For every transition where the head stays put, we can introduce a new state. And just have the head move one step to the right. And then one step back to the left. Gamma here means to match everything in the tape alphabet. And this puts the had back in the spot it started in, without affecting the tape's contents. Except for occasionally taking an extra movement step. This turning machine will operate in the same way as the stay put machine. More precisely, we say that two machines are equivalent if they accept the same inputs, reject the same inputs and loop on the same inputs. If we consider the tape to be part of the output equivalent machines also need to halt with the same tape contents. Note that other properties of the machine such as, the number of states, the tape alphabet, or the number of steps in any given computation do not need to be the same. Just the relationship between the input and the output needs to be the same. Since having multiple tapes makes programming with Turing machines more convenient, and since it provides a nice intermediate step for getting into more complicated models, we'll look at this Turing machine variant in some detail. As shown in the figure here, each tape has its own head. And what the Turing machine does at each step is determined solely by its current state. And the symbols under these heads. At each step, it can change the symbol under the heads, and move each head to the right or the left, or just keep it where it is. With the one tape machine, we always force the head to move. But if we required that condition for multitape machines, the differences in head positions would always be even, which leads to awkwardness in programming. So it's better to allow the heads to stay put. Except for those differences, multitape Turing machines are the same as single tape ones, we only need to redefine the transition function. For a machine with k tapes, the new transition function is from the Cartesian product of the possible states of the machine, and the possible symbols under the tape heads to the new states of the machine times the symbols to write under the heads times the possible directions to move the heads. Everything else stays the same. Let's see a multi tape turning machine in action. Input always comes in on the first tape and all the heads start at the left end of the tapes. Our task will be to duplicate the input separated by a hash mark. So for example, if the input string is 1, 0, 1, 1. We would want to turn that into that same input followed by a hash, followed by a copy of that same input, 1, 0, 1, 1. Of course we need to start in some fixed initial state. And the first thing we need to do, is to mark the beginning of this second tape since we're going to have to rewind it at some point. So, regardless of what symbol we read on the first tape, we're going to write a blank symbol to the first slot on the second. By the way, I'm not going to write out every transition as I draw the state diagram here. Instead I'll just include a description of the key transitions. Next, we walk forward on both tapes, copying the contents of the first tape to the second. Once we hit a blank symbol on the first tape, we stop. Then we write the separating hash to the first tape. And next we want to rewind the second one, keeping the head of the first tape over the hash the whole time. We stop rewinding once we hit that blank symbol we wrote to the second state in the first step. And now we're ready to start copying. And we keep doing so until this head here hits the blank symbol. Once that happens we're ready to halt. We've taken this string here, put a hash mark and then copied it past it. Next, we'll do a little exercise to practice using multitape Turing machines. Again, the point here is not so that you can put experienced programming multitape Turing machines on your resume. That is to get you familiar with the model, so that you can really convince yourself of the Church-Turing thesis, and understand how Turing machines can interpret their own description in a later lesson. With that in mind your task is to build a two tape Turning machine that decides the language consisting of strings of the form x hash y, where x and y are both binary strings and x is a substring of y. So for example, the string 101 is a substring of 01010. There it is as a substring right there. But on the other hand, 001 is not a substring of this one. Sure there's a zero here, a zero here, and a one here, but they aren't consecutive, so it doesn't count as a substring. Here's an outline of my answer and again I'll build it up piece by piece. The first part is similar to what we did for duplicating a string. We first write a blank to the second tape. Then we copy the first part of our input string to it. The x part. And then we rewind the second tape. We can illustrate this idea like this. We started by copying x from Tape 1 to Tape 2. And positioned the head on Tape 1 at the start of y, that is the text we want to search in. And then we rewound the head on Tape 2, so that it's positioned at the beginning of the key string, the thing we're going to look for within the text y. At this point we move to state q3. In here we walk forward on both tapes as long as we have a match. If we reach the blank cell at the end of x, then we've found a matching substring and we accept. If we reach a blank on the first tape, then we reject, since that means that x is longer than the remaining portion of y, the search. And if we find a mismatch between these two at any point, then we rewind one step on Tape 2, and then we rewind both tapes until we get back to the beginning of x. And then we can repeat this time will be checking x against a substring of y that's one further space down. Now I will argue that these enhanced Multitape Turing machines have the same computational power as regular Turing machines. Multitape machines certainly don't have less power. By ignoring all but the input tape, we obtain a regular Turing machine. Let's see why Multitape Turing machines don't have any more power than regular machines. On the left, we have a Multitape Turing machine in some configuration. And on the right, we have created a corresponding configuration for a single tape Turing machine. On the single tape, we have the contents of the multiple tapes with each tape's contents separated by a hash. So here's the first tape, here's the second tape, here's the third tape. Also note these dots here. We're using a trick that we haven't used before, expanding the size of the alphabet. For every symbol in the tape alphabet of the multitape machine, we have two symbols on the single tape machine. One with a dot, and one without. We use the mark symbols to indicate that the head of the multitape machine would be over this position on the tape. So the 1 is marked here, the 1 is marked here, and the 0 is marked here. Simulating a step of the multitape machine with the single tape version happens in two phases, one for reading and one for writing. First, the single tape machine simulates the simultaneous reading of the heads of the multitape machine. By scanning over the input and noting which symbols have marks on them. So here it would first note that one is under the head of the first tape, then that another one is under the head of the second tape. And then that a 0 is under the head of the third tape. That completes the first phase where we read the symbols. Now I need to update the tape contents and the head positions or markers as part of the writing phase. Let's say that the multitape machine changes this 0 to a 1 and moves the head to the left, like so. Then on the single tape machine, we need to change this 0 dot to a 1 and change this 0 to a 0 dot. Note that it's possible that one of these strings will need to increase its length when the multitape writes to a position it hasn't reached before. In that case, we just right shift the tape contents to allow room for the new symbol to be written. Once all that work is done, we return the head back to the beginning to prepare for the next path, like so. And notice that I updated the tape contents appropriately. So all the information about the configuration of a multitape machine can be captured in a single tape. And it's not too hard to convince yourself that all the logic needed to read from multiple places and remember all that contents. In order to figure out the right writing response, can also be implemented with a state transition logic. It's a good exercise to work that out if it's not obvious to you. Now for an exercise. No more programming Turing machines. Instead I want you to try to figure out how long the simulation process takes. We'll let m be a multitape Turing machine, and we'll let s be its single tape equivalent, as we've defined it. If on input x m holds after t steps, and we can assume t is greater than the size of the input, then the single tape machine halts after how many steps? Give the most appropriate bound. This question isn't easy, so spend a few minutes thinking about it and take a guess. The answer is on the order of t squared steps. The reason for this comes from two observations. The first, is that simulating a step of the multitape machine with a single tape machine, involves a number of steps proportional to the total number of symbols used. Recall that each step of the multitape machine was simulated with two passes over the tape. With k tapes, up to k minus one more passes are possible, for shifting cells to the right, but we're treating k as a constant here. The second observation, is that at step i of the multitape machine, the number of symbols used is at most the original size of the input, plus k times i. Since at most k symbols can be added during each step of the multitape machine M. Summing over the different steps of the multitape machine, we can put a t squared bound on the first term here. And then the second term just becomes the arithmetic sequence, which we can write like this. And that also is order t squared. So that's the answer. In addition to having multiple tapes, there are other curious variance of the basic turning machine. We can restrict them so that a symbol on any given square can only be changed once. We can let them have two way infinite tapes. Or even let them be non-deterministic. We'll examine this idea when we get to complexity. All these things are equivalent to turning machines in the sense that we've been talking about, and it's good to know that they're equivalent. Ultimately, however, I doubt that the equivalent of those models does much to convince anyone that Turing machines capture the common notion of computation. To make that argument, we'll show that a Turing machine is equivalent to the random access model, which very closely resembles the basic CPU register memory paradigm behind the design of modern computers. Here is a representation of the RAM Model. Instead of operating with a finite alphabet like a Turing machine, the RAM model operates with non-negative integers, which can be arbitrarily large. It has registers, useful for storing operands for basic operations, and an infinite storage device analogous to the tape of a regular Turing machine. I'll call this memory, for obvious reasons. There are two key differences between this memory and the tape of a regular Turing machine. One is that each position on this device stores a number. And the other is that any element can be read with a single instruction instead of having to move ahead over the tape to the right spot. In addition to this storage, the machine also contains the program itself, expressed as a sequence of instructions. And a special register called the program counter, which keeps track of which instruction should be executed next. Every instruction is one of a finite set that closely resembles the instructions of assembly code. For instance, we have the read instruction, read j, which reads the contents from the jth address on the memory. And places it in register zero. Register zero, by the way, has a special status. And is involved in almost every operation. We also have a write operation, which writes to the jth address in memory from R0. If we're moving data between registers, we have load, which writes to R0. And store, which writes from it, as well as add which increases R0 by the amount in Rj. All of these operations cause the program counter to be incremented by one after they're finished. To jump around in the list however, we have a series of special jump instructions that changed the program counter, sometimes depending on the value in R0. And finally, of course, we have the halt instruction to end the program. The final value in R0 determines whether we accept or reject. Note that in our definition here there's no multiplication, we can achieve that through repeated addition. We won't have much use for the notation surrounding the RAM model. But nevertheless it's good to write things down mathematically as this sometimes sharpens our understanding. In this spirit, we say that a random access Turing machine consists of a natural number K indicating the number of registers, and a sequence of instructions pi. The configuration of a random access machine is defined by the counter value which is 0 for the state. And indicates the next instruction to be executed otherwise. Also, we have the register values and the values in memory, which can be expressed as a function. Note that only a finite number of the addresses will contain a nonzero value, so this function always has a finite representation. We'll use one-based indexing. Hence the domain for the tape is the natural numbers starting from 1. Now we're ready to argue for the equivalence of our random access model and the traditional Turing machine. To translate between the symbol representation of the Turing machine and the numbers of RAM, we use a one-to-one correspondence, E. Note that E of the blank symbol is 0. That way, the uninitialized parts of the tape correspond to uninitialized parts of RAM. First, we argue that RAM can simulate a single tape Turing machine. The role of the tape played in the Turing machine will be played by the memory. We'll keep track of the head position in a fixed register say, R1 and the program and the program counter will implement the transition function. Here I've written out in pseudocode what this might look like for the simple Turing machine shown over here, which just replaces all the ones with zeroes and then halts. Being in state q0 corresponds to having the program counterpoint here. So the RAM will execute a sequence of tests for what the symbol under the head would be and it adjusts the values of the tape and memory accordingly. Sometimes, just moving to the right by changing the value in this register and then saying, hey, let's return to state q0. Or sometimes, a write is necessary, say, for this transition, here. And then again, we need to move the head to the right and then transition back to state q0. And sometimes we need to move to a new state, like when we encounter the blank symbol and then in q accept we halt. Now we argue the other way, that a traditional Turing machine can simulate a RAM. Actually, we'll create a multi-tape machine to implement a RAM since that's a little easier to conceptualize. Of course, anything that can be done on a multi-tape machine can be done with a single tape as we've already seen. We will have one tape per register and each tape will represent the numbers stored in the corresponding registers. We also have another tape that's useful for scratch work in some of the instructions, the one's that involve constants like add 55, then we have two tapes corresponding to the random access device. One is just for input and output and the other is for simulating the contents of the memory device during execution. Storing the contents of the random device is the more interesting part. This is done just by concatenating the index value pairs using some standard syntax, like parentheses and commas. The program of the RAM must be simulated by the state transitions of the Turing machine. This can be accomplished by having a subroutine or sub-Turing machine for each instruction in the program. The most interesting of these instructions are the ones involving memory. We simulate those by searching the tape that stores the contents of the RAM for one of these pairs that has the proper index and then reading or writing the value as appropriate. If no such pair is found, then the value in the memory device must be zero and we can write a new pair if it's a writing structure. After the work of the instruction is completed, the effect of incrementing the program counter is achieved by transitioning to the state corresponding to the start of the next instruction. That is unless the instruction was a jump in which case, that transition is affected. Once the halt instruction is executed, the contents of the tape simulated in random access device are copied out on to the I/O tape. Given this description about how a traditional Turing Machine can simulate a random access model, I want you to think about how long the simulation takes. We'll let R be a random access machine, and we'll let M be its multi-tape equivalent. n will be the length of the binary encoding of the input to R. And we'll let t be the number of steps taken by R. Then M takes how long to compute R? This is a tough question so I'll give you a hint. The length of the representation of a number increases by most a constant in M, as it simulates a step of R. The best answer is the second one here. The argument goes like this. As I stated in the hint, the longest representation of a number after i steps is order n plus i. Addition can lengthen the binary representation of a number by at most one bit. Next, observe that the total number of symbols used, let's say l, increases by at most order ri as the i step is simulated. We might get this increase by writing a number to the random access device to a location that we haven't written to before. Thus, after step j, the total number of symbols used, is going to be order n + j squared. Finally, each step only takes a constant number of passes over the symbols used on the tape, so we can bound the total number of steps with this sum here, to get the result. Once you know that a turing machine can simulate a ram then you know it can simulate a standard CPU. Once you can simulate a CPU you can simulate any interpreter or compiler and thus any programming language. So anything you can run on your desktop can be simulated by a turing machine. The turing machine can tackle any problem that the computer on your desk can solve. What about multicore, cloud computing, problematic, quantum and DNA computing? We won't do it here, but you can prove turing machines can simulate all those models as well. The church turing thesis has truly stood the test of time. Models of computation come and go but none have been, or ever will be, any match for the turing machine. Why should we care about the church turing thesis? Because there are problems that turing machines can't solve. We argued this with counter arguments in the first lecture and we'll give specific examples in future lectures. If these problems can't be solved by turing machines, they can't be solved by any other computing device. To help us describe specific problems that one can not compute, in the next lecture we discuss two of turines critical insights. That a computer program can be viewed as data as part of the input to another program and that one can have a universal turing machine. That one that can simulate the code of any other machine. One machine to rule them all. Every linear program, it turns out, has a dual program which mirrors the behavior of the original. In this lesson we'll examine this phenomenon. To give us a chance to apply some of the knowledge we gained about linear programs. As well as to deepen our understanding of some of the other problems that we've already studied. See if you can guess which problems as the lesson goes along. I want to start off our discussion, with a little exercise where we try to find an upper bound on the value of a linear program. We'll start with this one here, and we're going to take a linear combination of these inequality constraints, to obtain an upper bound on the objective function. Multiplying the first inequality by y1, and the second by y2. And adding them together gives us this inequality here. Where I have collected terms. Note that it's important that the y's be non-negative to avoid reversing the inequalities. Note that if we choose y1, and y2 such that 2 y1- y2 is at least 6. And also that, y1 + 2 y2, is at least negative 2, well then this becomes, an upper bound on the objective function, for our linear program. And for the exercise, I want you to choose y1 and y2, to make this bound as tight as possible. And the answer is that Y1 should be 3 and Y2 should be 0. >From looking at the bound we see that we want them both to be small. And looking at the constraints we see that the non negativity implies that this one will always be met. So we can more or less ignore that constraint. An increase in Y2 here actually enforces us to increase Y1 as well. So it should be clear that Y2 is zero, and from that it follows that Y1 is 3. Now you might have used some reasoning like that, or you could have recognized that what you were doing here was really solving another linear program. I asked you to minimize a linear function subject to a set of linear constraints and the non-negativity of the variables. This, then, is the essence of duality. In trying to obtain a tight bound on one linear program, we end up with another one. Associated with every linear program is a so-called dual program, which is also a linear program. This definition is most elegant when stated in terms of the symmetric form. Indeed, now you see why this form gets the name symmetric. As we saw in the exercise, the dual program can be thought of as the problem of minimizing an upper bound on the primal program. Observe that for all feasible y, we have that b transposed y is at least y transposed Ax. Here we've just used this constraint here from the primal and the non-negativity of y from the dual. And this in turn, is at most cTx, by the constraints of the dual here, and the non-negativity of x in the primal. In fact, we just proved the weak duality lemma, which states that if x is feasible for the primal, and y is feasible for the dual, then cTx is at most, bTy. That is to say, that the value of the primal is at most the value of the dual. And this makes sense because we built the dual as essentially an upper bound on the primal. Another thing to note here, is that if your primal problem isn't in this exact form, you can always convert it and then look at the corresponding dual and simplify it. Often, however, it's easier just to remember that the dual is the problem of bounding the primal as tightly as possible. For instance, if we change the primal inequality to an equality, then we can proceed by the same argument, only this first inequality becomes an equality. And I don't have to rely on the y being non-negative. Everything else stays the same. Here's the picture so far. We have a primal program over here trying to be maximized, and we have a dual program over here trying to be minimized, and the obvious question is do they ever meet? Well, the answer here is yes, they always do. More precisely, we state this as follows in the duality theorem. If either the primal problem or the dual has a finite optimum solution. Then so does the other. And, the optimum objective values are equal. And if either problem has an unbounded objective value, then the other is infeasible. Note that it is possible for both the primal and the dual to be infeasible. We'll start the proof by showing the second part. If either problem has an unbounded objective value, then the other has no feasible solution. So we'll suppose that the primal is unbounded and that y is feasible for the dual. Well, by weak duality, b transpose y must be at least c T times x for all feasible x. But since the primal is bounded, there has to be an x that's larger. And that creates a contradiction. The case where the dual instead of primal is unbounded is analogous. Now we turn to the first part of the proof. And we want to show that if either the primal or the dual has a finite optimal solution, then so does the other, and even more that the optimum values are equal. Let's start with the primal having a finite optimal solution. Well, that then implies that it has a finite optimal basic solution by the Fundamental Theorem of Linear Programming. Let's let the basis be the first m columns of the matrix a, and we'll divide up x and c in the usual way. Remember that B stands for basic here. Recall then from the simplex algorithm that this vector r D, which represented the effects of moving along one of the directions in x D had to be non-positive Otherwise this basic solution wasn't optimal. Now we're actually going to construct a solution for the dual. We'll let y transpose be CB transpose times B inverse. And then from the non-positivity of r, it follows that y transpose D has to be at least as great as CD transpose. And then we can show feasibility by arguing that y transpose A, and we can just rewrite that in terms of sub matrices B and D. Just by definition, the first part is just CB transpose, and the second part is this expression here, which again, from our The fact that our D is non positive is at most Cb times T. Well that's just our vector C and we can see that y does in fact meet the feasibility constraint. Moreover, Y transposed B, that is the objective function for the duel program. Is equal to CV transpose times B inverse, times B. But B inverse B, that's equal to XB, and C transpose B times XB was the value of the objective function for the primal. So these are equal, and then by weak duality it follows that our choice of Y is optimal. With this proof, we've actually shown something even stronger than the duality theorem which we set out to show. Because we actually gave a way to determine the dual optimal solution. We'll start with a linear program in standard form as usual. And we'll let the columns of the matrix B form an optimal basis, meaning that it generates an optimal basic solution. Then y transpose, defined to be CB transpose times B inverse is an optimal solution to the dual problem by our previous argument. Moreover, the optimum values of the problems are equal. Let's do an exercise on this idea of a dual optimal solution. Given that X, as shown here, is an optimal solution to this linear program, I want you to find the dual optimal solution. We'll let Y1 correspond to this first constraint, and Y2 correspond to the second. And the answer is that Y1 should be and Y2 should be 2. >From the solution for X, we can tell that the basis is the first two columns here. So we'll go ahead and call that B. And we want to perform row operations on this, so that we hit this part of the objective value, exactly, and achieve equality. We'll call this portion of the objective coefficients, CB. Setting up the linear equations, we have that Y transpose B should be equal to CB transpose. And by simple back substitution, we achieve the right answer. We can check the answer by making sure that it gives us the same objective value. So substituting in here, we have 2 plus 4 times 2, which is equal to 10. And for the dual, our solution is 10 as well. We have 1 times 6, which is 6 and 2 times 2, which is 4. And adding them together, we also get 10. We should also check the feasibility of our dual solution by checking that the bound on the coefficients for the objective function, behave themselves for these coefficients as well. And indeed, they do. We get 1 here, and -2 here, for a total of -1, which is indeed greater than negative 2. And regarding X4, we have 1 times 2, which is 1 minus 1, times 2, so that's 0. And 0 is indeed greater than the -1 here. So our choice for Y, was indeed feasible. By now, we have seen this picture several times, where there's one quantity that we're trying to maximize. And we have another quantity that serves as an upper bound for it, that we're trying to minimize. And luckily, the two meet at some point that is optimal for both. We've just seen this with our primal and dual linear programs. But we saw earlier in the semester, as well, with our max-flow min-cut problem, and also with our max-matching in bipartite graphs and our min vertex cover in bipartite graphs. It's natural to ask, are all these phenomena related? Well yes they are. And probably the easiest way to see that, is to realize that all of these can be characterized as liner programs and their duals. Lets take a look at duality in the context of maximum matching and by bipartite graphs first. Well let the variable xij indicate whether the edges (i,j) should be included in the matching. As a linear program, the problem becomes to maximize the number of matches edges, subject to the constraints that no vertex in R can be matched more than once. And no vertex in L can be matched more than once. And of course we can't have negatively matched edges. To build the dual program, we'll let yi and yj be the variables corresponding to these constraints. And we'll want to minimize their sum because the coefficients on the constraints here are all 1. For the constraints, observe that the coefficients in the objective function are 1. And that any Xij appears once in the equation for i and once in the equation for j. Hence, yi plus yj is at least 1. And of course, yi and yj can't be negative. The interpretation here is straightforward. Vertex i is in the cover, if and only if yi is 1. And similarly, vertex j is in the cover if and only if yj is 1. Every edge must have at least one vertex in a cover. And we're trying to minimize the size of the cover. So, we've just seen how maximum bipartite matching can be expressed as a linear program, and it's dual also turned out to have a natural interpretation as a vertex cover problem. This is really neat. Every decision problem in p can be converted into a linear program ultimately, just because linear programming is P-complete. But not every conversion will result in variables and a dual program that have such intuitive interpretations. When this happens, it often gives a way to gain deeper insight into the problem and its structure. As you might have guessed, this happens also for the max-flow min-cut problem, and we'll explore that next. For completeness we'll go ahead and explore duality in the context of a maximum flow problem as well. We can cast it as a linear programming by letting fuv be the flow, and letting cuv be the capacity along an edge u,v. Our goal is to maximize the flow out of the source subject to the conservation of flow constraint and the capacity constraints. And of course flows must be non negative as well. To express the dual, we'll use Yu for the conservation constraints and Yuv for the capacity constraints. Two subscripts mean a capacity constraint, one subscript means a conservation constraint. The dual problem is to minimize the sum over all the edges of Yuv times Cuv. Note that the Yus have no role in the objection function because their coefficients are 0. The constraints for the dual involve several cases. We'll consider first those arising from the objective function coefficients for edges out of the source being 1. These flows appear once in the capacity constraints, and once for the receiving vertex for the flow in their conservation equation. The case for edges going into the sync is analogous. The flow is present in the capacity constraints and in the conservation of flow constraint for the sending vertex. For all other edges the flow appears in the capacity constraint. And in the conservation equations for both the sending and the receiving vertex. And of course these dual variables have to be non-negative. The interpretation of these dual variables can be a little tricky, so I'm going to rearrange them to isolate the two indexed variables, the ones corresponding to edges and the ones that appear in the objective function. On the left hand side. This makes it a little easier to see what's going on. Actually, I think this would make a good exercise. Suppose that Y is a basic optimal feasible solution for the given linear program. Which of the statements here are part of an interpretation of Y as a minimum S-T cut A, B? And the answer is the first three. This last one doesn't make any sense, because Yuv must be non-negative. If u were in B, and Y were in A, then Yuv would be 0, which shouldn't count towards our objective function. One thing to notice immediately about this program is that because the capacities are all positive, the Yuvs will be as small as possible. Next, the parallelism between these equations suggest that we think of, ys, being a variable fixed as 1, and yt, be a variable here, fixed as 0. All of these edge variables then only need be greater than zero when the left hand variable over here is one and the right hand one is zero. This immediately suggests that variables with a single sub script indicate which side of the partition in the s-t cut the corresponding vertex is on. If they differ in the right direction. Then Yuv must be one, it's an edge cut by the cut and the capacity of that edge counts towards the objective function. The first three points follow from this realization. In this lesson, we define the dual of a linear program and showed how this dual program can be seen as the problem of making a certain kind of bound on the primal program as tight as possible. Then we saw how maximum flow and maximum bipartite matching can be expressed as linear programs, and how the minimum ST cut and vertex cover problems were their duals. If this idea of duality appeals to you, I suggest looking at the primal dual algorithm for solving the near programs. Although this is not a practical strategy for solving general linear programs, it has served as the inspiration for many algorithms for specific types of problems. In fact, the Ford-Fulkerson algorithm which we studied and the famous Hungarian algorithm for finding minimum cost matchings, can be seen as primal dual algorithms. Those are good places to start. More broadly, the type of duality that we've studied has profound implications for game theory, convex geometry, and convex optimization. Keep this lesson in mind when you encounter those topics. In this and in the remaining lessons for the course, we turn our attention to the study of polynomial algorithms. First however, a little perspective. In the beginning of the course, we defined the general notion of a language. Then we began to distinguish between decidable languages and undecidable ones. Remember that there were uncountably many undecidable ones, but only a countably many decidable ones. In comparison, the decidable ones should be infinitesimally small, but I'll give them this big circle here anyway, because they're so interesting to us. In the section of the course on complexity, we considered several subclasses of the decidable languages. P, which consists of those languages decidable in polynomial time. NP, which consists of those languages that can be verified in polynomial time. And within this we have this little bend of NP complete problems, which are the hardest problems in NP. If any one of these turned out to be polynomial, then P would expand and swallow all of NP. Or equivalently, we can think of NP collapsing down into this class P. In this section, we're going to focus on this class P, the set of polynomially decidable languages. The overall tone here will hopefully feel a little more optimistic. In the previous sections of the course, many of our results were of a negative nature. No, you can't decide that language, or no that problem isn't trackable unless P is equal to NP. Here, the results will be entirely positive. No longer we'll be the excluding problems from a good class. We'll just be showing the problems are in this good class, P. Yes, you can do that in polynomial time or it's even a low order polynomial, and we can solve huge instances in practice. Our first lesson is not on one algorithm in particular, but on a design technique called, dynamic programming. This belongs in the same mental category as divide and conquer. It's not an algorithm in itself, because it doesn't provide fixed steps to follow. Rather, it's a general strategy for thinking about problems, that often produces an efficient algorithm. The term dynamic programming, by the way, is not very descriptive. The word programming is used in an older sense of the word, that refers to any tabular method. And the word dynamic, apparently, was just used to make the technique sound exciting. With those apologies for the name, let's dive into the content. Here's the plan for this lesson. We'll start by outlining this general design technique of dynamic programming. Then we'll see three examples in the sequence alignment or edit distance problem. This is also a very minor generalization of the longest common subsequence problem. Also in the chain matrix multiplication problem. Where we're given a sequence of matrices to multiply and we want to place the parentheses in the right order to do it efficiently. And the all-pairs shortest paths problems for the graph. Good references for these topics are Dasgupta, Papadimitriou, and Vazirani, their book on Algorithms. And the classic Cormen, Leiserson, Rivest, Stein, Introduction to Algorithms. The absolutely indispensable element of dynamic programming is what we'll call an optimal similar substructure. By this I mean that we have some hard problems that we want to solve and we think to ourselves, oh, if I only had the answer to these two similar, smaller subproblems this would be easy. And because the subproblems are similar to the original, we can keep playing this game, letting the problems get smaller and smaller, until we've reached a trivial case. Well, at first, this feels like an ideal case for recursion. Since those subproblems are similar, perhaps we can use the same code and just change the parameters. Starting from this hard problem, we could recurse back to this one, and then to this one, and so forth using this whole subgraph here to solve this subproblem. But when we recurse through this branch we would come to this node again and do all this recomputation again, which would be terribly wasteful. This is sometimes called one of the perils of recursion. And it's often illustrated to beginning programmers with the example of computing the Fibonacci sequence. Each number in the Fibonacci sequence is the sum of the previous two numbers, and the first two numbers are one to get us started. This hard problem of computing the nth number in the sequence depends on solving the slightly easier problems of computing the n-1 and the n-2 elements in the sequence. Figuring out n-1 depends on n-2 as well as n-3. N-2 depends on knowing the answer to n-3 and n-4, and so forth. Thinking about how the recursion will operate here, notice that we need to compute Fn-2 once for Fn, and once also for Fn-1. So there's going to be some repeated computation here, and it's going to get worse and worse the further to the left we go. How bad does it get? Well, the top-level problem of computing the nth number will only be called once, and here we'll call the f-1 subproblem once as well. Computing S of n- 2 needs to happen once for each time that the 2 computations that depend on it are called. Once for n- 1 and once for n. Similarly, n -3 needs to be called once for every time that the subproblems that depend on it are called. Twice for Fn-2 and once for Sn-1 for a total of 3. Fn-4 gets computed once for each time that Fn0-3 is called and once for each time that Fn-2 is called for a total of 5. Notice that each number here is the sum of the two numbers to the right. So this actually is the Fibonacci sequence, and the number of the times that the n-kth number is computed will be equal to the kth Fibonacci number, which is roughly the golden ratio raised to the kth power. So yikes! This recursive strategy is exponential. Of course, computing the Fibonacci numbers doesn't need to be exponential. We could just start on the left here, initializing the first two numbers to 1 and 1. And then, compute the next one as the sum of the previous two with a simple for loop. There are two ways to cope with this problem of repeated computation. One, is to memoized the answer to the subproblems. After we solve it the first time we write ourselves a memo with the answer and before we actually do the work of solving a subproblem we always check our wall of memos to see if we have the answer already. So for instance, for this subproblem here I would have memoized it, written myself a memo to remember that I've already done it. And so, when I come this path here, I wouldn't redo all this computations. I'd just look up the answer. Alternatively, we can solve the subproblems in the right order, so that any time we want to solve one of these problems we're sure that we have the answer to its subproblems already. This can always be done because the dependency relationships among the subproblems must form a directed graph. If they're a cycle, then we would have a circular dependence and the recursion wouldn't work either. So we just find an appropriate ordering of the subproblems so that all the edges go left to right, and then we solve the subproblems in left to right order. This is the approach we'll take for this lesson. It tends to expose more of the underlying nature of the problem, and create faster implementations than using recursion and memoizing. The first problem we'll consider, is the problem of sequence alignment or at a distance. This often comes up in the study of genetics, or as part of spell checkers. For example suppose that we are given two genetic sequences and we want to line them up as best as possible, minimizing the needed insertions, deletions, and changes to the characters, that would be needed to convert one to the other. And we'll do this according to some cost function that represents how likely these changes are to have occurred in nature. Another way to think about this, is to say that I have aligned these elements of the sequence here. And left some of the elements unmatched. In general we say that we're given two sequences, X and Y, over some alphabet. And we want to find an alignment between them, that is, a subset of the pairs of elements of the sequence, so that each letter appears only once in this alignment and the pairs don't cross, like in this example here. The cost of an alignment comes from two sources. What is the number of unmatched characters, which we can write mathematically as n + m minus twice the size of the alignment. This corresponds to the number of insertions or deletions, or matching with these dashed characters in this figure. The other part of the cost comes form the matching of the two characters. We will notate that cost with this function alpha. Typically alpha is zero, when the two characters are the same. And the problem here of course, is to find the alignment that minimizes this cost. To make sure we understand this cost function, I want you to suppose that the function alpha is 0 if the two characters are the same. And that it's 1 otherwise. And to calculate the cost of this alignment here. The answer is 4. We have one, two, three insertions or deletions. And T is not the same as A, so we incur an extra cost of 1 from alpha here. And that is a total of 4. The key optimal substructure for the sequence alignment problem comes from aligning prefixes of the sequence that we want to align. We define c(i,j) to be the minimum cost of a aligning the first i characters of x with the first j characters of y. Since x has m characters total, and y has n characters, our overall goal then is to compute c(m,n) and the alignment that achieves it. Let's consider this problem of computing c(m,n), and in particular, we'll consider what we do with the last characters of each sequence. There are three cases to consider. First suppose that we match the last two characters of the sequence together like so. Then the cost would be the minimum cost of aligning the first m minus 1 character to the x with the first n minus 1 characters of y, plus the cost associated with matching these two characters. Another possibility is that we leave the last character of the x sequence unmatched. Then the cost would be the minimum cost of aligning the first m minus one characters of x, with the first n characters of y, plus one for leaving Xm unmatched. In the case where we leave Yn unmatched is analogous. Of course c(m,n) is defined to be the minimum cost of aligning the sequences, so it must be the minimum of these three possibilities. Notice, however, that there was nothing special about the fact that we were using m and n here. And the same argument applies to all combination of i and j. The problems are similar. So we can replace m and n with i and j like so. Adding the base cases where the cost of aligning a sequence with the empty sequence is just the length of the sequence. We have a recursive formulation. The optimal solution, expressed in terms of optimal solutions to similar subproblems. As you might imagine, a straightforward recursive implementation would involve an unfortunate amount of recomputation, so we'll look for a better solution. Notice that it's natural to organize our subproblems in a grid like this one. Knowing C(3,3), the cost of a line in the full sequence, depends on knowing C(3,2), C(2,2) and C(2,3). Knowing C(2,3) depends on knowing C(2,2), C(1,2) and C(1,3). And indeed, in general, to figure out any cost, we need to know the cost to the west, to the northwest, and to the north to fill out this grid. These dependencies form a directed acyclic graph, and we can linearize them in any number of ways. We might do it in scan line order like this. Or in left to right fashion like this. Or even maybe along the diagonals like this. We'll keep it simple and go in scan line order. First we need to fill out the base cases, this first row and this first column. And then it's just a matter of passing over the grid and taking the minimum of the three possibilities outlined earlier. Once we've finished, we can retrace our steps by looking at the west, north, and northwest neighbors to figure out what step we took. And this, then, will allow us to reconstruct the alignment. Let's practice computing these costs and extracting the alignment with an exercise. We'll align SNOWY with SUNNY, and we'll use an alpha value of 1.5 for flipping a character. This makes flipping a character more expensive than an insertion or deletion. Compute the minimum cost by filling out this table here, and write out the alignment here using a single dash to indicate where a character isn't matched. Here's the answer. The process of filling out the table was rather mechanical, so I won't walk through that step. We end up with a cost of 3.5, so that must have come from this neighbor, here, which also has a value of 3.5. Diagonal steps like this correspond to matching the two letters. So we write down that alignment here. Next we have 3.5, which could've come from either these two neighbors here. Let's choose this one for now, that corresponds to leaving w unmatched. So we'll write that down in our alignment. Next we have a cost of 2.5, which must have come from this north west neighbor here, who has a cost one and that corresponds to matching these two characters together o and n. Then we have a cost one which must have come from this north west neighbor so these ends get matched. Again we have a cost of one and since u doesn't match s this must have come from the west neighbor. Meaning that u is left unmatched and we can write that down like so. And, lastly, the s's must be matched together. Often there will be more than one path through the grid. In this example there was some ambiguity at this step here. Instead of skipping w, I could have matched it with n and then we could have left the o unmatched. And that would of left us with this alignment here which has the same minimum cost. Now we'll sum up the application of dynamic programming to the sequence alignment problem. Recall that we gave an algorithm, which given two sequences of length m and n, found a minimum cost alignment in O(mn) time. We did a constant amount of work for each element in the grid that we filled out. Dynamic programming always relies on the existence of some optimal similar substructure. In this case, it was the minimum cost of aligning prefixes of the sequences that we wanted to align. The key recurrence was that the cost of aligning the first i characters of one sequence with the first j of the other, was the minimum of the cost of matching the last characters that sequence together, of leaving the last character of the first sequence unmatched, and of leaving the last character of the second sequence unmatched. Now we turn to another problem that we'll be able to tackle with the dynamic programming paradigm, chain matrix multiplication. We are given a sequence of matrices of sizes m0 by m1, m1 by m2, and so forth. And we want to compute the product A1 through An efficiently. Note how the dimensions here are arranged so that such a product is always defined. The number of columns in one matrix always matches the number of rows in the next. First, recall that the product of a m by n matrix with an n by p matrix produces a matrix that is m by p, and that the cost of computing each entry is n. Each entry can be thought of as the dot product between the corresponding row of the first matrix, and the corresponding column of the second. Both these vectors have n entries. In total, we have about m times n times p additions, and as many multiplications. Next, recall that matrix multiplication is associative. Thus, as far as the answer goes, it doesn't matter whether I multiply A and B first and then multiply that product by C. Or if I multiplied B and C first and then multiplied that product by A. The product of all these three will be the same, but the number of operations may not be. Lets see this with an example. We will take matrices with these dimensions here and count the number of multiplications using both of these strategies. If we multiply A and B first, then we spend 50 times 20 times 50 computing AB. This matrix will have 50 rows and 50 columns, so its product with C will take 50 times 50 times 1, for a total of 52,500. On the other hand, if we multiply B and C first, this costs 20 times 50 times 1. This produces a 20 by 1 matrix. And multiplying it by a costs 50 times 20 times 1, for a total of only 2,000, a big improvement. So it's important to be clever about which order we choose for multiplying these matrices. In dynamic programming, we always look for optimal similar substructure, and try to find the recursive solution first. We can gain some insight into the substructure of the chain matrix multiplication problem by running out the expression trees for a various ways of placing parentheses. For example, ((AB)C) and then multiplying all that by D, corresponds to this tree here. (AB)(CD) corresponds to this tree here. And A((BC)D) yields this tree here. This suggests that we try all possible binary trees and pick the one that has the smallest computational cost. Starting from the top level, we would consider all n minus one ways of partitioning this into left and right subtrees. And for each one of these possible partitions, for example this one here, we would figure out the cost of computing the subtrees and then multiplying them together. To figure out the cost of the subtrees themselves, we would need to consider all possible partitions into left and right subtrees in here, and so forth, and so on. More precisely, we'll let c(i,j) be the minimum cost of multiplying Ai through Aj. For each way of partitioning this range, the cost is the minimum cost of computing each subtree, plus the cost of combining the two resulting matrices. And we just want to take the minimum over all such partitions. The base case, is the product of just one matrix, which doesn't cost anything. Now that we have this recursive formulation, we're ready to develop the algorithm. Let's convince ourselves first that the recursive formulation would indeed involve some needless recomputation. Take the two top level partitions, A B C times D E on the one hand and A B C D times E on the other. Clearly, we will have to compute the minimum cost of multiplying A B C in the left problem. But we are also going to have to compute it on the right as well since we need to pull D off from this chain at some point. And we end up doing all the work involved in figuring out how best to multiply A B C over again. Fortunately for us, there are only n choose 2 sub-problems. So we can create a table, and do the sub-problems in the right order. The entries along the diagonal are the base cases, which have cost 0. The product of 1 matrix doesn't cost anything, and our goal is to compute this value in the corner here, C 1 through n. The cost of multiplying matrices a1 through an. Consider which subproblems this depends on. When K equals one, we're considering these costs here, C11 and C2N. When K equals two, we're considering these problems here, C12 and C3N. When K equals three, we consider these and so forth. In general, every entry depends on all the elements to the left and downwards of it. There are many ways to linearize this diagonal of dependence, but the most elegant is to fill out the diagonals moving towards the corner like this. We'll let s indicate which diagonal we're working on. And then we will fill in the diagonal entry by entry from row 1 as far as it goes. For this row and diagonal, the column j will be equal to i + s. And we compute the entry in the table as the minimum of all the possible splits of the subtree. The last step, of course, is just to return this final cost. The binary tree that produced this cost can be reconstructed from the k that were chosen that yielded these minimum values. We just need to remember the split that we used for each entry in the table. Let's do an exercise to practice this chain matrix multiplication algorithm. Consider matrices A, B, C, D with dimensions given below. Use dynamic programming to fill in the table to the right, and then place the parenthesis in the proper place below to minimize the number of scalar multiplications. Don't use more parenthesis than necessary in your answer. Here's my answer. We'll go diagonal by diagonal. The first one is easy since it's just the cost of the pairwise product. Next we'll consider this entry here corresponding to the product of ABC. First, consider breaking it up as A(BC). That would be 0 for computing A, + 9 for computing C, + 6 x 3 x 3 for computing the product, for a total of 63. The other partition, (A B) x C, has cost 18 for (A B). 0 for C and 6 x1 x 3 for combining the products, for a total of 36. That ends up as the smaller of the two possibilities, so we'll go ahead and write that information in the table here. Now we move on to the other entry along this diagonal, the one for the product BCD. We'll consider first computing this as B(CD). That costs 0 for B, 24 for CD, and for computing the product, 3 x 1 x 8, for a total of 48. The other partition, BC x D, has cost 9 for BC, 0 for just grabbing D and 3 x 3 x 8 for the product, for a total 82. 48 is the minimum cost and the better partition is B x the quantity CD. So, we'll record that information in the table. Now, for the last entry. Consider first the partition A x the quantity BCD. The cost of A by itself is 0. The cost of BCD, was 48 and the cost of the product was 6 x 3 x 8 for a total of 192. The products AB x CD costs 18 for AB, 24 for CD, and 6 x 1 x 8 for combining these products for a total of 90. Lastly, we have this partition the quantity ABC x D. ABC cost 36, D by itself costs 0, and the product costs 6 x 3 x 8 for a total of 180. The middle one here is clearly the best, so we record that fact, and that gives us the partition, which we can write in these squares like so. Let's summarize what we've learned about the Chain Matrix Multiplication problem. There is an algorithm which, given the dimensions of n matrices, finds an expression tree that computes the product with the minimum number of multiplications in order n to he cubed time. Recall that there were order n entries in the table, and we spend order n time figuring out what value to write in each one. The optimal similar substructure that we exploited was the minimum cost of evaluating the subchains. The key recurrence said that the cost of each partition was the minimum cost of computing the left subtree plus the minimum cost of computing the right subtree, plus the cost of combining them. And of course, we want to take the minimum over all such partitions. Now we turn to our last example of dynamic programming, and consider the problem of finding shortest paths between all pairs of vertices in the graph. If you're resting on ideas like paths and graphs, or Dijkstra's algorithm, it might help to review these ideas before proceeding with this part of the lesson. Note that the idea of shortest, is given in terms of weights or costs on the edges. These might be negative, so we don't refer to them as lengths or distances themselves to avoid confusion. But we retain the word shortest with regards to path, instead of saying lightest or cheapest. Unfortunately, the use of this mixed metaphor is standard. We're given a graph G and a weight is associated with every edge in the graph. And our goal is to find a shortest path between every pair u and v in the graph. That is to say, a path that minimizes the sum of the weights involved in the path. Note that the idea of shortest here, is in terms of these weights. These weights might be negative, so we don't refer to them as lengths or distances to try to avoid confusion. But on the other hand, we still refer to the paths as the shortest, instead of saying lightest or cheapest. Unfortunately, the use of this mixed metaphor is standard. Recall that for the single source problem, where we want to figure out the shortest path from one vertex to all others, we can use Dijkstra's Algorithm. Which takes V times log V plus E time, when used with the Fibonacci cube. But this algorithm requires that the weights all be non-negative, an assumption that we don't want to make. For graphs with negative weights, the standard single-source solution is the Bellman-Ford algorithm, which takes order V times E time. Now we can run these algorithm multiple times. Once with each vertex as the source. If we visualize the problem of finding the shortest path between all pairs, as filling out a grid like this one. Then Dijkstra or Bellman-Ford would correspond to filling out this one row at a time. One call to the algorithm to fill out one row of the table. This strategy involves running each of these algorithms v times. So we could just add a factor of v to their running times, to get the all pairs shortest path version. Where the weights are non-negative, this strategy of using Dijkstra's algorithm is quite competitive. For the case where the weights could be negative, however, and especially when the graph is dense, we're looking at an algorithm that is on the order of v to the fourth for Bellman-Ford. Using dynamic programming, we're going to be able to do better. Since we're using dynamic programming the first thing we look for is some optimal similar substructure. Recall that the key realization in understanding the single source shortest path algorithms like Dijkstra and Bellman Ford was that subpath of shortest paths are our shortest path. So if this is a shortest path between U V and it happens to go between X and Y. Then the subpath must be a shortest path between X and Y. If there were a shorter one, then it could replace this subpath in the path from U to V, and that would then give us a shorter path from U to V. This type of argument is called cut and paste for obvious reasons. By the way, throughout, I'll use squiggly lines to indicate the path between two vertices, and straight ones to indicate a single edge. Unfortunately, by itself, this structure is not enough. Sure, we might be able to argue that a shortest path from U to V must take a shortest path from U to a neighbor of V first, but how do we find the shortest path? The sub problems end up having circular dependencies. One idea is to include the notion of path length, defined by the number of edges used. If we knew all shortest path that only used k minis one edges, then by examining the neighbors of V, we could figure out the shortest path with K edges to V. We'll let delta U V F K be the weight of the shortest path that uses at most K edges. Then the recurrence is that delta U V K is the minimum of delta U V K minus one. This is where we don't use that potential kth edge. Or it's the minimum of the distances from. U to one of the neighbors of V using only k minus one edges. Plus the weight of that last edge to get us to V. This strategy works, and it yields the matrix multiplication shortest path algorithm that runs in time V cubed log V. See Cormen, Leiserson, Rivest, Stein for details. We're going to take a different approach that will yield a slightly faster algorithm, and allow us to remove this log factor. We're going to recurse on the the set of potential intermediate vertices used in the shortest path. Without loss of generality, we'll assume that the vertices are one through N for the convenience of notation, consider the last step of the algorithm, where we've allowed vertices one through N to be intermediate vertices. And just now we're considering the effect of allowing N to be an intermediate vertex. Clearly our choices either using the old path, or taking the shortest path from U to N. And then from N to V. In fact, this is the choice not just for N, but for any K between one and N. To get from U to V using only intermediate vertices one through K, we can either use K by taking these shortest paths, or not use it. Therefore, we define delta U V F K, to be the minimum weight of a path from U to V. Using only one through K as the intermediate vertices. Then, the recurrence becomes delta U V F K. Is the minimum of delta U V K minus one that is not using K. That is an intermediate vertex. And using K that is taking the shortest allowed path to K from U and then going as quickly as possible from K to V. In the base case where no intermediate vertices are allowed the weights and the edges provide all the needed information. With this recurrence defined, we are now ready for the Floyed-Warshall Algorithm. Note that if we were to simply implement this with recursion, we would do a lot of recomputation. Delta u k, k minus one would be computed for every vertex v. And delta k, v, k minus 1 would be computed for every vertex u, in every iteration. As you might have guessed, we're going to fill out a table. The sub problems here have three indices, but thankfully, only two will be required for us in the algorithm. We create a two-dimensional table, d, indexed by the source and destination vertices of the path. We initialize it for k equals 0, where there are no vertices allowed, just based on the edges. Then we add potential intermediate vertices one by one, in this outer loop here. For each source destination pair we account for the possibility of using k as an intermediate vertex, with this update equation. Note that when i or j is equal to k this weight here won't change, since a vertex can't be useful in the shortest path to itself. Hence, we don't need to worry about using outdated value in this loop here. To extract not just the weights of the shortest paths, but also the paths themselves, we keep a predecessor table, which we'll call pi, that contains the second to last vertex on a path from u to v. Initially, when all paths are just single edges, this is just the other vertex on the incident edge. In the update phase, we either leave the predecessor alone, or if we are going to change the path, then we switch it to the predecessor of the second half of the path from k to j. To check our understanding of the Floyd-Warshall algorithm, let's do an iteration as an exercise. Suppose that the table d is in the state below after the k=3 iteration. Which is where vertex 3 was allowed to be an intermediate vertex along a path. I want you to fill in the table on the right with the values of d after the final iteration, where k is equal to 4. Here's my answer. The first thing I did, was to fill in the diagonals. These should all be 0 unless there's a negative cycle. Thankfully, there's only one negative path found so far, and I can easily check that it doesn't create a negative cycle. When we use 4 as an intermediate vertex, I could go from 3 to 4, that would cost -3. But going back from 4 to 3 would cost 6, and so then the overall cost would be 3 which is greater than 0. 0 is better. Next, I filled in the values where 4 was either a source or a destination, since vertex 4 can't help as an intermediate vertex in those situations. Now for the remaining entries. The choices for going from 1 to 3 are either to use the old path, which costs 4, or to go from 1 to 4. And then from 4 to 3, that costs 7. So, the old path is better. How about going 2 to 1? Well, there is no old path. But there is a path through 4. I could go from 2 to 4, that would cost 1. And then from 4 to 1, which would cost 2 more, for a total of 3. That's good. Going from 2 to 3 was impossible before, but using vertex 4, I can go 2 to 4 for a cost of 1. And then 4 to 2 for a cost of 6 more for a total of 7. 3 to 1 is another path that's impossible without going through 4. But if I do go through 4, then going 3 to 4 costs -3. And going 4 to 1 costs 2, for a total of -1. Lastly, we have that going from 3 to 2 used to cost 2, but we can improve on that, by going to 4 first, which costs -3. And then going form 4 to 2, which costs 1, for a total of -2. And that completes the answer. Now let's summarize what we've shown about the all-pairs shortest path problem. The Floyd-Warshall algorithm, which is based on dynamic programming, finds the shortest path for all pairs of vertices in a weighted graph in time cubic in the number of vertices. Recall that we have to fill out that whole table, which has size v squared v times. The key optimal similar substructure came from considering optimal paths by a restricted set of vertices, 1 through k. And that gave us the recurrence, where the shortest path between two vertices either used the new intermediate vertex or it didn't. The Floyd-Warshall Algorithm has neat connection to finding the transitive closure of mathematical relations that I want to cover briefly. Consider a relation R over set A. That is to say, R is a subset of A cross A. For example, A might represent sports that one can watch on television and the relation might be someone's viewing preferences. Maybe this individual prefers NBA to college basketball and NBA to college football, and we also know that he prefers college football to pro football. Since a relation is just a collection of ordered pairs, it makes sense to represent them as a directive graph. And given these preferences, we would like to be able to infer that this individual prefers the NBA over the NFL as well. In effect, if there's a path from one vertex into another, we'd like to add a direct edge between them. In set theory, this is called the transitive closure of a relation. Given what we know already, there's a fairly simple solution. Just give each edge weight 1, and then run Floyd-Warshall. The distance will be infinity if there's not a path, and it will be the minimum number of edges traversed otherwise. This is more information than we really need however. We really just want to know whether there is a path, not how long it is. Hence, in this context, we use a slight variant where the entries in the table are all booleans, either 0 or 1, instead of integers. But otherwise, the algorithm is going to be essentially the same. We initialize the table so that the entry [i][j] is 1, if (ai, aj) is in the relation, and 0 otherwise. Note that I'm letting a1 through an be the set A here. Then we keep on adding potential intermediate elements, and updating the table accordingly. Here [i][j], are in the relation, if they're either in the relation already or if k acts as a link between them. Often, we're interested not in the transitive closure of relation, but in the reflexive transitive closure. In this case, we just set the diagonal elements to be 1 from the beginning here. There are many other applications of dynamic programming that we haven't touched on, but the three we covered provide a good sample and help illustrate the general strategy. Perhaps the most important lesson, is that any time you've found a recursive solution to a problem, think carefully about what the subproblems are and whether there's an overlap that you can exploit. Note that this won't always work. Dynamic program will not yield a polynomial algorithm for every problem with a recursive formulation. Take satisfiability, for example. Pick a variable and set it to true, and eliminate the variable from the rest of the formula. We're left with another Boolean formula. And if it's satisfiable, then so is the original. The same is true if we set the variable to false. And the original formula will have a satisfying assignment if either of these other formulas do. This is a perfectly legitimate recursive formulation, yet there isn't enough overlap between the subproblems to create a polynomial algorithm, at least not unless p is equal to np. In the next lesson, we'll examine the fast Fourier transform. This is much more a divide and conquer problem, than it is a dynamic programming one. Though many of the same themes of identifying subproblems and reusing calculation will appear as we study it. Keep this in mind. In this lesson, we will examine a fast Fourier transform and apply it in order to obtain an efficient algorithm for convolving two sequences of numbers. If you have seen the Fourier transform before in the context of mathematics, physics or engineering, this lesson may have a different flavor from what you are used to. We won't be using it to solve differential equations or to characterize the behavior of an electrical circuit. Instead, we'll be focused on the much more mundane task of multiplying polynomials and doing it quickly. This algorithmic aspect of the Fourier transform is actually almost as old as the Fourier transform itself, appearing in an early 19th century paper by Gauss on interpolation. The transform itself, by the way, gets its paper from Jean Baptiste Fourier's 1807 paper on the propagation of heat through solids. Gauss' trick seems to have largely been forgotten, until Cooley and Tookey published a paper on the fast Fourier transform in 1965. Tookey was apparently somewhat reluctant to publish the paper, because he thought it was a simple observation and the how to questions of algorithms were still considered second class at the time. Well much has changed since then. Their paper's now one of the most cited scientific literature and the idea is considered one of the most elegant in algorithm design. Before beginning this lesson, it might be worth brushing up on a few concepts, so that you won't have to go back and review them later. We will be using complex numbers, and in particular their polar representation, just a familiarity with the basics is required here. We will also be using a little linear algebra, the ideas of matrix inverse and systems of equations being the most important. And since we will be using a divide and conquer strategy, it might be a good idea to review the master theorem briefly. The fast Fourier transform is an instance of the discrete Fourier transform, which as we've said has its own significance in various branches of mathematics, physics and engineering, signal processing most especially. In the study of algorithms, however, the fast Fourier transform is most interesting for its role and a very practical and very fast way of convolving two sequences of numbers. We'll illustrate convolution by an example. We're given two sequences of numbers shown here, we'll call them a and b. And we want to obtain a new sequence defined by this formula here, where the kth element in the sequence is the sum from 0 to k over i and ai times b sub k minus i. We can visualize convolution by reversing the order of b and lining it up with a, so that the the 0th element of b is under the kth element of a. This is the alignment for k equals 0, then we multiply all elements that overlap and add up these products. For k equals zero, this is 2 times 1, which is 2. So we'll write down that C0 is equal to 2. For k equals 1, we shift the sequence over, multiply the corresponding elements and add those up, that is a total of zero and that becomes C1. For K equals 2 we shift the B sequence over again, multiply the corresponding elements and add them up for a total of 5 and we continue sliding b along doing these products and then summing all things together and we keep doing this until there is no more overlap left. Convolution has many applications, but the one that will be most convenient for us to talk about is multiplying polynomials. Given two coefficients of polynomials, we can find the coefficients of the product just by convolving the two sequences of coefficients. In fact, we can easily repeat the example we just did, but in the context of polynomial multiplication. First, I lined things up, so that we get the constant terms and that product is two. Then we line them up, so that we get the linear terms, 0 times x. Then we line them up, so we get the quadratic terms. 5 time x squared and so forth and so on. How long does this process take? Well, for each element in the longer sequence, we had to do as many multiplications and additions as there are elements in the shorter sequence. Sometimes it will be a little shorter around the edges, because the shorter one ran off the edge of the longer one. But on average, it was at least about half. Therefore, we can say that convolving two sequences via the naive strategy outlined here, takes theta n times m operations where n and m are the lengths the sequences. The Fourier Transform will give us a way to improve on this. So far, we've assumed that polynomials are represented by their coefficients. For example, A is equal to -x + 1x,+ 1x squared, for example. If you've ever worked with polynomial interpolation or fitting before, however, you will know that an order n polynomial is uniquely characterized by its values at any n distinct points. The order of a polynomial by the way is the number of coefficients used to define it or the degree plus one. Hence we might just as well represent a polynomial by it's values at a sequence of inputs as by it's coefficients. For example, that same polynomial could be represented by saying that A (-1) = -2, A(0) = -2, and A(1) = 0. Going from the coefficient representation to the value representation can be though of as matrix multiplication. To calculate A of some value, I take the dot product of the corresponding row of the matrix consisting of the powers of the argument X with this column consisting of the coefficients of the polynomial a. This matrix is important enough that it gets its own name and it's called the Vandermonde matrix. Its determinant is the product of the differences of all the values of x. So as long as these are distinct the determinant will be non-zero and that means the matrix is invertible. So given the values then, we should be able to recover the coefficient. Now that we've seen an alternative way of representing polynomials, let's turn back to the problem of multiplying them. Multiplying via the convolution equation takes order nm time, as we've seen. If the polynomials are represented as values, however, we can just multiply the corresponding values to obtain the values of the product. Note that I did have to start with a number of points that was equal to the order of the product here. Not the order of the polynomials that we were multiplying. The fact that multiplying in the value representation is so much faster than in the coefficient representation suggests that it might make sense to convert to the value representation, do the multiplication there, and then interpolate back to the coefficient representation. We'll visualize the process like this. First we convert from the coefficient representation to the value representation. Then we multiply the corresponding values to get the values of the product polynomial, C. And then we interpolate back to get the coefficients of the product polynomial. Let's multiply two polynomials together with this process. Start with the coefficient here. Write the values of the polynomials here. Multiply them together and I'll do the interpolation, since that computation is a bit tedious. Here's the answer. We'll start with the polynomial a and for this I get the values 2, 5, 2, 5. You could have done the calculation however you liked. To emphasize that we can view this process as matrix multiplication, I've written out the Vandermonde Matrix here. For b, we just change the coefficients. And we get 3, 5, -1, -3 for the values of the polynomial. Multiplying in the value domain is simple. We just multiply the corresponding values at these inputs, and that gives me 6, 25, -2, and -15. This gives me a matrix equation here in terms of the desire to coefficients of the product polynomial C, and solving this gives me the answer 1 + 2x + x squared + 2x cubed. Which, you can easily verify is correct by convolving those two sets of coefficients in the normal way. As you might have intuited from the exercise, some more cleverness will be needed to make this process efficient. Even the evaluation of polynomials at arbitrary points will take quadratic time. For each point, we need to do a number of multiplications and additions that is proportional to the number of coefficients. The most efficient way to do this is via Horner's rule. You can find the link in the instructor's notes. You can also think about filling out the Vandermonde matrix and then doing the matrix multiplication. Regardless, for arbitrary points, we end up with the quadratic running time. Multiplying in the value domain takes order (n+m) time. Since we just multiply the values for the corresponding inputs, xj, this part is fine. Interpolation involves solving a system of equations with n + m equations and n + m unknowns. By Gaussian elimination, this would take order (n + m) cubed time in the worst case. For Vandermonde matrices, however, there's also a method called Lagrange interpolation that allows us to do this in time that's just quadratic. Note, however, that this still isn't good enough. In fact, we're at essentially the same running time as we have for convolution. Is there any hope? Well, yes there is. All of these running times pertain to an arbitrary set of values for X here. But since we're only interested in the coefficients of the product, polynomial C, we get to choose all these X. And as it turns out, this freedom is very, very powerful. At this point, we've seen how polynomials can be represented by their values, as a set of distinct inputs, and how multiplying polynomials is easy, when they're represented in this way. The problem is that we're really interested in the coefficients. Recall that the coefficients of the two polynomials can represent any two sequences that we want to convolve. To exploit the speed of multiplying in the value representation, therefore, we need an efficient way, first to evaluate a polynomial at some distinct set of input points, and an efficient way to interpolate back the result to the coefficient representation. We'll focus on optimizing for quick evaluation of the polynomials first. Our goal is to evaluate a polynomial A of order N at N distinct points. Note that I've made the order of the polynomial and the number of points the same here. We can always pad the coefficients with zeroes, effectively increasing the order, and we can always add more points. So there's no loss of generality here. As you did the calculation for the exercise, you may have taken advantage of the fact that the input values were arranged in positive and negative pairs. For higher order polynomials, this advantage becomes even greater. All the even terms are the same for x and negative x, and the odd terms are just negatives of each other. With this in mind let's define A sub e to be the polynomial whose coefficients are the even coefficients of the original polynomial A. And we'll define Ao to be the polynomial whose coefficients are the odd coefficients of the original polynomial A. Then we can write A as A sub e of x squared, plus x times A sub o of x squared. And we can get A of minus x just by changing this one little sign here. And essentially we've found two values for A for the price of one. More formally, let's say that we choose x sub i such that x of i is equal to negative x of I plus N over two. Then we can compute the values of the polynomial two at a time by computing Ae and Ao at Xi squared and then using them in these two equations. Overall we have changed the problem from evaluating a polynomial of order and at end points to evaluating two polynomials of order N over two at N over two points. Plus doing the work of N more additions and multiplications. This is good, but at its best we've only reduced the running time by a constant factor. We need to be able to apply this strategy recursively to change the asontonic running time. A set of points that would allow us to do this would be very special indeed. Th desire to apply the trick of using positive and negative pairs recursively, leads us to a very special set of points, called the Roots of Unity. Recall that our goal is to compute two values of a polynomial for the price of one, by dividing the terms up into even and odd powers. >From here on, we will assume that the number of points is equal to the order of the polynomial. And that this number is a power of 2. In the context of polynomial multiplication, N will be the power of 2, that is at least as great as the number of coefficients of the product of the two polynomials. And we will pad the coefficients of the polynomials being multiplied with the zeros, as needed to make their number equal to N as well. In order to be able to do this computation recursively, we need the sequence X to have the following proprieties. First, they should all be distinct, otherwise our efforts will be wasted, and we wont have enough points to interpolate back to find the coefficients. We also need the points to be symmetric or anti symmetric, depending on how you look at it. That is to say, we want the point with index J plus N over 2 to the negative of X of J, and lastly, we want all these properties to apply to the squares of these numbers, so that we can use this trick recursively again and again. If your polynomials are over some unusual field, then it may make sense to choose an unusual set of values for X. For most applications however, the coefficients will be integers or reals or complex numbers. And the choice of X will be the complex roots of unity. We define omega N to be e to the 2 pi i divided by N, and we let Xj be omega of N raised to the jth power. Let's visualize these points in the complex plane. I'll do this for N equal to 8. All the points have magnitude 1, so they'll be arranged around the unit circle, and the angle from the positive real axis, is given by the exponent here. Thus, omega to the j, will be j Nths around the circle. Let's confirm that all the desired properties hold. Indeed, the points are unique. j here is always less than N, so there's no wrap around the circle. The symmetric property holds, because adding N over 2 to j has the effect of increasing this angle by pi, or half the circle. And so that's equivalent to negating and flipping over both axes as well. The recursive property is the hardest to confirm. Notice however, that for all the points that have odd powers in the exponent here, squaring these numbers makes the exponent even. Thus omega 3 when squared, becomes omega to the 6th. Omega fifth when squared, will become omega to the 10th, which wraps around and then becomes omega to the 2nd. Moreover, each of the even powers here is the square of exactly two other points. Which points? Well, just divide the exponent by 2. That gives you one of them. For example, omega to the 4th, well his square root is omega to the 2. What's the other point? Well it's on the opposite side of the circle, of course. The additional N over 2 in the exponent, comes an additional N when we square it, so it's just an extra trip around the circle, and it's the same place. For, omega squared, half is omega to the 1st power, and then the opposite side is omega to the 5th. The result of all this, is that when we square these numbers, the odd powers of omega go away, like so. Once we're left with, only the even powers however, It doesn't make sense to express these points in terms of omega sub eight anymore. The factors of two in the numerator j and the denominator N cancel. So we can just divide both by two. And voila, we end up with the fourth roots of unity, instead of the eighth roots. And the same logic applies to all N, where N is a power of 2. And that then, shows that all these key properties that we wanted for X are true, when we choose them to be the Nth roots of unity, where N is a power of 2. It's worth noting here, how few of the properties of complex numbers were necessary for the recursion we used. In fact, there are a number of theoretic algorithms that use modular arithmetic, and avoid the precision difficulties associated with working with complex numbers. At the end of the lesson, you'll get a chance to implement one of those yourself. Let's illustrate the Fast Fourier Transform scheme by looking at the case where N = 4. That means omega = i, and we want to evaluate our polynomial at the points 1, i, -1, and -i. Recall that we use our even-odd decomposition. And that we recycle as much of the computation as possible. Here I've written out the four needed sub-problems. Thus, we've reduced the problem of computing A at the 4th root subunity to computing Ae and Ao at the second root subunity. Plus some effort to combine the results. To compute Ae and Ao, at the second roots of unity, we apply the same strategy again. First writing them in terms of the even and odd sub-polynomials and then recycling as much of the computation as possible. Each of the two previous problems has been reduced to evaluating an order one polynomial at one point. But this is trivial since it only involves the constant term. The upward pass of the recursion then fills in all these intermediate values, eventually giving us the values of A at the fourth roots of unity. To help solidify our understanding of how this process works, let's do an example. I want you to use the fast Fourier transform to evaluate the polynomial 1+0x +x squared-x cubed at the 4th roots of unity. Here's how I found the answers. To evaluate A at these fourth roots of unity, I first evaluated this polynomial Ae at the second roots of unity. Note that Ae here is 1 plus x. Thus Aee, the even of those terms, is just 1. And Aoe is the odd of those terms, the coefficient here is 1 as well. So I can just write 1 and 1 in those slots. Applying these formulas then tells me that Ae of 1 is 2 and Ae of negative 1 is 0. With this done, now I'm ready to find the values of Ao at its roots of unity. Ao consists of the odd coefficients here, so it's 0 minus x, and the even of these is the 0 and the odd of those is negative 1. So I go ahead and write those in those slots. Applying these formulas then tells me that Ao of one is negative 1 and Ao of negative 1 is 1. With all these values computed we can now apply the rules for our final layer to obtain the answer which I calculated to be this. Having seen an example for n equals four, now let's state the Fast Fourier transform precisely for the general case. As input, we have a sequence of numbers, a 0 through a n minus one, or N is a power of two, and we want to return the values of the corresponding polynomial at the nth roots of unity. We'll state this as a recursive algorithm, and the base case is where n is equal to 1. And in that case, we want to return the single element sequence, consisting just of a 0, the constant term. If n is greater than 1, then we call the Fast Fourier transformer cursively. Once with the even coefficients. And once with the odds. Then we combine the results, taking care of paired values together. Notice the difference in the sign on the contribution from the odd powers, which became S prime. How long does this take? Well, we traded one problem of size N for two problems of size N over 2, plus some order N work for all the arithmetic in this loop. By the master theorem, this gives us a running time of order and log in. This is much better then the N squared running time, from the naive evaluation by Horner's rule or by matrix multiplication. There's one other wrinkle that I want to add to the algorithm, and that is to say that this omega parameter here, can really be any primitive nth root of unity. All that matters is that all the powers be distinct, and that those be the roots of unity. So we can add this as a parameter to our algorithm as well. We now take in as well, that omega is a primitive nth root of unity, and we'll use that in our update rules in here. We'll see how this comes in handy later. Before moving on from this question of evaluation, I want to take another look at the connections between the various subproblems. This network is called a butterfly network because the connections over here on the left look a little like a butterfly. Also note that there is a unique shortest path between all nodes on the right and those on the left. Another thing to note is that this sequence of evens and odds on the subscripts of these polynomials can be translated to binary with e's being translated to 0s, and o's being translated to 1s. And these numbers, then, tell us which coefficient of the original polynomial to return here. And this makes sense, because when we chose the even coefficients, we were choosing those that had 0 in the lowest order bit. And when we chose the odd coefficients here, we were choosing those with 1 in the lowest order bit. It will also be instructive to write out the power applied to omega here on the right-hand side. It turns out that these numbers on the left act as instructions for any path to this node from the right. And these numbers on the right act as instructions for how to get there from any node on the left. Let's go right to left first. Reading this bit sequence on the right, we find that 0 tells us to go the even route first, that is, the higher of these paths everywhere. Then the 1 tells us to go to the lower route, towards the odds. And then the last 1, again, tells us to go the lower route towards the odds. And indeed, that provides instructions for going from all of these nodes over here to this one over here. The reverse is analogous. We think about the least significant bit first, so to get to 6, going from here to here, we go to the omega that has the lower of the possible powers. And then in the next step, we go towards the one that has the higher of the possible powers of omega in all these cases. And the same thing in the last step. Again, going towards the higher power of omega, which is the downward of these paths. This network is well worth some study. It serves as an important example in graph theory, and the topology here has been used on some massively parallel computers for their memory architecture. Recall that our original goal was not just to evaluate a polynomial at the roots of unity but rather to multiply two polynomials together or even more generally to convolve two sequences of numbers together. The faster way to transform would seem only to get us a little past halfway. So let's go back and look at our road map to see where we are in multiplying two polynomials together. Let's take a step back and see where we are in trying to find a faster way to multiply polynomials. We have an N logN way to evaluate the polynomials, we can then multiply them in the value representation easily in N time, but the interpolation step remains a problem. Remember that this runtime involved inverting or applying Gaussian elimination to a Vandermonde matrix. Or to get the square here, we could have used Lagrange interpolation. Let's see what the Vandermonde matrix looks like at the complex roots of unity. Each row corresponds to the powers of one of the roots. So for 1, the powers are all 1. The powers of Omega are 1, then Omega, then Omega square, all the way up to Omega to the N- 1. The next value is Omega to the second, so we go omega squared to the 0th power is 1, to the first power, what that ends up being is omega squared, and then we have omega to the 4th, omega to the 6th, etc. In general, the kj-th element is omega with the k -1 x j- 1 power. This matrix has some very special properties. For our purposes, however, the key one can be summarized with the following claim. We'll let omega be a primitive Nth root of unity. This means that all of its powers, 0 through N-1 are unique. Well, then, the Vandermonde matrix, parameterized by omega as defined here, times the one parameterized by omega inverse is = N x the identity matrix. For the proof, consider the kj-th element of the product. For this we're multiplying the K throw of M of N of omega with the J column of M or N of omega inverse. So we're taking corresponding powers of omega to the K- 1, and omega to the -J- 1. Multiplying those together, and then adding all of those up. Gathering terms from the exponent, this becomes the sum over L, of omega to the K- J, x L power. Now if K is equal to J, then every turn here is 1. And overall sum is N. Otherwise, we recognize this as a geometric series, and rewrite it as this ratio here. Note however, that omega, to the J- Kth power is a root of unity. And then raising that to the Nth power, must necessarily be 1. So, this numerator expression is 0. The whole thing is 0. And that then proves our claim. This is terribly important. Recall that evaluating a polynomial at the roots of unity, correspond to multiplying the coefficients by this matrix here, and of omega. And we use the fast foria transform to do that. Now we see that we can multiply these values by essentially the inverse of this matrix. Also using the fast forier transform and this will allow us to recover the coefficient's given values. This is why it was so key that the fas foria transform work with any route of unity. This realization about the Vandermonde matrix leads us to the inverse fast Fourier transform. We're given the values of a polynomial at the roots of unity, and we want to recover the coefficients of that polynomial. The algorithm is fantastically simple, given what we've established so far. Just run the regular fast Fourier transform only passing in the values, and the inverse of the root of unity that we used the first time. Then divide the result by N. Recall that the values that we receive as input here were equal to the Vandermonde matrix times the coefficient. By multiplying the vectors of these values by the Vandermonde matrix parameterized by omega inverse, we end up with n times the identity matrix times those coefficients, and so we just need to divide this result by N in order to get back the original coefficients exactly. To solidify our understanding of this inverse Fast Fourier Transform, let's do an exercise. Suppose that A is at most a cubic, and the values at the fourth roots of unity are as shown here. And I want you to find the coefficients of A. The trickiest part of this problem is setting it up. We need to figure out the correct placement of the values on the left-hand side on the butterfly network. Even though as we go forward here, omega is going to be negative i instead of i because we're doing the inverse transform. We need to order the values according to the omega that was used in the forward transform. So thus, omega to the 1 is i, and omega to the third power is negative i here. Carrying the calculation forward a level we obtain these results here. Then for this last level we have 4 plus 8 which is 12, good. Then for this next one we want omega to the first power times this and because we're doing the inverse transform that omega is negative i. And that gives us this result here of 4. For the next one, we multiply the odd term by negative 1. So that gives us negative 4 here. And for the last one we multiply by our omega cubed which is i. We multiply that by the odd term, and combining that with this gives us a total of 8. To get the original polynomial, we divide by n, 4 in this case. Thus we have that the original polynomial was 3 plus x minus x squared plus two X cubed. And you can check that that all works out. Note that there is a sense in which the ordering over here was arbitrary. We could have done the forward transform with omega equal to -i, in which case I would've flipped these and then the inverse transform would've used i here and -i here. And that would have given us the same result. Recall that we started the lesson with this idea for multiplying polynomials. Convert to the value representation, multiply the polynomials there, just by multiplying the corresponding values, and then convert back. Rounding up to the nearest power of 2, we might have written these running times like so, in terms of our parameter N. With the Fast Fourier Transform, we were able to do the evaluation, not in quadratic time, but on linearithmic time and log N. And even more remarkably, we were able to apply this same trick to do the interpolation as well. We just had to pass in the inverse of the root of unity that we had used in the evaluation. The conclusion is that order N polynomials can be multiplied in order N log N time. Remember that polynomial multiplication was just a convenient way to think about the more general problem of convolution. Therefore, in general, convolving an N long sequence with an m long sequence takes time on the order of n+m times the log of n+m operations. This is a truly remarkable and powerful result. That concludes our discussion of the Fast Fourier Transform. >From a practical perspective, probably the most important thing to remember from the lesson is that convolution can be done in n log n time, not n squared as one might naively think. Don't be needlessly intimidated by the need to perform convolution in an application. And be aware that image and signal processing libraries use this technique. >From the perspective of algorithm design, the Fast Fourier transform falls in the category of divide and conquer algorithms, along with merge sort and Strassen's algorithm for matrix multiplication, for those who are familiar with those algorithms. It has some resemblance to dynamic programming, too, however. Instead of just having one problem, we have multiple problems as we want to evaluate a polynomial at multiple values. And the associated subproblems overlap in a way captured by that butterfly network. The butterfly network, by the way, is a fascinating structure in its own way and it's sometimes used in massively parallel computers. It also serves as an important example in graph theory. Another thing to appreciate from the lesson is the strange twists and turns that our development of the algorithm took. We started by thinking about the general problem of convolution, but the algorithm came much more specifically from thinking about the special case of polynomial multiplication. We started by only considering sequences of integers yet, complex numbers became an essential part of the algorithmic solution. Sometimes the ideas you need come from unexpected places. So soak up as much mathematics as you can. You never know when it might come in handy. Hello, and welcome to Computability, Complexity, and Algorithms, the introductory theoretical computer science course for the Georgia Tech Masters and PhD programs. I'm Lance Fortnow, professor in the School of Computer Science at the Georgia Institute of Technology. And I'm Charles Brubaker, a PhD graduate from Georgia Tech, and Udacity course developer. In this course, we'll ask the big questions. What is a computer? What are the limits of computation? Are there problems that no computer will ever solve? Are there problems that can't be solved quickly? What kinds of problems can we solve efficiently? And how do we go about developing these algorithms? Understanding the power and limitations of algorithms helps us develop the tools to make real world computers smarter, faster, and safer. We step away from the particulars of programming languages and computer architectures and instead take an abstract view of computing. This viewpoint allows us to understand where the computation problem is inherently easy or hard to solve. Independent of the specific limitation of a machine we plan to use. These results have stood the test of time, being as valuable to us today as they were when they were developed in the 70s. While the topic of the course is theory, understanding the material can have very practical benefits. You will learn a wealth of tools that will help you recognize when problems you encounter in the real world are intractable and when there is an efficient solution. This can save you countless hours that would otherwise have been spent on a fruitless endeavor or in reinventing the wheel. Certainly by having followed the rigorous arguments made in this course, you'll have given yourself a kind of training. The athlete doesn't just practice on the field or the court where the competition is held, he goes to the gym. And when he returns, he finds he is better prepared for the game. In a similar way, by taking this course, you'll improve your thinking, as you make engineering, or even business decisions. You will find that you have better reasons to prefer one strategy over another, and will be able to articulate more rigorous arguments in support of your ideas. Shall we get started? Let's hit that gym! I want to begin our discussion of computability by taking you back to early high school or late middle school, when you first learned about the notion of a function. Here we have the teacher, and here are some very earnest students sitting in the front row. The teacher begins by defining the notion of a relation, saying that it is a set of ordered pairs, where the first element comes from a set called the domain, and the second comes from a set called the range. And a function then is a special kind of relation where each element of the domain is paired with exactly one element of the range. And of course this rather abstract definition is well beyond the maturity of everyone in the class. Then, the teacher writes some examples of functions on the board. Like f(x)=3x+2, or g(x)= -x2+1. And then the students get a flash of insight and say to themselves, oh I get it. A function is what you do to x in order to get y. I mean f(x). In effect, the student understands the function as an algorithm, a finite number of steps to follow that lets you find the output given an input. Notice, however, that the teacher's definition was very different, being given in terms of the rather abstract notion of sets. Now for the rest of this algebra class, and maybe for as much math as this student studies, the difference between the student's computational understanding and the teacher's definition, won't be important. The student will know how to solve all the problems and probably won't give the teacher's definition much more thought. In reality however there's a profound difference between these two notions. And indeed this difference between computation on the one hand and functions on the other, will be the subject of the first part of this course. Before we can make precise statements about what is and is not computable, we need to define what we mean by computation. We'll start with the notion of some kind of machine or computer. And this machine takes in an input and after some finite amount of time produces an output, or so we hope. In this lesson, we'll focus on the input and output parts of this picture. And postpone the mathematical definition of the machine itself. The input is formed from an alphabet, which is a finite set of symbols or characters. The finite part is important. So if I wanted to use binary, my alphabet would consist of the two symbols, 0 and 1. For genetic sequences it's typical to use A, C, G, T which stand for the four possible nucleotides in a DNA sequence. In practical applications, we often use ASCII or Unicode for our alphabet. Now creating a new symbol for every possible input and output would not be very convenient. So, instead, we use finite sequences of symbols which we call strings. For example, over the alphabet sigma we might have the string 1010, which might represent the number 10. Or over our alphabet gamma, we might have the string x = AACAG, which might represent part of a DNA sequence. We notate the empty sequence, which doesn't have any symbols in it. With the special character, Epsilon. Sometimes we will talk about machines having string outputs, just like the inputs. But more often than not the output will just be binary, an up or down decision about some property of the input. So we might think about the machine as just turning on one of two lights. Either one for accept or one for reject. Once the machine has finished computing. With these rules, an important type becomes a collection of strings. Maybe it's the set of strings that some particular machine accepts. Or maybe we're trying to design a machine so that it accepts strings in a certain set and no others. Or maybe we're asking if it's even possible to design a machine that accepts everything in some particular set of strings and no others. In all these cases, it's a set of strings that we're talking about. So, it makes sense to give this type its own name. We call a set of strings a Language. For example, a Language could be a list of names, Charles {'Charles', 'Lance'}. It could be the set of binary strings that represent even numbers. Notice that this set is infinite, or it could be the empty set. Any set of strings over an alphabet is a language. In addition to defining the notion of a language itself, we want to find some operations on languages, and establish some notation. We'll illustrate using these two languages over the zero-one alphabet. Since languages are sets, we have the usual operations of union, intersection, and complement. For example, A union B consists of these three strings, 0 from both A and B, 10 from A, and 11 from B. The intersection contains only those strings in both languages, just the string 0 here. To define the complement of a language, we need to make clear what it is that we're completing. It's not sufficient just to say that it's everything not in A. For this particular A, that would include strings with characters besides 0 and 1, or maybe even infinite sequences of 0s and 1s, which we don't want. The complement, therefore, Is designed so as to complete the set of all strings over the relevant alphabet in this case the binary alphabet. Usually the alphabet over which the language is defined will be clear from context. In this case, the complement of A will be infinite, including the empty string, not 0 however. It will include 1. It will include 00. It will include 01. It will not include 10 however. It will include 11. It will include 000, and so on and so forth for all longer strings. In addition to the standard set operations, we also define an operation for concatenating two languages. The concatenation of A and B is just all strings that you can form by taking a string from A, let's say x, and appending to it a string y from B. In our examples, the set AB would be 00, the first 0 coming from A and the second from B. 011, with the 0 coming from A and the 11 coming from B and so forth. Of course, we can also concatenate a language with itself as I've done here in this example. Instead of writing AA, we often write A squared. And in general, when we want to concatenate the language with itself k times, we write A to the k power. Note that for k equals 0, this is defined as the language containing exactly the empty string. When we want to concatenate any number of strings from the language together to form a new language, we use an operator known as Kleene star. This can be thought of as the union of all possible powers of the language. When we want to exclude the empty string, we use the plus operator instead, which insists that at least one string from A be used. Notice the difference in the starting indices. So, for example, this string here is in A. There's a way that I can break it up so that each part is in the language A. Note that A* doesn't include infinite sequences of symbols. Each individual string from a must be a finite length, and you're only allowed to concatenate a finite number of those together. For those who have studied regular expressions, this should seem quite familiar. In fact, one gets the notation for regular expressions by treating individual symbols as languages. For example, 0* is the set of all strings consisting entirely of 0s. We'll also commonly refer to sigma*, meaning all possible strings over the alphabet sigma. Let's do a quick exercise to check our understanding of these operations on languages. We'll let sigma consist of the symbols zero and one. And we'll let gamma consist of the symbols a and b. For each statement, indicate whether the given statement is true or false. The answer is that the first one is false and the rest are true. The first one is false because a a b a is not from sigma star, it's from gamma star. In the order here of say, gamma star and sigma star matters. If we were to reverse these, then it would be in the language. b is in sigma star b. Because the empty string is in sigma star. And we can concatenate that with b, to get b. Sigma union gamma is the set of strings 0, 1, a, and b. So applying star to that gives us all strings over all those symbols including this one here. And finally, because the empty string is not in gamma plus, it is in the complement of gamma plus. So this statement is true as well. Besides introducing the terminology of alphabets, strings, and languages, the major goal of this lesson is to convince you that some functions are not computable. Central to our argument will be the difference between countably infinite sets and uncountably infinite sets. If you weren't familiar with this distinction already, you may be thinking to yourself, infinity's a strange enough idea by itself. Now I have to deal with two of them? Well, it isn't as bad as all that. A countably infinite set is one that you can enumerate. That is, you can say, this is the first element, this is the second element, and so forth. And, this is the really important part, eventually give a number to every element in the set. For some sets, an enumeration is straight forward. Take the even numbers. We can say that two's the first one, four is the second, six the third and so forth. For some sets like the rationals, you've to be a little bit clever to find an enumeration. We'll see the trick for enumerating them in a little bit. And for some sets, like the real numbers, it doesn't matter how clever you are, there simply is no enumeration. These are the uncountable sets. In what follows, we'll argue that the set of all strings over an alphabet is countable and give a few other results that will be useful for thinking about countability in general. Then we will show that the set of languages is uncountable. Let's make this notion of countable and uncountable sets precise with a formal definition. We say that a a set is countable if it is finite or if there is a one-to-one correspondence with the natural numbers. A one-to-one correspondence, by the way, is a function that is one to one, meaning that no two elements of the domain get mapped to the same element of the range. And also onto, meaning that every element of the range is mapped to by an element of the domain. For example, here is a one-to-one correspondence between the integers, 1 through 6, and the set of permutations of three elements. Now, in general, the existence of a one-to-one correspondence implies that the two sets have the same size. That is, the same number of elements. And this actually holds even for infinite sets. That is why we say that there are as many even natural numbers as there are natural numbers. Because it's easy to establish a one-to-one correspondence between the two sets. F of N is equal to 2 N, for example. For our purposes, we are most interested in showing that for any finite alphabet, the set of strings over that alphabet is countable. The proof is relatively straightforward. Recall that sigma star is the union of the strings of size 0 with those of size 1, with those of size 2, etc. And our overall strategy will be just to assign the first numbers to sigma 0. And the next to sigma 1, and the next to sigma 2, etc. More formally, we can can define N sub K to be the total number of strings of size at most K, and then use this to create the correspondence. Thus, Nk will be the last number assigned to a string of length k. So in this way, 1 will get assigned to the empty string, the only element of sigma 0. And then the next number N0+1 through N1 that'll get assigned to all the strings of length 1. And in general, Nk-1+1 through Nk, those set of numbers will get assigned to sigma k, the strings of length k. Within each one of these arrows, you can do the correspondence in any way. Lexographical order is as good as any. For example, we can enumerate all the strings over the binary alphabet like this. First we assign one to the empty string, then we enumerate all strings of length one, then those of length two, ans so forth. This is the main theorem that we will need here concerning countability. But while we're on the subject, we'll say a few words about countability in general as a kind of bonus. The same argument here, shows that a countable union of finite sets is countable. Suppose that our collection of sets is S0, S1, etc. And without loss of generality, we're suppose that they are disjoint. If they happen not to be disjoint, we can make them so by just subtracting out from Sk. All the elements it shares with the sets, s0 through Sk-1 then the argument proceeds just as before. We assign the first numbers to S0, the next to S1, etc. Now it turns out that we can actually prove something even stronger than the original statement here. We can replace the word finite with the word countable, and say that a countable union of countable sets is countable. Notice that our current proof doesn't work. If we tried to count all of the elements of S0 before any elements of S1. We might never get to the elements of S1 because S0 could be infinite, nevertheless this theorem is true. For convenience of notation, we'll let the elements of SK be XK0, XK1, etc. And then we can make each SK a row in a grid as shown here. Again, we can't go row by row here because we might not ever finish the first row if S0 is infinite. On the other hand, we can go diagonal by diagonal since each diagonal is finite. The union of all the Sk is the union of all the rows, but it's also the same thing as the union of all the diagonals. Each diagonal being finite, we can apply the original version of this theorem to prove that a countable union of countable sets is countable. You can also just translate the indices here on X into an enumeration as well. Note that this same idea proves that the rationals are countable. Imagine putting all the fractions with a one in the denominator in the first row. All those with a two in the denominator in the second row, all those with three in the denominator in the third row, and so on and so forth. So far we've seen that the set of strings over any finite alphabet is countable. How about the set of all languages? Here is an attempted proof that shows that the set of languages over an alphabet is countable. Indicate the steps that you think are incorrect. The answer is that this third step here is problematic. And it's problematic because there are languages where the maximum string length is unbounded. Remember that languages can have an infinite number of strings. So there are languages where, no matter what length you pick, there is a string in the language that is longer. This union excludes any such language, because each Si is restricted to having strings of length at most i. In fact, none of the languages in this union even contain an infinite number of strings. One particular language that is not in this union is sigma star, the language containing every string. Okay, so that proof didn't work. Is there one that does? Well no. As it turns out, the set of languages is uncountable. We'll prove this by contradiction. So, suppose not. That is to say, we'll let L1, L2 be the set of languages over an alphabet sigma. Here, I'm giving them an enumeration. And we'll also let X1, X2 etc be the strings in sigma star, we know that has an enumeration. We are then going to build a table where the columns correspond to the strings in sigma star, and the rows correspond to the languages. In each element in the table will put a one. If the string is in the language and we'll put a 0 if it is not. Now we're going to consider a very sneaky langauge defined as follows. It consists of those strings Xi for which Xi is not in the language Li. In effect, we've taken this diagonal here in the table and just reversed it. Since we're assuming that the set of languages is countable, this language must be Lk for some k. But is Xk in Lk or not? You see from the table here we've defined the row Lk such that in every column the entry is just the opposite of what is on the diagonal. But this diagonal entry can't be the opposite of itself. If Xk is Lk, then according to the rules for Lk, it should not be an Lk. On the other hand, if Xk is not an Lk, then it should be an Lk. Another way to think about this argument, is to say that this language here must be different from every row on the table because it is different on the diagonal element. In any case we have a contradiction and can conclude that this enumeration of the languages was invalid since the rest of our reasoning was sound. This argument here is known as the diagonalization trick and we'll see it come up again later, when we discuss undecidability. Although they may not be immediately apparent, the consequences of the uncountability of languages are rather profound. We'll let sigma be the set of ASCII characters. These are all the ones you would need to write a computer program. And observe that the set of strings that represent valid python programs is a subset of sigma star. Each program is a finite string after all. And note here that I chose python arbitrarily. You could choose any language or set of languages if you like. Well, since this set is a subset of the countable set sigma star, we have that it is countable. Thus, there are a countable number of python programs. On the other hand, consider this fact, for any language L, we can define f sub L to be the function that is 1 if x is in L, and 0 otherwise. All such functions are distinct. So the set of these functions must be uncountable. Or in other words, the set of functions from sigma star to (0,1) is uncountable. Here's the profound point. Since the set of valid python programs is countable, but the set of functions is not, it follows that there must be some functions that we just can't write computer programs for. In fact, there are uncountably many of them. So going back to our picture of the high school classroom, we can see that the teacher, perhaps without realizing it, was talking about something much more general than what the student ended up thinking. There are only countably many computer programs that can follow a finite number of steps as the student was thinking. But there are uncountably many functions that fit the teacher's definition. In this lesson, we've seen that there are some problems that we can not compute in a traditional programming language like Python. But somehow this feels a bit unsatisfying, since all we showed was there are more problems to solve than there are computer programs to solve them. But we haven't actually given you any specific problem that can't be solved on a computer. The set of problems with finite descriptions accountable, so at this point we haven't even ruled out that every problem we can describe we can solve in a computer. Turns out this isn't the case, there's some very natural problems that cannot be solved by computers, now or ever. To get to that point we need more tools. We need to find some model of computation simple enough to study yet powerful enough to capture everything we mean by computing. In the next lecture we do exactly that and introduce you to the wonderful Turing machine. The subject for this lesson is linear programming. We've seen some very general tools and abstractions in this course. It would be hard to argue that any other combines the virtues of simplicity, generality and practicality as well as linear programming. It's simple enough to be captured with a few matrix expression and inequalities. General enough so that it can be used to solve any problem in p and polynomial time, and practical enough that it helped revolutionize business and industry in the middle of the 20th century. In many ways, this is algorithms at its best. This lesson begins by reviewing the two dimensional linear programming problems that high school students often solve in their algebra two classes. Then it extends the equations to n dimensions and captures the essential intuition that optimal solutions are at the corners of the feasible region. And we state this rigorously, with the fundamental theorem of linear programming. And then finally, the lesson will cover the simplex algorithm, a very practical way for solving these optimizations. There are many good references on linear programming. This treatment will follow most closely, David Luenberger's Linear and Nonlinear Programming. Before we begin, however, I should mention that parts of this lecture will use notation and some ideas from linear algebra. For notation, we will use capital letters for matrices, thus A might be this matrix here. For column vectors, we'll use lower case letters, so x might be this vector here. Also, we'll often use the lowercase version of a matrix to indicate a column. Thus, a two here refers to the second column of the matrix A. We indicate the transpose of the matrix by giving it a T in the superscript, and this just reverses the meaning of the rows and columns. This notation, then allows us to use C transpose X for the dot product. This is a row vector, times a column vector, which ends up just as a single entry matrix, which, by convention, we'll interpret as a scalar. As far as concepts go, the ideas of how to represent a system of equations as matrices of linear independence of vectors, matrix rank and matrix inverse should all be familiar. If these ideas aren't familiar then it would be a good idea to refresh your understanding before watching the rest of this lesson. As always, it is recommended that you watch with pencil and paper handy, so that you can pause the video and work out details on your own, as needed. Let's do a quick exercise on this high school linear programming. Here's the problem. A movie actor is developing a workout plan. He will burn 12 calories for every minute of step aerobics he does, and 4 calories for every minute of stretching he does. The workout must include 5 minutes of stretching for warmup and cannot last more than 40 minutes total. The actor wants to burn as many calories as possible. We'll let x be the minutes spent on step-aerobics. And we'll let y be the minutes spent on stretching. And I want you to express this actor's problem as a linear program and give the optimum values for x and y here. Here's the answer I came up with. The actor wants to maximize the number of burned calories and he burns 12 calories for every minute of step-aerobics and four for every minute of stretching. He can't spend more than 40 minutes total. So I expressed that constraint by saying that x + y is at most 40. And expressing that he has to spend five minutes stretching, is a little unintuitive because of the way the inequality is directed here. But I can do it by saying that -y is at most -5, like so. Using the graph and check method of your high school teacher, gives the optimum solution as 35 minutes of step aerobics and 5 minutes of stretching. Now you probably didn't bother graphing the polytope as I've done here. The answer was too obvious. But why was it so obvious? Maybe you realized that more exercise was better, and started thinking about constraints somewhere along here, that used up all 40 minutes. Because step-aerobics burn more calories, you realize that you would want to do as many of them as possible, pulling you along this constraint, downward towards this vertex. Or maybe you realized that just five minutes of stretching, and zero minutes of step aerobics would satisfy the constraints. And then you decided to increase the number of minutes spent on step aerobics, until you ran into another constraint. Another case, your intuition led you to think very much along the lines of the simplex method, that we'll see in a little bit. Linear programming is largely just the generalization of this sort of problem solving to n dimensions, instead of just the two that we've seen so far. We can write our optimization problem in a form like this one. In matrix form this becomes maximize c transpose x such that A x is at most b and x is nonnegative. Note that inequality, when applied to matrices, means that it holds for each element in the matrix. When we first encounter a linear programing optimization problem it might not be in this form. In fact the only requirements for an optimization being a linear program is that both the objective function and the constraints be linear, whether they are inequalities or equalities. If this is true, then we can always turn it into a canonical form like this. Here are the key transformations, to convert a max problem to a min problem, or vice versa, just negate the vector c, which contains the coefficients for the objective function. Thus, max c transpose x becomes minimizing -c transpose x, and minimizing c transpose x becomes maximizing -c transpose x. To convert an upper bound to a lower bound, or vice versa, in one of the constraints, we simply negate the inequality. We can convert an equality into two inequalities by giving both an upper and lower bound constraint. Here I have played the trick from transformation to make these both at most constraints. Things get a little more interesting when we go from one of the inequalities to an equality. Here, we introduce a new variable, either a slack or a surplus variable, depending on the inequality. There's also the problem of free variables that are allowed to be negative. There are two ways to cope with one of these. If it is involved in an equality constraint, then we can often simply eliminate it through substitution. Otherwise you can replace it with two new non-negative variables, and just substitute the difference for x everywhere. Let's do a quick exercise to practice these transformations. Use the transformations discussed to transform the linear program on the left to the form on the right. And note here that x1 and x2 should be the same, but x3 might be a different variable in these two cases. Here's my solution. I started with the original problem and decided first to eliminate this pesky free variable, x3, which wasn't constrained to be nonnegative. >From this equality constraint, I know that x3 = x- 1 and substituting that into the objective function then gives me that I need to minimize 4x1- 2x2. There's some other constant out here, but that's not going to effect my choice for the optimum value for x. So, I can simply drop that. We end up with 3x1 in the inequality constraint. And the -2 from this side, increases the bound on the side to 7. The equality constraint goes away altogether, with the elimination of the variable. Now I need to turn this inequality into an equality, so I introduce a new slack variable, x3. And then lastly, I just need to negate the coefficients in the objective function to convert the minimization problem to a maximization problem. By these transformations, it's possible to write any linear program in a variety of forms. Two forms, however, tend to be the most convenient and widely used. First is what we'll call the symmetric form. We'll see why it gets that name when we consider duality and second is the standard form. The key difference between the two is that we have changed the inequality for an equality. To better understand the relationship between these two forms, I'm going to write the standard form in terms of the symmetric form. To convert the m inequalities to equalities, we introduce n slack variables. Xn plus 1 through xn + m. And of course, this means that we need to augment our matrix a as well, so that the slack variables can do their job. And C also needs to be augmented, so that we can multiply it with the new x without changing the value of the objective function. It's interesting to think about the differences between these two forms geometrically, as well. In the symmetric, we're optimizing over a polytope in end dimensions. In the standard form, however, these equality constraints here define something called the flat, which is rather like a subspace, only it's away from the origin and then we're intersecting that with the cone defined by the positive coordinate axes. And of course, we're in a higher dimension. Note that we expect an optimum for the symmetric problem to be at one of the vertices of the polytope. That is to say, of these m plus n constraints here. N must hold with equality or be tight in the common products. Some might come from A, some might come from the non-negativity constraints, but there will always be n of them. Over here in standard form, the notion of whether the constraints of A are tight or not is captured by the slack variables that we introduced. A slack variable is zero if and only if, the corresponding constraint is tight. Thus, at least n of the variables will be zero when we're at a vertex of the original polytope. In fact, if I tell you which n variables are zero and the ones that are nonzero correspond to a linearly independent set of constraints, then you can construct the rest of the variable based on the equality constraint. We'll see this when we talk about basic solutions. Now so far, I've kept on using the number of variable n and the number of constraints m from the symmetric form, even as we talk about the standard form. In general, however, when discussing the standard form, we redefine n to be the old n plus m. So don't get confused about that. One other thing to note about this equality form is that we can assume that A is full rank or has rank m where m is the number of rows. That is the say, the rows should be linearly independent. If the rows aren't linearly independent, then there are two possibilities. One is that the constraints are inconsistent, meaning that there is no solution. The other possibility is that the constraints are redundant, meaning that some of them can be eliminated. So from now on, we'll just assume that A has full rank. >From this point onward, we will consider linear programs in their standard form, where the constraints are expressed as e qualities. We will have an underdetermined system of equations, that has lots of solutions, and we'll try to find the one that maximizes the objective function. Now, if you've ever had to come up with solution to an underdetermined system on your own, you'll have noticed that it's easiest to find one by simply setting some of the coefficients to 0, so as to create a square system, and then solving for the rest of the coefficients. In effect, this is what solvers like MATLAB do, as well. These solutions are called basic solutions. And it turns out that to solve linear programs, basic solutions are the only ones that we will need to consider. The trick however, is figuring out which coefficients need to be set to 0. With that intuition in mind, let's dive into the details. Switching over now entirely to considering linear programs in standard form. I want to define some vocabulary that will be useful going forward. First, we say that a vector x is a solution if and only if Ax = b. A basic solution is one generated as follows. We'll pick an increasing sequence of m column numbers so that the corresponding columns are linearly independent. And we'll call the resulting matrix B. This is easiest to see when the chosen columns are the first m. And we'll use this convention for most of our treatment. We define XB to be B inverse times this B here, and then embed this in a longer vector x. Putting the value from XB for columns in the sequence and 0 otherwise. This x is a basic solution. Really, all that we're trying to accomplish here is to let XB get multiplied with the columns of B and 0 to get multiplied with the columns of D. Remember that post multiplication corresponds to column operations. So that then is a basic solution and we call it basic because it came from our choice of this linearly independent set of columns which forms a basis for the column space. >From this basis we get a basic solution. It is possible for more than one basis to yield the same basic solution if some of the entries of this XB are 0. Such a solution is called degenerate. This corresponds to the case where our vertex is at the intersection of more than n hyperplanes in the symmetric form. So far this vocabulary only addresses the equality constraints. Adding in the non-negativity constraints in the variables, we use the word feasible, thus a feasibly solution is a solution that has all nonnegative entries. And a basic feasible solution is one that comes from a basis as described above and has all nonnegative entries. Now for an exercise on basic solutions. Given the equations below, find a basic solution for X. Remember that since there are only two rows in your matrix, your solution must not have more than two non-zero entries. Here's the solution I came up with. I chose columns 1 and 3 for my bases. Note that I couldn't have chosen columns 1 and 2 because these are not linearly independent. If that were the matrix B, B wouldn't be invertible. Choosing 1 and 3 however, gives me this matrix here as B. And I need to solve this equation for the variables X1 and X3. Simple substitution suffices here. X1 must be equal to three and X3 must be equal to 2, as a consequence. And I just put in those answers here. So far, we've reminded ourselves of the basics of linear programming by examining it in just two dimensions. Then we built up some vocabulary and notation that allows us to extend these notions to n dimensions. Now we're ready for the culmination of all this work in the fundamental theorem of linear programming. Which captures the idea that optimal solutions should be at the corners, also called extreme points of the feasible region. And that tells us that we need only consider basic feasible solutions. Precisely, the theorem states that given a linear program in this standard form, where A is an m x n matrix of rank m. Then if there's a feasible solution, there is a basic feasible solution. And if there is an optimal feasible solution, there is an optimal basic feasible solution. We'll start by proving the first point. Let x be a feasible solution, and we'll consider only the positive entries. Without loss of generality, let's assume that those are the first p. Then it must be the case that this linear combination of the columns a1 through ap is equal to b. That is after all, part of what it means to be feasible. There are two cases. First, suppose that the columns a1 through ap are linearly independent. Then it's not possible that p should be greater than m, recall that these vectors are only m elements long. If p is equal to m, well then x is basic already and we're done. P could be less than m, that would mean that x is degenerate. But then we can just add columns to form a linearly independent basis and we would be all right. So that then covers the case where these columns are linearly independent. Next, suppose that those columns are linearly dependent. Well, that means they are coefficients y1 through yp. Such that this linear combination of the columns is equal to 0 with at least one of the coefficients, yi positive. Then we're going to choose this parameter epsilon in a very clever way. So that it's the minimum of these ratios, xi/yi among all these i's such that yi is positive. Then multiplying this equation by this epsilon, and subtracting it from this one, we end up with this equation here. These coefficients must represent another feasible solution. But by our choice of epsilon, we've set at least one of these coefficients to zero. All of the others have remained positive. So we've reduced the number of positive entries in x. And we can repeat this over and over until case 1 is reached, where a1 through ap are linearly independent. Now onto part two of the theorem, this will feel similar to the first part. We want to show that if there is an optimal feasible solution, there is an optimal basic feasible solution. We let x be an optimal feasible solution with non-zero entries x1 through xp. Well it's feasible, so this linear combination of the columns of a then must be equal to b. And because it's optimal, it's objective value, c transpose x must be the maximum over all possible feasible x. Again, we first consider the case where the columns a1 through ap are linearly independent, and the situation is the same as before. P being greater than m is impossible, equal means that it's a basic solution, and less just means that it's a degenerate solution. This case is simple. If these columns are linearly dependent, then there exists a set of coefficients y1 through yP. Such that the linear combination of the columns is equal to 0 with at least one of the y's being positive. Note however, for epsilon sufficiently close to zero in both the positive and the negative direction. X minus epsilon y is still feasible. Multiply epsilon times this equation, and add it to this equation, and we end up still with a set of coefficients equal to b. Thus, c transpose y has to be equal to 0. If it weren't, then c transpose x would be strictly less than c transpose times x minus epsilon y. We can choose the right sign on epsilon, so that this inequality holds. But this is impossible because x is optimal. We conclude that c transpose y is equal to 0 and then we can choose epsilon to be the same thing we chose it before. It'll still give us an optimal feasible solution, only this time it will have smaller support. Because we're able to choose it to set one of the coefficients of this vector here to zero, that wasn't zero before. And we can repeat playing this game over and over until we've reached the case where a1 through ap are linearly independent. We've just seen how the fundamental theorem of linear programming tells us that we can always achieve an optimal value for the program with a basic solution. Moreover, basic solutions come from the choice of m linearly independent columns for the bases. Remember this key point going forward. We've just seen how the fundamental theorem of linear programming tells us that we can always achieve an optimum value for the program with a basic solution. Moreover, basic solutions come from a choice of m linearly independent columns for the basis. This immediately suggests an algorithm where we just try all possible bases and take the best generated basic feasible solution, as outlined here. And my question for you is, why is this algorithm problematic or impractical? Check all that apply. The answer is that that algorithm is simply too slow. It would have to consider all n choose m possible bases, which, for problems of any reasonable size, is impractical. All of the other suggested problems really aren't problems. Every feasible solution is considered because we consider every possible basis. It's fine to consider just basic solutions, as argued in the fundamental theorem of linear programming. Indeed, the matrix B that we generate here might be singular, but we can just ignore that combination as the algorithm does here. And indeed, x might be degenerate, but this is okay, it would just show up in more than one iteration. Now, we'll talk about a much more efficient approach called the simplex algorithm. This actually will not be polynomial time in the worst case. But as a practical matter, this algorithm is efficient enough to be widely used. The simplex algorithm allows us to move from one basic feasible solution to a better one. And by moving to better and better basic feasible solutions, we eventually reach the optimum. For convenience, let's suppose that our current basis consists of the first m columns of A. We'll call this sub-matrix B and the remaining sub-matrix D. It will also be convenient to partition X and C in an analogous fashion. Over all then, we can write our standard form linear program like so. CB transposed times XB + CD transposed times XD subject to the constraints that B times XB + D times XD must be equal to B. And, of course, both XB and XD must be non-negative. For the simplex algorithm we want to consider the effects of swapping out one of the current basis columns for another one. To do this we first want to identify a good candidate for moving into the basis. One that will improve the objective function. As the program stands however, it's not immediately clear which if any are good candidates. Sure for some XI in XD here, the corresponding coefficient might be positive. But raising the value of that variable might force others to change because of the constraints. Making the whole picture rather opaque. Therefore, it would be convenient to parametrize our ability to move around in the flat defined by these constraints, solely in terms of the variables in XD, the variables that we're thinking about moving into the basis. By the way, I'll use the idea of moving variables and moving columns into the basis interchangeably throughout. To this end, we solve the equality constraint for XB, so that we can substitute for it where desired. First, we substitute it into the objective function, and through a little manipulation, we get this expression. This constant term here doesn't matter since we're only considering the effects of changing XD. This quantity that gets multiplied with XD however, is important enough that it deserves its own name. We'll call it rD. In our reframing of the problem therefore, we want to maximize rD transpose times XD. How about the other constraints? Well, the first one goes away with the substitution. This requirement however, that XB be non-negative, remains. Substituting our equation for XB, we get the constraint shown here. Of course, XD must remain non-negative, as well. Note that XD equals 0. The current situation for the algorithm is feasible, and it's easy to see a way to improve the objective value just by looking at the vector rD. Our real goal, however, is not just to climb up hill, but to figure out which column should enter the basis. Actually then, we'll make this a quiz. What makes for good candidate to enter the basis? Check the best answer. The answer is that any column corresponding to a positive entry of rD is a good candidate. We want the entry to be positive because the corresponding value of xD is also going to be positive as we increase it. Just picking the greatest entry, rD, doesn't work because this still might be negative. This idea then becomes the basis for the simplex algorithm. Pick a q such that rDq is positive and we'll let xD just be some scalar times the unit vector along the qth coordinate axis. This choice simplifies the optimization even further, since D times xD is now just gamma times the Dth column of the matrix D. We'll define this B inverse time this column as u, and we'll define B inverse times b as v. Of course, the greater the gamma the more I get to increase the objective value, so i want to make it as big as possible. But how big can I make it? Let's make this another quiz. What should the value of gamma be? Choose the answer. The answer is this first expression here, unless ui is positive, we can make gamma as big as we want without running into the constraint here. And of these constraints, we hit the one where this ratio is lowest first. Setting gamma to this value makes one of these constraints tight and sends the corresponding entry of xB to zero. Remember that this equation came from the constraint that xB be non-negative. We can then bring dq into our basis, and kick out the column corresponding to the constraint that became tight. We then want to repeat this process over and over, getting better and better basic feasible solutions. In more detail we can express the simplex algorithm as follows. First, calculate Rd. If Rd is non-positive, well then we've done as well as we can, and we can just return our current solution, B inverse times b in the basic variables, and zero elsewhere. Otherwise, pick q such that the qth element of rd is positive, and then let dq be the qth column of D. This is the column that we want to bring in to the bases. Next, calculate this column U as B inverse times the qth column of the Matrix D. If U doesn't have any positive entries, we can report that the problem is unbounded. We can just increase the value of x that corresponded to this positive rdq. As much as we want. If that's not the case, then we find the i which minimizes this ratio of Vi by Yi, where the ui is positive. This tells us which constraint we're going to run into first. And we let bi be the ith column of B. We then swap out bi from the basis, and swap dq in, and we keep on repeating this procedure. Let's see the simplex algorithm in action on an example from the first part of the lesson. I've changed the variable names here a little bit. So we'll let x1 correspond to the number of minutes spent doing step aerobics, and we'll let x2 correspond to the number of minutes spent stretching. I've also introduced a slack variable, x3, on this constraint for the length of the workout. And a surplus variable, x4, for how many minutes of extra stretching that our actor decides to do. And together, these allow me to trade the inequality constraints we had before for equality constraints. Let's suppose that our current solution is down here in this corner, meaning that x1 is 0, x2 is 5. And our slack variable here will be 35, and our surplus variable here will be 0. The basic variables here are clearly x2 and x3, and the sub matrix B of the constraint matrix A consists of these two columns here. Recall that we want to solve for these basic variables and essentially eliminate them. And here, we see that we have x2 = x4 + 5, from this equation here. And x3 then must be 35 minus x1 minus x4, and of course, these need to remain non-negative. Substituting these into the objective function, I now get this in terms of the non-basic variables, the ones that I'm thinking about moving into the basis, x1 and x4. And actually, both of these have positive entries in the rd matrix. rd here just has 12 and 4 as its coefficients. Well, arbitrarily, we'll go ahead and choose x4 as the variable to bring into the basis. So essentially, x1 gets automatically set to 0, and these here then become the constraints. The negative sign here means that I could send x4 as big as I wanted to, and that wouldn't affect this constraint at all. Here however, the coefficient on x4 is positive, so we do need to worry about this constraint, and that limits how much bigger I can make x4. Only up to 35. The fact that this constraint goes tight implies that x3 became 0. So x3 should go out of the basis and x4 should come in. Graphically, this corresponds to moving from this point down here to this one up here, with these being the new values for my x variables. And then I would repeat this process for the new basis, and eventually come down this edge here to the optimal solution. Now we will argue that the simplex algorithm is correct, giving us a basic feasible solution for bounded linear programs, and reporting that unbounded ones are unbounded. Let's consider the bounded case first. First we recognize that at each step we make some progress, usually improving the objective value. We have to be careful here in the case of degenerate basic solutions. Going back to the algorithm, remember that we picked a new column to go into the basis because it had a positive corresponding value in R D, and hence increasing it, increased the objective value. The trouble is that if the current basic solution is degenerate, that is to say that v has a zero entry, it's possible that we won't get to move in this direction at all. The nightmare scenario is that we end up in some kind of cycle. There are two ways of coping with this challenge. One is to perturb the constraints slightly. The other is to give preference to lower indexed columns for both entering and leaving the basis. This is known as blands rule. In either case we can be assured of making some progress in each step, but I'll put an asterisk here because the notion is a little tricky. Clearly, we have a finite number of steps, because there are only n choose m possible bases. And because we make progress, we don't cycle among them. Now we just need to make sure we don't stop too early. There are two possible ways the algorithm can terminate, either because u is nonpositive or rD is. Let's consider the termination because of u first. This turns out to be pretty trivial. If u is non-positive, then we get to keep going in the direction of dq here, as much as we want. Increase of the objective value the whole way. So clearly the problem is unbounded. And of course that can't happen, because we've assumed that the linear program is bounded. Let's consider what happens when we terminated on the RD condition then. We'll let x star be and optimal feasible solution that's not necessarily basic. And we'll let x be a basic sub optimal solution. While using the basis for this basic sub optimal solution, I can write xb star in these terms here. This just comes from the equality constraint. And the feasibility of x star. Substituting this expression back into the objective function gives me this result here. This expression here is the same rD that we obtained in the simplex method, and because x is suboptimal, we then have this strict inequality here. Because x is basic and B is its basis, however XD is 0, so we can simply annihilate this term. Also, XD star is non negative, so for the strict inequality to hold, at least one entry of rD must be positive. Hence, if there's a better solution, the simplex algorithm won't terminate. That wraps up the case where the program is bounded. How about when it's unbounded? Well by the same argument just given, we won't hit the case where rD is non-positive. And the algorithm can't run forever, because it avoids cycling. And thus can only visit each of the bases once. The only remaining possibility is that u will be non-positive at some point. And we'll report that the problem is unbounded, which, indeed, it is. If you've been playing careful attention, you will have noticed that the simplex algorithm started with the basic feasible solution. Now in some cases basic feasible solutions are easy to find just by inspection. But that's not always the case. For these harder cases, we can create an auxiliary program to help us. First, we negate the constraint equations so that we have B is non-negative. Then we create an auxiliary program as follows. We introduce artificial variables Y that represent the slack between Ax and b. We require these variables to be non-negative and then try to minimize their sum. For this auxiliary program it is easy to find a basic, feasible solution. Just that x equal to zero, and y equal the b. Therefore we can start simplex on this problem here. If the optimum here is zero, then we can start the simplex algorithm on this problem at the value we found for x, at the optimum to this one. On the other hand, if the optimum isn't zero, then we conclude that our original problem was in-feasible. This is sometimes called the two phase approach for solving linear programs. The Simplex method as it is described here, was first published by George Dantzig in 1947. Fourier apparently had a similar idea in the early 19th century, and Leonid Kantorovichhad already used the method to help the Soviet Army in World War II. In was Danzig's publication however, that led to the widespread application of the method to industry. As the lessons of operation research learned from the war, began to be applied to the wider economy and fuel the post-war economic boom. It remains a popular algorithm today. As practical as the algorithm was, theoretical guarantees on its performance remain poor. And in fact in 1972, showed that the worst case complexity is exponential. It wasn't until 1979, that Khachiyan published the ellipsoid algorithm, and showed that linear programs can be solved in polynomial time. His results were improved upon in 1984 by Karmarkar, whose method turned out to be practical enough to be competitive with the Simplex method for real world problems. Both of these algorithms take shortcuts through the middle of the polyhedron, instead of always going from vertex to vertex. In the next lecture, we'll talk about the duality paradigm, which rises out of linear programming. And has been the source of many insights and the inspiration for new algorithms. Even with a whole other lesson however, we're only going to be able to scratch the surface of the huge body of knowledge surrounding this fundamental problem. That has shown itself to be of deep importance in both theory and practice. Over the next four lessons, we'll discuss two very important problems. Finding a maximum flow on a graph and solving a linear programming optimization. These problems are P-complete, meaning that any problem that can be solved on a Turing machine in polynomial time, can be turned into one of these problems. We will also see an important restricted case in the bipartite matching problem. And then we'll explore the connections among all these problems, in a discussion of the principal of duality, which is the inspiration for a large class of approximation algorithms. In this first lesson, we discuss the problem of finding a maximum flow through a network. A network here is essentially anything that can be modeled as a directive graph. And by flow, we mean the movement of some stuff through a medium, from some designated source to a destination. Typically we want to maximize the flow. That is to say, we want to route as many packets as possible from one computer across a network to another, or we want to ship as many items from our factories to our stores. Even if we don't actually want to move as much as possible across the network, understanding the maximum flow can give us important information. For instance, in communication networks, it tells us something about how robust we are to link failures. And even more abstractly, maximum flow problems can help us figure out things that seem unrelated to networks. Like, which teams have been eliminated playoff contention in sports. Given the variety and importance of these applications, it should be no surprise that maximum flow problems have been studied extensively, and we have some sophisticated and elegant algorithms for solving them. Let's dive in. The only prior knowledge you need for this lesson is a familiarity with the basics of graphs and graph traverses. You should know what a graph is and the difference between directed and undirected graphs. You should be familiar with the common vocabulary surrounding graphs such as vertex degree. The ideas of neighbors and neighborhoods and paths in graphs. And we will also assume that you are familiar with the basic graph traversal algorithms like Depth-first search and Breadth-first search and their running times. The plan for the lesson is this. We begin by introducing the necessary definitions for a flow network, flow capacities, and residual networks, and residual capacities. Then, we present the Ford-Fulkerson method, and show that it finds a maximum flow with the help of a theorem that shows that the maximum flow in a network is actually equal to the minimum cut. Finally, we discuss two variations on the Ford-Fulkerson method, showing that these algorithms make it much more efficient than a naive version would be. This lecture will follow most closely the treatment in Kleinberg and Tardos, Algorithm Design, but any reference on the subject should suffice. We begin with a definition. A flow network consists of, among other things, a directed graph, G, and we will disallow anti-parallel edges to simplify some of the equation. This is not a serious limitation, as we'll see. We'll distinguish to special vertices in the graph. A source, typically labeled s. This is where whatever is flowing to the network starts from. And a sync typically labeled t, this is where the flow ends up. We call all other vertices internal. To keep our equations a little simpler, we'll assume there are no incoming edges into s, or outgoing edges from t. Associated with every pair of vertices is a capacity which indicates how much flow it is possible to send directly between two vertices. We will assume that these capacity's are always non negative integers. This will make some of the arguments easier and it's not a serious limitation in fact the last algorithm we see will overcome it. Also note that if there is no edge in the graph, then the capacity is defined to be 0. That's the flow network, the flow itself is a function from pairs of vertices to the non negative reels. Clearly then, it must be non negative, and it can't exceed the capacity for any pair of vertices. Also we require that between any two vertices, at least one direction be 0. And it doesn't make sense to have flow going from one vertex to another, only to send it back again. Lastly, we require that flow be conserved at every internal vertex. We define, f in, to be the flow, into a vertex, and f out, to be the flow out. And we require that the two be equal. So for example, in this node here, I have 4 plus 2 coming in, and I have 5 plus 1 going out. These are both 6, so we say that flow is conserved at this vertex. Intuitively, this just means that internal nodes can't generate or absorb any of the stuff that's flowing. Those are the jobs of the source, and of the sync. The overall value of the flow, is defined as the flow going out of the source, or equivalently, the flow going into the sync. Now, we'll do a quick exercise to sharpen our understanding of these rules for the flow. Fill in the blanks with the appropriate numeric value. Here's how I figured out the answer. I started with this node here, and I noticed that there were 3 plus 1 units coming in, so there must be 4 units going out. Looking at this node down here, I observe that 10 units are coming in, so 10 units need to come out. There are two unknown flows going out, but the capacities are 6 and 4, respectively, for a total of 10 capacity. This means that there's no ambiguity. Both edges must be used to their full capacity. That is to say, that they are saturated. Finally that leaves me with this edge. We have 4 plus 6 coming in, and 2 units going out already, meaning that 8 more units must go out flowing along this edge. And I can check my work at the end by confirming that the amount coming out of the source, 12 units in this case, is the same amount that's going into the sink. Perfect. In our definitions so far, we've limited our flow networks in several ways to make our treatment of them a little simpler. Before going any further, I want to show some tricks of the trade that will allow all of the theorems and algorithms that we'll study to be applied to more general networks. One such limitation is the need for all the capacities on the edges to be integers. We can extend all our arguments to rational capacities, just by multiplying the capacities by the least common multiple of the denominators to create integer capacities. So in this case, I would simply multiply all the capacities by six. This just amounts to a change of units in our measurement of the flow. Another limitation we've imposed is that no antiparallel edges are allowed in the network. This forces us to choose a direction for the flow between every pair of vertices. In general, however, it might not be clear in which direction the flow should go before solving the max flow problem. It's possible to cope with this situation with some slightly less elegant analysis or just to convince yourself that the theorems still hold, add an artificial vortex between the two nodes and create two edges to simulate a reverse flow in this direction. Another limitation of our model is that we've limited ourselves to having a single source and a single sink. At first, it would seem like we couldn't handle a network like this one where we have two sources and three sinks. Actually, however, this situation is quite easy to deal with. We can just add an artificial source node and connect that to the others with an infinite capacity. And similarly, add an artificial sink. So don't let that limitation trouble you either. Everything we've done so far, has been to set up the rules of the game. We define the network, and the notion of a flow over it. And we've justified some of the simplifications that we've made. Now we're going to turn to the task of actually finding a maximum flow. The search will be an incremental one. We'll start with a sub-optimum flow, and then we'll look for a way to increase it. Supposed that we are given a particular flow over a network, and we want to explore how we might change it. Perhaps we realize that we can increase the flow, by routing it through this link here, instead of through these two. And that would then allow us to add more flow, through this path. This is equivalent to adding a flow that goes like this. Notice that the flows through the middle, cancel out. By adding this flow to the original, I get the desired result. Alternatively, if we just wanted to reroute the flow through the top link, we could add a circular flow like this, which would then reroute the flow. In fact, all possible changes that we might make to our original flow, can be characterized as another flow, but different rules apply. Certainly, if we've used up some of the capacity on an edge, we can't use the full capacity in the flow we're going to add. We capture the rules for the flow that we are allowed to add, with the notion of a residual flow network. We start by defining the residual capacity, for all pairs of vertices. Let's consider a u,v edge. If u,v is an edge in the original graph, then the residual capacity is the original capacity, minus whatever flow we already sent across that edge. For reverse edges, where not u,v but v,u is the edge in the graph, then we have a capacity equal to the flow that we sent from v to u. We can unsend this much flow. Everywhere else, the residual capacity is zero. In the residual graph, the edges are those u,v pairs for which the capacity is positive. Keeping the network sparse, helps in the analysis. In this example here, notice how the direction of this edge is reversed. There's no residual capacity going forwards anymore. That's all the new stuff. But there is a residual capacity going backwards, since we can unsend the unit of flow that we sent up here. This edge here remains the same. We can still send 1 unit of flow along there. This edge in the middle gets interesting, because we can either send 1 unit of flow, that would still leave us inside of these capacity constraints here. Or we can unsend the unit of flow that we already sent. Now for a quick exercise on residual networks. Consider the flow over the network on the left. And I want you to mark the edges with errors in this attempt at a residual network on the right. Here are the errors that I found. The direction of this edge here, should be reversed. We've exhausted all the capacity going up. The remaining capacity on this edge is going down. We can unsend the two units of flow that we sent already. For this edge here, there are only two remaining units of flow that we could possibly send in this direction. And there should be five that we can unsend. So, those two are wrong. For this pair of vertices here, we can indeed unsend three units of flow. But we actually can't send any more forward in that direction. So this edge should be eliminated. And in this case here, between these two vertices, we were sending two units of flow, so we can indeed unsend them. But we only have one more unit of flow that we can send. So this capacity should be one. Ultimately, as we try to find the maximum flow, we're going to start with a suboptimal flow and then augment it with a flow we find in the residual network. Let's see how this works. We'll start with the flow f in a graph G. And then drawing the residual network, we'll let f' be a flow over this graph. Of course, it obeys the residual capacity constraint. Then we add these two flows together using a special definition. Note only one of these two values of f' can be positive. Now I claim without proof that this augmented flow is a flow in our original network G, and that its value is just the sum of the two values of the individual flows. This is pretty easy to verify with the equations, but it's not very illuminating. So we'll skip it here and focus on the intuition instead. Is this augmented flow a flow in the original network G? It fits within the capacity constraints essentially by construction of the residual capacities. And it conserves flow, because both f and f' do. So yes. And the flow out from the source is a simple sum, which makes it a linear function. So yes, it makes sense that the flow of the sum should be the sum of the flow values. Now with all of this notation and background behind us, we are ready for the Ford-Fulkerson method. As we'll see, it's not really specific enough to merit being called an algorithm, though sometimes we will call it that anyway. We begin by initializing the flow to 0. Then, while there is a path from the source to the sink in the residual graph, we're going to calculate the minimum capacity along this path. We'll call that b. And then we're going to augment our flow, where the augmenting flow has b along this path and 0 everywhere else. Once there are no more paths in the residual graph, we just return the current flow that we've found in this variable f. Let's see how this work on a toy example. We start with 0 flow and the capacities shown here. We find a path in the residual graph that looks like this. The minimum capacity is 1, so we'll add that value to every edge along the path in our original flow. The result is a new flow, and a new residual graph. Here, there's only one st path. So we'll go ahead and choose that. Adding it to the old flow will cause this flow across the middle to cancel out, leaving us with this flow and this residual network. At this point, there are no st paths in the residual graph, so we're done. Great, and it's easy to confirm that the algorithm worked in this case. But how about in general? First, it's easy to verify that f is indeed a flow. 0 is a flow at the beginning, and that each iteration we augment the current flow, via flow in the residual graph. And as argued earlier, this is indeed a flow. Does the algorithm terminate? Well, remember that the capacities are all integral, so each augmenting flow, f prime, has to have a value of at least 1. Otherwise the path wouldn't be in the graph. Therefore, we can't have more iterations that the maximum value for a flow. So, yes, it terminates. How much time does it spend per iteration? Finding a path can be done with search or depth first search in time proportional to the number of edges. Constructing the residual graph itself takes time proportional to the number of edges. And the number of edges in the residual graph is, at most, twice the number of edges in the original graph. Since we only possible created those reverse edges. And of course, updating the flow requires a constant amount of arithmetic per edge. So all in all then, we just have order e times each iteration. That is, time proportional to the number of edges. This is a good start for the analysis, but it leaves us with some unanswered questions. Most important perhaps, is whether the return flow is a maximum flow. Sure, it's maximal in the sense that we can't augment it any further. But how do we know that with a different set of augmenting paths or perhaps with an entirely different strategy altogether, we wouldn't have ended up with a greater flow? Also, this bound on the number of iterations is potentially exponential in the size of the input, leaving us with an exponential algorithm. Perhaps there's some way to improve either the analysis or the algorithm to get a better running time. These two questions will occupy the remainder of the lesson. We'll start by showing that Ford-Fulkerson does indeed produce a maximum flow, and then we'll see about improving its running time. If your intuition tells you that the incremental approach of the Ford-Fulkerson method produces a maximum flow, that's a good thing. Your intuition is correct. But it's important that it be correct for the right reasons. Not all incremental approaches work. Often in optimization, we get stuck in local minima or local maxima. We need to argue either that we never make a mistake, the analysis of greedy algorithms typically have this feel. Or we need to be able to argue that the rules for incrementing allows us to undo any mistakes that we have made along the way. The latter will be the case for maximum flow. The argument we'll make is complex, and will require us to introduce a new notion of a minimum cutograph along the way. But in the end it's rewarding, and provides an example of how to find elegance in analysis. To show that Ford-Fulkerson returns a maximum flow, we're going to introduce the concept of a cut, and then relate the capacity of a cut to a flow. Here's a picture of our understanding so far. We know that the network will have some possible non-negative flow values, but it's not exactly clear how high this range goes, or where the results of Ford-Fulkerson fit in. One observation is that the flow value can't exceed the capacity of edges coming out of the source. Remember that the flow is defined to be the sum of the flows over these edges. Separating S from T in this way constitutes a cut in the graph which, as we'll see, is an upper bound on the value of any flow. It's possible that there will be smaller cuts as well, which will shrink the range of possible flow values for us. Even better, however, we'll see that the flow produced by Ford- Fulkerson has the same value as the cut. And since this cut serves as an upper bound on all possible flows, this flow must be a maximum. That's where this argument will end up. We start with making a more precise definition of an s-t cut. Saying that it is a partition of the vertices such that the source s is in A and the sync t is in B. And B is defined to be the set of vertices minus those in A. For example, in this network here, the green nodes might be A and the orange ones B. Or we might have this cut here. The vertices within one side of the partition don't have to be connected. Next, we observe that f is an s-t flow, and (A, B) is an s-t cut. Then the value of the flow is equal to the flow out of A minus the flow into A. Which in turn is equal to the flow into B minus the flow out of B. In this example here, we have 1 plus 1 plus 5 plus 1 exiting A, and 2 units entering over here, for a total of 6. And similarly for this cut here, we have 2 exiting here and 2 exiting here, and 2 entering here, again for a total of 6. As you might imagine, the proof comes from the conservation of flow. We start with a definition of flow, and then add 0 in the form of the conservation equations for each node in the A part of the partition. For every edge where both vertices are in A, the terms simply cancel, because it'll pair once here, and once here. So we're left with the value of the flow as being the sum of the flows over edges. Between A and B, minus the sum of the flows over edges between B and A. But this is just the flow out of A minus the flow into A, which of course, is exactly what we wanted to prove. >From this notion of a cut, it's natural to think about how much flow could possibly come across? Well clearly it can't exceed the sum of the capacity of the crossing edges. This intuition gives rise to the idea of the capacity of the cut. Which is, as I've said, just the sum of the capacities of edges crossing over from A to B. In this case of this graph it's just 2 + 8 which is equal to 10. Switching up the partition a little bit, let's do a quick exercise on cut capacity. What s the capacity of this cut shown in the above flow network? The answer is 16, we have 3 plus 4, which is equal to 7, going from A to B here, and 1 plus 8 equals 9 from A to B over here, for a total of 16. Intuitively, it should be clear that the capacity of a cut represents an upper bound on the amount of flow that could go from s to t. We state this as the following lemma. Let f be a flow and let (A, B) and s-t cut in a flow network. Well then the value of the flow is at most the capacity of the cut. The proof goes like this. The value of the flow is the flow out of A minus the flow into A. We can just drop the second term, leaving us with just the sum of the flows across individual edges from A to B. This is bounded by the capacity of these edges. And this sum then is the capacity of the cut. Note that from this proof, the inequality will be tight if no flow goes back from B to A and if all the crossing edges are saturated. That is, used to their full capacity. Keep this in mind. We are now ready for the climatic big theorem of this lesson. The Max-Flow Min-Cut Theorem. It's states that the following are equivalent, f is a maximum flow in a flow network G. The residual graph Gf has no augmenting paths. And there exists an s-t cut (A,B) such that the value of the flow is equal to the value of the cut. This is the realization of the strategy outlined earlier where we're going to introduce the notion of a cut, show that it served as an upper bound on the flow, and then show that Ford-Fulkerson produced a flow with the same value as a cut. So as an immediate corollary, we have then that the Ford-Fulkerson approach returns a maximum flow. Let's see a proof of the theorem. We start by showing that if f is a maximum flow in G, then the residual graph has no augmenting paths. Well, suppose not. And let f' be an augmenting flow. Then, we can augment f by f', and as we've argued this will be a flow. And the value of the flow will be the sum of the values of the individual flows, which must be strictly greater than the value of f. But we assumed it was a maximum, so that creates a contradiction. Next, we show that the fact that the residual network has no augmenting path implies that there is a s-t cut who's capacity is the same as the value of the flow. This is the real heart of the theorem. We let A be the set of vertices we can reach from S. In the residual graph Gf and we let B be the complement to that. We'll make the next step in the proof an exercise. If (u,v) goes from A to B, what does that tell us about the values of the flow? And similarly, if (u,v) goes from B to A what does that tell us about the values of the flow? Here are the answers. If u,v goes from A to B, then u is reachable from s but v is not. We conclude that the edge u,v must not be in the residual graph. So the edge must be saturated, and the flow is equal to the capacity. If u,v happened not to be in the original flow, then the capacity would be 0 and the equation still holds. If on the other hand u is in B and v is in A, then there can't be any flow going from u to v. Otherwise, it would be possible to unsend this flow and the edge u,v would be in the residual graph. So this tells us that there is no flow going from B to A. We have concluded that any forward edge from A to B must be saturated, and any backward edge from B to A must be unused. Before going any further, let's illustrate this with an example. Here we have a flow on the left and the corresponding residual graph on the right. Note that there is no path from s to t in the residual graph. Let's mark the vertices we can reach from s in green and the other ones in orange. Now it's easy to see that all the edges from the green to the orange vertices are saturated, and the edges from the orange to the green are empty, just like the theorem claims. Recall that for any cut, the value of a flow is the flow out of the partition with the source minus the flow into the partition with the source. As we just argued, however, in this case there is no flow back into the source partition. Moreover, the flow saturates all the edges. So it's just the sum of the capacities across the cut from A to B, which is then defined as the cut capacity. That completes the second part of the theorem. Lastly, we need to show that the existence of a cut that has the same value as a flow implies that that flow is a maximum flow. This follows immediately from the fact that a cut is an upper bound. If it's not an upper bound, then cut capacity wouldn't be an upper bound on the value of the flow, so this would produce a contradiction. That then completes the theorem. These three things are equivalent, maximum flow is equal to minimum capacity cut, and the Ford-Fulkerson approach returns a maximum flow. If you followed all that, congratulations. The Max-Flow Min-cut Theorem is one of the classic theorems in the study of algorithms, and a wonderful illustration of duality, which we'll discuss in a later lesson. For now, however, we're not quite ready to leave maximum flow yet. So far, we've addressed the first of the remaining questions we had about the FordFulkerson algorithm. Now we turn to the second question regarding its running time. Recall that the only bound that we have so far on the number of iterations, is the value of the flow itself, since each iteration must increase the value of the flow that we find by one. Let's see if this bound is tight. Consider the flow network below, and give the maximum number of augmenting paths that Ford-Fulkerson might find. The answer is the worst possible, 200. Let's see how this might happen. We might start by finding this path here. And since the capacity of this middle edge is 1, and that's the lowest, that's all the flow that we can push. This then becomes my new flow. We might then find this augmenting path here. Adding that in, gives this total flow. Two iterations for two units of flow. And we can continue in this pattern all the way until the maximum flow of 200 is achieved. This means that the bound we had before on Ford-Fulkerson is tight. It can take as many iterations as there are units of flow and the maximum flow. In thinking about this example, it's pretty clear that we made some poor choices for picking the augmenting path. But how can we be sure to pick better ones? One idea is to prefer heavier paths, push flow along paths that have larger bottlenecks. Another idea is to prefer shorter paths, note that we were choosing longer paths here to create this bad case. We'll explore both of these ideas and some refinements to the Ford-Fulkerson algorithm that will give us better running times. This idea that we should prefer heavier flows brings us to the scaling algorithm. One idea is to find the heaviest possible flow. We could do this by starting with an empty graph and then adding an edge with the largest remaining residual capacity until there was an ST path. But this would be unnecessarily slow. Instead, we fix a schedule of thresholds for the residual capacity, and lower the threshold when there are no more augmenting paths in the residual network. We'll start by defining the residual network with threshold delta to include only those edges whose capacities is at least delta. Note that when delta is equal to 1, this is the same as our traditional definition for a residual network. We can state the algorithm as follows. We initialize the flow to 0 and the parameter delta to be the largest capacity coming out of the source. This is a trivial upper bound on the value of a single path flow. Then while delta's at least 1 we look for an augmenting path in the residual graph with threshold delta. And we use this path to augment the flow. Once all such paths are exhausted, we then, cut Delta in half. And repeat. Some of the analysis, we can do just by inspection. Letting C be the initial value for Delta, it's clear that we only use, log C iterations, of the outer loop, since we're cutting Delta in half, each time. And the steps of the innermost loop here, finding a path and using P to augment the path or certainly order E. The big question then is how many iterations of the inner loop are there? The key claim in the analysis of the scaling algorithm is that the maximum number of iterations for a scaling phase is at most twice the number of edges. By scaling phase, I mean an iteration of the outer loop here where we have a fixed value of delta. We start with the following lemma. If the residual graph, with threshold delta, has no s-t paths, then there exists an s-t cut, A, B, such that the capacity of the cut is at most the value of the flow, plus the number of edges times delta minus 1. The proof will fill up like the max flow min cut proof. We let A be the center of vertices reachable from s in our residual graph. And we let B be the complement of those set of vertices. Edges from A to B in this graph must have residual capacity at most delta minus 1, otherwise the vertex in B would be reachable from the source s and hence a part of A. And edges from B to A can't have flow more than delta minus 1, or their reverse edge would appear in the residual graph. The value of the flow then is the flow out of A minus the flow back into A. And using these bounds here we can bound this by the capacity minus delta minus 1. And similarly for these back edges, we can do that with the delta minus 1 bound. This then is just the capacity of the cut, and combining the delta minus 1 terms, well that can only appear in E number of terms here. So that gives us the remaining part of the bound. With this lemma complete, we now return to the main claim that we want to prove. The base case where delta is equal to C is trivial. Since each augmenting flow here saturates one of the edge out of the source. For subsequent iterations we let f be the flow after the scaling phase delta, and we'll let g be the flow before. That is, the flow after the previous scaling, which would be either 2 delta, or 2 delta + 1, depending on how the integer division rounding worked. The value of the flow f is at most the maximum flow, but this is at most the capacity of the s-t cut induced by the flow g, in the previous iteration, or the threshold was two delta. Our lemma then says, that this is at most, the flow of g, plus the number of edges, times 2 delta. Now we let k be the number of iterations, that we use to go between the flow g, and the flow f. Well then k times delta is a lower bound on the difference between the value of the flow f and the value of the flow g. Each iteration increased the flow by at least delta. But from this bound here we have that this is at most twice the number of edges times delta. And so from this then we conclude that the number of iterations is at most twice the number of edges. This then completes the analysis of the scaling algorithm. We have at most log C iterations at the outer loop, order E iterations of the inner loop, and each one of these iterations takes order E time. For a total of E squared log C as we state in this theorem. The scaling algorithm returns a maximum flow in time order e squared log c, where c is the maximum capacity of an edge coming out of the source. So far we've explored the idea that we should prefer heavier augmenting paths. It turns out that the idea of using shorter paths also gives rise to an efficient algorithm. This is the Edmonds-Karp algorithm, also discovered independently by Dinitz in the Soviet Union. Again, this is exactly the Ford-Fulkerson algorithm, only we are sure to choose a minimum length path. The cost of an iteration is order E, as always, as we just use breadth-first search to find the shortest s-t path. If we can bound the number of iterations, most likely we'll have a better bound than we did for the naive version of Ford-Fulkerson. Indeed, we will be able to bound the number of iterations here as order E times V. And this, then, will enable us to argue that Edmonds-Karp returns a maximum flow in time E squared V. Now for the analysis. Mostly, I'll just try to share the intuition. We want to show that order E times V augmentations are used. To see this, we define something called a level graph. The level of a vertex is defined to be the shortest path distance from the source vertex f. The level graph then, is a sub graph of the original, that includes only those edges from one level to a level that's one higher. For example, if this were my directed graph here, then the level graph would be this sub graph down here. These edges here have been deleted, because they go between vertices within the same level. And this one has been deleted, because it went backwards or up a level, in the graph. We first observed that augmenting along a shortest path, only creates paths that are longer ones. Say that we push flow along this path here. Then we've introduced back edges along this path. Note, however, that these edges are useless for creating a path of the same length. In fact, because they go back up a level, any path that uses one of them, must use two more edges than the augmenting path that we just used, the one that was the shortest path. Next, we observe that the shortest path distance must increase every E iterations. Every time that we use an augmenting path, we delete an edge from the level graph. The edge that got saturated. Maybe this one here got saturated in an augmentation along this path, so we would delete that. And then maybe we'd push another flow along this path, and end up deleting this edge. These edges won't come back into the level graph, until the minimum path length has increased. As we've already argued, the reverse edges are useless. Until we are allowed to use a longer path. So if each augmentation deletes one edge from the level graph, then certainly after E iterations, the level graph would be empty. And then clearly, there are no ST paths in this level graph, and so we need to build a bigger one that includes longer paths in order to be able to augment any further. And lastly, there are only V possible shortest path lengths. So that completes the theorem. For each possible path link, we might have up to E iterations. Going back to the theorem, note that we've eliminated the dependence to the running time on the capacities. This means that the algorithm is now strongly polynomial. And actually, we can eliminate the requirement that the capacities be integers entirely. There's one more refinement to the algorithm that I can't resist, and that's due to Dinic. He actually published his algorithm in 1970, two years before Edmonds-Karp. His key insight is that the work of computing the shortest path can be recycled so that a full recomputation only needs to happen when the shortest path distance changes, not for every augmenting path. As with all augmenting path strategies, we start with an initial flow of 0. Then we repeat the following. We build the level graph from the residual flow network, and let k be the shortest path length from s to t. Then, while there's a path from source to sink in the level graph that has length k, we will augment the flow along that path and then update the residual capacities. And we repeat this until there are no more s-t paths in the residual network. And finally we just return the flow. Turning to the analysis, we will call one iteration of this outer loop a phase. And we will be able to argue that each phase increases the length of the shortest s-t path in the residual network by one. The principle here is the same as for Edmonds-Karp. Augmenting by a shortest path flow doesn't create a shorter augmenting flow. Hence, once we've exhausted all paths of a given length, the next shortest path must be one edge longer. Within a phase, the level graph is built by breadth-first search, so that only costs order E time. The hardest part of the argument will be that this loop altogether takes only E times V time. We'll see this argument in a second. Altogether, we will show that Dinic's algorithm takes E times V squared time, which is an improvement over the E squared V time of Edmonds-Karp. We turn now to the key part of the analysis where we show that each phase of the Dinic algorithm takes V times E time. As with Edmonds-Karp, we will use a level graph. In this case, however, the algorithm actually builds the graph, whereas in Edmonds-Karp we simply used it for the analysis. The level graph can be built by running Bedford search and saving all forward edges while ignoring backwards and lateral ones. As we argued before, when we augment the flow along the path, say along this one here, we introduce reverse edges into the residual graph. Note that these are always backwards edges in the level graph. And hence they aren't useful in building a path equal to or shorter than the previous shortest path. Well if the new edges are useless, why rebuild the level graph of the residual graph when the old one will serve just as well? We can just update the residual capacities. More precisely, given the possibly outdated level graph, we can build a path from source to sync just by making the first vertex on the adjacency list the next vertex in the path. If this generates a path to t, then we augment the flow and update the residual capacities. If it doesn't, then we delete the last vertex in the path from the level graph. In this example, we would, first find a path from s to t, and, let's say that this edge here is a bottle neck. It's capacity gets set to 0, and it gets deleted. Next, we will build a path from s again, and this time, we would run into a dead end. So we delete this vertex, and, continue. There are only V vertices in the graph. So we can't run into more than V dead ends, and every augmentation deletes the bottleneck edge, so we can't delete more than E edges. Overall then, we won't try more than E path. This process however, of building the path by first in the adjacency list, and then augmenting or deleting a vertex as appropriate. Each time that set of operations is done, only costs us order V time. So, overall, then, we're doing order V work, order E times. So, overall, this work takes order V times E. Taking this all together, then, we have V phases. And each one of the individual phases costs order E times V time, for a total of E times V squared time overall, which is an improvement over Edmonds-Karp. We've now seen a few improvements to the naive version of the Ford-Fulkerson method. But this isn't the end of the story. There are some even more sophisticated approaches that improve the run time even further, also based on the idea of augmenting flows. There's also a family of algorithms called push relabel, that allow the internal vertices to absorb flow during the intermediate phases of the algorithm. In practice, these seem to perform the best. Beyond just seeing the algorithms in this lesson, we examined the max-flow, min-cut theorem. This was more than just a trick for proving the correctness of forward focusing. It's part of a larger pattern of duality that provides important insight in a variety of contexts. We'll discuss this more in a later lesson This lecture covers the theory of NP completeness. The idea that there are some problems in NP, so general, and so expressive, that they capture all the challenges of solving any problem in NP in polynomial time. These problems will provide important insight into the structure of P and NP, and from the basis for the best arguments we have, for the attractability of many important, real world problems. With our previous discussion of the classes, P and NP in mind, you can visualize the classes like this. Clearly P is contained inside of NP. And we're pretty sure that this containment is strict. That is to say there are some problems in NP but not in P. Where the answer can be verified efficiently, but it can't be found efficiently. But no one knows how to prove this yet. In this picture then, you can imagine the harder problems being at the top. Now, suppose for a moment that you encounter some problem where you know how to verify the answer but where you think that finding an answer is intractable. Unfortunately, you boss or maybe your advisor doesn't agree with you, and keeps asking for an efficient solution. How would you go about showing that the problem is, in fact, intractable? One idea is to show that the problem is not in P. That would indeed show that it is not intractable, but it would do much more. It would show that P is not equal to NP. You would be famous. As we talked about in the last lecture, whether P is equal to NP is one of the great open questions in mathematics. So I'm going to cross that out. I don't want to discourage you from trying to prove this theorem necessarily. You should just know what you're getting into. Another option is to reduce another intractable problem to your problem. If you were working in the early or mid-1970s, you might have reduced linear programming to your problem. Practical methods for solving some linear programs were available, but most people thought that in general it was not polynomially solvable. Since the solution to your problem would solve any linear program your problem must be harder. The trouble with this approach is that it was later shown that linear programming actually was polynomially solvable. Hence the fact that your problem is as hard as linear programming doesn't mean much anymore. The Class P swallowed linear programming. Why couldn't it swallow your program, as well? This type of argument isn't worthless, but it's not as convincing as it might be. It would be much better to reduce your problem to a problem that we knew was one of the hardest in NP. So hard that if P were to swallow it, it would've had to have swallowed all of NP. In other words, it would have to move this line here all the way up to the top. Such a problem would have to be NP complete. Meaning that we can reduce every language in NP to it. Remarkable as it may seem, it turns out there are lots of such languages, satisfiability being the first one for which this was proved. In other words, we know that it has to be at the top of this image. Turning back to how to show that your problem is intractable. Short of proving that P is not equal to NP, the best we can do is reduce an NP-complete problem like SAT to your problem. Then your problem would also be NP-complete and the only way that your problem could be polynomially solvable is if this line moves up here and everything in NP Is polynomially solvable. There are two parts to this argument. The first is the idea of a polynomial reduction. We've seen reductions before in the context of computability. Here, are the reductions we not only have to be computable, but to be computable in polynomial time. This idea will occupy the first half of the lesson. The second half, we'll consider this idea of NP completeness and we'll go over the famous Cook-Levin theorem. Which showed that bouillon satisfiability is NP-complete Now for the formal definition of a polynomial reduction. This should feel very similar to the reductions that we considered when we were talking about computability. A language, A, is polynomial reducible to a language, B, and we write this as A less than or equal to B with the subscript P over the less than or equal to sign, if there is a polynomial function, f where for every string w, w is in A, if and only if f(w) is in the language B. The key difference from before is that we have now required that the function be computable in polynomial time, not just that it be computable. We will also say that f is a polynomial time reduction of A to B, or sometimes from A to B. Here's the key implication of there being a polynomial reduction of one language to another. Let's suppose that I want to know whether a string x is in the language A. And suppose, also, that there exists a polynomial decider, M for the language, B. Then all I need to do is take the machine or program that computes this reduction function. Let's call it N. And feed my string x into it. And then feed that output into M. M will tell if f(x) is in B. But by definition of a reduction, this also tells whether x is in A, which is exactly what I wanted to know. I just had to change my problem into one encoded by the language B and then I could use B as decider. By the way, this square bracket notation means 1 if the expression inside is true, and 0 otherwise. Therefore, the composition of M with N is a decider for A, by all the same arguments we used in the context of computability. But is it a polynomial decider? We just found ourselves considering the running time of the composition of two turning machines. Think carefully, and give the most appropriate bound for the running time of the composition of m, which runs in time q with N, which runs in time p. You can assume that both N and M read all of their input The answer is (q(p(n))). It's tempting to think that we run N and then we run M. So, the total running time should just be the sum of the two individual running times. The trouble with this, is that it doesn't account for the possibility that the output of N could be much longer than the input. In fact, it could be almost p(n). Most of the program's instructions could have been print statements, or writes to the end of the tape for Turing machines. So that the output of N is p(n) long and would then take q(p(n)) time. And then that becomes the dominant factor in our bound. In particular, this means that if both p and q are bounded by polynomials, then the running time of the composition will also be bounded by a polynomial, since the composition of two polynomials is a polynomial. We've just argued that if N is polynomial, and we take its output, and feed that into M, which is also polynomial in its input size, then the resulting composition of M with N is also polynomial. So, we can add in polytime to our claim here. And by this argument we've proved the following theorem. If a is a polynomial reducable to B and B is in P then A must be in P. Just convert the input to prepare it for the decider for B and return whatever the decider for B says. Here's a question to test your understanding of the implications of the existence of a polynomial reduction. Suppose that A reduces to B. Which of the statements below follow? The answer is the middle two. This first one might not be true. A may well be in P. But B could be something much harder and not be in P. The fact that we can turn an instance of problem A, into an instance of problem B, doesn't tell us anything. The same logic applies to the last theorem. The answer then, is the middle two. The second one is just what the previous theorem said. And the third, is just the contrapositive of this statement. And it's this contrapositive version that we use most in the study of complexity. To illustrate the idea of a polynomial reduction, we're going to reduce the problem of finding an independent set in the graph, to that of finding a vertex cover. In the next lesson, we're going to argue that if P is not equal to NP, then independent set is not in P, so vertex cover can't be either. But that's getting ahead of ourselves. First, for the benefit of those not all ready familiar with these problems, we will state them briefly. First will consider the independent set problem and see how it is equivalent to the clique problem that we talked about when we introduced the class NP. Given a graph, a subset of the vertices is an independent set if there are no edges between the vertices in the set S. For example, these two vertices here do not form an independent set because there's an edge between them. However, these three vertices do form an independent set because there are no edges between them. Clearly each individual vertex forms an independent set since there isn't another vertex in the set for it to have an edge with. And the more vertices we add, the harder it is to find new ones to add. So, finding a maximum independent set is the interesting question. Or, phrased as a decision problem, the question becomes given a graph g, and an integer k, at most, the number of vertices, does g have an independent set of size k? Note that this problem is in NP. Given a subset of the vertices, it's easy to count them to make sure that there are indeed k, and check that there are no edits between them. This problem is so close to the clique problem that the two are usually considered as one. The set is independent in a graph if and only if there's a clique in the complement graph. By the complement graph, I mean the one where there is no edge if there was an edge between two vertices in the original, and there is an edge if there wasn't one in the original. Not having any edges between a set of vertices in the original graph is the same as having all the edges among the vertices in the complement graph. In order to understand the reduction, it's critical that you understand the independent set problem. So here's a quick question to test your understanding. Mark an independent set of size 3. The only independent set of size 3 is this one here. Clearly, we can only include one vertex from this triangle over here. And the best we can do from these three over here, is to select these two. And this combination allows us to do both. Now let's define the other problem that will be part of our example reduction. Vertex Cover. Given a graph G, a subset of the vertices is a vertex cover, if every edge is incident on a vertex in S. To illustrate, consider this graph here. The two shaded vertices do not form a vertex cover. We haven't covered this edge here, or this one here. On the other hand, these two vertices do form a vertex cover, because every edge is incident on one of these two. Clearly, the set of all vertices is a vertex cover, so the interesting question is, how small a vertex cover can we get? Or, phrased as a decision question, we're given a graph G and an integer k, at most the number of vertices. The question is, does G have a vertex cover of size k? Note that this problem is also in NP. It's easy enough to check whether a subset of vertices is of size k, and whether it covers all the edges. Going forward, it will be very important to understand this vertex cover problem. So we'll do a quick exercise, to give you a chance to test your understanding. Mark a vertex cover of size 3 in the graph below. The answer is these three here, and that's the only combination that works. Note that these three are the compliment of the ones that we chose when we were looking for an independent set. Makes you wonder. Having defined the independent set and vertex cover problems, we will now show that vertex cover is as hard as independent set. In general, finding a reduction can be very difficult. Sometimes however, it can be as simple as playing around with the definition. You will notice that in both these examples here and in the exercises, the set of vertices used in the vertex cover was the complement of the vertices used in the independent set. Let's see if we can explain this. A set S is an independent set if there are no edges within S. By within, I mean that both endpoints are in S. That's equivalent to saying that every edge is not within S, or that every edge is incident on V- S. A-ha! But that just says that V- S is a vertex cover. So of course the complement of an independent set was always a vertex cover and vice versa. Thus, we have the observation that a subset of the vertices S is an independent set if and only if V- S is a vertex cover. As a corollary then, G will contain an independent set of size at least K, if and only if it has a vertex cover of size at most V- K. The reduction then is fantastically simple. Given a graph G and a number K, the reduction just keeps the same graph G and changes K to the number of vertices- K. The polynomial reducability of one problem to another is a relation, much like the at most relation that you've seen since elementary school. And while it doesn't have all the same properties, it does have the important property of transitivity. Like the traditional at most sign for integers or real numbers, the polynomial reducability sign is transitive, meaning that if A is reducable to B and B is reducible to C, then A is reducible to C. For example, we've seen how independent set is reducible to vertex cover, and I claim that vertex cover is reducible to Hamiltonian path, a problem closely related to the traveling salesman problem, and from these facts, it follows that independent set is reducible to Hamiltonian path. Let's take a look at the proof of this theorem. We'll let M be the program that reduces A to B, and we'll let N be the program that reduces B to C. To turn an instance of the problem A into an instance of the problem C we just pass it through M and then pass that result through N. This whole process can be thought of as another computable function, R. Note that like M and N, R is polynomial-time. Thus x is in A if, and only if, M of x is in B. Remember, M implements that reduction. And m of x is in B if and only if N of M of x is in C because N implements that reduction. The composition of N and M is the reduction R so overall we have the x is in A if and only if R is in C, just as we want. I claimed that prove, that the composition of two polynomial reductions was polynomial. But why was that the case? Think carefully, and give the most appropriate bound for the running time of the reduction R. Which is the composition of m. Which runs in time p. And N, which runs in time q. You can assume that both m and n read all their input. The answer is q of p of n. It's tempting to think, that we run M, and then we run N, so the running time should be the sum of the two polynomials. The trouble is, that this doesn't account for the possibility, that the output of M, could be much longer than the input. In fact, it could be almost p of n. Most of the program's instructions, could have been print statements or rights to the end of the tape for Turing machines. So that the input to n, is p of n long. N would then take q of p of n time, and that becomes the dominant factor. If you have followed everything in the lesson so far, then you are ready to understand NP completeness, an idea behind some of the most fascinating structure in the P versus NP question. You may have heard optimists say that they are only one algorithm away from proving that P is equal to NP. What they mean is that if we could solve just one NP complete problem in polynomial time, then we could solve all of them in polynomial time. Here's why. Formally, we say that a language L is NP-complete if L is in NP, and if every other language in NP can be reduced to it in polynomial time. Recalling our picture of P and NP from the beginning of the lesson, the NP complete problems were the ones at the very top. And we called them the hardest problems in NP. We can't have anything higher that's still in NP, because if it's in NP, then it can be reduced to an NP complete problem. Also, if any one of the NP complete problems were shown to be in P then P would extend up and swallow all of NP. It's not immediately obvious that an NP complete problem even exists. But it turns out that there are lots of them and in fact they seem to occur more often in practice than the problems in this intermediate zone which are not NP complete and so far have not proved to be an NP either. Historically, the first natural to be proofed to be NP-complete is boolean formula satisfiability, or SAT for short. This was shown to be NP-complete by Stephen Cook in 1971 and independently by Leonid Levin in the Soviet Union around the same time. The fact that this problem is NP-complete is extremely powerful, because once you have one NP-complete problem, you just need to reduce it to other problems to show that they, too, are NP-complete. Thus, much of the theory of complexity can be said to rest on this theorem. This is the high point of our study of complexity. So what exactly is this satisfiability problem? For our purposes, we won't have to work with the most general satisfiability problem. Rather, we can restrict ourselves to a simpler case, where the boolean formula has a particular structure called, conjunctive normal form, or CNF. Like this one here. First, consider the operators. The v indicates a logical or. The wedge indicates a logical and. And the bar over top of one of the variables indicates logical not. For one of these formulas we need to define a collection of variables, x, y, and z, in this example. And these variables appear in the formula as literals. A literal can be either a variable, or the variables negation. For example this x or this y bar, etc. At the next higher level we have clauses, which are disjunctions of literals. You could also say logical ors of literals. One clause is what lies in between a pair of parenthesis. And finally, we have the formula as a whole, which is a conjunction of clauses. That is to say, all the clauses get anded together. Thus this whole formula is in conjunctive normal form. In general, there can be more than two clauses that get anded together. That covers the terms we'll use for the structure of a CNF formula. As for satisfiability itself. We say that a boolean formula is satisfiable if there is a truth assignment for the formula. A way of assigning variables true and false such that the formula evaluates to true. The CNF satisfiability problem is, given a CNF formula, determine if that formula has a satisfying assignment. Clearly this formula is in NP. Since given any truth assignment, it takes time, polynomial, and the number of literals to evaluate the formula. Thus we've accomplished the first part of showing that satisfiability is NP complete. The other part, showing that every problem in the NP is polynomial time reducible to it, will be considerably more difficult. To check your understanding of boolean expressions, I want you to say whether the given assignment satisfies this formula. The answer is that it does not. You can see that it doesn't satisfy that last clause here. A is false, B bar is false, and C bar is false. Therefore this clause is false. And because it's anded with the rest, the whole expression is false. Just because one particular truth assignment doesn't satisfy a Boolean formula, doesn't mean that there isn't a satisfying assignment. See if you can find one for this formula. Unless you're a good guesser, this will take longer than just testing, whether a given assignment satisfies the formula. Here is one of several possibilities. >From the first clause, A bar is true. >From the second, D is true, from the third, B bar is true, and from the last, B bar and C bar are true. Now that we've understood the satisfiability problem, we're ready to tackle the Cook-Levin theorem. Remember that we have to turn any problem in NP into an instance of SAT. So it's natural to start with the thing that we know all problems in NP have in common. There's a non-deterministic machine that decides them in polynomial time. That's the definition, after all. Therefore, let L be an arbitrary language in NP, and let M be a non-deterministic Turing machine that decides L in time at most p(n), where, p is a polynomial. In accepting computation, or sequence of configurations for the machine M, can be represented in a tableau like this one here. Each configuration in a sequence is represented by a row. Where we have one column for the state, one column for the head position, and columns for the values of the first p event squares of the tape. Note that no other squares can be written to because there just isn't time to move the head that far in only p event steps. Of course, the first row must represent the initial configuration. And the last one must be in an accepting state in order for the overall computation to be accepted. Note that it's possible that the machine will enter an accepting state before step P event, but we can just stipulate that when this happens, all the rest of the rows in the table have the same values. This is like having the accept state always transition to itself. The Cook-Levin theorem then, consists of arguing that the existence of an accepting computation is equivalent to being able to satisfy a CNF formula that captures all the rules for filling out this table. Before doing anything else, we first need to capture the values in the tableau with a collection of boolean variables. We'll start with this State column here. We let Qi represent whether after step i, the state is q sub k. Similarly, for the Head Position, we define H, i, j, to represent whether the head is over square j after step i. That captures this column. And lastly, for the Tape Contents, we define s, i, j, k, to represent whether after step i, square j, contains the tape symbol enumerated as k. That captures all of these columns, here. Note that as we define these variables, there are many truth assignments that are simply nonsensical. For instance every one of the q variables could be assigned a value of true. But in a given configuration sequence, the Turing machine can't be in all states at all times. Similarly we can't assign them to all be false. The machine has to be in some state at each time step. We have the same problems with the head position variables, and with the variables for the squares on the tape. All of this is okay. For now, we just need a way to make sure that any accepting configuration sequence has a corresponding truth assignment. For any way of filling out the tableau, the corresponding truth assignment is uniquely defined by these meanings. It's the job of the boolean formula to rule out truth assignments that don't correspond to a valid accepting computation. Having defined the variables, we're now ready to move on to building the boolean formula. We're going to start with the clauses needed so that given a satisfying assignment, it's clear how to fill out the table. Nevermind for now about whether the corresponding sequence has valid transitions. For now, we just want the individual configurations to be well-defined. First, we have to enforce that at each step, the machine is in some state. Hence, for all steps i, at least one of the state variables for that step has to be true. Note that r denotes the number of states here. In this context, it's just a constant. The input to our reduction is just the string that we need to transform into a boolean formula. The machine also can't be in two states at once. So we need to enforce that constraint as well by saying that for every pair of state variables for a given turn step, one of the two has to be false. Together, these sets of clauses enforce that the machine corresponding to a satisfying tooth assignment. Is in exactly one state after each step. For the position of the head, we have a similar set of clauses. The head has to be over some square on the tape, but it can't be in two places at once. And lastly, each square on the tape has to have exactly one symbol. Thus, for all steps I and squares J, there has to be some symbol on the square, but there can't be two. The other clauses related to individual configurations come from the start and end states. The machine must be in the initial configuration at step zero. This means that the initial state must be q0, that the head be over the first position on the tape, and that the first part of the tape contained input w. The rest of the tape must be set to blank to start. And finally, the computation must end in an accepting state. This we can enforce with the single clause that the state after step p(n) be the accepting state. Any truth assignment that satisfies the clauses we've talked about so far specifies how the tableau should be filled out and moreover, the tableau will always start on the initial configuration and end up in an accepting one. What is not guaranteed yet is that the transitions between the configurations are valid. To see how we can add constraints that will give us this guarantee, we'll use this example. Suppose that the transition function tells us that if the machine is in state q3, and it reads a 1 then it can do one of two things. It can move to state q0, leaving the 1 alone and move the head to the right, or it can move to state q4, write a 0 and move the head to the left. To translate this rule into clauses for our formula, it's useful to make a few definitions. First, we need to enumerate the tape alphabet so that we can refer to the symbols by number. Let's say that the blank is symbol 0, 0 is symbol 1 and 1 is symbol 2. Next, we define the tuple k, l, k prime, l prime, delta to be valid if the triple of the state qk prime, sl prime and delta is in the set delta of qk, sl. That is to say, this has to be one of the valid transitions, given that we are in state qk and that we read the symbol sl. For example, the tuple 3, 2, 0, 2, R is valid. The first two numbers here indicate which transition rule applies. And note here that I'm using the enumeration of the alphabet to translate the symbols here. 3 means state three and 2 means the symbol one. The last three numbers indicate the transition being made. In this case it is in the set defined by delta. We're moving to state q0, we're writing symbol 2, which is a 1. And we're moving to the right. On the hand, this transition is endowed, the machine can't switch to state four, write a one, and move the head to the right. That's not one of the valid transitions, given that the machine is in state three, and that it just read a blank. Now there are multiple ways to create the causes needed to ensure that only valid transitions are followed. Many proofs of Cook's theorem start by writing out Boolean expressions, that directly express the requirement that one of the valid transitions be followed. The difficulty with this approach, is that the intuitive expression isn't in conjunctive normal form. And some Boolean algebra is needed to convert it. That is to say, we don't immediately get an intuitive set of short clauses to add to our formula. On the other hand, if we rule out invalid transitions instead, then we do get a set of short intuitive clauses that we can just add to the formula. To illustrate, in order to make sure that this invalid transition is never followed, for every step i and position j, we add a clause that looks like this. It starts with three literals that ensure the transition rules for being in state q3 and reading a blank symbol actually do apply. If the head isn't in the position we're talking about, the state isn't q3, or the symbol being read isn't the blank symbol, then the clause is immediately satisfied. The clause can also be satisfied if the machine behaves in any way that's different from what this particular invalid transition would cause to happen. The head could move differently, the state could change differently, or a different symbol could be written. Another way to parse this clause is as saying if the Q3 blank symbol transition rule applies, then this invalid transition can't have been followed. Logically remember that A implies not B is equivalent to not A or not B. That's the logic we're using here. A being, does the transition rule apply, and B being, was the invalid transition followed. Now let's state the general rule for creating all the transition clauses. Recall that a tuple k, l, k prime, l prime, delta, is valid, if switching to state qk prime, writing out the symbol sl prime and moving into action delta is an option, given that the machine is currently in state qk and reading symbol sl. For every step I, position j, and invalid transition, k, l, k prime, l prime, delta, we include in the formula a clause like this, the first part tests whether the truth assignment is such that the transition rule actually applies. And the next three rules ensures that this invalid transition wasn't followed. This is just the generalization of the example that we saw earlier. We're almost done with the proof that satisfiability is NP complete. But before making the final arguments, I want to remind you what we're trying to do. Consider some language in NP, and suppose that someone wants to be able to determine whether strings are in this language. The Cook-Levin theorem argues that because L is in NP, there's a non-deterministic turn machine that decides it. And it uses this fact to create a function computable in polynomial time that takes in any input string X, and outputs a boolean formula that is satisfiable. If and only if, the string X is in the language L. That way, any polynomial algorithm for deciding satisfiability will be able to decide every language in NP in polynomial time. Only two questions remain. Is the reduction correct, and is it polynomial in the size of the input? Let's consider correctness first. If X is in the language, then clearly the output formula is satisfiable. We can just use the truth assignment that corresponds to an accepting computation of the non-deterministic turning machine that accepts X, that will satisfy the formula F. That much is certainly true. How about the other direction? Does the formula being satisfiable imply that X is in the language? Take some satisfying assignment for F. As we've argued, the corresponding tableax is well-defined. Only one of the state variables can be true at any time, etc. The Tableax also starts in the initial configuration. Every transition is valid, and the configuration sequence ends in an accepting state. That's all that's needed for a non-deterministic turn machine to accept. So yes, this direction is true as well. Now we have to argue that the reduction is polynomial. First, I claim that the running time is polynomial in the output formula length. There just isn't much calculation to be done besides iterating over the combination of steps, head positions, states and tate symbols, and printing out the associated terms of the formula. Second, I claim that the output formula is polynomial in the input length. Let's go back and count. These clauses, pertaining to the states, require order p of n times log n string length. The number of states to the machine is a constant in this context. The p factor comes from the number of steps of the machine. The four i here. The log in factor comes from the fact that we have to distinguish the literals from one another. This requires log in bits. In fact, in all these calculations, that's where the log factor comes from. For the head position, we have order p of nq to log n string length. One factor p comes from the number of steps, and we have two coming from all pairs of head positions. They're order p squared combinations of steps and squares. So this family of clauses requires order p squared log n length as well. The size of the alphabet is a constant in this context. As for the constraints pertaining to the initial configuration, this require order p log n string length. The dominant factor being the constraints about the contents of the tape. And the constraint that the computation be accepting requires only log n string length, since it just has to write out this single literal one clause here. The transition clauses might seem like they would require a high order polynomial of symbols. But remember that the size of the non-deterministic turning machine is a constant in this context. Therefore, the fact that we might have to write out clauses for all pairs of state and tape symbols, doesn't affect asymptotic analysis. Only the range of the indices i and j depend on the size of the input string. So we end up with order p squared log n string length for all these transition clauses. Summing the length of all those clauses up together, we have a sum of polynomials, which is of course a polynomial. So yes, the overall reduction is a polynomial and Cooks theorem is proved. Congratulations, you have seen how one problem, satisfiability, captures all of the complexity of the P versus NP problem. An efficient algorithm for satisfiability when imply P equals NP. If P is different from NP, then it can't be any efficient algorithm for satisfiability. Satisfiability has efficient algorithms if, and only if, P equals NP. Steve Cook, professor of the University of Toronto, presented his theorem on May 4, 1971, at the symposium on the the theory of computing at the then Stouffer's Inn in Shaker Heights, Ohio. But his paper didn't have an immediate impact as a satisfiability problem was mainly of interest to logicians. Luckily a Berkeley professor, Richard Karp, also took interest, and realized that you use satisfiability as the starting point. If you can reduce satisfiability to another NP problem, then our problem must be NP complete as well. Karp did exactly that, and a year later he published his famous paper, Reducibility Among Combinatorial Problems, showing that 21 well-known combinatorial search problems were also MP complete, including the clique problem. Today tens of thousands of problems have been shown to be MP complete. The ones that come up the most often are practiced tend to be closely related to one of Karp's originals. In the next lesson, we'll examine some of the reductions that prove these classic problems to be MP complete and try to convey a general sense for how to go about finding such reductions. This sort of argument might come in handy if you need to convince someone that a problem isn't as easy to solve as they might think. In the last lecture, we proved the Cook-Levin theorem which shows that CNF satisfiability is MP complete. We argued directly that arbitrary MP can return into the satisfiability problem with efficient reduction. Of course, it is possible to make such a direct argument for every MP complete problem. But usually this is not necessary. Once we have one MP complete problem, we can simply reduce it to other problems to show that they, too are MP complete. In this lesson we use this strategy to prove that a few more problems are MP complete. The goal is to give you a sense of the breathe of the class of MP complete problems, and introduce some of the techniques used in those reductions. Here's the state of our knowledge about NP-complete problems, given just what we proved in the last lesson. We've shown that we can take an arbitrary problem in NP and reduce it to CNF satisfied ability. That's what the Cook-Levin theorem said. We did do another polynomial reduction, one of the independent set or clique problem to the vertex cover problem. Again, I'm treating Clique and Independent Set here as one because the problems are so similar. Much of the lesson will be concerned with connecting up these two links into a chain. First, we're going to reduce general CNF SAT to 3-CNF SAT. Where each clause has exactly three literals. This is a critical reduction because 3 SAT is much easier to reduce to other problems than general CNF SAT. Then we're going to reduce 3-CNF to independent set and by transitivity this will show that this whole chain here is NP complete. Note that this is very convenient because it would have been pretty messy to try to reduce every problem to NP to these problems directly. Finally, we will reduce 3-CNF SAT to the Subset Sum problem to give you a fuller sense of the types of arguments that go into reductions. Our first reduction is of CNF SAT to three CNF SAT. Before we begin however I want you to help me articulate what we're trying to do. So here's a question. Choose from the options at the bottom and enter the corresponding letter in here so that this description matches the reduction we are trying to achieve. The first part of the answer is that we're trying to map a CNF formula into a 3-CNF one. We're trying to show that 3-CNF is as hard as CNF. So that if we could solve 3-CNF, we would solve CNF as well. The such that part is a little subtle. It's important to realize that the 3-CNF formula we get doesn't have to be logically equivalent to the general CNF formula that we put in. As we'll see, it won't even make sense to talk about having the same truth assignment satisfy both formulas, because they'll have different sets of variables. Here's our overall strategy. We're going to take a CNF formula f, and turn it into a 3 cnf formula f prime, which will include some additional variables, which we'll label y. And this formula f prime will have the property, that, for any truth assignment t for f. T will satisfy f. If and only if there exists a way of extending t, we'll call this ty, over the variables Y, such that t extended by ty satisfies the new clause f prime. Let's illustrate such a mapping for a simple example. Take this disjunction of four literals. Note that the zi are literals here, so they could be x1, x1 bar, etc. Remember that disjunction means logical or, so this whole clause is true if any one of the literals is. We will map this clause of size four into two clauses of size three by introducing a new variable y, like so. Let's confirm that this property holds. If the original clause is true under some truth assignment, then we can set y to be Z1 or Z2. This clause will be satisfied by one of the Zs and this one will be satisfied by Y. Going the other direction, suppose that we have a truth assignment that satisfies both of these clauses. On the one hand, Y might be true. But that then implies that Z1 or Z2 is true, and so this clause must be true. On the other hand, Y might be false, but that then implies that Z3 or Z4 is true, and hence this original clause would be true as well. If you've understood this, then you've understood the crux of why this reduction works. More details to come. We're going to start by looking at how to convert just one clause from CNF to 3-CNF. This is going to be the heart of the argument because we're going to be able to apply this procedure to each clause of the formula independently. So consider this one clause consisting of literals Z1 through ZK. We're going to have three cases depending on the number of literals in each clause. Note that I have left off K equals 3 since nothing needs to be done in that case. When there are two literals in the clause, the situation is easier to understand. We just need to boost up the number of literals to three. To do this we include a single extra variable and trade in the clause z1 or z2 for these two clauses here. One with the literal y1, and one with the literal y1 nod. If in a truth assignment, z1 or z2 is satisfied, then clearly both these have to be true. On the other hand, taking a satisfying assignment from these clauses here, the Y literal has to be false in one of these two clauses. And so therefore z1 or z2 must be true. We can play the same trick when there's just one literal, z1 in the original clauses. This time we need to introduce two new variables which we'll call y1 and y2. Then we replace the original clause with these four. Note that if z1 is true, then all four of these clauses are true as well. On the other hand, for any given truth assignment for z1, y1 and y2, that satisfies these four clauses, the y literals have to both be false in one of these and that then implies that z1 is true. Lastly we have the case where K is greater than three. Here we introduce K minus three variables and use them to break up the clauses according to this pattern. Let's illustrate this idea with an example. Consider this sixth literal clause here. Looking at this, we might wish that we could have a single variable that captured whether any of these first four literals were true. Let's call this variable y3. Then we can express this overall clause with just three of the literals, y3, or z5 or z6. At first I said that we wanted y3 to be equal to z1 or z2 or z3 or z4. But actually it's efficient and considerably less trouble for y3 just to imply the truth of one of those literals. This idea is easy to express as a clause. Either y3 is false, in which case the implication doesn't apply, or one of z1 through z4 better be true. Together, these two clauses imply the first one. If y3 is true, then y3 bar is false, and so one of the z1 through z4 must be true, and so must the original clause. On the other hand, if y3 is false, then z5 or z6 must be true, and so must the original clause. Also, if the original clause is satisfied, then we can set Y3 equal to the disjunction of z1 through z4, and these two clauses will be satisfied as well. Note that we went from having a longest clause length of six to having one of length 5 and another of length 3. We can apply this strategy again to the clause that's too long, by introducing another variable y2. And trade this long clause in for a shorter clause, this time one of length 4 and one of length 3. Of course we can play this trick again. And eventually we have just three literal clauses, this example is for k equals 6 but an inductive argument will show that the argument works for any k greater than 3. Here's an exercise intended to help you solidify your understanding of the relationship between the original clause and the set of three clauses that the transformation produces. I want you to extend the truths table below so that it satisfies the given 3CNF formula. This one is the image of this clause under the transformation that we've discussed. The truth assignment that I used is false, true, true. I used the rule that yi is equal to the disjunction of the first i plus one literals, and any satisfying assignment yi being true implies this disjunction. But yi can sometimes be false, even when this expression is true. For example, another way to extend this assignment is to let y2 be false. Regardless of how we set y2 this clause here is still satisfied by x3, and this clause here is still satisfied by x4 bar. Thus, there isn't a unique way to extend the original assignment, but it is true that this way of extending it always works. Recall that our original purpose was the find a transformation of the formula, such that a truth assignment satisfied the original, if and only if it could be extended to the transformed formula. And we've seen how to do this transformation for a single clause. But actually, this is enough. We can just transform each clause individually. Introducing a new set of variables for each. All the same arguments about extending or restricting the truth assignment will hold. Let's illustrate what a transformation of a multi clause formula looks like with an example. Consider this formula here, where I've indexed the literal Z with two indices now. The first, referring to the clause it's in, and the second being it's enumeration within the clause. The first clause only has two literals. So we transform it into these two clauses with three literals by introducing a new variable, y11. It gets the first one because it was generated by the first clause. We transform the second clause with five literals into these three clauses, introducing two new variables y21, and y22. Note that these are different from the variables used in the clauses generated by the first original clause. Since all these sets of variables are disjoint, we can assign them independently of each other and apply all the same arguments as we did to individual clauses. That's how CNF can be reduced to 3-CNF. And of course, this transformation runs in polynomial time, making the reduction polynomial. We just reduced the problem of finding a satisfying assignment to a general CNF formula, to the problem of finding a satisfying assignment to a 3-CNF formula. At this point, it's natural to ask, can we go any further? Can we reduce this problem to a 2-CNF? Well, no, not unless p is equal to np. There is a polynomial time algorithm for finding a satisfying assignment to a 2-CNF formula, based of finding strongly connected components in a graph. Therefore, if we could reduce 3-CNF to 2-CNF, then p would be equal to np. So, 3-CNF is as simple as the satisfiability question gets. And it admits some very clean and easy to visualize reductions that allow us to show that other problems are np complete. We'll go over two of these. First, the reduction to independent set, or clique, and then to subset sum. At the beginning of the lesson, we promised that we would link up these two chains and use the transitivity of reductions to show that vertex cover and independent set are NP-complete. We now turn to the last link in this chain, and we'll reduce 3-CNF SAT to independent set. As we've already argued these problems are N and P so that will complete the proof that they are NP-complete. Here's the transformation, or reduction, we need to accomplish. The input is a 3-CNF formula, with M clauses and we need to output a graph that has an independent set of size m if and only if the input is satisfiable. We'll illustrate on this example. For each literal in the formula we create a vertex in a graph. Then, we add edges between all vertices that came from the same clause. We'll refer to these as within clause edges or simply the clause edges. Then we add edges between all literals that are contradictory, b and b bar can't both be true for instance. We refer to these as the between clause edges or the contradiction edges. The implementation of this transformation is simple enough to imagine, and I'll leave it to you to convince yourself that it can be done in polynomial time Here's a question to make sure that you understand the reduction just given. Consider the formula below and the associated graph. Indicate the edges that would not have been output by the transformation just described. The answer is that these two edges don't belong. All of the other edges are either within clause edges, which we always include, or they connect contradictory literals, a and a bar, b and b bar, etc. These two edges connect literals that are the same, and we don't want to include these edges for reasons that become clear with an understanding of the proof. Next we're going to prove that the transformation just described does in fact reduce 3-CNF satisfiability to independent set. We start by arguing that if f is satisfiable then the output graph G has an independent set of size m. We'll let t be a satisfying assignment for f. In our example, let's take the one that makes a true, b false, c false, and d false, and we'll set the complements accordingly. Then, we choose one literal from each clause in our original formula that is true under the truth assignment, and this will form our set S. So in our example, I might choose this a, this b bar and this b bar here. Clearly the size of this set is m, the number of clauses. Because the vertices come from distinct clauses, there can't be any within clause edges and because the truth assignment t doesn't contradict itself, there can't be any contradiction or between clause edges either, therefore S must be an independent set. And indeed it is in this example. Let's prove the other direction next. If G has an independent set of size m, then the formula f, is satisfiable. We start with an independent set of size m in the graph. Here I've marked an independent set in our example graph. The fact that there can be no between-clause edges in S implies that the literals in S come from distinct clauses. And the fact that there are no between-clause edges implies that the literals in S are non-contradictory. Therefore any assignment consistent with the literals of S will satisfy the original formula f. Here the choice of literals a, b and c implies that a, b and c all be true. We can set d however we like, true or false, and we still have a satisfying assignment. So that completes the proof that independent set is as hard as 3-CNF. And that completes this chain here, showing that both independent set and Vertex Cover are NP-Complete. Now we're going to branch out, both in the tree here and in the type of arguments we'll make, by considering the Subset Sum problem. Before reducing 3-CNF to subset sum, we have to define the subset sum problem first. We're given a multi set of numbers, A, consisting of numbers a1 through am, where each element is a positive integer. A multiset by the way, is just a set that allows the same element to be included more than once. So this set might include three 5's, two 20s, and just one 31, for example. And we're also given a number k. The problem is to decide whether there is a subset of this multiset, whose members add up to k. One instance of this problem is partitioning. And I'll use this particular example to illustrate. Imagine that you want to divide assets evenly between two parties. Maybe we're picking teams on the playground, trying to reach a divorce settlement, dividing spoils among the victors of war, or something of that nature. Then the question becomes, is there a way to partition a set so that things come out even? Each side getting exactly one half. In this case the total is 18. So we should choose k to be equal to 9 and ask if there is a way to get a subset to sum to 9. And indeed there is. We can choose two 1s and a 7, and that will give us 9. So, yes. So would 4 and 5, of course. That's the subset sum problem. Note that the problem isn't NP because given any subset, it takes polynomial time to add up the numbers and see if the sum equals k. Finding the right subset, however, seems much harder. We don't know of a way to do it that's necessarily faster than just trying all two to the m subsets. We're going to show that subset sum is np complete. But, here is a simple algorithm that solves it. w is a two dimensional array of bulions, and wij indicates whether there's a subset of the first i elements that sums to j. There are only two ways that this can be true. Either there's a way to sum to j using the elements one through i-1, or there's a way to sum to j-ai, using elements 1 through i-1, and then ai brings the total up to j. For now, I just want you to give the highest valid bound on the running time of this algorithm on a random access machine. The answer is that the running time is order M times K. There are no tricks about that. These are simple iterated for loops. Now at first this bound might seem to suggest that subset sum is in P. But no, this number K takes only log K bits to represent. So this number K is exponential in the input length. And therefore this is an exponential algorithm. Because it's exponential nature comes from something that is a simple parameter to the problem, like K, algorithms of this sort are sometimes called pseudo polynomial. As we'll see however, subset sum is still in P complete because we can reduce three. Here's the reduction of 3 CNF SAT to SUBSET-SUM. I'm going to illustrate the reduction with an example, because writing out the transformation in it's full generality can get a little messy. Even with just an example, it may not be clear why I'm making some of the intermediate steps at the time, it should all become clear at the end, however. We're going to create a table with columns for the variables, and columns for the clauses. The rows of the table are going to be the numbers in our subset. We'll have this column represent the one's place, this one the ten's place, and so forth. The collection of numbers will consist of two for every variable in the formula. One that we include when Xi is true. We'll notate that with Yi, and one that include when Xi bar is true. We'll notate that with Zi. In the end, we want to include either Yi or Zi for every i since we have to assign the variable Xi one way or another. To get a satisfying assignment we also have to satisfy each clause. So we want the numbers Yi and Zi to reflect which clauses are satisfied, as well. Here then, is how we fill out the table. Y1 sets X1 to be true. So we put a 1 in that column, to indicate that choosing Y1 means assigning X1. And this also satisfies clauses 1 and 3 since that's where X1 appears. Including Z1 we'd also assign the variable X1 a value, but it wouldn't satisfy any of the clauses. So all these are written as 0s. We do the analogous procedure for Y2 and Z2. The literal X2 appears in the first clause, and the literal X2 bar appears in clauses 2 and 3. And we do the same for the variables X3 and X4. These then are the numbers that we want to include in our set A. Now, we need to choose our desired total k. For each of the variable clauses, the desired total is exactly 1. We assign each variable one way or the other. For these clause columns however, the total just has to be greater than 0. We just need one literal in the clause to be true in order for the clause to be satisfied. But that doesn't give me a specific k that we need to sum to. The trick is to add more numbers to the table. These all have 0s in the place corresponding to the variables. In exactly 1,1 in a column for our clause. Each clause j gets 2 numbers that have a 1 in that column. We'll call them gj and hj, so thus, g2 and h2 have a 1 in the column for c2, and 0s everywhere else. This allows us to set the desired number to be 3 in the clause columns. Given a satisfying assignment, the corresponding choice of Y and Z numbers will have at least a 1 in all the clause columns, but not more than 3. All the 1s and 2s can be boosted up by including the g and h numbers. Note that if some clause is unsatisfied, then including the g and h numbers isn't enough to bring the total to 3. Because they're only two of them. So there's the construction. For each variable include one of the two variables, Y or Z, which will correspond to the truth setting. And for each clause include g and h, so as to boost up this total in the clause column to 3 where needed. This construction just involves a few four loops. So it's easy to see that the construction of the set of numbers is polynomial time. Next, we prove that this reduction works. Let f be a 3CNF formula with n variables and m clauses. And we'll let A and k be the result of the transformation. First, we show that if f is satisfiable, then there's a subset of A summing to k. Let t be the satisfying assignment. Then we include yi in our set S, if xi is true under the truth assignment, and we include zi otherwise. As for the g and h families, if fewer than three literals of a clause are satisfied, then include g. If fewer than 2, then include h as well. In total, the sum of these numbers must be equal to k. In the other direction, we argue that if there is a subset of A summing to k, then there's a satisfying assignment. Suppose that S is a subset of A summing to k. Then the impossibility of carrying the digits in any sum implies that exactly one of yi or zi must have been set to true. Therefore, we can define a truth assignment, t where t of xi is true if yi is included, and it's false otherwise. This must satisfy every clause, otherwise there would be no way that the total in the clause places could be 3. All together, we've seen that subset-Si, is an NP. And we can reduce 3-CNFSAT, an NP-complete problem to it. So Subset-Sum must be NP-complete. Here's an exercise to encourage you to carefully consider the reduction just given. We interpreted the rows of the table we constructed as the base ten representation of the collection of numbers that the reduction output. What other bases would have worked as well? Check all that apply. The answer is that base 6, and base 16 would've worked as well. The key is to avoid carrying, so that the effects of including a number on one column, never spill over into another. 2 and 3 won't work. 6 and 16 on the other hand do work, since there is a total of at most five in any column. Actually, base four and base five work as well, thought the argument is a little more complicated. I'll leave it to you to work out as an exercise. So far, we've built up a healthy collection of NP-complete problems. But given that there are thousands of known NP-complete problems, we've only scratched the surface. In fact, we haven't even come close to Karp's mark of 21 problems from his 1972 paper. If you want to go on and extend the set of problems that you can prove to be NP-complete, you might consider reducing subset sum to the knapsack problem. Where one has a fixed capacity for carrying stuff, and wants to pack the largest value of subset items that will fit. Another classic problem is that of the traveling salesman. He has a list of cities that he wants to visit, and he wants to know the order that will minimize the distance that he has to travel. One can prove that this problem is NP-complete by first reducing vertex cover to the Hamiltonian Cycle problem, which asks if there's a cycle in the graph that visits each vertex exactly once. And then Hamiltonian Cycle to traveling salesman. Another classic problem is 3D matching. 2D matching can be thought of as the problem of making as many compatible couples as possible from a set of people. 3D matching extends this problem further. By matching them with a home that they would enjoy living in together, and of course, there are many others. The point of this lesson, however, is not so that you can produce the needed chain of reductions for every problem known to be NP-complete. Rather it is to give you a sense for what these arguments look like, and how you might go about making such an argument for a problem that is of particular interest to you. The reductions we've given as examples are a fine start. But if you want to go further in understanding how to use complexity to understand real-world problems, take a look at the classic text by Garey and Johnson on Computers and Intractability. Even though it's from the same decade as the original Cook-Levin result, and Karp's list of 21 NP-complete problems, it's still one of the best. As Lance says, a good computer scientist shouldn't leave home without it. Now that you have a better understanding of the nature and scope of the P versus NP question, we'll return to high-level in the next lesson. The next question's like, what would happen if P were equal to NP? What good might we be able to do? And in a world where P is not equal to NP, what are the hopes for coping with the NP-completeness of problems that we would really like to solve? So far in this course, we've answered the question, what is computable? We modeled computability by Turing machines, and showed that some problems, like the halting problem, cannot be computed at all. In the next few lectures we ask, what can we compute quickly? Some problems, like adding a bunch of numbers or solving linear equations, we know how to solve quickly on computers. But how about playing the perfect game of chess? We could write a computer program that can search the whole game tree, but that computation won't finish in our lifetime or even a lifetime of the universe. Unlike computability, we don't have clean theorems about efficient computation, but we can explore what we probably can't solve quickly through what is known as the P versus NP problem. P represents the problems we can solve quickly like your GPS finding a short route to your destination. NP represents problems where we can check that a solution is correct such as solving a Sudoku puzzle. In this lesson we will learn about P NP and a role in helping us understand what we may or may not be able to solve quickly. We'll illustrate the distinction between P and NP by trying to analyze a world of friends and enemies. By the way, Lance gets credit for this example. His daughters were the right age for the Twilight movie when it came out. Everyone in this world is either a friend or an enemy. And we'll represent this by drawing an edge between all the friends like so. Given all this information, there are several types of analysis that you might want to do. Some easier and some harder. For instance, if you wanted to run a dating service, you'd be in pretty good shape. Say that you wanted to maximize the number of matches that you could make and enhance the number of happy customers. Or perhaps you just want to know if it's possible to get everyone a date. Well, we have efficient algorithms for finding matching, and we'll see some in a future lesson. Here in this example, it's possible to match everyone up, and such a matching is fairly easy to find. Contrast this with the problem of identifying cliques. By clique I mean a set of people who are all friends with each other. For instance, here is a clique of size three. Every pair of members has an edge between. And cliques of that size aren't too hard to find. As we look for larger cliques, however, the problem becomes harder and harder to solve. In fact, finding a clique of size 4, even for a relatively small graph like this one, isn't necessarily easy. See if you can find one. Here's the clique. Now I had a hard time finding the clique, the first time that I looked for it. But once it was pointed out to me, it was easy to confirm that it was indeed a clique. I just had to check that there was an edge between every pair of it's members. And this is precisely the point. Some problems, like matching for the dating service, are easy to solve. Others, like this clique problem, are hard to solve, but easy to check. Being a little more formal now, we define P to be the set of problems solvable in polynomial time. By polynomial time, we mean that the number of Turing machine steps is bounded by a polynomial. We'll formalize this more in a moment. Bipartite matching was one example of this class of problems. NP we define as the class of problems verifiable in polynomial time. This includes everything in P, since if a problem can be solved in polynomial time, a potential solution can be verified in that time too. Most computer scientists strongly believe that this containment is strict, that is to say, there are some problems that are efficiently verifiable, but not efficiently solvable. But we don't have a proof for this yet. The clique problem that we considered is one of the problems that belongs in NP, but we think does not belong in P. There's one more class of problems that we'll talk about in this section on complexity, and that is the set of NP complete problems. These are the hardest problems in NP, and we call them the hardest because any problem in NP we can efficiently transform into an NP complete problem. Therefore, if someone were to come up with an polynomial algorithm for even one NP complete problem, then P would expand out in this diagram, making P and NP into the same class. Finding a polynomial solution for clique would do this. So we say that clique is NP complete. Since solving problems doesn't seem to be as easy as checking answers to problems, we're pretty sure that NP complete problems can't be solved in polynomial time. And therefore, that P does not equal NP. To computer science novices, the difference between matching and clique might not seem to be a big deal. And, it's surprising that one is so much harder than the other. In fact, the difference between the polynomial solvable algorithm and an NP complete one, can be very subtle. Being able to tell the difference is an important skill for everyone who'll be designing algorithms for the real world. As we discussed, one way to see the subtlety of the difference between problems in P and those that are in NP-complete, is to compare what it takes to solve seemingly similar real world problems. Consider the shortest path problem. You're given two locations, and you want to find the shortest valid route between them. Your phone does this in a matter of milliseconds, when you ask it for directions. And it gives you an exact answer, according to whatever model for distance it is using. This is computationally tractable. On the other hand consider this warehouse scenario where a customer places an order for several different items. And a person or a robot has to go around and collect them before going to the shipping area for them to be packed. Of course, we want to do this as quickly as possible. This is called the traveling salesman problem. And it is NP-complete. This problem also comes up in the unofficial guide to Disney World which tries to tell you how to get to all the rides as quickly as possible. This explains why your phone can give you directions, but supply chain logistics, just figuring out how things should be routed is a billion dollar industry. Actually however we don't even need to change the shortest path so much to get a NP-complete problem. Instead of asking for the shortest path, we can ask for the longest simple path. We have to say simple so we don't just run around in cycles forever. That also would give us an NP-complete problem. This isn't the only possible pairing of similar P and NP-complete problems either. I'm going to list some more. If you aren't familiar with these problems yet, don't worry, you will learn about them by the end of the course. Vertex cover in bipartite graphs is polynomial, but vertex cover in general graphs is NP-complete. A class of optimization problems called linear programming is NP, but if we restrict the solutions to integers, then we get and NP-complete problem. Finding an Eulerian cycle in the graph, where you touch each edge once is polynomial. On the other hand, finding a Hamiltonian cycle that touches each vertex once is NP-complete. And lastly figuring out whether a boolean formula with two literals per clause is polynomial, but if there are three literals per clause, then the problem is NP-complete. So you see, problems in P aren't always easy to tell from those that are NP-complete. Yet in the real world, when you encounter a problem, it's very important to know which sort you are dealing with. If your problem is like one of the problems in P, then you know that there should be an efficient solution. And, you can avail yourself of the wisdom of many other scientists who have thought hard about how to efficiently solve these problems. On the other hand, if your problem is as hard as one of the NP-complete problems, then some caution is in order. You can expect to be able to find exact solutions for small enough instances. And you may be able to find a polynomial algorithm that will give you an approximate solution that is good enough, but you should not expect to find an exact solution that will scale well. Being able to know which situation you are in is one of the many practical benefits of studying complexity. Now we're going to drill down into the details and make many of the notions we've been talking about more precise. First, we need to define running time. We'll let M be a Turing machine. Single tape, multi tape, random access, the definition works for all of them and we assume that it halts on all inputs. The running time then is a function over the natural numbers where F of N is the largest number of steps taken by a machine for an input string length of n. We can extend this definition to machines that don't halt as well, by making their runtime infinite. We're always considering the worst case here. Let's illustrate this idea with an example. Consider this single tape machine that takes in a binary input. And tests whether the input contains a 1. On encountering a 0, it simply loops back into this q0 state and moves the head to the right. When it encounters a 1, it accepts and halts immediately and if it encounters the blank symbol, indicating the end of the input without ever having encountered a 1, it rejects. Let's figure out the running time for a string of length 2. We need to consider all possible strings of length 2. So we'll make a table like this and count the number of steps. The largest number of steps is 3. We had to read the 0, the 0 and the blank after it. And so, that then is the overall running time. F(2) = 3. Now it's not very practical to write down every running time function exactly, so computer scientists use various levels of approximation. For complexity, we use asymptotic analysis. I'm going to do a very brief review here, but if you haven't seen this idea before, you should take a few minutes to study it on your own before proceeding with the lesson. We define big O of a function F to be the set of functions G where there are numbers C and big N such that for all little N, at least as big as big N, G of N is at most C times F of N. In other words, C times F of N is at least G of N for all sufficiently large N. Even though we've defined big O as a set, we write g(n) = O (f(n)) instead of using the inclusion sign. We also say that G is order F. This definition can be a little confusing. But it should feel like the definition of a limit from your calculus class. In fact, we can re-state this condition to say that the ratio of G over F converges to a constant under the lim soup. An example also, helps. Take this function, g of n is equal to n squared minus n plus ten. We can argue that this is order n squared, by choosing c is equal to one. And big N equal to ten. For every little n greater than ten, N squared, this red line here, is bigger than the function g, the blue line. We also could have chosen c equal to ten and big N equal to one again in that case. The red line, the c times n squared, is bigger than the function g, the blue line. Note that the big O notation does not have to create a tight bound. Thus, g(n) is also order n squared, setting c equal to 1 and N equal to 3 works for this. Once we've established the running time for an algorithm, we can analyze other algorithms that use it as a subroutine much more easily. Consider this question. Suppose that an algorithm A has running time of order n. And suppose that algorithm B calls algorithm A logn times. And then algorithm B spends an additional order og squared time afterwards. And my question to you is. What is the tightest bound that we can put on the running time of B? The answer here is order n logn. The order n procedure a is called logn times by b. So that takes n times logn time. Then, we do log squared n work. But this term is dominated by the other term. Hence, only the first term matters. If this sort of analysis is not familiar to you, then it would probably be a good idea to review these ideas in one of the classic algorithms textbooks, like Cormen, Leiserson, Rivest, and Stein. We are now ready to formally define the Class P. Most precisely, P is the set of languages recognized by an order n to the k deterministic Turing machine for some natural number k. There are several important things to note about this definition. First, is that P is a set of languages. Intuitively, we talk about P as a set of problems, but to be rigorous, we have to ultimately define it in terms of languages. A second point is the word deterministic. We haven't seen a nondeterministic Turing machine yet, but one is coming. Deterministic just means that given a current state and tape symbol being read there's only one transition for the machine to follow, but in order n to the k time Turing machine here I mean, a Turing machine with running time order n to the k as we defined the terms a minute ago. Perhaps the most interesting thing about this definition is the choice of for any k in the natural numbers. Why is this the right definition? After all, if k is 100, then deciding the language isn't tractable in practice. The answer is that, the notion of the class P doesn't exactly capture what is tractable in practice. It's not clear that any mathematical definition would stand the test of time in this regard, given how often computers change. Or how it could be relevant in so many context. This choice however does have some very nice properties. First, is that it matches tractability better than one might think. In practice k is usually low for polynomial algorithms and there are plenty of interesting problems not known to be in P. Second, is that the definition is robust to changes in the model. That is to say P is the same for single tape, multitape machines, random access machines, and so forth. In fact, we pointed out that the running times for each of those models are polynomially related when we introduced them. Lastly, P has a nice property of closure under the composition of algorithms. If one algorithm calls another algorithm as a subroutine a polynomial number of times, and that algorithm is still polynomial and the problem it solves is in P. In other words, if we do something efficient a reasonably small number of times then the overall solution will be efficient. P is exactly the smallest class of problems containing linear time algorithms and which is closed under composition. We've defined P as a set of languages, but ultimately we want to talk about it as a set of problems. Unfortunately, this isn't as easy as it might seem. The encoding rules we use for turning an abstract problem into a string can effect whether or not the language is in P. Let's see how this might happen. Consider the question, does a graph, G, have a Hamiltonian cycle? That is, a cycle that visits all the vertices exactly once. Here's a graph, and here's it's adjacency matrix. A natural way to represent the graph as a string, is to write out it's adjacency matrix in scan line order. First a 0, then a 1, then a 0. Then a 1, then a 0, then a 1. Then a 0, then a 1, then a 0. But this isn't the only way to encode the graph. We might do something rather inefficient. The scan line encoding for this graph represents the number 170 in binary. We could choose to represent the graph essentially in unary. We might represent the graph as 342 zeros, followed by 170 ones. The fact that there are two to the ninth equals five 12 symbols total indicates that it's a three by three adjacency matrix. And converting the 170 back into binary gives us the entries of the adjacency matrix. This is a very silly encoding, but there's nothing invalid about it. This language, it turns out, is in P. Not because it allows the algorithm to exploit any extra information, or anything like that, but just because the input is so long. The more sensible, concise encoding, isn't known to be in P. And it probably isn't, by an overwhelming consensus of complexity theorists. Thus a change in the encoding, can affect whether a problem is in P. Yet it's ultimately problems that we're interested in, independent of the particulars of the encoding. We deal with this problem essentially by ignoring unreasonable representations like this one. As long as we consider any reasonable encoding, think about what XML or JSON would produce from how you would store it in computer memory. Then the particulars won't change the membership of a language in P, and hence we can talk, at least informally, about problems being NP or not. Now that we have to find P, I want to illustrate how easy it can be to recognize that a problem is in P. Recognizing that a problem is not in P is a little harder. So for this exercise, assume that if the group force algorithm is exponential, so is the best algorithm. Consider a finite set of integers written in binary. And I want you to check the problems here that you would think are in P. Again, if there isn't an obvious polynomial-time algorithm, assume there isn't one. The answer is the first two. Does U contain a number greater than 100? Well just scan through. If you find one, it does. If not, there wasn't one. This can be solved with a linear time algorithm. Do any three elements sum to zero? Well, we can just try all and choose three possibilities. This is only cubic in the number of input numbers, so this brute force algorithm is polynomial. I should say that there happen to be faster algorithms as well. And lastly, does any subset of U sum to zero? Here, the brute force strategy is to try all subsets of U, and this is exponential in the size and input. Indeed, this problem is NP complete. >From the class P we now turn to the class NP. At the beginning of the lesson I said that NP is the class of problems variable in polynomial time. This is true but it's not how we typically define it. Instead, we define NP as the class of problems solvable in polynomial time on a nondeterministic Turing machine, a variant that we haven't encountered before. Nondeterminism in computer science is often misunderstood. So put aside whatever associations you might have had with the word. Perhaps the best way to understand nondeterministic Turing machines is by contrasting a nondeterministic computation with a deterministic one. A deterministic computation starts in some initial state, and then the next state is exactly and uniquely determined by the transition function. There's only one possible successor configuration. Into that configuration, there's only one possible successor, and so on, and so forth, until an accepting or rejecting configuration is reached, if one is reached at all. On the other hand, in a nondeterministic computation, we start in a single initial configuration, but it's possible for there to be multiple successor configurations. In effect, the machine is able to explore multiple possibilities at once. This potential splitting continues at every step. Sometimes, there might just be one possible successor state. Sometimes there might be three or more. For each press, we have all the same possibilities as for the deterministic machine. The machine can reject, it can loop forever, or it can accept. If the machine ever accepts in any of these branches, then the whole machine accepts. The only change to the seven triple definition of the deterministic machine that we need to make is to modify the transition function. The range is no longer a single state, tape symbol being red, direction tuple, but a whole collection of such possibilities. The set of all subsets is often called a power set. The only other change we need to make is when the machine accepts. It accepts if there is any valid sequence of configuration that results in an accepting state. Naturally, it also rejects only when every branch reaches a reject state. If there's a branch that hasn't rejected yet, then we need to keep computing in case it accepts. Therefore, a nondeterministic machine that never accepts and that loops on at least one branch, will loop. Here's the state transition diagram for a simple non-deterministic Turing machine. The machine starts out in Q zero, and then it can move the head to the right upon reading a zero or a one. Or, upon reading a one, it can transition to state Q one, again moving the head to the right. This is the non-deterministic part. For a given state and symbol red, there's more than one possible transition to follow. In branches where this transition is followed, the machine reads one more zero one and then expects to hit the end of the input. Remember that by convention if there's not transition specified in one of these state diagrams, then the machine simply moves to the reject state and halts. That keeps the diagrams from getting cluttered. My question to you, then, is what language does this machine recognize? Check the appropriate answer. The answer is this language. {0,1}*1 {0,1}. Remember, that in order for the machine to accept the string, it just has to accept it in any branch of it's computation. This self loop, on the initial state, corresponds to the initial {0,1}*. This transition can happen for the whole length of the string, or it can happen none at all. In any excepting computation however, it has to stop exactly two symbols before the end. So that a 1, and then either a 0 or a 1, can be read just before the end of the input is reached. To get more intuition for the power of nondeterminism, let's see how much more efficient it makes deciding the language of composite numbers that is numbers that are not prime. The task is to decide the set of string representations of numbers that are the product of two positive numbers greater than one. One deterministic solution looks like this. Think of the flow diagram as capturing various modules within the deterministic Turing machine. We start by initializing some number p to 1, then we increment it and test whether P square is greater than x. If it is, then trying larger values of p won't help us and we can reject. If p squared is no larger than x, however, then we test to see if p divides x. If it does, then we accept. If not, then we go back and try the next value for p. Each iteration of this loop requires a number of steps that is polynomial in the number of bits used to represent x, not so bad. The trouble is that we might end up needing square root of x iterations of this outer loop here in order to find the right p or to confirm that one doesn't exist. This is what makes the deterministic algorithm slow. Since the value of x is exponential in its input size, remember that it's represented in binary, the deterministic algorithm is exponential. On the other hand, with nondeterminism, we can do much better. We initialize p, so that's it's represented on its own tape as the number one written in binary. We then nondeterministically modify p by having two possible transitions for the same state symbol pair. We can nondeterministically append a bit to p. By the way, I'll mark all the nondeterministic transitions in orange here. Next, we check to see if we've made p too large. If we did, then there's no point in continuing, so we reject. On the other hand, if p is not too big, then we nondeterministically decide either to append a zero to p, append a one to p or to leave p as it is and go see if it divides x. If there's some p that divides x, then some branch of computation will set p accordingly. That branch will accept and so the whole machine will. On the other hand, if no such p exists, then no branch will accept and the machine won't either. In fact, the machine will always reject, because every branch of computation will be rejected in one of these two places. This nondeterministic strategy is faster, because it only requires log x iterations of this outer loop. The divisor p is set one bit at a time and can't use more bits that x, the number it's supposed to divide. Thus, while the deterministic algorithm we came up with exponential in its input length, it was fairly easy to come up with a nondeterministic one that was polynomial We are almost ready to define the class NP. First, however, we need to define running time for a non-deterministic machine, because it operates differently from a deterministic one. Since we think about these possible computations running in parallel, the running time for each computation path is the path length from the initial configuration. And the running time of the machine of the whole, is the maximum number of steps used on any branch of the computation. Note that once we have a bound on the length of any accepting configuration sequence, we can avoid looping just by creating a time out. NP, then, is the set of languages recognized by an order n to the k nondeterministic Turing machine, where k is some natural number. Or, in other words, it's the set of languages recognized in polynomial time by a non-deterministic machine. NP stands for non-deterministic polynomial time. Non-determinism can be a little confusing, but it helps to remember that a string is recognized if it leads to any accepting computation, ie, any accepting path in this tree. Not that any Turing Machine that is a polynomial recognizer for a language, can easily be turned into a polynomial decider by adding a timeout, since all accepting computations are bounded in length by a polynomial. At the beginning of the lesson we identified NP as those problems for which an answer can be verified in polynomial time. Remember how easy it was to check that a clique was a clique? In the more formal treatment however, we identified NP as those languages recognized by non-deterministic Turing machines in polynomial time. Now we get to see why these mean the same thing. To get some intuition, we'll revisit the example of finding a clique of size four. We already discussed how a clique of size four is easy to verify. But how easy is it to find with a non-deterministic machine? The key is to use the non-determinism to create a branch of computation for each subset of size four. And then use our verification strategy to decide whether any of those subsets correspond to a clique. Remember, that if any sequence of configuration accepts, then the non-deterministic machine accepts. One branch of computation might choose these four, and then reject, because it's not a clique. Another branch of computation might choose these four, and also reject. But one branch will choose the correct subset, and this will accept. And that's all we need. If one branch accepts, then the whole non-deterministic machine does as it should. There is this clique of size four here. Now for the more formal argument. A verifier for language L is a deterministic Turing machine where L is the set of strings w for which there is another string c such that V accepts the pair w, c. In other words, for every w in the language, there's a certificate c that can be paired with w, so that V will accept and for every w not in the language L, there's no such c. It's intuitive to think of w as a statement and of c as a proof. If the statement is true, then there should be a proof for it that the verifier V can check. On the other hand, if w is false, then no proof should be able to convince the verifier that it's true. A verifier is polynomial, if its running time is bounded by a polynomial in the length of the string w. Note that this w is the same as the one in the definition here, it's the string that's a candidate for the language. If we included the certificate in the bound, then it becomes meaningless since we could make c as long as necessary. So that's a polynomial verifier and we claim that the set of languages that have polynomial time verifiers is the same as NP. The key to understanding this connection is once again, this picture of the tree of computation performed by the nondeterministic machine. If a language is in NP, then there's some nondeterministic machine that recognizes it. Meaning that for every string in the language, there's an accepting computation path. Now a verifier can't simulate the whole tree of the nondeterministic machine in polynomial time, but it can simulate a single path. It just needs to know which path to simulate, but this is what the certificate can tell it. The certificate can act as directions for which turns to make in order to find the accepting computation of the nondeterministic machine. Hence, if there's a nondeterministic machine that can recognize the language, there's a verifier that can verify it. Now, argue in the other direction. Suppose that V verifies a language, then we can build a nondeterministic machine whose computation tree will look a bit like a jellyfish. At its very top, we'll have a high degree of branching as the machine nondeterministically appends a certificate c to its input, then it just deterministically simulates the verifier. If there's any certificate that causes V to accept, the nondeterministic machine will find it. If there isn't one, then the nondeterministic machine won't. Now that we've defined NP, and defined what it means to be verifiable in polynomial time, I want you to apply this knowledge to decide if several problems are in NP. First, is a given graph connected? Second, does a graph have a set of K vertices with no edges between? This is called the independent set problem. And lastly, will a Turing Machine accept exactly one string? The answer is that the first two are in NP. Graph connectivity can be solved in polynomial time via depth-first search, no certificate necessary. This problem is in P, so it's definitely in NP. Does a graph have a set of k vertices with no edges between? Well, given the right k vertices, we can verify this by checking order k squared edges in the graph. The certificate just needs to give us this information. This last problem is not decidable, as we've seen before, and hence it simply can't be in NP. Note that there are decidable questions that are not in NP too. The easiest one to articulate, given what we've covered so far, is deciding if a non-deterministic Turing machine will halt after k steps, where k is encoded in binary. Unfortunately, the proof is beyond the scope of this course. But it's worth knowing that such problems exist. In this lesson we have introduced the P versus NP problem. As we said in the beginning, the class P informally captures the problems we can solve efficiently on a computer, and the class NP captures the problems whose answers we can verify efficiently on a computer. Our experience with finding a clique in a graph is just that these two classes are not the same. Cliques are hard to find, but they are easy to check. Intuitively solving a difficult problem should be harder than just checking a solution. Nevertheless, no one has been able to prove that P is not equal to NP. Whether these two classes are equivalent is known as the P versus NP question and is the most important problem in theoretical computer science today. If not in all of mathematics. We'll discuss the full implications of the question in a later lecture. But for now, I'll just mention that in the year 2000, the Clay Mathematics Institute named the P versus NP problem as one of the seven most important open questions in mathematics. And has offered a $1 million bounty for a proof that determines whether or not P equals NP. Fame and fortune await the person that settles the P versus NP problem, but many have tried and failed. Next class we look at the NP complete problems. The hardest problems in NP. In this lesson, we're going to introduce a new element to our algorithms. Randomization. In a full course on complexity or one on randomized algorithms, we might go back to the definition of turning machines, include randomness in the model and then argue that other models are equivalent. Here, we're just going to assume that the standard built in procedures available in most programming languages work. Of course in reality these only produce pseudorandom numbers, but for the purpose of studying algorithms, we assume that they produce truly random ones. The lesson we'll use a few simple randomized algorithms to help motivate probability theory, and then use the basic theorems to characterize the behavior of a few slightly more sophisticated algorithms. Some ideas that come up include independence, expectation, Monte Carlo versus Las Vegas algorithms, derandomization. And the end, we'll tie our study of algorithms back to complexity with a brief discussion of probabilistically checkable proofs. Our first randomized algorithm will be one that verifies polynomial identities. Suppose that you're working at a company that is building a numerical package for some parallel or distributed system. A colleague claims that he has come up with some clever algorithm for expanding polynomial expressions into their coefficient form. His algorithm takes in some polynomial expression and outputs a supposedly equivalent expression in coefficient form, but you're a little skeptical that his algorithm works for the large instances that he claims it works on. You decide you want to run a test. That is to say, you want to verify that the polynomial represented by the input expression A is equivalent to the one represented by the output expression B. Being slightly more general, we state the problem like this. Given representations of polynomials A and B having degree d, we want to decide whether they represent the same polynomial. Note that we're being totally agnostic about how A and B are represented. We're just assured that A and B are indeed polynomials of degree d and we have a way of evaluating them. Well, here's a fantastically simple algorithm for deciding whether these two polynomials are equal. Just pick a random integer in the range 1 to 100 x d. The value of the polynomials at this point and say that the polynomials are equal, if they're equal at this point. Why does this work? Well, the so-called fundamental theorem of algebra says that any nonzero polynomial can have only d roots. So if A and B are different, the bad case is that it has d roots and what's worse is that all of them are in this range 1 to 100d in the integers. But even so, the chance that the algorithm picks one is still only 1 in 100. So if the polynomials are the same, we will always say so. But if they're different, then we will say they're the same only with chances 1 in 100. This is pretty effective and if it is found that A is not equal to B in some case, your algorithm is so simple, but there can't be much dispute over which piece of code is incorrect. In the analysis of the algorithm for polynomial identity verification, I avoided the word probability because we haven't defined what it means yet. We'll do that now and use the algorithm to illustrate the meaning of the abstract mathematical terms. A discrete probability space consists of a sample space omega, that is finite, or accountability infinite. This represents the set of possible outcomes for whatever random process we are modeling. In the case of the previous algorithm, this is the value x that is chosen. A discreet probability space also has a probability function with these three properties. It must be from the set of subsets of the sample space to the reals. Typically, we call a subset of the sample space an event, say E. For every event, the probability must be at least 0 and at most 1. The probability of the whole sample space must be 1, and for any pairwise disjoint collection of events, the probability of the union of the events must be the sum of the probabilities. To illustrate this idea with a polyequal example, let's define the event Fi to be the set consisting of the single element i. This corresponds to i being chosen as the value at which we test the polynomials. And we define the probability of Fi as one over omega, the size of our sample space or one over 100d. Now these Fi aren't the only possible events. They're just the single element subsets of the sample space. We need to define our function over all subsets. But actually we've done some implicitly already because of this property here. For any subset s of the sample space, we have that this subset is the union of some Fi, the individual elements of the sample space. These are all disjoined, so the probability of the union is the sum of these individual probabilities. And so the result is that the overall probability is the size of the set, s, divided by the size of the sample space, as we would expect. Let's now confirm that this function meets all of the criteria for the definition. This third property holds because the size of a disjoint union is the sum of the sizes of the individual set. Clearly the probability of the whole sample space is one, as we see from this ratio here and the probability of any event is between zero and one. So this example here is in fact a discrete probability space. By the way, this probability function is called uniform, because the probability of every single element event, that is to say the Fi here, they're all the same. Not all probability functions are like that. Heres a quick exercise on probability spaces. Consider one where the sample space is the integers 1 through 10 and where the probability of the set 1,2 is three-tenths. And the probability of the set 2,3 is four-tenths. And I want you to give the tightest possible bounds on the probability of the set 1,2,3. The tightest lower bound is 4, and the tightest upper bound is 7. For the upper bound, we first use the so called inclusion exclusion principal. To write the probability like so. Note that in the sum of the first two terms, we've counted the probability of 2 twice. So the third term subtracts it out. Now all probabilities are non-negative. So we can obtain an upper bound by just taking the first two terms and dropping the third. This is called the union bound. The probability of the union of any two sets is at most the probability of the sum of the two sets. We might be counting something twice but we don't miss anything. This then gives the upper bound of 7/10. And this is as good as it gets. The probability of 2 might be 0,while the probability of 1 is exactly 3/10. And the probability of 3 is 4/10. For the lower bound, we break the probability up in two ways. The quality holds here because these sets are disjointed. And again these probabilities are non-negative. So we obtain two lower bounds, 4/10ths being the tighter of the two Let's return to our polynomial identity verification algorithm and see if we can improve it. So far we've seen how if the two polynomials are equal, then the algorithm will always say so. But if the polynomials are different, there's up to a 1 in 100 probability that the algorithm will say that they're the same anyway. Maybe this isn't good enough. We want to do better. Well, one idea is to just change out this number 100 for a larger number. This works, but on a real computer we might start running into range problems. We don't want our algorithm to strain the difference between the theoretical models in practice. Another solution is repeated trials. Instead of just testing for equality at one point, we'll do it at several random ones. Such an algorithm might look like this. We start out by assuming that the two polynomials are equal. Then we try different values for x, and if we ever find one where the two polynomials are not equal, then we know that they aren't. Note that we could terminate as soon as a difference is found, but this version of the algorithm makes the analysis a little more clear. For simplicity, we'll make this k equal to 2 so that we can visualize the sample space with a 2D grid like so. The row corresponds to the value of x chosen in the first iteration, and the column to the value of x chosen in the second iteration. Now the size of the sample space is 100d quantity squared. And since there are at most d squared pairs of roots for the difference between A and B, at most d squared of these possibilities will make the algorithm fail. We'll let F be the event that the algorithm fails on unequal polynomials. By symmetry we can argue that all elements of the sample space should have equal probability. So the probability of the algorithm failing on two unequal polynomials is just d squared divided by 100d quantity squared. Which is 1 over 100 squares. That's 1/100th of the probability for just one trial. We can also make the argument by following the actual process of the algorithm more closely. We'll let E1 be the event that the polynomials are equal at the value for x chosen in the first iteration. In terms of our grid, this subset is a subset of the rows. As we argued before, this probability is at most 1 over 100. Similarly, we let E2 be the event that the polynomials are equal at the value x chosen in the second iteration. In the grid this corresponds to a certain subset of the columns. Again, these columns take up at most 1/100th of the whole probability mass. We're interested in the probability of both E1 and the E2 happening, i.e., the intersection of these two events represented as the black region in our grid. What fraction of the probability mass does it take up? Notice that in order for a sample to fall into the black region, the first iteration must restrict us to the blue region. The probability of this happening is just the probability of E1. Then from within the blue region, we ask what fraction of the probability mass does E2 take up? Well, that's just the probability of E1 and E2 divided by the probability of E1. And we want to multiply this quantity with the probability E1 to get the result. This sort of ratio is common enough that it gets its own name and notation. We notate it like so, and we read this as the probability of E2 given E1. This is called a conditional probability. The interpretation is that it gives the probability of E2 happening given that E1 has already happened, or that it's somehow destined to happen. More specifically, it is the probability that the second iteration will pick a value where the two polynomials are equal given that the first iteration did. Well, of course, this is the same probability as E2 happening, regardless of what happened in the first iteration. So this is just the probability of E2. This condition is known as independence, and it corresponds to our intuitive notion that one event is not dependent on another. Substituting these values, we find that this approach gives the same result as the other one. We just used the ideas of conditional probability and independence in the context of our polynomial verification algorithm. Now let's discuss these ideas in general. We define the conditional probability that an event E occurs, given that an event F occurs, as the probability that E and F occur divided by the probability that F occurs. We can visualize this quantity using a traditional Venn diagram. We draw the whole sample space as a large rectangle. And within here we draw the set F, like so. When we talk about the conditional probability of E, given F, we are restricting ourselves to the set F within the sample space. Thus, only the portion of E that is in F is important to us. And to make this a proper probability, we have to renormalize by dividing by the probability of F. That way the probability of E, given F, and the probability of not E, given F sum up to one. An interesting situation is where the probability of an event E, given F, is the same as the probability when F isn't given. This implies that E and F are independent, one event doesn't depend on the other. Formally, we say that two events, E and F, are independent if the probability of E and F is the probability of E times the probability of F. Note that this is a slightly more general statement than this equality. The probability of E, given F, isn't defined if the probability of F is zero. Here's a quick question on independence. Suppose that there's a 0.1 probability that Sam will get a bullseye each time that he throws a dart. What is the probability that he gets 5 bullseyes in a row? The answer is ten to the minus five. The standard assumption here is that the throws are independent, so the probability of getting five in a row is just the product of the probabilities of getting a bulls eye each time. Let's go back to our polynomial verification algorithm with repeated trials and review its behavior. If the two polynomials are equal, then the probability that the algorithm says so is 1. On the other hand, if the polynomials are different, there is a chance that the algorithm will get the wrong answer, but this only happens with probability 100 to the -k, where k is the number of trials that we did. We just need to extend the argument we made before, where k equals 2 to general k. The fact that our algorithm might sometimes return an incorrect answer makes it what computer scientists call a Monte Carlo Algorithm. And because it makes mistakes only when the polynomials differ, it is called a one-sided Monte Carlo Algorithm. This idea can be extended to arbitrary languages. Here, strings in the language represent equal polynomials. This algorithm only makes mistakes on strings not in the language. Of course, it's possible for the situation to be reversed, so that the algorithm makes mistakes on strings in the language. This is another kind of one-sided Monte Carlo Algorithm. I should say that there also two-sided Monte Carlo Algorithms where both arrows are possible, but regardless of which input is given the answer is more likely to be correct than not. Suppose, however, that any possibility of error is intolerable. Can we still use randomization? Well, yes we can. Instead of picking a new point uniformly at random, from the 100d possible choices, we pick one from the choices that we haven't picked before. This is known as sampling without replacement. Since we don't place the sample we took back in the pool before choosing again. Since there are only d possible routes. By the time we've picked the d+1 point, we must have picked 1 of the non routes. This algorithm uses randomization, but nevertheless, it always gives a correct answer. If the polynomials are equal, the probability that the algorithm says so is 1. If they're unequal, the probability that it says so is 0. The fact that this algorithm never returns an incorrect answer makes it a Las Vegas Algorithm. The randomization can affect the running time but not the correctness. If the polynomials are equal the algorithm definitely takes d+1 iterations, but when they're unequal, it gets a little more complicated. Let Ei be the event that A and B are equal at the ith element of the order array. Then, we characterize the probability that the algorithm takes at least k steps as the intersection of E0 through Ek- 1. Note, however, that these events are no longer independent. If A and B are equal at the first element of the order list, then that's one fewer root that could been have chosen to be the second. So how do we go about calculating this probability? Returning to the Las Vegas version of our polynomial identifier, we can write the probability that we don't detect a difference in k iterations as this product here. With a little more work, we can get a tighter bound than this. But for our purposes, this simple bound works. Note that even though the probabilities here are the same, the meanings are different. In the Monte Carlo Algorithm, the probability represents the chances that the algorithm will return an incorrect answer. In the Las Vegas Algorithm, it says something about the running time for an algorithm that will always produce the correct answer. Let's make this calculation for a concrete example and exercise. Suppose that difference A- B is a polynomial of degree 7 with 7 roots in the set 1 through 700, all integers. An algorithm samples three samples elements from S uniformly without replacement. Give an expression for the probability that all three elements are roots of the polynomial A- B. The expression I used is 5 by 698 times 6 by 699 times 7 by 700. In the first iteration we might choose any one of the 7 groups among the 700 possibilities. Given that we chose a root in the first iteration, however, that leaves only 6 roots left to choose from in the second iteration, and there are only 699 numbers to choose from in S, as well. Remember that we are sampling without replacement. We don't put back the sample we chose in the first iteration, so that we might choose it again. And the logic for the last term is the same. We've used up two numbers already, both of which are roots. In terms of the probability theory that we've established, we can formalize this argument as follows. Let Ei be the event that the ith element chosen from S is a root. The probability that all three samples are roots is the probability of E1 and E2 and E3. Which we break up into this product using the definition of conditional probability. We play the same trick with the second term here to get this expression, replacing it with the probability of E2 given E1 times the probability of E1. Now, we have the probability of each event conditioned on the previous ones. Given that 2 routes have been chosen already, the probability of picking another is is 7-2, by 700-2, and we end up with the same argument, and the same expression as before. So far, all the probabilistic objects that we've talked about have been events. Things that either happen or don't happen. As we go further in our analysis however, it will be convenient to talk about other random quantities. What is the value of a random die roll, or how many times did we have to repeat that procedure before we got an acceptable outcome. For this, we introduce the idea of a random variable. A random variable is just a function from the sample space to the reals. For example, let's let x be the sum of two die throws. Then the sample space is the set of all pairs of numbers between 2 and 6. And the function x just adds the two numbers together. If I roll a four and a seven, then the total is 11. We use the notation x equals some constant a, to indicate the event that x is equal to a. Thus is the set of elements s from the sample space, for which x is equal to a. In our example, the event x equals 3, is the set consisting of the samples 1, 2 and 2, 1. Having defined a random variable, it now makes sense to talk about a random variable's average or expected value. Formally, we define the expectation of a random variable X, like this. We consider the set of values that X could take on, notating it as X of omega. The image of the sample space under X, and we sum over all these possible values, giving each a weight according to how probable the value is. Thus the expectation is a weighted average of all the values that a variable could take on. For example, let the random variable X be the number of heads in three fair coin tosses. Then, according to this definition, this would be 0 times one-eighth for getting no heads, plus 1 times three-eighths for getting one head. There are three possible tosses that could've come up heads. Also, 2 times three-eighths for getting two heads. Similarly there are three possible tosses that could have come up tails so as to give us two heads, and lastly there's only a one-eighth chance of getting all three heads. Adding these up we get twelve-eighths or 3 over 2. Now if I asked you casually how many heads will there be in three coin tosses on average. You probably would have said three over rather quickly without doing all this calculation. Each toss should get you half a head, so with three, you should get three halves, you might have reasoned. In terms of our notation, we can express the argument like this. We let Xi be 1 if the ith fair coin toss is heads. And we let it be 0 otherwise. Then we say that the average number of heads in three tosses is the expectation of X1 + X2 + X3. And this is the expectation of X1 plus the expectation of X2 plus the expectation of X3 and this totals to 3 over 2. The key step in the proof is this equality here, which says that the expectation of the sum is the sum of the expectations. This is called the linearity of expectation. And as we will see, this turns out to be a very powerful idea. In general, for any two random variables X and Y and any constant a, we have these two equalities. The expectation of the sum is the sum of the expectations, and we can just factor out constant factors from expectations. Remember this theorem. Here's an exercise that will help illustrate the power of the linearity of expectations. In a random permutation, over 100 elements, what is the expected number of 3-cycles? We can think aout the permutation as defining a directed graph over 100 vertices. Where each vertex has exactly one outgoing and one incoming edge. Thus if pi of 55 is equal 57 we draw this edge here. If pi 57 is equal to 58 we draw this edge here. And if pi 58 is equal to 55, we draw this edge here. And together these form a 3-cycle. Use the linearity of expectation, and write down the expect of three cycles, as a ratio here. The answer is one-third. Let's first consider the probability that a fixed cycle say, this one illustrated here is in the permutation. The probability that this first edge here is made is one-hundredth. Some edge has to come out of 55 and all 100 destinations are equally likely. Next, we consider the probability that this next edge in the cycle is made. 57 already has an edge going in from the first edge, so we can't have a self loop here. That leaves only 99 destinations to choose from, all equally likely. So there's a 1 in 99 chance that we pick 58 and we applied the same logic to this last edge here to give us another factor of one over 98. Overall then, this gives the probability that any fixed recycle will exist in a random permutation. We can also think of this as a random variable that is one, if the cycle is in the permutation and zero, otherwise. Therefore, this is the expected number of times that this particular 3-cycle will occur in a permutation. Well, to calculate the expected number of 3-cycles overall, we just need to sum up this quantity for all possible 3-cycles. All of their expectations will be the same, so it becomes just a matter of counting them. Counting 3-cycles is the same as counting permutations of length three, except that this overcounts by a factor of three, because the starting point is arbitrary. There are 100 ways to choose the first element, 99 the second, 98 the third. But in counting this way, we counted every 3-cycles three times. Once for each possible starting point. Of course, in this calculation, everything cancels out, except for the 3 and we get one-third for the answer. This argument is actually much more general than the question. Note that the fact that we were using 100 vertices didn't show up anywhere in the answer. And in fact, we can replace the 100 with some arbitrary n and we get the exact same answer. The cancellation happens in exactly the same way. And in fact, we can also replace the 3 with k. The exact same cancellation will occur and we get an expected number of 1 over k cycles of length k. At this point, we've covered the basics of probability theory, so we'll be able to turn our focus on the algorithms themselves and their analysis. Up first is the classic randomized Quicksort algorithm. In case you don't recall randomized Quicksort from a previous algorithms course, here's the pseudo code. To keep things simple we'll assume that the elements to be sorted are distinct. This is a recursive algorithm with the base case being a list of 0 or 1 elements, where the list can simply be returned. For longer lists we choose a pivot, uniformly at random from the elements of the list, and then split the list into two pieces. One, with those elements less than the pivot. And one with those elements larger than the pivot. Then, we recursively sort these shorter lists, and then join them back together in sorted order like so. The efficience of the algorithm depends greatly on the choices of these pivots. I'm going to visualize a run of the algorithm by drawing out the recursion tree over here. I'll write out the list in sorted order, so that we can see better what's going on, though the algorithm itself will likely have these elements in some unsorted order. The ideal choice of pivot is always the middle value in the list. This splits the list into two equal sized sublists, one consisting of the larger elements, the other consisting of the smaller elements. Then in recursive calls we split these lists into two pieces until we get down to the base case of the list of length one or zero. Because the size of the lists get cut in half with each call, there are only login levels to the tree, and every level gets compared with a pivot in a call to Quicksort. So there are order n comparisons at each level, for a total of n log n comparisons overall. That's if we're lucky and pick the middle element every time. How about if we're unlucky? Suppose we pick the largest element in every iteration. Then the size of the list only decreases one in each iteration. So they are n levels. The first level requires n minus 1 comparisons. The second n minus 2, and so forth. So that the total number of comparisons is an arithmetic series. And therefore is order n squared. This is as bad as a naive algorithm like insertion sort. The natural question to ask then is, how does Quicksort behave on average. Is it like the best case where we pick the pivot in the middle of each list? Or is it like the worst case that we have here, where we're always picking the largest element in the list? Or is it somewhere in between? Now, I'll analyze the average case performance of Quicksort, and show that it is order n log n, just like the best case. Suppose that randomized Quicksort is used to sort a list consisting of elements a1 through an, all distinct. We'll let Eijk be the event that the element ai is separated from the element aj by the kth choice of a pivot. And we'll let xij be a variable that's 1 if the algorithm actually compares ai with aj, and we'll let this variable be 0 otherwise. And we'll let xij be a variable that's 1 if the algorithm actually compares ai with aj, and we'll let xij = 0 otherwise. The sum of these xij will count the number of comparisons that the algorithm actually uses. Now, I claim that the expected value for xij is 2 divided by j- i + 1. We'll look at this argument in some detail, so we'll need a new canvas. By the definition of expectation, the expected value of xij is this expression here. So it's really just the probability of xij being equal to 1. In fact, this is true of all 0, 1 variables. Now, ai has to be separated from aj by some pivot in the algorithm, and it won't be separated by 2. Therefore, these Eijk events are disjoint. And this sum must be the probability that xij is 1. This argument is known as the Law of Total Probability, by the way. Each term here can be written in terms of a conditional probability. And it turns out that this quantity here is the one that we're going to want to reason about. Given that ai is going to be separated from aj, it must be that they haven't been separated yet. So whatever the current list is, it must include ai, aj, every element in between, and possibly some more elements to the outside. Given that the separation does occur, however, the pivot must be chosen in this range here, including the ai and the aj. ai will actually only be compared to aj, however, if one of the two is chosen as a pivot. Therefore, given that the separation is going to occur, the probability that it will actually require a comparison of these two particular elements is only 2, the chances of choosing one of them as a pivot. Divided by j- i + 1, the possible number of choices of pivot, here. Substituting this value into here, will then give us the answer. Our sum then, just becomes this ratio times the probability of the Eijks. We can factor out the constant term here however. And since i must be separated from j by some pivot, the remaining sum is just 1. So we obtain the desired result. With this claim established now, we argue that by the linearity of expectations, the expected number of total comparisons is just the sum of the expectations for these xijs. Going for simplicity rather than tightness, we can bound this expression by this one here and see that summing up the 1 over js is rather like integrating 1 over x. So this inner sum here becomes a log. And then with the outer sum we end up with n log n, overall. To sum things up then, we can state our result as follows. For any input of distinct elements Quicksort, with pivots chosen uniformly at random, compares order n log n elements in expectation. The average case is on the same order as the best case. This is comforting, but by itself is not necessarily a good guarantee of performance. It's conceivable that the distribution of running times could be very spread out, so that it would be possible for the running time to be a little better than the guarantee or potentially much worse. It turns out that this is not the case. The running times are actually quite concentrated around the expectation, meaning one is unlikely to get much more or much less than the average. This sort of argument is called a concentration bound. And if you ever take a full course on randomized algorithms, a good portion will be devoted to these types of arguments. Next, we consider a randomized algorithm for finding a minimum cut in a graph. Likely this won't be as familiar as quick sort. We're given a connected graph G and the goal is to find a minimum set of edges whose removal from the graph causes the graph to have two connective components. This set of edges is called a minimum cut-set. Note that this is a different problem from the minimum s-t cuts that we considered in the context of maximum flow. There are no two particular vertices that we're trying to separate here. Any two will do, and all the edges have equal weight here. We can use the Minimum S-T Cut Algorithm to help solve the problem but I think you'll agree that this randomized algorithm is quite a bit simple. The algorithm proceeds by repeatedly contracting edges so as to join two together. And once there are only two vertices left they define the partition. I'll illustrate with an example. Consider this graph, and suppose that we decide to contract this edge here. Then these two vertices are going to get collapsed into one. And we're going to delete this edge, like so. Note that we keep all the other edges in the graph. Even if they end up being parallel that is between the same two vertices. >From this graph, suppose that we choose this edge here. Then contracting this edge will delete this edge as well as this parallel edge here, like so. >From this group, suppose that we contract this edge. Deleting these two here, as well. Then the final result is this graph here. And from the result, we can read off both the partition, 1, 2, 3, 4, on 1 side, and 5 on the other, and we can also read off the cut-set represented by the 2 remaining edges. Just 3, 5 and 4, 5. That is these two edges here in the original graph. Now, this particular choice of edges, led to a minimum cut-set. But not all such choices would have. How then should we pick an edge to contract? Well, it turns out that just picking a random edge is a good idea. More often than not this won't yield a correct answer. But as well see even so it will yield a correct answer often enough to be useful. So it turns out that the previous algorithm outputs a minimum cut set with probability at least 2 / n(n- 1). Now, at first you might look at this result and ask, what good is that? The algorithm doesn't even promise to be right more often than not. The trick is that we can call the algorithm multiple times, and just take the minimum of these results. If we do this roughly n squared times the log of 1/epsilon times, then there's a 1- epsilon chance that we will have found the minimum cut set. The proof for this corollary is that each call to the algorithm is independent. So, the probability that all the calls failed is given by this expression here. Then in general, (1-x) is at most, e to the -x. So, applying that quantity to this case here, we have the n-1 and the 2 factors cancel. Leaving us with this expression here, which then just simplifies to the quantity epsilon. So the probability that all the iterations fail is at most epsilon. This, down here, by the way, is extremely useful in analysis, and is one you should always have handy in your mental toolbox. So then, we have that if this theorem is true, we can boost it up into an effective algorithm, just by repeating it. But why is this theorem true? Consider a minimum cut set C, and we'll let Ei be the event that the edge chosen in the the ith iteration is not in C. Note that there could be other minimum cut sets, as well. But we're going to focus on just one of them, which we're going to call C. Returning the cut set C means not picking an edge in C in each iteration. So it's the intersection of all the events Ei, which we can turn into this product as we've done before. We are just conditioning the probability of avoiding C in the ith iteration, given that we have avoided it in all previous ones. Actually that quantity will be a little easier to analyze if we write it like this. And I claim that this is at most n-j-2 / n-j. We'll warm up just by considering the probability of E1, that is, avoiding the cut in the first iteration as we choose an edge. Letting k be the size of the cut, we have that every vertex must have degree at least k. Otherwise, the edges incident on a smaller degree vertex would be a smaller cut-set. This then implies that the total number of edges is at least nk/2. Every vertex must have degree at least k and summing up the degrees for every vertex counts every edge exactly twice. Therefore, the probability of avoiding the cut set in the first iteration Is 1 minus the size of C divided by the over all number of edges. This is at least this expression here, which then simplifies to n-2/n. The more general argument will be similar, given that the first j edge is chosen, we're not in C. Then C is still a minimum cut-set for the current graph. If not, then taking the edges in the smaller cut would also be a smaller cut for the original graph. And note throughout, as we count edges we're always counting the parallel ones. So again, we can let k be equal to the size of the set C. And we have that there have to be at least k(n- j) / 2 edges left. The n-j now comes from the fact that there are only n-j vertices left after the j iterations. With the same argument as before, we have that the probability of avoiding C in the j+1th iteration, given that no edges in C has been chosen yet, is at least n-j-2 / n- j. Which was what we were trying to show. Substituting this into our expression here, we have that we are down to a 1/3 probability in the last iteration. 2/4 in the iteration before that, etc. This product telescopes and leaves us with a bound of 2/ n(n-1), which was what we were trying to show. Altogether then, this extremely simple procedure has given us a fairly efficient algorithm for finding a minimum cut set. For our last algorithm, we'll consider Maximum 3-SAT. This will tie together randomized algorithms, approximation algorithms, and complexity. We're given a collection of clauses, each with three literals all coming from distinct values and we want to output an assignment to the variables such that a maximum number of clauses is satisfied. First we're going to show that for any formula there is an assignment that satisfies 7/8 of the clauses. Consider an assignment that's chosen uniformly at random. And we'll define Yi to be 1 if the clause Ci is satisfied and we'll let Yi be 0 otherwise. And we'll let Y be the sum of all the Yis. The then expectation of Y is just the sum of all the possible values of Y times the probability of that value. This is just the definition of expectations. Bilinearity, this is the sum of the expected values of the individual Yis, and I claim that each of these is 7/8. Since the literals all come from distinct variables, there are 8 possible ways to assign them true and false values. But only one of those assignments will cause the YI to be equal to 0. The rest satisfy the clause and cause YI to be 1. Therefore, we get an expected value of 7/8. And summing over all M clauses, we get 7M over 8. The key realization is that this value here represents a kind of average. That means that not all the v in this sum here can be less than this average. There has to be a v where the probability is positive and the associated value of v is at least 7m/8. Because the probability is positive, however, that means that there has to be a way to assign the variables to get that value of v. Therefore, there's always a way to satisfy 7/8ths of the clauses in any three set formula. This technique of proof by the way, is called the expectation argument. And it's part of a larger collection of very powerful tools called the probabilistic method, which were developed and popularized by the famous Paul Erdos. Note that the statement of the theorem doesn't involve probability at all. But the proof did use probability as part of the argument. That's characteristic of the probabilistic method. So far we've seen how there must be an assignment that satisfies at least seven-eighths of the clauses in any 3-CNF formula. And in fact, the same argument gives us an algorithm that satisfies seven-eighths on average. Just pick a random assignment. By itself however, this does not guarantee that we'll actually find an assignment that satisfies seven-eighths of the clauses. To obtain such a guarantee, we're going to use a technique called derandomization. That will take the randomness of our algorithm out and give us a deterministic algorithm with a guaranteed seven-eighths factor approximation. An important part of the algorithm will be a subroutine that assigns a value to a variable and simplifies the causes. We call this procedure instantiate. Let's say that we want to set a variable, x1, to true. Then any clauses not involving x1 are simply left alone. And if a clause has an x1 in it, then it gets set to true. If a clause has a not x1 in it, then we just eliminate that literal from the clause. If not x1 is the only literal in the clause, then we just set the clause to false. Another important set routine will be one that calculates the expected number of clauses that will be satisfied if the remaining variables are assigned true or false uniformly at random. Of course, if a clause is just true, then it gets assigned a value 1. And a clause that's just false gets assigned a value 0. A single literal gets assigned a value one-half. Two literals get assigned a value of three-quarters. And three literals gets a value of seven-eighths. Remember that there's just one way to assign the relevant variables so that the clause is false. The E of Y procedure simply calculates these values for every clause and then sums them up, using the linearity of expectations. With these separate teams defined, we can now write down our derandomized algorithm as follows. We start with an empty assignment for the variables. Then for each variable and term, we consider the formula resulting from the variable Xi being sent to true. And the one resulting from Xi being set to false, between these two we pick the one that gives a larger value for E of Y. That is the one where the expected number of satisfied clauses is greater, assuming that the remaining variables are set at random. Note that we're using our knowledge of how a random assignment would behave here. But we actually aren't using any randomization. Having picked a better way of assigning the variable Xi, we update the set of clauses and record our assignment of the variable Xi. The reason this algorithm works is that it maintains the invariant that the expected number of clauses of C that would be satisfied if the remaining variables were assigned uniformly at random is at least 7m over 8. This is true at the beginning, just by our previous theorem. But this expectation is just the average of the expected number that would be satisfied in Cp and the expected number that would be satisfied in Cm. So by picking C to be the one for which this E of Y is greater, the lavariant is maintained. Of course, at the end of the algorithm, all the variables have been assigned, so this expectation is just counting up the number of true clauses. This technique is known as the method of conditional expectation and it has a number of clever applications. Overall then we've shown that there's the deterministic algorithm which given any 3-CNF formula finds an assignment that satisfies at least 7/8 of the clauses. Remarkably, it turns out that this is the best we can do assuming that P is not equal to NP. For this argument we turn to the concept of probabilistically checkable proofs. PCP stands for probabilistically checkable proofs. It turns out that if you take the verifiers we talked about when we defined the class NP, and one, you give them access to random bits. And two, you give them random access into the certificate or proof, then they become extremely efficient in a certain sense. These types of verifiers are called probabilistically checkable proof systems, and the famous PCP theorem relates the set of languages they can verify under certain constraints back to the class NP. In a course on complexity, we would place these proof systems within the larger context of other complexity classes and interactive proof systems. For our purposes however, we can state the PCP theorem in this way, which is much more accessible. We'll let capital phi denote the set of all 3CNF formulas. Remember we're assuming that all clauses have exactly three literals, and that they come from three distinct variables. Then a version of the PCP theorem can be stated like this. For any constant alpha greater than seven-eighths, there is a polytime computable function f such that for every formula phi over a sufficient number of variables, the following two things are true. One, if the formula phi is satisfiable, then f of phi is also satisfiable. And if phi is not satisfiable, then every assignment for f of phi satisfies fewer than an alpha fraction of the clauses. So if phi is satisfiable, there's a way to satisfy all the clauses of f of phi. If phi is unsatisfiable, however, then you can't even get close to satisfying all the clauses of f of phi. We've introduced a kind of gap here, and this gap is extremely useful for proving the hardness of approximation. Many, many hardness of approximation results follow from this theorem. The most straightforward of them, however, is that you can't do better than the seven-eighths algorithm from MAX-SAT that we just went over. Not unless P is equal to NP, anyway. Why? Well suppose that I wanted to test whether strings were in some language, where the language is in NP. And at my disposal, I had a polyton alpha approximation for 3-SAT, where alpha is strictly greater than seven-eighths. Then I could use the Cook-Levin reduction to transform my string into an instance of SAT that would be satisfiable if and only if X is in L. Then I can use the function f from the PCP theorem to transform this into another set of 3-SAT clauses. Where either all the clauses are satisfiable or fewer than an alpha fraction of them are, depending on whether phi was satisfiable. That way, I just run the alpha approximation on f of V and see if the fraction of clauses satisfied is greater than alpha or not. If it is, then from the PCP theorem, I can reason that phi must have been satisfiable and so from the Cook-Levin reduction, X must have been in L. On the other hand, if the fractional clauses satisfied is less than alpha, then f of phi can not have been satisfiable, so phi must not have been satisfiable either, so from the Cook-Levin theorem, X must not be in L. Using this reasoning, we just found a way to decide an arbitrary language in NP in polynomial time. So if such an alpha approximation exists, then P is equal to NP. Since P probably is not equal to NP, it seems like such an alpha approximation can't exist. Many hardness of approximation proofs can be done in a similar way. All that's necessary is to stick in another transformation here, transforming the three step problem that has this gap into another problem, while preserving a gap, and thus showing that certain approximation factors would imply that P is equal to NP. As we've seen, randomization can be a very useful tool in the design and analysis of algorithms. And it turns out that this is true in practical programming too, as many real world computer programs rely on pseudorandomness to achieve their desired behavior. Nevertheless, it's an open question whether randomization actually helps in the sense that the complexity classes that we discussed earlier on in the course. It simply isn't known whether there's a language that can be decided in polynomial time with a two sided Monte Carlo algorithm that can't be decided with the normal turning machine in polynomial time. This is known as the P equals BPP question, and it's one of the major open problems in complexity. In the last lesson, we talked informally about what it meant to be computable. But what does it mean to be computable? What even is a computer? What kind of problems can we solve on a computer, or problems can't be solved? To answer these questions, we need a model of computation. There are many, many ways we could define a notion of computability. Ideally, something simple and easy to describe, yet powerful enough so this model can capture everything any computer can do now or in the future. Luckily, one of the very first mathematical models of a computer serves us quite well, a model developed by Alan Turing. In the 1930's decades before we had digital computers, Turing developed a simple model that captured the thinking process of a mathematician. This model which we now call the Turing machine is an extremely simple device and yet completely captures our notion of computability. In this lesson, we'll define a Turing machine. What it means for the machine to compute and either accept or reject a given input. In future lessons, we use this model to give specific problems that we cannot solve and more. In the last lesson, we began to define what computation is with a goal of eventually being precise about what it can and cannot do. We said that the input to any computation can be expressed as a string and may assume that whatever the instructions that were for turning input into output, that these two could be expressed as a string. Using a counting argument, we were able to show that there were some functions that were not computable. In this lesson we're going to look at how input gets turned into output more closely. Specifically, we're going to study the Turing machine and the classical model of computation. As we'll see in a later lesson, Turing machines can do everything that we consider as computation and because of their simplicity they are a terrific tool for studying computation and its limitations. Massively parallel machines, quantum computers, they can't doing anything a that a Turing machine can't do. Turing machines were never intended to be practical but nevertheless several have been built for illustrative purposes. Including this one, from Mike Davey. The input to the machine is a tape, unto which the string input has been written. Using a read right head, the machine turns input into output, through a series of steps. At each step, a decision is made about whether and what to write to the tape, and whether to move it to the right or to the left. This decision is based on exactly two things. The current symbol under the read write head and something called the machine state. Which also gets updated as the symbol is written. That's it. The machine stops when is reaches one of two halting states, named accept and reject. Usually we're interested in which of these two states the machine halts in. Though when we want to compute functions from strings to strings, then we pay attention to the tape contents instead. It's a very interesting historical note that in Alan Turing's 1936 paper, in which he first proposed this model, the inspiration does not seem to come from any thought of an electromechanical device, but rather from the experience of doing computations on paper. In section 9 he starts from the idea of a person who he calls the computer, working with pen and paper, and then argues that his proposed machine can do what this person does. So here I am, a computer, with my pen and paper. And we'll follow Turing's argument that his machine can do what I can. Let's take an example. He's talking about computable numbers, so I'll compute a very simple number, Alan Turing's age when he wrote the paper. So we'll take 1936- 1912, which is equal to 24. He argues that any calculation like this can be done on a grid. Like a child's arithmetic book her says. I assume he means something like wide ruled graph paper. He argues that all the symbols can be made to fit inside one of these squares. Then he argues that the fact that the grid is two dimensional is just a convenience. So he takes away the paper and says that computation can be done on a tape. As someone who has to carry out the computation, I don't much care for this. But I can still do my job. Then he points out that there are limits to the width of my perception. So if I happen to be reading a very long mathematical paper, the phrase "hence (applying this big Theorem number) we have. Then when I look back, I probably wouldn't be sure at a glance that I had found the right theorem. I would have to check, maybe three or four digits at a time, crossing off the ones I had matched, so as not to lose my place. Something like this. So I match the first four, then the next four, and so on. And so forth. Eventually, I will have matched them all and can re-read the theorem. Now, since Alan Turing was going for the simplest machine possible, he takes this idea to extreme and only lets me read one symbol at a time and only move right or left one square on the tape at a time trusting to the strategy of making marks on the tape like I did with this theorem here, and my state mind to accomplish the same things as I would under normal operation with pen and paper. And with those rules, I have become a Turing machine. So that's the inspiration, not a futuristic vision of the digital age, but probably Alan Turing's own everyday experience of computing with pen and paper. Now that we have some intuition for the Turing machine, we turn to the task of establishing some notation for our mathematical model. Here, I've used a diagram to represent the Turing machine and its configuration. We have the tape, the read/write head, which is connected to the state transition logic, and the little display here that will indicate the halt state. That is, the internal state of the Turing machine when it stops. Mathematically, a Turing machine consists of a finite set of states Q. Everything used to specify a Turing machine as finite, by the way, that's critically important. Next, we need to specify an input alphabet of allowed symbols. This must not include the blank symbol, which we will notate with this square cut most of the time. For some of the quizzes where we need you to be able to type the character, we will use B. We can't allow the input alphabet to blank symbol or we wouldn't be able to tell where the input string ended. The whole rest of the tape where the input string is not written, by the way, has blanks on it. We also define the tape alphabet of symbols. And that's the set of symbols that the read/write head can use, and this will include the blank symbol. A Turing machine also specifies a transition function from a state tape symbol pair to a state tape symbol direction triple. This of course tells the machine what to do. For every possible current state and symbol that could be read, we have the appropriate response, the new state to move to, the symbol to write to the tape, you can make this the same as the input one to leave the tape alone, and the direction to move the head relative to the tape. Note that we can always move the head to the right, but if the head is currently over the first position of the tape, then we can't actually move left. When the transition function says that the machine should move the head left, then we just have it stay in the same position by convention. A Turing machine also specifies a start state. The machine always starts in the first position on the tape, and in this state. And finally, we have an accept and a reject state. When these are reached, the machine halts its execution and displays the final state. At first, all this notation may seem overwhelming. It is a seven tuple after all. Remember, however, that all the machine ever does is respond to the current symbol it sees based on its current state. Thus, it's the transition function that is at the heart of the machine. And most all of the important information, like the set of states and the tape alphabet, is implicit in it. For our first training machine example, I've chosen one that tests the oddness of a binary representation of a natural number. Note that I've cheated here in the transition function by including only state symbol pairs in the domain that we would actually encounter during computation. By convention, if no transition is specified for the current state symbol pair, then the program just halts in a reject state. One convenient way to represent the transition function by the way, is with a state diagram. Similar to what is often used for a finite automata, for those familiar with that model of computation. Each state gets its own vertex in a multigraph, and every row of the transition table is represented as an edge. The edge gets labeled with the remaining information besides the two states. That is it gets the symbol red followed by an arrow, then the symbol to right, and the direction to move the head. Now let's trace through the operation of this Turing machine on this input 1, 0, 1, 1. The head first reads a 1 from the tape, and run state q0. So I go up here and consult the transition table. And here, I see that I should follow this edge, since that has state q0, and we read a 1. And this transition tells me that I should write a 1 and leave the tape alone, and move the head to the right. And change the state to q0. So let's go ahead and update our figure. We're still in state q0, and the position of the head has been moved one space to the right. So this becomes our new configuration. In the next step, we'll read a 0, consulting the transition diagram. We see that this means that we should leave the 0 alone and move the head to the right again, and stay in state q0. We read a 1 from the tape, and that means we follow this transition here, and move the head to the right again. And the next step is similar. Finally, the head reads a blank symbol, the transition table tells us to write a blank symbol and move the head to the left. And to switch the state to q1. Now, we read the symbol under the head, that's a 1, so we follow this transition rule here, and we end up in an accept state. Recall that a Turing machine always starts in the initial state and with its head over the first position on the tape. As it computes, its internal state, the tape contents, and the head position will change. But everything else will stay the same. We call this triple of state tape content and head position, a configuration. And any given computation can be thought of as a sequence of configurations. It starts with the initial state, the input string, and with the head on the first location, and it proceeds from there. Now it isn't very practical to always draw a picture like this every time that we want to refer to a computation of a Turing machine, so we've developed some notation that captures the idea. Actually, we'll rewind the computation we just did, and do it again, this time writing down the configuration using this notation. We write the start configuration as q01011. The part to the left of this state represents the tape contents to the left of the head. It's just the empty string, in this case. Then we have the state of the machine. And then the rest of the tape contents. After the first step, a 1 is to the left of the head. We're still in state q0, and this is the rest of the string, 011, 011. In the next configuration, 1, 0 is to the left of the head. We're still in state q0 and 1, 1 is the rest of the tape content. And so on and so forth. This notation is a little awkward, but it's convenient for type settings. It's also very much in the spirit of Turing machines, where all structured data must ultimately be represented as strings. If a Turing machine can handle working with data like this, then so can we. At a slightly higher level, a whole sequence of configurations like this captures everything that a Turing machine did on a particular input. And so we will sometimes call such a sequence a computation. And actually this representation of a computation will be central as we discuss the Cool-Levin theorem in the section on complexity. Next, we're going to get some practice tracing through Turing machine computations and programming Turing machines. The point of these exercises is not so that you can put on your resume that you're good at programming Turing machines. And if someone asks you in an interview to write a program to test, say, whether an input number is prime, I wouldn't recommend trying to draw a Turing machine stake diagram. Somehow, I doubt that will land you the job. Rather, the point is to help you convince yourself that Turing machines can do everything that we mean by computation. And if you really had to, you could program a Turing machine to do anything you could write a procedure to do in your favorite programming language. There are two ways to convince yourself of this. One is to just practice so that you build up enough of a facility with programming Turing machines. That is to say, it just becomes easy enough for you, so that it seems intuitive that you could do anything. Another way to convince yourself of the power of Turing machines is to show that a Turing machine can simulate the action of models that are closer to real world computers, like the random access model. We'll do that in a later lesson. But to be able to understand these simulation arguments, you need a pretty good facility with how Turing machines work anyway. So make the most of these examples and exercises. To illustrate some of the key challenges in computing with Turing machines and how to overcome them, we'll examine this task here, where we're given an input string, and we want to tell if it is of the form w#w, where w is a binary string. Otherwise, we reject it. I'll illustrate the basic idea on this example. First, we x out the first symbol on the tape. And then we're going to go look to see if the first element past the # is the same symbol. So the fact that I read a 0 here, somehow has to encoded in the machine's state. So we'll go along to the first symbol after the # and indeed it is a 0. If it wasn't, then we can reject the input right away. Those two strings on either side of the # are not equal. We x out that 0, so that we know not to check it again. And we move left, past the #, over to the first x after that that we encounter. And now we're ready to repeat that process. We x out the 1 this time. Remembering the fact that it was a 1 in the machine state. We can forget that the previous symbol was a 0 by now. And then we move right, past the #, past all the x's, to the next non-x symbol that we see. This is a 1, which matches what we x'd out before. That's great. We x it out and then we rewind past the #, to the first x that we encounter. At this point, the next symbol is a #, so we know that the string to the left of the # matches the first part of the string to the right. But, there might might be more to the right, so we check that we crossed off all of the 0's and 1's to the right of the #. If we didn't, then we reject because there wasn't a match. The string on the right was too long. As Turing machines go, this is a pretty simple program, but as you can see here, the state diagram gets a little messy. Like the Sipsor text book, I have used a little shorthand here in the diagram. When two symbols appear to the left of the arrow, I mean, to match either one those. It's easier than writing out a whole other edge. Also, sometimes I will only give a direction on the right, interpret that to mean that the tape should be left alone. Here is our initial state, q0. >From here, we usually read either a 0 or 1, and proceed down one of these two parallel paths. We need parallel paths here, because we have to remember whether it was a 0 or a 1 that we read initially. We move past the rest of the first string to the # and then past any x'd out symbols to the next either 0 or a 1. Actually, if we encounter a blank, that means that the second string was too short and we can reject immediately. Or if we encounter a 1 when we're looking for a 0, that also means that we reject. If we encounter a 0 in this side, however, that means that we found a match and we can begin the rewind process. And this path is exactly an alogus. When we do find the match, we x it out, and then begin the rewind process. Rewinding past all the xs. Then we encounter a #, and then we rewind past all the 0s, and 1s until we encounter the first x. And then we move the symbol to the right so that we're reading the next symbol in the first string that was input. And then we're ready to begin this process again. When in this phase, we encounter the #. That means that we've matched all of this first string, here. And so we just need to check that this string here isn't any longer. So we moved past all the xs and if we encounter a blank next, that's great, we accept. If we encounter something else, then we need to reject. Take a minute to look this all over carefully and convince yourself that it's right. It helps to trace through an example. We'll do this for the input 01#01. Of course we start in the initial state q0, and we read a 0, so we transition to the state q1, xing out that 0. So this becomes our new configuration. Next, we read a 1 so we stay in this state and just move the head to the right. Next we encounter the #, so we move to state q3 and move the head to the right. Now, I'm not going to finish tracing this sequence all the way through, but I think it would be a good idea for you to do so. If it's worth studying Turing machines, then it's worth getting your hands dirty with the gory details, at least a few times. So tell me what the 4th configuration is, what the 7th configuration is, and what the 11th configuration is. Here are the answers that I came up with. If you trace through the configuration sequences carefully, you should get the same. Now, I want you to actually build the Turing machine. The goal is to right shift the input, and place a dollar sign symbol in front of it, and accept. Unlike our previous examples, accept versus reject isn't important here. This is more like a subroutine within a larger Turing machine that you might want to build. Though you could think about it as a Turing machine that computes a function from all strings over the input alphabet to a string over the tape alphabet if you like. I'll illustrate my recommended strategy with the example of shifting the string 01. First, we move the head to the end of the tape and replace the symbol with a blank. Then, we write this symbol one past where it was before. Then we rewind two steps and repeat. First, erasing the symbol, and then writing it one past where it was before, like so. Then we rewind two steps. And in rewinding, we detect that we have reached the beginning of the tape because we end up finding the blank symbol here. We try to rewind once, and then we try to rewind again, that would put us off the end of the tape, but we end up staying here at this location instead. So we end up reading a blank symbol, and that tells us that we're finished. And so we can just write the dollar symbol, and we're done. Here's the transition function that I used. I'll write it out piece by piece. First, we need to move to the end of the input string. So we have q0 transition to itself on 0n1 and only move into state q1 when it encounters a blank symbol. Since we rewound to the left after reading the first blank symbol, the head now reads the last symbol of the string. We overwrite this symbol with a blank, but we remember what the symbol was by going into state q2 if it was a 0, and then to state q3 if it was a 1. In both cases, we move to the right. The next symbol will necessarily be a blank and we want to replace it with a 0 or a 1 as appropriate. We move the head to the left, and then left again to return to the last symbol of the unshifted string. Eventually, the whole string will have been shifted, and this move will try to move past the beginning of the tape. In this case, the head stays where it is, and so we read the blank symbol again. We use this as a signal to indicate the machine should halt. So we write the dollar sign and do so in the accepting state. Let's program one final turn machine. We're given a binary string, and we want to accept if it has an equal number of 0's and 1's, and reject otherwise. This is going to be an exercise, but I'll recommend a strategy that leads to a relatively small number of states. Starting from the initial configuration, we'll accept if the current symbol is blank. An empty string has a balanced number of 0s and 1s. Again, because this is an exercise, we'll use the character b, instead of the square cup, to indicate blank. And if the current symbol isn't blank then we can replace it with a blank. Now, if that old symbol was a 1, then we move right until we encounter the next 1. And we'll change it to an x. And in fact, if we find a 0, we go hunting for a 1, and if we find a 1, then we want to go hunting for the matching 0. These have the effect of cancelling each other out, essentially. If we don't find a match for whichever symbol we found here, then we reject. We found the blank, that means we reached the end of the string. If we did find a match, however, then we need to rewind to the next blank, that would be the one that we over rode here, and change that to X to indicate that we've crossed it off. Then we move right past all the xs and then we're ready to repeat. Either finding a 0 (1) and then hunting for its match. Let's consider the sequence of tape values for the example input 00101. First, we blank out the first entry and then we go looking for a 1 to match that 0. Once that is found, we rewind the tape and mark off this blank here. And then we step forward until the next x'd out symbol. That's a 0, so we cross it off with a blank, and then we go look for a 1 to cancel it out. And we find one here. Then, we rewind back to the blank, cross it out, and then advance to the next uncrossed character. We find this 0, blank it out, and then we go looking for a 1, but we don't find one, so we reject. This string is imbalanced. You may use this strategy, or one of your own, to construct a turn machine that decides this language. Good luck. Here's my solution. The initial state accepts on the blank symbol. The empty string is balanced naturally. While encountering a 0, we move to state q1, and go hunting for a 1. On encountering a 1, we move to state q2, and go hunting for a 0. As we hunt for a 1, we skipped past all the zeros and x's. And if we do find a 1, then we move on to state q4, which will rewind for us. And we do the analogous thing for q2. On the other hand, if we encounter a blank instead of a 1, then we reject. There were no 1's to match. Or in this case, if we're looking for a 0 and encounter a blank instead, that means that there's no 0 to match. Assuming we do find a match, we end up in state Q4 which then rewinds. It moves left past all the Xs, the zeros, and the ones until it finds that blank symbol, which we crossed out initially in this transition here. It exes that out and then moves to the right, so that we're ready to process the next symbol. If you completed all those exercises then you're well on your way to understanding how Turing machines compute and hopefully also are ready to be convinced that they can compute as well as any other possible machine. Before we move onto that argument however there's some terminology about Turing machines and the languages that they accept and reject and those that they might loop on that we should set straight. Some of the terms may seem similar, but the distinctions are important ones, and we'll use them freely throughout the rest of the course so pay close attention. First, we define what it means for a machine to decide a language. We say a Turing machine decides a language L if it accepts every string in the language and it rejects every string not in the language. For example, the Turing machine we just described decided the language L consisting of strings of the form w#w where w was a binary string. We also might say that the Turing machine computed the function that is 1 is X is in the language and 0 otherwise. Or even just that the Turing machine computed the language at all. Now for a question that is a little tricky. Consider the language that consists of all binary strings that contain a symbol 1. Does this turning machine here decide that language? If the string does not contain a 1, however, the Turing machine will never halt. It will just stay in this loop here and state q0 forever. This possibility of Turing machines looping forever leads us to define the notion of a language recognizing. We say that a Turing machine recognizes a language if and only if it accepts every string in the language and does not accept any string not in the language. Thus we can say that the turning machine from the quiz does indeed recognize the language consisting of strings that contain a 1. It accepts those containing a 1. And it doesn't accept the others. Instead, it loops on them. Contrast this definition with what it takes for a Turing machine to decide a language. Then, it must not only accept every string in the language but it has to reject every string not in the language. It can't loop like this Turing machine. If we wanted to build a decider for this language, we would need to modify the Turing machine so that it detects the end of the string and moves into the reject state like so. At this point, it also makes sense to define the language of the machine. Which is just the language that the machine recognizes. After all, every machine recognizes some language even if it's the empty one. Formally, we define L(M) to be the set of strings that m accepts. That's the language of the machine M. In this lesson, we examined the working of touring machines. And if you completed all the exercises, you should have a strong sense of how to use them to compute functions and decide languages. We've also seen how unlike some simpler models of computation, touring machines don't necessarily halt on all inputs. This forced us to distinguish between language deciders and language recognizers. Eventually, we'll see how this problem of halting or not halting will be the key to understanding the limits of computation. We've shown Turing machines can test the quality of strings, not something that can be computed by simpler models like finite or pushdown automata that you might have seen in your undergraduate classes. But equality is a rather simple problem. Can Turing machines solve the complex tasks that we have our computers do? Can a Turing machine do your taxes, or play a great game of chess? Next lesson we'll see that Turing machines can indeed solve any problem that our computers can solve, and truly do capture the idea of computation. As a computer scientist, you have almost surely written a computer program that, when you run it, just sits there spinning its wheels. You don't know whether the program is just taking a long time, or you have made some mistake in the code and the program is in an infinite loop. You might have wondered why nobody put a check in a compiler. That would test the code to see whether it would stop or loop forever. The compiler doesn't have such a check because it can't be done. It's not that the programmer's not smart enough or the computer's not fast enough. It is simply impossible to check arbitrary computer code to determine whether or not it will halt. The best it can do is simulate the program to know when it halts. But if it doesn't halt, you can never be sure if it will halt in the future. In this lesson, we'll prove this amazing fact, and beyond. Not only can you not tell whether a computer halts, but you can't determine virtually anything about the output of a computer. We build up to those results, starting with a tool we've seen from our first lecture, diagonalization. The Diagonalization argument comes up in many contexts, and is very useful for generating paradoxes and mathematical contradictions. To show how general the technique is, let's examine it in the context of English adjectives. Here, I've created a table with English adjectives both as the rows and as the columns. Consider the row to be the word itself, and the column to be the string representation of the word. For each entry, I've written a 1 if the row adjective applies to the column representation of the word. For instance, long is not a long word, so I've written a 0. Polysyllabic is a long word, so I've written a 1. French is not a French word, it's an English word so I've written a 0, and so forth. So far we haven't run into any problems. Now let's make the following definition, a heterological word is a word that expresses a property that its representation does not possess. We can add the representation of the word to the table without any problems. It is a long, polysyllabic, non-French word. But when we try to add the meaning to the table, we run into problems. Remember, a heterological word is one that expresses a property that its representation does not possess. Long is not a long word, so it is heterological. Polysyllabic is a polysyllabic word, so it is not heterological. French is not a French word, so it is heterological. In effect, we've taken the values along this diagonal and flipped them to create this row. What about heterological, however? If we say that this heterological, causing us to put a 1 here, then it applies to itself, so it can't be heterological. On the other hand, if we say that it is not heterological, causing us to put a 0 here, then it doesn't apply to itself and it is heterological. So there really is no satisfactory answer here. Heterological is not well defined as an adjective. For English adjectives, we tend simply to ignore the paradox, and politely say that we can't answer that question. Even in mathematics, the polite response was simply to ignore such questions until around the turn of the 20th century. When philosophers began to look for more solid logical foundations for reasoning, and for mathematics in particular. Naively, one might think that a set could be an arbitrary collection. But what about the set of all sets that do not contain themselves? Is this set a member of itself or not? This paradox posed by Bertrand Russell, wasn't satisfactorily resolved until the 1920s with the formulation of what we now call Zermelo-Fraenkel set theory or from mathematical logic. Consider the statement, This statement is false. If this statement is true, then it says that it's false, and if the statement is false, then it says so and should be true. It turns out that falsehood, in this sense, isn't well-defined mathematically. At this point, you've probably guessed where this is going for this course. We're going to apply the diagnalization trick to turning machines. Here's the diagonalization trick applied to Turing machines. We'll let M1, M2, etcetera, be the set of all Turing machines. Turing machines can be described as strings, so there are a countable number of them, so we can enumerate them like this. And we'll create a table as before. I'll define the function f(i,j) to be one, if machine i accepts the description of machine j and we'll define it as zero otherwise. For this example I'll fill out the table somehow. The actual values aren't important right now. Now consider the language L consisting of string descriptions of machines that do not accept their own descriptions. And let's add a Turing machine, which we'll call ML that recognizes this language to the grid. Well, again, we run into a problem. This row is supposed to have the opposite values of what's along the diagonal. But what about the diagonal element of this row? What does the machine do when it's given its own description? If it accepts itself, then ML is not in the language, so ML should not have accepted itself. On the other hand, if ML does not accept its string representation, then ML isn't a language, so ML should have accepted its string representation. Thankfully in computability, the resolution to this paradox isn't as hard to see as it is in set theory or mathematical logic. We just conclude that the supposed machine ML that recognizes the language L doesn't exist. Here, it is natural to object, well, of course it exists. I just run M on itself and if it doesn't accept, we accept. The problem is that M on itself might loop, or it might just run for a really long time and there's no way to tell the difference. The end result, then is that the language, L, of string descriptions of machines that do not accept their own descriptions is not recognizable. Recall that in order for a language to be decidable, both the language and it's compliment have to be recognizable. Since L is not recognizable, it's also not decidable. And neither is it's compliment, that is the language where the machine does accept it's own description. We'll call this DTM, D standing for diagonal. These facts are the foundation for everything that we will argue in this lesson. So make sure that you understand all these claims. If you think back to the diagonalization of Turing machines. You'll notice that we hardly refer to the properties of Turing machines at all. In fact, except at the end, we might as well have been talking about a different model of computation, say, the doomaflitchy. Perhaps, unlike Turing machines, doomaflitchies halt on every input,these models exist. A model that allowed one step for each input symbol would satisfy this requirement. How do we resolve the paradox then? Can't we just build the dumaflache that takes the description of a dumaflache as input and then runs it on itself? It has to halt so we can reject if it accepts, and accept if it rejects, achieving the needed inversion. What's the problem? Take a minute to think about it. The answer here is no. The Dumaflache cannot exist. It is possible to have models of computation that halt on every input. Finite state automata and pushdown automata have this property. And it is possible to have models of computation that can interpret their own descriptions. And negate the results like Turing machines. But, as the diagonalization argument shows, you can't have both. If the Dumaflache did exist, then we could create a program like this one, which I'll call Inverse D. It takes a Dumaflache description as input and simulates it on itself. If this simulation accepts, then InvD rejects. And if the simulation rejects, then inverse D accepts. The Dumaflache model allows us to do this, and the Dumaflache model must always halt. Yet what happens when we run inverse D on its own description? If inverse D on itself accepts, well then it must have rejected this time and that can't have happened. On the other hand, if it rejected then it must have accepted this time, and that can't happen. Hence the dumaflache can not exist. So far, we've only proven that one language is unrecognizable. One technique for finding more is a mapping reduction, where we turn an instance of one problem into an instance of another. Formally, we say that a language A is mapping reducible to a language B, if there's a computable function F. Where for every string w, w is in A, if and only if, f(w) is in B. We write this relation between languages using the less than or equal to sign. With a little m on the side to indicate that we're referring to mapping reducibility. It helps to keep in your mind a picture like this. Here we have the language A, a subset of sigma star. And here we have the language B, also a subset of sigma star. In order for the computable function f to be a reduction, it has to map each string in A to some string in B. And each string not in A to some string not in B. The mapping doesn't have to be one-to-one or onto, it just has to have this key property. Before using reductions to prove that certain languages are undecidable. It sometimes helps to get some practice with the idea of reduction itself, as a kind of warm up. WIth this in mind, we've provided a few programming exercises. Good luck. Now that we understand reductions, we're ready to use them to help us prove decidability and even more interestingly, undecidability. Suppose that we have a language A that's reducible to B, and let's say that I want to know whether some string x is in the language A. Well, if there's a decider for B then I'm in luck. I can use the reduction, which is a computable function, remember, that takes in one string and outputs another. I just need to feed in x and take the output and feed that into the decider for B. If B accepts, then I know that x is in A and if B rejects, then I know that it isn't. This works because by the definition of a reduction, x is in a, if and only if, r of x is in b and by the definition of a decider, this is true if and only if, d accepts r of x. Therefore the output of d tells me whether x is in a. If I can figure out whether a string is in b or not. Then by the properties of the reduction, this also lets me figure out whether the original string x was in A. That means that we can say that the composition of the reduction and the decider for B is itself a decider for A. Thus, the fact that A is reducible to B has four important consequences for decidability and recognizability. The easiest to see is that if B is decidable, then A is also decidable. As we've seen, we can just compose the reduction with the decider for B. The same logic also works if there's only a recognizer for B. If we input a string X that is in A, then R of X will be in B, so the recognizer will accept. And if we input a string that is not in A, then R of X will not be in B, so the recognizer will not accept. It might loop. But we know it won't accept and that's good enough for our recognizer for A. The other two consequences are just the counter positives of these. If A is undecidable, then this composition of the reduction and the decider can't be a decider for A. Since we're assuming that there is a reduction, the only possibility is that the decider for B doesn't exist, hence B is undecidable. And the same logic holds for the unrecognizability of A and B. Let's do a quick question on the consequences of there being a reduction between two languages. I find that the easiest way to keep things straight, is to think about this relation as referring to hardness. Thus, I would read this as saying B is as hard as A. The word reduction also makes sense in this light. We are reducing the hardness of A to that of B. The answer of course is the first and the last. If there's a decider for B then that let's us build the decider for A just by composing with the reduction. And the last statement is just a counter positive of the first. The second statement is false because it might very well be that A is decidable, but B is not. A could be easy while B could be hard. And the same logic applies to the third state. Now, we're going to use a simple reduction to show that this language B, consisting of the descriptions of Turing machines that accept something is undecidable. Our strategy is to reduce the diagonal language to it. In other words, we'll argue that deciding B is at least as hard as deciding the diagonal language. Since we can't decide the diagonal language, we can't decide B either. Here's one of many possible reductions. The reduction is a computable function whose input is the description of a Turing machine M. And it's going to build another machine N in this python-like syntax. First, we write down the description of a Turing machine by defining this nested function. Then, we return that function. An important point is that the reduction, R, never actually runs the machine N. It just writes the program for it. Note here, that in this example, N totally ignores the actual input given to it. It just runs M on it's own description. And does whatever that computation does. Hence, and it's either going to be a machine that accepts nothing, if M doesn't accept its own description, either by looping or rejecting. Or if M does accept its own description, then N is going to accept everything. In the one case, the language of N will be the empty set. And in the other case, it will be sigma star. A decider for this language B, would be able to tell the difference, and therefore tell us whether M accepted its own description. Therefore, if B had a decider we would be able to decide the diagonal language, which is impossible. So B cannot be decidable. Now we turn to the question of halting. As we have seen, not being able to tell whether a program will halt or not plays a central role in the diagonalization paradox. And it's at least partly intuitive that we can't tell whether a program is just taking a long time or if it will run forever. It shouldn't be surprising then that given an arbitrary program input pair, we can't decide whether the program will halt on that input but actually the situation is much more extreme. We can't even decide if a program will halt when it is given no input at all just the empty string. Let's go ahead and prove this. The language of descriptions of Turing machines that halt on the empty string is undecidable. We'll do this by reducing from the diagonal language. That is, we'll show that the halting problem is at least as hard as the diagonal problem. Here's one of many possible reductions. The reduction creates a machine, N That simply ignores the input and then runs the machine M on itself. If M rejects, then N is going to loop. Otherwise, N accepts. Note that because we're switching from a language about accepting to a language about halting, We have to return a rejecting non-acceptance into a looping one. The end result here is that if M accepts its own description, then N accepts every string, the empty one in particular. On the other hand, if M either rejects or loops on itself, then N will loop. A decider for the halting problem, could tell the difference between accepting everything and looping on everything. So the reduction is complete. At this point, it might seem that we've just done a bit of symbol manipulation. But let's step back and realize what we've just seen. We showed that no Turing machine can tell whether or not a computer program will halt or remain in a loop forever. This is a problem that we care about and we can't solve it on a Turing machine, or any other kind of computer. You can't solve the halting problem on your iPhone. You can't solve the halting problem in your desktop, no matter how many cords you have. You can't solve the halting problem in the cloud. Even if someone invents a quantum computer, it still won't be able to solve the halting problem. To misquote Nick Selby If you want to solve the halting problem, you're at Georgia Tech, but you still can't do that. It's not just the halting problem we can't solve, but we can't determine pretty much anything about what a computer does. Let's keep going. So far the machines we've made in our reductions ie the ends, have been relatively uncomplicated. They all either accepted every string or they accepted no strings. Unfortunately, reductions can't always be done that way. Since the machine that always loops and the machine that always accepts. Might both be in or might not be in the language we're reducing to. In these cases, we need N to pay attention to its input. Here's an example were we will need to do this. The language of descriptions of Turing machines, where the Turing machine accepts exactly one string. It doesn't make much of a difference which undecidable language we reduce from. So this time, we'll reduce from the halting problem to this single acceptance language. Again, there are many possible reductions. I like this one. We run the input machine M on the empty string. If M loops, then so will N. We don't accept one string, we accept none. On the other hand, if M does halt on the empty string, then we make N act like a machine in this language, S. We'll have it just accept the empty string, my favorite string. This works because if M loops on the empty string than N will loop on every input and hence accept nothing. On the other hand, if M does halt on the empty string, then N will accept exactly one string, the empty string. Once again, in one case, the language of N is the empty set. In the other case, the language of N is the empty string. A decider for the language S can tell the difference. And therefore, would be able to decide if M halted on the empty string or not. Since this is impossible, a decider for this single acceptance language cannot exist. Now you get a chance to practice doing your reductions on your own. I have been using a python-like syntax in these examples, so they shouldn't feel terribly different. I want you to reduce the language, which I'll call loop, which consists of descriptions of machines that loop on the empty string. And we want to reduce that to the language L, which consists of Turing machines that do not accept the number 34 written out in binary. As we'll see in a later lesson, it's not possible for me to perfectly verify your software. But, if you do a straightforward reduction, then you should pass the test provided. Here is my answer. We run the input program M on the empty string. If this loops then N will as well. On the other hand, if this halts, then we just accept everything including the string 34. In one case, the language of N is the empty set. In the other, it is sigma*, every input string. And a decider for L could tell the difference. Now for a slightly more challenging reduction. We want to reduce the halting problem to the language L of Turing machines that accept strings of the form 0 to the n, 1 to the n. Here's my answer. This is just a souped up version of the filtering idea that we saw before. We run the input machine M on the empty string. If it loops, then so will N, and it will accept nothing. So N is not in the language L. On the other hand, if M halts, then N behaves like a machine in the language returning true if the input string matches 0 to the n, 1 to the n and false otherwise. In the one case, the language of N is the empty set. In the other case, it is the set of strings of the form 0 to the n, 1 to the n. And, of course, a decider for L can tell the difference. We've seen a few examples, and you've practiced writing reductions on your own at this point. Now I want you to test your understanding by telling me which of the following statements about these two languages are true. This one is the familiar halting problem. This one we'll call the empty problem. The set of descriptions of Turing machines, where the Turing machine accepts nothing. Think very carefully, and write out on pen and paper, any reductions that you find. The answer is that only this first statement is true, and it shows that the empty problem is undecidable. The reduction is straightforward. The output machine just runs the input machine on the empty string. And if it halts, it accepts. If the input machine is in H bar that means that it loops on the empty string. And so will the output machine, and it will accept nothing. On the other hand, if the input machine is not an HTM, that means it does halt. And then the output machine will accept everything. Use pencil and paper to confirm that this reduction works for yourself. If you try to use a similar argument for these next reductions, you'll find that the machines output by the reduction, except when you want them to loop or reject, and loop or reject when you want them to accept. But, proof by Lack of imagination isn't very satisfactory. So let's actually show that if these statements were true, then we would get a contradiction. If we could find these reductions, then this one would directly imply that the halting problem is decidable, and this last one would imply that the empty problem is decidable. Let's erase the rest and focus on this contradiction here. We'll let R be the reduction such that the description of the machine M is in H if and only if R of them is in E. Then we let A be a recognizer for the compliment of E. The compliment of E is the set of machines that except something. We talked about how we can recognize this language with the dove tailing strategy before, so assume that A implements that. So to decide H we run M on empty string and A on R of M in parallel. If M halts on the empty string, we recognize that fact. And if it does not, we'll then R of M must be an E complement so A must recognize that. And running these in parallel allows us then to decide the halting problem. One way or another, we're going to ge tan answer from one of these two machines. Overall then, the moral of the story is that you can't mapping reduce a recognizable language, like the halting problem to a co-recognizable like the empty problem. A co-recognizable language, by the way, is one whose complement is recognizable. And vice versa doesn't work either. Keep this in mind when you're trying to prove undecidability. Once you've gained enough practice, these reductions begin to feel a little repetitive. And it's natural to wonder whether there is a theorem that would capture them all. Indeed there is, and it is traditionally called Rice's Theorem, after H.G Rice's 1953 paper on the subject. This is a very powerful theorem, and it implies that we can't say anything about a computer just based on the language that it recognizes. So far the pattern has been that we've wanted to show that some language L was undecidable, and this language L was about descriptions of Turing machines whose languages, that is the set of strings that they accept, have a certain property. Two things have to be true about this language. First, as we have said, that the membership can only depend on the language of the machine, not about its particular implementation like the number of states or the tape alphabet, and so forth. Second is that the language can't be trivial, either including or excluding every Turing machine. We'll assume that there's some machine M1 in the language and another machine M2 outside the language. That's the only additional assumption we need. Recall that in all the reductions, we created a machine N, that either accepts nothing, or else it has some other behavior, depending on the behavior of the input machine M. Similarly, there are two cases for Rice's Theorem. Either the empty set is in P in and therefore, every machine that doesn't accept anything is in the language L, or else the empty set is not in P. Similarly, there are two cases for Rice's theorem. Either the empty set is in P, and therefore every machine that doesn't accept anything is in the language L, or else the empty set is not in P, and every language that doesn't accept anything is not in L. Let's take a look at the case where the empty set is not in P first. In that case, we reduce from the halting problem. The reduction looks like this, and just runs M with the end to end input. And if M halts, then we define N to act just like this machine M1, which is in the language. Thus, N acts like M1 if M halts on the empty string, and it loops otherwise. This is exactly what we want. In the one case, the language of N is the language of M1, and hence, N must be in the language. In the other case, the language of N is the empty string, meaning that it's not in L. And a decider for L can thus tell the difference and tell us whether M halted on the empty string or not. Now for the other case where the empty set is in P. Thus every machine that doesn't accept any string shouldn't be in the language. In this case, we just replace M1 by M2 in the definition of the reduction, so that N behaves like M2, if M halts on the empty string. This is fine, but we need to reduce from the complement of the halting problem. That is, from the set of descriptions of Turing machines that loop on the empty input. Otherwise, we would end up accepting when we want to not accept and vice versa. All in all then, we have proved the following theorem. We'll let L be a subset of strings representing Turing machines having two key properties. First, if M1 and M2 recognize the same language, that is the same set of strings, then their descriptions are both in or out of the language. This just says that the language only depends on the behavior of the machine. And, not on its implementation. Second, the language can't be trivial. There must be a machine whose description is in the language and a machine whose description is not in the language. If these two properties hold, then the language L is undecidable. Using Rice's theorem, we now have a quick way of detecting whether certain questions are decidable. Use your knowledge of the theorem to indicate which of the following properties is decidable. For clarity's sake, let's say that a virus is a computer program that modifies the data on the hard disc in some unwanted way. And the answer here is the last two. The question of is a program a virus is a question about the program's behavior as I've defined it. Also, some programs are viruses and some aren't, so this can't be decided in general. We can look for specific viruses and we can monitor the behavior of programs for suspicious activity. Doing this is a billion dollar industry. But we're never going to get an all purpose virus detector. Nor am I ever going to be able to write the perfect test code to see if the Turing machine that you programmed to decide balance strings actually works. I can write test cases, and detect common errors. But I could never decide completely whether your programs are right or wrong. On the other hand, I can write a program to test whether your reduction of HTM to the language of Turing machines that accept nothing is decidable, I simply reject. Remember that no such reduction can exist, because HTM is recognizable, and ETM is corecognizable. And lastly, this question about whether a program will halt in 10 to the 200 steps is decidable. I just run the program for up to that many steps. Of course that number is so large that in practice, even on the fastest computers, the heat death of the universe will have occurred by then. But nevertheless, the question is decidable. The reason Rice's theorem doesn't apply here is because this question is about machines, not about languages. The purpose of this quiz is not to tell you that undecidability is a concept with no practical implications. Rather, it is to help you to see what it does and does not tell us. Just because a problem is decidable doesn't mean that it's tractable, or practical to solve. Also, just because a question is undecidable in general, doesn't mean that we can't write programs to tackle specific cases, as we do everyday with malware detection and software testing. It does however say that we should not expect an all-purpose solution, and any attempt to achieve that end is doomed to fail. I want to end this section on computability, by revisiting the scene we used at the beginning of the course. Where the teacher was explaining functions in terms of ordered pairs, and the students were thinking of what they had to do to X in order to get Y. The promise of our study of computability was to better appreciate the difference between these understandings. And I hope you will agree that we have achieved that. We've seen how there are many functions that are not computable in any ordinary sense of the word, by counting argument. We made precise what we meant by computation, going all the way back to Turing's inspiration from his own experience with pen and paper, to formalize the Turing machine. And we argued for the Church Turing thesis, that this model can compute anything that a computer today, or any one envisioned for tomorrow can. Lastly, we described a whole family of uncomputable functions through Rice's theorem. What's next in the study of computability? Let's ask Lance. Ever since the pioneering work of Turing and his contemporaries, such as Alonzo Church and Kurt Gdel, mathematic logicians have studied the power of computation and connecting it to prove ability, as well as giving us new insights on the nature of information, even philosophy and economics. Computability has a downside, just because we can solve a problem on a Turing machine, doesn't mean that we can solve it quickly. What good is having a computer solve a problem, if our sun explodes before we get to the answer? So for now we'll leave the study of computable functions and languages, and move to computational complexity. Trying to understand the power of efficient computation. And learning about the famous P verses NP question. In 1936 when Alan Turing wrote his famous paper on computable numbers he not only created the Turing machine but had a number of other major insights on the nature of computation. Turing realized that the computer program itself could also be considered as part of the input. There really is no difference between a program and data. We all take that as a given today. We create computer code in a file that gets stored in the computer and no differently than any other type of file. And data files often have computer instructions embedded in them. Even modern computer fonts are basically small programs that generate readable characters at arbitrary sizes. Once Turing took the view of program code as data he had the beautiful idea that a Turing machine could simulate that code. There is some fixed Turing machine, a universal Turing machine, that can simulate the code of any other Turing machine. Again this is an idea that we take for granted today as we have interpreters and compilers that can run the code of any programming language. But back in Turing's day, this idea of a universal machine was a major breakthrough. It allowed Turing, and will allow us to develop problems that Turing machines, or any other computer cannot solve. Before we can simulate a Turing machine, we first have to represent it using a string. Notice that this represents an immediate challenge. Our universal Turing machine must use a fixed alphabet for its input and have a fixed number of states. But it must be able to simulate Turing machines with arbitrarily larger alphabets and an arbitrary number of states. As we'll see, one solutions is essentially to enumerate all the symbols and states and represent them in binary. There are lots of ways to do this. The way we're going to do it, is a compromise of readability and efficiency. So we'll let M be a turning machine with states q0 through qn- 1. And we'll let it's tape alphabet be gamma a1 through am. We'll define i and j so that 2 to the i is at least the number of states. And 2 to the j is at least the size of the tape alphabet. Then we can encode a state qk as the string qw, where w is the binary representation of k. For example, if the number of states is 6, then we would need i to be 3, and so we would encode q3 as the string q, and then 3 written out in binary, 011. By the way, we'll use the convention that q followed by the binary representation of 0 is the initial state. q followed by the binary representation of 1 is the accept state, and q followed by the binary representation of 2 Is the reject state. We encode symbols much in the same way as we encoded states. We'll use a followed by w, where w is the binary representation of this k to indicate the kth symbol. For example, if there are 10 different symbols, then we need 4 bits to represent them all. And we might encode a5, which could be any symbol. Maybe it's a star. We would encode that as the string, a, followed by the binary representation of 5, 4 plus 1 is 5. Let's see an encoding for an example. This example decides whether the input consists of a number of zeros, that is a power of two. To encode the Turing machine as a whole, we really just need to encode the transition function. We'll start by considering this edge here. We're going from state 0, so I'll write that this way. We see the symbol 0, so I'll write the encoding for that like so. And then we're to state 3, so i'll write out that here. And then we need to write the $, so i'll write out the encoding for that, like so. And we're supposed to move to the right. So I'll write the R there. Remember that the order is input state, input symbol, output state, output symbol, and then direction. Next, let's do this blue transition here. We go from state 3, and we read a 0. So I encode that that way. And then we're going to state 4. So, I'll write the encoding for that with q, followed by the binary representation of 4. And we're supposed to write x. So I look up the encoding for x, that's a10, like so. And we want to move the head to the right. So I include an R here. So that's the convention we'll use to write out the transition function. It's just a sequence of these five tuples of this form, input state, input symbol, output state, output symbol and then direction. I'm not going to write out all of the transitions for this machine, but I think it would be a good idea for you to do just one more. So, use this red box here, to encode this red transition. The answer is this here. We start from state 4, so I write that out, as q, followed by the binary representation of 4. We read an x, so I write out the representation of that symbol, and then we go back to state q4. So I write that out again here. And then we want to leave this x alone, so I write x, or rather the encoding of x, as the symbol to write. Then we want to move to the right. So I write R as the last element of the pool. Now we're ready to describe how to build this all powerful universal Turing machine. As input to the universal machine, we give the description of the machine we want to simulate, and the input we want to simulate it on, these two things separated by a #. The goal is to simulate m's execution when given the input w. That means we loop, accept, or reject, just as M would on w. And if M does halt, we want the universal machine to output the encoding of the tape contents that M would have on w. We'll describe as three tape Turing machine. That achieves this goal of simulating M on W. The input comes in on the first tape. And here I've just written out the mathematical description. And first, we'll copy this description to the second tape. And we'll put the initial state on the third tape. For example, the tape contents might end up like this. Then we rewind the heads and begin the second phase of actually simulating the execution. Here we searched for the appropriate tupal in the description of the machine. The first element has to match the current state stored on tape three. And the symbol part has to match the encoding on tape one. If no match is found, then we just halt the simulation, and put the universal machine in an accepting or rejecting state, according to the current state of the machine being interpreted. If there is a match, however, then we apply the changes to the first tape. Moving ahead to the right position and so forth, and then repeat this process. So, actually, interpreting a Turing machine description is surprisingly easy. We've just seen how Turing machines are indeed reprogrammable, just like real world computers. This lends further support to the Church Turing Thesis, but it also has a significance beyond that. Since the input to a Turing Machine can be interpreted as a Turing Machine, this suggests that programs are a type of data. But arbitrary data can also be interpreted as a possibly invalid Turing Machine. So is there any difference between data and program? I'll leave this for you to discuss with your friends over lunch or in the forums. At this point, the character of our discussion of computability is going to change significantly. We've established the key properties of Turing machines. That they can do anything we mean by computation, and that we can pass a description of a Turing machine to a Turing machine as input for it to simulate. With those points established, we won't need to talk about the specifics of Turing machines much anymore. There will be little about tapes, or states, transitions, or head positions. Instead, we'll think about computation at a very high level. Trusting that if we really had to, we could write out the Turing machine to do it. If we need to write out code, we'll do so mostly in pseudocode or with pictures. What is there left to talk about? Well remember from the very first lesson that we argued that not all functions are computable or as we later said not all languages can be decided by Turing machines. We're now in a good position to talk about some of these undecidable languages. We had to wait until we established the universality of Turing machines. Because these languages are going to consist of strings that encode Turing machines. The rest of the lesson we'll review the definitions of recognizability and decidability and then we'll talk about the positive side of the story. The languages about Turing machines that we can decide or recognize. As we'll see later, there are plenty that we can't. Recall that a Turing machine recognizes a language if it accepts every string in the language and does not accept any string not in the language, it could either reject or loop. This Turing machine here recognized the language of binary strings containing one, but it looped on those that don't contain a one. In order to decide the language, the Turing machine must not only accept every string in the language, but it has to reject every one not in the language. This machine achieves that goal for the same language by explicitly rejecting the input when we reached the end of the string. Ultimately, however, we're not interested in whether a particular Turing machine recognizes or decides a language, rather we're interested in whether there is a Turing machine that can recognize or decide the language. Therefore, we say that a language is recognizable if there is a Turing machine that recognizes it. And we say that a language is decidable, if there is a Turing machine that decides it. Now looking at this someone might object shouldnt we say, recognizable by a Turing machine and decidable by a Turing machine. Well, of course, we could and the statements would still be true, but we don't. And the reason we don't is that we strongly believe that if anything can do it, a Turing machine can. That's the Church turn thesis. So in some absolute sense, we believe that a language is only recognizable by anything if a Turing machine can recognize it. And a language is only decidable by anything, if a Turing machine can decide it and we use terms that reflect this belief. Now, other terms are sometimes used instead of recognizable and decidable. Some say that Turing machines compute languages. So to go along with that, they say that languages are computable if there's a Turing machine that computes them. Another equivalent term for decidable is recursive. Mathematicians often prefer this word and those that use that term will refer to recognizable languages as recursively innumerable. Some also call these languages Turning acceptable or semi or partially decidable. We should also make clear the relationship between these two terms. If a language is decidable, then it's also recognizable. The same Turing machine works for both. It feels like it should also be true that if a language is recognizable and it's compliment is also recognizable, then the language is decidable. This is true, but there's a potential pitfall here that we need to make sure to avoid. Ultimately however, we're not interested in whether a particular Turing machine recognizes or decides a language. Rather, we're interested in whether there is a Turing which machine that can recognize or decide the language. Therefore, we say that a language is recognizable if if there is a Turing machine that recognizes it. And we say that a language is decidable if there is a Turing machine that decides it. Now looking at this someone might object, shouldn't we say recognizable by a Turing machine and decidable by a Turing machine well of course we could and the statements would still be true. But we don't. And the reason we don't is that we strongly believe that if anything can do it, a Turing machine can. That's the Church-Turing thesis. So as it's absolute sense, we believe that a language is only recognizable, by anything, if a Turing machine can recognize it. And the language is only decidable, by anything. If the Turing machine can decide it. And we use terms that reflect this belief. Now other terms are sometimes used, instead of recognizable and decidable. Some say that Turning machines compute languages, so to go along with that, they say that languages are computable, if there's a Turing machine that computes them. Another equivalent term for decidable is recursive. Mathematicians often prefer this word. And those who use that term, will refer to recognizable languages, as recursively enumerable. Some also call these languages Turing acceptable, or semi or partially decidable. We should also make clear the relationship between these two terms. If a language is decidable, then it's also recognizable. The same Turing machine works for both. It feels like it, it should also be true, that if a language is recognizable and its complement is also recognizable, then the language is decidable. This is true, but there's a potential pitfall here that we need to make sure to avoid Suppose that we are given one machine M1 that recognizes a language L and another machine M2 that recognizes L's compliment. If we were to ask your average programmer to use these machines to decide the language, his first guess might go something like this. Run M1 on x, and accept if it accepts. Otherwise, run M2 on x, and reject if it accepts. One of these two has to produce an accept after all. This program will not decided L, however. And I want you to tell me why. So look at these, and check the best answer. The problem here is that M1 might not halt. If M1 accepts, then the program is fine. If M1 rejects, then we'll get to here, M2 will accept, and D will reject. That's also correct. If M1 loops, however, then this whole program will loop. Yet we need it to reject in order for D to decide the language L. Note that M2 looping can't be a problem because it can only loop when x is in the language, but then M1 would of accepted and we would have accepted by that point. Also, if this program returns any answer, it will be correct. So both of these are wrong for that reason. All right, so how do we fix this problem of looping? Well one idea is to run the two programs parallel, and accept if M1 accepts, and reject if M2 does. Now we can't actually run the two programs in parallel, but we can do the next best thing by switching between the two at every step. First running a step of M1, then one of M2, then one of M1, etc. Here's the alternating trick in some more detail. Suppose that M1 recognizes L and M2 recognizes L complement. We want to decide the language L. In pseudo code the alternating strategy might look like this. In every iteration we execute both machines, one more step than we did the last time. Note that it doesn't matter if we save the machines' configuration and start where we left off, or start over. The question is whether we get the right answer, not how fast. The string has to either be an L, or an L complement. So one of these has to halt after some finite number of steps. And when i hits that value this program will give the right answer. Overall then we have the following theorem. A language L is decidable if and only if L and its compliment are recognizable. Now we're going to go through a series of languages and try to figure out if they and their compliments are recognizable. First, let's examine the set of strings that describe a turning machine that has at most a 100 states. You can assume the particular encoding for turning machines that we used, but any encoding will serve the same purpose. Indicate whether you think that L is recognizable. And whether you think L complement is recognizable. Now we don't have a way of proving yet that a language is not recognizable, so I've labeled the no option also as unclear. The answer is Yes for both languages. We just count up the number of states in the machine, and compare this to 100. And that gives us a decision both ways. Note that this doesn't involve actually running M at all. And because both L and its complement are recognizable, this language is decidable. Next, we considered the set of Turing machines that halt on the number 34 written in binary. Indicate whether L and L complement are recognizable. The answer is that L is recognizable. A machine that recognizes L is fairly simple. It just feeds the input 100010 into M and then accepts after M halts. If M loops, so then so will this machine. If it doesn't loop, then this machine will accept. Note that it's not clear if it's possible to recognize the complement. If you haven't taken a class on the theory of computation before, then you might think it possible that there are only some dozens or hundreds of ways that infinite loops occur, and that you could check all of those cases. This is at least unclear, and as we'll see later, it actually turns out to be false. Simulation defer this recognized in this language doesn't work either. We can't just run the machine and wait until it doesn't halt. That doesn't make any sense. Another possible idea is to monitor the machine by keeping track of every configuration it has been in, and if it ever repeats a configuration, to declare that it doesn't halt. This might detect some non halting behavior, but because a Turing machine's tape is infinite, it can run forever without actually ever repeating a configuration. Students will sometimes object that this strategy of monitoring for loops would work on any computer with finite memory. The trouble is that it would require more memory than the machine being monitored would have access to. So the monitor would have to be a different computational model than the machine it's monitoring. As a practical method of detecting cycles, this strategy doesn't work either. Consider the fact that a computer with a gigabyte of RAM has over 8 billion bits and therefore over two to the 8 billion possible configurations. That's a lot of possible configurations before you detect a cycle. The bottom line is that this objection isn't satisfactory, either in theory or in practice. Monitoring and looking for cycles of configurations just doesn't cut it. Let's consider another language. This time, instead of Turing machine descriptions. Where the Turing machine accepts nothing. Tell me, are either L or L complement recognizable? The answer is that L is not recognizable, but L complement is. Note that there are some Turing machines that accept nothing that we can tell accept nothing. We can examine the code, for instance of the Turing machine that immediately transitions to the reject state no matter what. And we can see that such a Turing machine never accepts anything. But it's not clear that we can do that in general. On the other hand, it turns out that we can recognize L complement, the set of Turing machines that accept something. We just run the Turing machine on all strings and see if accepts any of them. Now, if you answered no here because you thought of this possibility, but were worried that some of these computations might not halt and that that would ruin it, then actually good for you. It shows that you're thinking carefully. This is a real problem, because we can't just run M on the empty string, and then run it on the zero string, and so forth. The computation on the empty string might never halt. The solution is essentially to run all these computations in parallel. Note that this is challenging, but it can be done through a strategy called dovetailing. Here's the dovetailing trick which lets you run a countable set of computations all at once. We'll illustrate the technique for the case where we're simulating a machine M on all binary strings with this table here. Every row in the table corresponds to a computation, or a sequence of configurations that the machine goes through for the given input. Simulating all these computations means hitting every entry in the table. Note that we can't just simulate M on the enpty string first. Or we might just keep going forever filling out the first row, and never getting to the second. This is the same problem that we encountered when trying to show that a countable union of countable sets is countable, or that the set of rational numbers in countable. And the solution is the same too. We go diagonal by diagonal, first simulating the first computation for one step, then the second computation for one step. And the first computation for two steps. Then the third computation for one step. The second computation for two steps. And the first computation for three steps. And we just keep going, diagonal by diagonal. Eventually every configuration in the table is reached. Thus, if we're trying to recognize the language of Turing machine descriptions, where the Turing machine accepts something. Then a Turing machine in this language must accept a string after some fixed finite number of steps. Say that configuration appears here in the table. Well then we'll eventually reach it with the dovetailing strategy and we can accept. Let's consider one last language. The set of descriptions of turing machines that halt on every input. Think carefully and indicate whether you think that either L or L compliment is recognizable. The answer is that neither language is recognizable or at least it isn't clear. We can't just simulate every input because no matter how long we run the simulation. Even with dovetailing, there will be some input that we haven't checked yet. Now consider the confident language, those Turing machines that don't halt on some input. Well I could try simulating all inputs until I find one it does not halt on. But how would I tell that it doesn't halt? It's unclear for now and we will see how actually we can't tell in general. In this and our previous lessons, we developed a set of ideas and definitions. To set the stage for understanding what we can, and cannot solve on a computer. We've seen the Turing machine, an amazingly simple model that can only move a tape back and forth, while reading and writing on that tape. Yet we see that this model capture the full power of computers now and forever. You see now how to consider Turing machine programs as data themselves. And create a universal Turing machine that can simulate those programs. At the end of this lesson, we saw some languages defined using program as data, that don't seem to be easily decidable. In the next lecture, we will show how to prove many languages cannot be solved by a Turing machine. Including the most famous one, the halting problem.