So welcome to Artificial Intelligence for Robotics. You are entering at exciting 7-week class in which you'll learn how to program self-driving cars. And just to motivate what we're trying to achieve in this class, let me show you some videos. My interest in self-driving cars started with the DARPA Grand Challenge in 2004 in which my team at Stanford developed Stanley, a robot that could drive itself through the Mojave Desert. The vehicle was based on a Volkswagen Touareg that was equipped with all kinds of sensors like GPS and laser, and it was able to make its own decisions without any human input whatsoever. The DARPA Grand Challenge was a government-sponsored race that took place in 2005. Here we see our robot Stanley moving through the desert completely without a human on board. The task was to drive a desert trail for about 130 miles, and whoever was fastest would win the race. Here we're passing a different robot by Carnegie Mellon University about 110 miles into the race. Our robot was able to navigate really steep mountainous roads and able to avoid collisions with rocks or falling down a cliff all based on its ability to use what I'm going to teach you in this class. After almost 7 hours and 131 miles our robot returned all the way to the starting base as the first robot to ever finish a DARPA Grand Challenge winning Stanford University 2 million bucks and Stanley a place in the Smithsonian Museum of American History. This work led to the Urban Challenge, in which we built another robot called Junior that eventually took second place. The Urban Challenge was a followup race by DARPA in which cars were asked to drive in traffic, so whereas the Grand Challenge was kind of a motionless desert floor, this was a mock urban city where the robot was able to interact with other traffic and had to follow the traffic rules as in this left turn over here. It had to be stay on lanes with very high precision, accommodate oncoming traffic and just drive confidently in a situation that really resembled a small city. This led at Google to a sequence of experiments known as the Google self-driving car. I believe these are the best robotic cars out there today. Here we see one of our Priuses on University Avenue in Palo Alto they are undetected, driving just like a human driver, but this car was driving by itself. Our cars have been able to drive hundreds of thousands of miles all across California and some of Nevada, in downtown areas like San Francisco, on busy highways. Here in Monterey, a small coastal city in California with lots and lots of pedestrians. These are all completely self-driving moments where the car is able to accommodate things like deer in the headlights in the middle of the night or even crooked Lombard Street in San Francisco as shown in this video. This is what I'm doing on my day job. I really love, with my team, building self-driving cars. We believe it's going to really change the world, and in this class that's what I hope to enable you to do. Let's dive in. The very first problem I'm trying to solve is called localization. It involves a robot that's lost in space. It could be a car. It could be a mobile robot. So here is the environment, and the poor robot has no clue where it is. Similarly, we might have a car driving on a highway, and this car would like to know where it is. It is inside the lane or is it crossing lane markers? Now the traditional way to solve this problem is by using satellites. These satellites emit signals that the car can perceive. That's known as GPS, short for "global positioning system," and it's what you have in your dashboard if you have a car with GPS that shows you the maps and shows you where you are. Now unfortunately, the problem with GPS is its really not very accurate. It's really common for a car to believe to be here but it has 2 meters all the way up to 10 meters of error. So if you try to stay in the lane with 10 meters of error, you're far off, and you're driving right over here, and you crash. So for our self-driving cars, to be able to stay in lanes using localization, we need something like 2-10 centimeters of error. Then we can drive with GPS in lanes. The question is, how can we know where were are with 10 cm accuracy? That's the localization question. In the Google self-driving car, localization plays a key role. We record images of the road surface and then use the techniques I'm just about to teach you to find out exactly where the robot is. It does so within a few centimeters of accuracy, and that makes it possible to stay inside the lane even if the lane markers are missing. So localization has a lot of math, but before I dive into mathematical detail, I want to give you an intuition for the basic principles. I want to tell you the story of how we will localize this, and then we can go through the math together so you can understand it. I also want to let you program your own localizer so you can program a self-driving car. Let me begin my story in a world where our robot resides. Let's assume the robot has no clue where it is. Then we would model this with a function--I'm going to draw into this diagram over here where the vertical axis is the probability for any location in this world, and the horizontal axis corresponds to all the places in this 1-dimensional world. The way I'm going to model the robot's current belief about where it might be, it's confusion is by a uniform function that assigns equal weight to every possible place in this world. That is the state of maximum confusion Now, to localize the world has to have some distinctive features. Let's assume there are 3 different landmarks in the world. There is a door over here, there's a door over here, and a 3rd one way back here. For the sake of the argument, let's assume they all look alike, so they're not distinguishable, but you can distinguish the door from the non-door area--from the wall. Now let's see how the robot can localize itself by assuming it senses, and it senses that it's standing right next to a door. So all it knows now is that it is located, likely, next to a door. How would this affect our belief? Here is the critical step for localization. If you understand this step, you understand localization. The measurement of a door transforms our belief function, defined over possible locations, to a new function that looks pretty much like this. For the 3 locations adjacent to doors, we now have an increased belief of being there whereas all the other locations have a decreased belief. This is a probability distribution that assigns higher probability for being next to a door, and it's called the posterior belief where the word "posterior" means it's after a measurement has been taken. Now, the key aspect of this belief is that we still don't know where we are. There are 3 possible door locations, and in fact, it might be that the sensors were erroneous, and we accidentally saw a door where there's none. So there is still a residual probability of being in these places over here, but these three bumps together really express our current best belief of where we are. This representation is absolutely core to probability and to mobile robot localization. Now let's assume the robot moves. Say it moves to the right by a certain distance. Then we can shift the belief according to the motion. And the way this might look like is about like this. So this bump over here made it to here. This guy went over here, and this guy over here. Obviously, this robot knows its heading direction. It's moving to the right in this example, and it knows roughly how far it moved. Now, robot motion is somewhat uncertain. We can never be certain where the robot moved. So these things are a little bit flatter than these guys over here. The process of moving those beliefs to the right side is technically called a convolution. Let's now assume the robot senses again, and for the sake of the argument, let's assume it sees itself right next to a door again, so the measurement is the same as before. Now the most amazing thing happens. We end up multiplying our belief, which is now prior to the second measurement, with a function that looks very much like this one over here, which has a peak at each door and out comes a belief that looks like the following. There are a couple of minor bumps, but the only really big bump is this one over here. This one corresponds to this guy over there in the prior, and it's the only place in this prior that really corresponds to the measurement of a door, whereas all the other places of doors have a low prior belief. As a result, this function is really interesting. It's a distribution that focuses most of its weight onto the correct hypothesis of the robot being in the second door, and it provides very little belief to places far away from doors. At this point, our robot has localized itself. If you understood this, you understand probability, and you understand localization. So congratulations. You understand probability and localization. You might not know yet, but that's really a core aspect of understanding a whole bunch of things I'm going to teach you in the class today. So let's move into our first programming exercise, and let's program together the very first version of robot localization. Here's a bit of program code--an empty list. And what I'd like you to program is a world with 5 different cells or places where each cell has the same probability that the robot might be in that cell. So probabilities add up to 1. Here's a simple quiz for the cells x1 all the way to x5. What is the probability of any of those x's? Index i goes from 1 to 5. And the answer is 0.2, which is the total probability, 1, divided by 5 grid cells. So now in our Python interface, I'd like to take this code over here, which assigns to p an empty list and modified into code where p becomes a uniform distribution over 5 grid cells as expressed in a vector of 5 probabilities. Here's an easy solution. You just initialize the vector with five 0.2s. Let's see if we can modify this to make a vector of length n where I can vary the value of n and get a resulting vector with n elements. So if n equals 5, we'd get the same result as before, but for n equals 10, I should get a vector of length 10, each of which would have value zero point one. The answer is simple. Use a for loop as shown here, and you append to the list n elements, each of size of 1/n. The dot over here is really important. It gives you the floating point version. Unfortunately, if we leave it out, the result will just be zeros, which is not what you want. Now we are able to initialize the initial belief of the robot in the world over here. Let's look at the measurement of this robot in its world with 5 different grid cells-- x1 through x5. Let's assume 2 of those cells are colored red whereas the other 33 are green. As before, we assign uniform probability to each cell of 0.2, and our robot is now allowed to sense. What it sees is a red color. How will this affect my belief over different places? Obviously, the one's for x2 and x3 should go up, and the ones for x1, x4, and x5 should go down. So I'm going to tell you how to incorporate this measurement into our belief with a very simple rule--a product. Any cell where the color is correct--any of the red cells-- we multiply it with a relatively large number--say, 0.6. That feels small, but as we will see later, it is actually a large number. Whereas all the green cells will be multiple with 0.2. If we look at the ratio of those, then it seems about 3 times as likely to be in a red cell than it is to be in a green cell, because 0.6 is 3 times larger than 0.2. Now let's do the multiplication. For each of those 5 cells, can you tell me what the result would be multiplying in the measurement in the way I've stated. Please, for these 5 boxes, fill out the number. The answer is obviously for the red cells we get a 0.12 whereas for the green cells we get a 0.04, which is the product of 0.2 x 0.6 versus 0.2 x 0.2. This is in principle our next belief. It only has one problem, which is it isn't a valid probability distribution. The reason why is probability distributions always have to add up to 1. If I ask you what the sum of all these values, then we find out it doesn't add up to 1. Please type in the sum of all these values. If you add up all these values, you get 0.36. To turn this back into a probability distribution, we will now divide each of these numbers by 0.36. Put differently, we normalize. Please, in these 5 fields enter your result for dividing 0.04 or 0.12 by 0.36. Please check that the sum of those truly adds up to 1. So 0.12 divided by 0.36 is the same as 12 divided by 36 is the same as 1/3 or 0.333. And 0.04 divided by 0.36 is the same as 4 divided by 36, that is 1/9. If you look at that numbers, 1/3, there's 1/3 plus 3/9 is another 1/3 gives exactly 1. So this is a probability distribution, which is often written in the following way. The probability of each cell, i where i could range from 1-5, after we've seen our measurement Z. The probabilist would also call it posterior distribution of place xi given measurement Z. Let's implement all this. Here's our distribution again. Here's our factor for getting the color right or for getting it wrong, and let's first start with a non-normalized version. Write a piece of code that outputs p after multiplying in pHit and pMiss at the corresponding places. One way to do this is to go explicitly through all these 5 different cases from 0 to 4 and multiply in manually the miss or hit case. This is not particularly elegant, but it does the job, and as I hit the "Run" button, we get the correct answer that is not normalized. My next question is can you print out the sum of those to normalize them? Modify the program so you get the sum of all the p's. Well, it turns out Python gives you a function called "sum," and if you now hit the run button, you get the correct answer. I want to make this a little bit more beautiful. I will introduce a variable called "world," and for each of the 5 grid cells, world specifies the color of the cell-- green, red, red, green, green. Further, I define the measurement Z to be red. Can you define a function, called "sense," which is the measurement update, which takes as input the initial distribution p and the measurement Z and all the other global variables and outputs a normalized distribution called "Q" in which Q reflects the non-normalized product of our input probability, which will be 0.2 and so on, and the corresponding pHit or pMiss in accordance to whether these colors over here agree or disagree? When I call sense(p, Z), I expect to get the vector as output as before, but now in the form of a function. The reason I'd like to have a function here is because later on as we build our localizer we will apply this to every single measurement over and over again. This function should really respond to any arbitrary p and arbitrary Z, either red or green, and give me the non-normalized Q, which gives me the vector 0.04 or 0.12 and so on and so on. Here's my solution. I start with an empty list over here, and I build it up using the append command over time. I do so by iterating over all the elements in my probability p, and I set a binary flag whether my measurement that I received is the same I would expect at the ith grid cell over here from this list over there. If the case hit is positive, it's true, so we're going to multiply p with pHit. If it's false, then the flag hit will valuate to zero, 1 - hit will be 1. You're going to multiply pi with pMiss. I build up the list, return it, and run it, and out comes 0.04, 0.12, 0.12, 0.04, and 0.04 as expected. Let's take that same piece of code and modify it to give me a valid probability distribution. Please modify this code so it normalizes the output of the function sense so it add up to 1. Here are the three lines of code I used to program this in. First I computed the sum of a vector Q using the function sum, which makes it really easy. Then I go through all the elements in Q and just divide it by s, which is the normalization, and when I run it, I get 1/9, 1/3, 1/3, 1/9, and 1/9. So I've just implemented the absolute key function of localization, which is called the measurement update. Let's just go back to our example and see what an amazing thing you've just programmed. We had a uniform distribution over places. Each place had a probability of 0.2. Then you wrote a piece of code that used the measurement to turn this prior into a posterior, in which the probability of the 2 red cells was a factor of 3 larger than the posterior of the green cells. You've done exactly what I gave you intuitively in the beginning as the secret of localization. You manipulated a probability distribution over places into a new one by incorporating the measurement. In fact, let's go back to our code and test in your code whether we get a good result when we replace our measurement red by green. Please type green into your measurement variable and rerun your code to see if you get the correct result. I'm now replacing the red by green over here, and I rerun my code and out come these funny numbers. Somewhere in there is the division by 44, but you can see that the 1st, the 4th, and the 5th grid cell have a much larger value than the grid cells in the middle. So let's dive in. In fact, I'd like you to modify this code a little bit more in a way that we have multiple measurements. Instead of Z, we're going to make a measurement vector called "measurements." We're going to assume that we're going to first sense red and then green. Can you modify the code that so it updates the probability twice and gives me the posterior after both of these measurements are incorporated? In fact, can you modify it in a way that any sequence of measurements of any length can be processed? The modification is simple. We will call the procedure sense multiple times, in fact, as often as we have measurements, which is the for loop over here, we grab the kth measurement element and apply it to the current belief. Then recursively update that belief into itself. In this case, we run it twice. We print the output. For this specific example, we get back the uniform distribution. These are all 0.2s approximately. The reason is we up multiplied each field once for the 0.6 and down multiplied for the 0.2. And these effects were in total the same for each cell. As a result, we get the same output over here. It's quite remarkable. Before we're done with localization, I'd like to talk about robot motion. Suppose we have a distribution over those cells-- such as this one: 1/9, 1/3, 1/3, 1/9, and 1/9-- and even though we don't know where the robot is, the robot moves, and it moves to the right. In fact, the way we're going to program is we will assume the world is cyclic, so if it drops off the right-most cell it finds itself in the left-most cell. Suppose we know for a fact the world moved exactly 1 grid cell to the right, including the cyclic motion. Can you tell me for all these 5 values, what the posterior probability is after that motion? The answer is all of these are shifted to the right. The 1/9 in the left-most cell goes over here, the 1/3 over here, and finally the right-most 1/9 finds itself on the left side. In the case of exact motion, we have a perfect robot. We just shift the probabilities by the actual robot motion. Now, that's a degenerate case. Let's program this one. I define a function "move" with a distribution p and a motion number "U," where U is the number of grid cells that the robot is moving to the right or to the left. I want you to program a function that returns the new distribution Q after the move where if U equals zero, Q is the same as p. If U equals 1, all the values are cyclically shifted to the right by 1. If U equals 3, they are cyclically shifted to the right by 3. If U equals -1, they're cyclically shifted to the left. Please call the function with argument p and a shift to the right by 1. I've commented out my measurement part because for now I don't want to do measurement updates. In addition to this, I will use a very simple p, that has a 1 at the second position and zeros elsewhere. Otherwise, if we were to use the uniform p, we couldn't even see the effect of the motion whether that's programmed correctly or not. Here is the solution. We start with the empty list. We go through all the elements in p. This is the tricky line. We will construct Q element-by-element by accessing the corresponding p, and p is shifted by U and if this shift exceeds the range of p on the left, we apply the modulo operator with the number of states as an argument. In this case, it'll be 5. Now, the reason why there is a minus sign is tricky. To shift the distribution to the right, U = 1, we need to find in p the element 1 place to the left. Rather than shifting p to the right directly, what I've done is I've constructed q by searching for where the robot might have come from. That's of course, in hindsight, from the left. Therefore, there is a minus sign over here. So think about this, as it's a little bit nontrivial, but it's going to be important as we go forward and define probabilistic convolution and generalize this to the noisy case. Let's talk about inaccurate robot motion. We are again given 5 grid cells. Let's assume a robot executes its action with high probability correctly, say 0.8, but with 0.1 chance it finds itself short of the intended action, and yet another 0.1 probability it finds itself overshooting its target. You can define the same for other U values. Say U = 1. Then with 0.8 chance it would end up over here, 0.1 it stays in the same element, and 0.1 it hops 2 elements ahead. Now this is a model of inaccurate robot motion. This robot attempts to go U grid cells, but occasionally falls short of its goal or overshoots. It's a more common case robots as they move accrue uncertainty, and it's really important to model this, because this is the primary reason why localization is hard, because robots are not very accurate We're now going to look into this first from the mathematical side. I will be giving you a prior distribution, and we're going to be using the value of U = 2, and for the motion model that shifts the robot exactly 2 steps, we believe there is a 0.8 chance. We assign a 0.1 to the cases where the robot over or under shoots by exactly 1. That's kind of written by this formula over here where the two gets a 0.8 probability, the 1 and the 3 end up with a 0.1 probability. I'm going to ask you now for the initial distribution that I'm writing up here, can you give me the distribution after the motion? The answer is for our intended field over here 0.8, the 2 neighbors 0.1 and a zero and zero over here. Well done. Let's do this again for a different initial distribution. Let's assume we have a 0.5 in this cell and a 0.5 in this cell. Remember that this is a cyclic-motion model, so whatever falls off on the right side, you'll find on the left side. Can you again for U = 2 fill in the posterior distribution? This is a pretty tricky question, which I'm going to answer in two phases. Let's just look at the 0.5 over here, 0.8 of that, which is 0.4, ends up over here, and 0.1 of this, which is 0.05 ends up over here. The reason why I write it so small is because this is not the correct answer quite yet. Let's look at the other 0.5. 0.4 goes two steps--1, 2--and ends up over here on the left side, but 0.1 falls short and makes the 0.05 over here 0.05 in the second grid cell. And interestingly enough, for the cell on the right side, there's two possibly ways you could've gotten there. Either by overshooting starting in the second cell, or undershooting starting in the right cell. So the total is the sum of these two things--0.1. This is the final answer: 0.4, 0.05, 0.05, 0.4, and 0.1. Let me give you a final example in which I assume a uniform distribution, and I want you to fill in for me the distribution after motion. The answer as it turns out will be just 0.2 everywhere, and the reason is with every grid cell being equally likely, applying this motion model will still make each grid cell equally likely. Let's pick one of them--say this one over here. We could have arrived here in 3 different ways. Perhaps we started in x2 and our motion went well. This gives us a 0.2 x 0.8. Perhaps we started in x1 and we overshot, which gives us a 0.2, for the cell x1, times a 0.1 for overshooting. Or perhaps we started in x3 and we undershot, which gives us 0.2 x 0.1. If we add those up, then we find it is the same as 0.2 x 1, because the factors over here add up exactly to 1, which makes 0.2. The result is 0.2. You can apply this same logic to all the other cells. This guy over here could have come from this guy, this guy, this guy, where this one is weighted with 0.8 and the other two with 0.1. That's called a convolution, and as well see later, there's a very nice way to write this mathematically as something called Theorem of Total Probability. But for the time being, I'd like to program this in. I'm going to give us a pExact of 0.8, pOvershoot of 0.1, and pUndershoot of 0.1. I'd like you to modify the move procedure to accommodate these extra probabilities. Here's one way to implement this. We're going to introduce the auxiliary variable "s," which we build up in three different steps. We multiply the p value as before for the exact set off by pExact. Then we add to it two more multiplied by pOvershoot or pUndershoot where we are overshooting by going yet 1 step further than U or undershooting by cutting it short by 1. Then we add these things up and finally append the sum of those to our output probability q. When we run this, we get for our example prior of 0, 1, 0, 0, 0, the answer 0, 0.1, 0.8, 0.1, and 0. Here's a question for you that is somewhat involved, and I really want to check your intuition. Suppose we have 5 grid cells as before with an initial distribution that assigns 1 to the first grid cell and 0 to all the other ones. Let's assume we do U = 1, which means with 0.8 chance in each action we transition 1 to the right. With 0.1 chance we don't move at all, and with 0.1 chance again we skip and move 2 steps. Again, let's assume the world is cyclic, so every time I fall off on the right side, I find myself back on the left side. The question is suppose I run infinitely many motion steps. Then I actually get a what's called a "limit distribution" What's going to happen to my robot if it never senses but executes the action of going 1 to the right on our little cyclic environment forever.? What will be the so-called limit or stationary distribution be in the very end? You might have guess it correctly. It's the uniform distribution. There's an intuitive reasoning behind this. Every time we move, we lose information. That is, in the initial distribution we know exactly where we are. One step in we have a 0.8 chance, but the 0.8 will fall to something smaller as we move on--0.64 and so on. The distribution of the absolute least information is the uniform distribution. It has no preference whatsoever. That is really the result of moving many, many times. There is a way to derive this mathematically, and I can prove a property that's highly related, which is a balance property. Say we take x4, and we'd like to understand how x4 at some time sub t corresponds to the previous time distribution over all these variables. For this to be stationary, it has to be the same. Put differently, the probability of x4 must be the same as 0.8p(x2) + 0.1p(x1) + 0.1p(x3). This is exactly the same calculation we did before where we asked what's the chance of being x4. Well, you might be coming from x2, x1, or x3, and there's these probabilities are 0.8, 0.1, and 0.1, they govern the likelihood you might have been coming from there. Those together must hold true in the limit when things don't move anymore. Now, you might think there are many different ways to solve this and the 0.2 is just one solution, but it turns out 0.2 is the only solution. If you plug in 0.2 over here and 0.2 over here and 0.2 over here, you get 1 x 0.2, and that's 0.2 on the right side. Clearly, those 0.2s over here meet the balance that is necessary to define a valid solution in the limit. Now let's go back to our code and move many times. Let's move twice, so please write a piece of code that makes the robot move twice, starting with the initial distribution as shown over here--0, 1, 0, 0, 0. Here's a piece of code that moves twice by the same amount as before, and the output now is a vector that assigns 0.66 as the largest value and not 0.8 anymore. Let's move 1,000 times. Write a piece of code that moves 1,000 steps and give me the final distribution. Here's my code. We have a loop for 1,000 steps. We move 1,000 times, and we print the corresponding distribution over here. It's 0.2 in each case as expected. Wow, you've basically programmed the Google self-driving car localization even though you might not quite known it yet. Let me tell you where we are. We talked about measurement updates, and we talked about motion. We called these two routines "sense" and "move." Now, localization is nothing else but the iteration of "sense" and "move." There is an initial belief that is tossed into this loop if you. If you sense first, if comes to the left side. Then localization cycles through these--move, sense, move, sense, move, sense. move, sense, move, sense cycle. Every time the robot moves, it loses information as to where it is. That's because robot motion is inaccurate. Every time it senses it gains information. That is manifest by the fact that after motion, the probability distribution is a little bit flatter and a bit more spread out. and after sensing, it's focused a little bit more. In fact, as a foot note, there is a measure of information called "entropy." Here is one of the many ways you can write it: [-Ʃp(xi)log p(xi)] as the expected log (logarithmic) likelihood of the probability of each grid cell. Without going into detail, this is a measure of information that the distribution has, and it can be shown that the update step, the motion step, makes the entropy go down, and the measurement step makes it go up. You really losing and gaining information. I would now love to implement this in our code. In addition to the 2 measurements we had before, red and green, I'm going to give you 2 motions--1 and 1, which means the robot moves right and right again. Can you compute the posterior distribution if the robot first senses red, then moves right by 1, then senses green, then moves right again? Let's start with a uniform prior distribution. Here's the routine. It's very short. It goes through the measurements. It assumes it has as many motions as measurements. It first applies the measurement as before. Then it applies the motion. When it's done with it, it prints the output, and the output is interesting. The world has a green, a red, a red, and a green, and a green field. The robot saw red, followed by a right motion, and green. That suggests that it probably started with with the highest likelihood in grid cell number 3, which is the right-most of the two red cells. It saw red correctly. It then moved to the right by 1. It saw green correctly, moved right again. It now finds itself most likely in the right-most cell. This is just looking at these values over here without any probabilistic math and any code limitation. Let's look at the output--0.2, 0.1, 0.08, 0.16, 0.38. Very correctly, then it would most likely assign this position to the right-most cell as should be, given the sequence of observations over here. Let's pick a different base. Let's assume the robot saw red twice. It senses red, it moves, it senses red, it moves again. What is the most likely cell? Let's run the program, and we find that the most likely cell is the 4th cell. That makes sense, because the best match of red, red to the world is red over here and red over here. After seeing the 2nd red, the robot still moved 1 to the right and finds itself in the 4th cell as shown over here. Now I want to celebrate with you the code that you just wrote, which is a piece of software that implements the essence of Google's self-driving car's localization approach. As I said in the beginning, it's absolutely crucial that the car knows exactly where it is relative to the map of its road. While the road isn't painted green and red, the road has lane markers. Instead of those green and red cells over here, we plug in the color of the lane markings relative to the color of the pavement. It isn't just one observation per time step, it's an entire field of observations, an entire camera image, but you can do the same with a camera image as long as you can correspond a camera image in your model with a camera image in your measurements. Then a piece of code not much more difficult than what you coded yourself is responsible for localizing the Google self-driving car. You just implemented a major, major function that makes Google's car drive itself. I think you should be really happy and proud of yourself. You should say to yourself, I just implemented localization. Now why on earth did it take Google that long to build a product that drives itself. Well, the truth is the situation is a little more difficult. Sometimes road get paved over and changed, and we're working on this. But what you've implemented is the core of Google's self-driving car localization idea. Let me just summarize the essential things we've learned. We learned that localization maintains a function over all possible places where a road might be, where each cell has an associated probability value. The measurement update function, or "sense," is nothing else but a product in which we take those probability values and multiply them up or down depending on the exact measurement. Because the product might violate the fact that probabilities add up to 1, there was a product followed by normalization. Motion was a convolution. This word itself might sound cryptic, but what it really means is for each possible location after the motion, we reverse engineered the situation and guessed where the world might have come from and then collected, we added, the corresponding probabilities. Something as simple as multiplication and addition solves all of localization and is the foundation for autonomous driving. I want to spend a few minutes and go over the formal definition of localization. I'm going to introduce probability and ask you lots of questions. Formally, we define a probability function to be P(X), and it's a value that is bounded below and above by 0 and 1. X often can take multiple values. We had the case of 5 grid cells. Suppose it can only take 2 values--there's only 2 grid cells, x1 and x2. If the probability for x1 is 0.2, what would be the probability for x2? Please enter the number. It's a quiz, obviously. The answer is 0.8. The reason being that probabilities always add up to 1. Let me ask a second question, and I know that's not particularly difficult. What if P(X1) = 0? The answer is 1. You got it. For our world with 5 different grid cells, suppose we know that the first 4 of them have a 0.1 probability. What would be the probability of the 5th and final grid cell? The answer is 0.6. They have to add up to 1. We subtract 4 x 0.1, which is 0.4, which is 0.6. That's a valid probability. Let's look into measurements, and they will lead to something called "Bayes Rule." You might have heard about Bayes Rule before. It's the most fundamental consideration in probabilistic inference, but the basic rule is really, really simple. Suppose x is my grid cell and Z is my measurement. Then the measurement update seeks to calculate a belief over my location after seeing the measurement. How is this computed? Well, it was really easy to compute in our localization example. Now I'm going to make it a little bit more formal. It turns out that Bayes Rule looks like this. That will likely be a little bit confusing, but what it does is it takes my prior distribution, P(X), and multiplies in the chances of seeing a red or green tile for every possible location and out comes, if you just look at the denominator here, the non-normalized posterior distribution we had before. Recognize this? This was our prior. This was our measurement probability. If we do this for all the grid cells, so we put a little index "i" over here, then just the product of the prior of the grid cell times the measurement probability, which was large if the measurement corresponded to the correct color and small if it corresponded to a false color. That product gave us the non-normalized posterior distribution for the grid cell. You remember this because that's what you programmed. You programmed a product between the prior probability distribution and a number. The normalization is now the constant over here--p(Z). Technically, that is the probability of seeing a measurement devoid of any location information. But let's not confuse ourselves. The easiest way to understand what's going on is to realize that this is a function here that assigns to each grid cell a number, and the p(Z) doesn't have the grid cell as an index. No matter what grid cell we consider, the p(Z) is the same. Here's the trick. No matter what p(Z) is, because the final posterior has to be a probability distribution, by normalizing these non-normalized products over here, we will exactly calculate p(Z). Put differently, p(Z) is the sum over all i of just this product over here. This makes Bayes Rule really simple. It's a product of our prior distribution with a measurement probability, which we know to be large if the color is correct and small otherwise. We do this and assign it to a so-called non-normalized probability, which I'll do with a little bar over the p. Then I compute the normalizer, which I'LL call "α," is the sum of all these guys over here. Then I just normalize. My resulting probability will be 1/α of the non-normalized probability. This is exactly what we did, and this is exactly Bayes Rule. Let me ask you Bayes Rule in the context of a completely different example to see if you understand how to apply Bayes Rule. This time it's about cancer testing. It is an example that is commonly studied in statistics classes. Suppose there exists a certain type of cancer, but the cancer is rare--only 1 in a 1000 people has the cancer-- where as 999 in 1000 people don't have it, illustrated by the probability of cancer and the probability of not cancer. Suppose we have a test, and the test can come out positive or negative. The probability that the test triggers positive if you have cancer is 0.8, and the probability that the test comes out positive given that I'm cancer free is only 0.1. Clearly the test has a strong correlation to whether I have cancer. Here's a really difficult question. Can you compute for me the probability of cancer given that I just received a positive test. Let me emphasize this is not an easy question, but you should be able, based on what I've taught you, to calculate this result. Think of the cancer/non cancer as the robot position and think of the positive as whether the colored door observed is the correct one. And the answer is 0.0079. In other words, there's only 0.79% chance, 0.79 out of 100 that, despite the positive test result, that you have cancer. And we're going to apply the exact same mechanics as we did before. The result of Bayes Rule, non-normalized of C given POS is simply the product of my prior probability, 0.001 times 0.8, which is the probability of a positive result in the cancer state. And that ends up to be 0.0008. The non-normalized probability for the opposite event, the non-cancerous event, given a positive test, is 0.999 times 0.1. And that's obviously 0.0999. Our normalizer is the sum of both of those, which is 0.1007 just add these two values up over here. So dividing 0.0008, the non-normalized probability, by 0.1007 gives us 0.0079. We just applied Bayes Rule to compute a really involved probability of having cancer after seeing a test result. Let's look at motion, which will turn out to be something we will call total probability. You remember that we cared about a grid cell "xi," and we asked what is the chance of being in xi after robot motion? Now, to indicate the after and before, let me add a time index. T up here, is an index for time. I write it superscript so there is no confusion with the index i, which is the grid cell. You might remember the way we computed this was by looking at all the grid cells the robot could have come from on time step earlier-- indexed here by j. We looked at the prior probability of those grid cells at time t - 1. We multiply with the probability that our motion command would carry us from xj to xi. This is written as a condition distribution as follows. This was exactly what we implemented. If there was our grid cells over here and we asked one time step later about a specific grid cell over here, we would combine 0.8 from over here, 0.1 from over here, and 0.1 from over here into the probability of this grid cell. It's the same formula as here. This is now xi, and the way we find the posterior probability for xi is to go through all possible places from which we could have come, all the different j's. Look at the prior probabilities, multiply it by the probability that I transition from j to i given my motion command, which in this case is go 1 to the right side. Now in probability terms, people often write it as follows: P(A) = Ʃ p(A│B) p(B). This is just the way you'd find it in text books, and you can see directly the correspondence of A as a place i of time t and all the different Bs as the possible prior locations. That is often called the Theorem of Total Probability. The operation of a weighted sum over other variables is often called a "convolution." Let me test total probability in a quiz. Suppose I flip a coin, and the coin comes up tails or heads. Suppose it's a fair coin. The probability of tails or of heads is both 1/2. Now let's say that the coin comes up tails, and I just accept and don't do anything. But suppose it comes up heads, and I flip it again, and after 1 flip, I accept the result. My quiz for you is what is the probability that the final result is heads? That's an example of total probability. The answer is 1/4. It's easy to see that the probability of heads in step 2 is the probability of heads in step 2 conditioned on heads in step 1 times probability of heads in 1 plus, that's the sum, probability of heads in step 2 given we had tails in step 1 times probability of tails in step 1. Now, the way I set it up, those things here are equally likely. However, if we did have tails in step 1, we would never toss the coin again and just accept it. It's impossible that in step 2 I flip over the heads. It's probability zero. Whereas if I found heads, I would flip again and then the 0.5 chance I arrive at heads. If I look at this, then this all becomes zero, and these guys multiply to 1/4, and the final answer is 1/4. Let me make a final quiz in which I have a coin which probability I don't know. There are multiple coins. One is fair and one is loaded. The fair coin has a probability of heads of 0.5. The loaded coin has a probability of heads of 0.1. Here's what I'm going to do. I'm going to grab a random coin with 50% chance. The fair coin will be chosen with 50% chance, and the loaded coin will be chosen with 50% chance, but I don't know which one it is. I flip it and I observe heads. What's the probability that the coin I hold in my hand is fair? Apply anything you've learned before-- one of the rules you've learned before is exactly the right one to apply here. Let's work this out. What I'm really asking you is the probability of a fair coin "f" given that I observed H. This has nothing to do with total probability and all with Bayes Rules, because I'm talking about observations. The non-normalized probability according to Bayes Rule is obtained as follows: the probability of observing H for the fair coin is 0.5, and the probability of having grabbed the fair coin is 0.5 as well. The non-normalized probability of not F given H, which is the loaded coin, is probability of H given not f, which we know to be 0.1--that's the one over here-- times the probability of not picking the fair coin, which is 0.5, ends up to be 0.05. When we now normalize, we α = 0.25 + 0.05, which is 0.3. If we now normalize the 0.25 over here with the 0.3, we get 0.833, which is the same as 5/6. That's our posterior probability we hold the fair coin after we observed H. All right. With this I'd like to finish this class. I just want to tell you what you've learned, because you did amazingly well. You learned about localization. You wrote an algorithm that implements what's called Markov localization. You learned about probabilities, Bayes Rule, and total probability. You implemented all of this, and you solved interesting quiz questions. This is an enormous amount of work. You are now able to make a robot localize, but you also have a very intuitive understanding of some very interesting and important probabilistic methods that are commonly called "filters." Next class, we're going to see new filters, and particularly, we're going to talk about particle filters and Kalman filters. We will apply both of those to really interesting problems in the Google self-driving car, In particular, to the problem of finding and predicting other cars. I'll see you in the next class. Welcome to my second class on Kalman filters. I want to take you on a little tour to where it all began--Stanford University. Behind me is Vale, Stanford's Research Center. Let's go inside. This is Junior, Standord's most recent self-driving car. It's the child of Stanley, whom you can find in the National Museum of American History in Washington, D.C. Let me tell you something about the equipment that's on this car that makes it self-driving. This rotating thing over here is a laser-range finder that takes distance scans 10 times a second, about a million data points. It'll be really important for the Kalman filter class I'm teaching you today. It's major function is to spot other cars so you don't run into them. There is also a camera on top. There is a stereo camera system over here. In the rear there are antennas for a GPS--global positioning system-- that allows us to estimate where the car is in the world. This is a supplemental system to the localization class I just taught you. This is the data that comes from the laser. This is the car parked in the garage right now. We see the back wall. These are all range measurements that tell you how far things are away, and they are essential as the input to the Kalman filter that we're going to learn about today. I'd like to take my students on a little journey to Stanford and show them our self-driving car that uses sensors to sense the environment. So let me dive into the class pretty much right now. In our last class, we talked about localization. We had a robot that lived in an environment and that could use its sensors to determine where in the environment it is. Here you can see the Google self-driving car using a road map localizing itself. But in addition, what is shown here in red are measurements of other vehicles. The car uses lasers and radars to track other vehicles and today we're going to talk about how to find other cars. The reason why I would like to find other cars is because we wouldn't want to run into them. We have to understand how to interpret sensor data to make assessments, not just where these other cars are as in the localization case, but also how fast they're moving so you can drive in a way that avoids collisions with them in the future. That's important--not just for cars. It matters for pedestrians and for bicyclists. Understand where other cars are an making predictions where they're going to move is absolutely essential for safe driving in the Google car project. [Tracking] So in this class we're going to talk about tracking. The technique I'd like to teach you is called a Kalman filter. This is an insanely popular technique for estimating the state of a system. It's actually very similar to the probabilistic localization method we taught in the previous class--Monte Carlo localization. The primary differences are that Kalman filters estimate a continuous state whereas in Monte Carlo localization we are voiced to chop the world into discrete places. As a result, the Kalman filter happens to give us a unimodal distribution-- and I'll tell you in a second what that means-- whereas Monte Carlo was fine with multimodal distributions. Both of these techniques are applicable to robot localization and tracking other vehicles. In fact, in a later class, we're going to learn about particle filters, which are yet another way to address the same problem, and indeed they are actually continuous and multimodal. But for the time being let's look into Kalman filters and ignore these other two families of methods. Let me start with a example. Consider the car done here. Let's assume the it sees as its measurement, an object here, here, here, here, and here for the times t = 0, t = 1, t = 2, and t = 3. Where would you assume the object would be at t = 4? Check one of those 3 boxes. And he asked you to expect it right over here. From those observations you would say that the velocity points in the direction of this vector. Assuming no drastic change in velocity, you expect that the 5th position would be over here. The common filter takes observations like these and estimates future locations and velocities based on data like this. Today I'm going to teach you how to write a piece of software that let's you take points like those--even if they're noisy and uncertain-- and estimate automatically where future locations might be and at what velocity the object is moving. The Google self-driving car uses methods like these to understand where other traffic is based on radar and laser-range data. So let's dive in! You remember our Markov model where the world was divided into discrete grids, and we assigned to each grid a certain probability. Such a representation of probability over spaces is called a histogram in that it divides the continuous space into a finite many grid cells and approximates the posterior distribution by a histogram over the original distribution. The histogram is a mere approximation for this continuous distribution. In Kalman filters the distribution is given by what's called a Gaussian. A Gaussian is a continuous function over the space of locations, and the area underneath sums up to 1. Here's our Gaussian again. If we call the space x, then the Gaussian is characterized by two parameters-- the mean, often abbreviated with the Greek letter μ, and the width of the Gaussian, often called the variance, and for reasons I don't want to go into, it's often written as a quadratic variable σ^2. And Gaussian in 1D, which means the parameter space over here is 1 dimensional, is characterized by μ and σ^2. Rather than estimating the entire distribution as a histogram, our task in Kalman filters is to maintain a μ and a σ^2 that is our best estimate of the location of the object we're trying to find. The exact formula is an exponential of a quadratic function where we take the exponent of this complicated expression over here. The quadratic difference of our query point x relative to the mean μ divided by σ^2 multiplied by -1/2. Now, if x equals μ then the numerator becomes 0, and we have x of 0, which is 1. It turns out we have to normalize this by a constant-- 1 over the square root of 2πσ^2-- but for everything we'll talk about today, this constant won't matter, so ignore it. What matters is we have an exponential of a quadratic function over here. Let me draw you a couple of functions, and you tell me which ones you believe are Gaussian by checking the box on the right side. Please excuse my poor drawing skills here. The answer is this one is a Gaussian, this one, and this one. They are all characterized by this exponential drop-off on both sides that are symmetrical, and they have a single peak. They are what's called "unimodal." This is a bimodal function that has two peaks and as a result is not Gaussian. The same is true over here and over here, so these guys don't qualify. Let me ask you again about your intuition and draw three more Gaussians, and, again, excuse my poor drawing skills here. Now I'm going to ask you about the covariance. For each of those check exactly one box. Is the covariance large, medium, or small? Obviously, one of those is the largest, one is a medium, and one is small. The answer is this is the largest covariance. It's the widest spread. This is the smallest, and this is medium. To see how this is being found in the formula over here, the difference between x and y is being normalized by the covariance. The larger this value, the less the difference over here matters, and, as a result, the more the function is spread out. So functions with a very wide spread have the largest covariance, whereas functions with small spread have the smallest covariance. Put differently, the sigma-squared covariance is a measure of uncertainty. The larger sigma-squared, the more uncertain we are about the actual state. This is a very certain distribution where expected deviation is small. This is a relative uncertain distribution where we know very little. [Which to prefer] If we track another care with our Google self-driving car, which Gaussian would we prefer? The first, second, or third? The answer is the third, because that's the one that's most certain, and because it is most certain, it makes a chance of accidentally hitting another car the smallest just by the fact that we know more about the car than in the two other distributions. You learned something really important. You learned the definition of a Gaussian. You learned about the fact that these are unimodal distributions. They are also symmetrical. And you learned a little bit about how to use them as a belief in a probabilistic filter. Let's go and program a Gaussian. Let me ask you to calculate the value of x-- you will need a calculator for this-- for the following values: mu equals 10, sigma-squared equals 4, and x equals 8. The approximate answer is 0.12. x minus mu squared is 4, this is 10 minus 8 to the square divided by 4 equals 1. The expression of the exponential is -1/2, which is about 0.6. This guy over here is approximately 0.2, which gives us as a product 0.12 I won't torture you with any more questions like these, because they're really not fun, but we can program this now. Starting with the following source code, I'm looking for a completion of this one line over here that returns the Gaussian function with arguments mu = 10, sigma2 = 4, and x = 8, and I want the output to be approximately 0.12. Here's my solution. This is the constant: 1/sprt(2pisigma2). Then I multiply with the exponential of (-.5(x-mu)*2/sigma2). Applying that to the following numbers over here gives me 0.12. Now, here's a question for you. How do I have to modify x = 8 to be the maximum return value for this function f? The answer is assess with the same value as mu, in which case this expression over here becomes zero, and we get the maximum. We get the peak of the Gaussian. We set x to the same value as mu, to 10, and the output is 0.2 approximately. Now let's look at the Kalman filter. The Kalman filter represents all distributions but Gaussians. Just like in the last class where we talked about measurement cycles and motion cycles, the Kalman filter iterates two different things--measurement updates and motion updates. This is identical to the situation before in localization where we got a measurement and then we took a motion. Here the max changes, but the basic principle applies. Let's do a quiz to see if we remember the material from the last class. You might remember that one of the two steps, measurement or motion, required a convolution and the other one a product. Please check the corresponding box. Measurements were implemented using products, and motions using a convolution. If you don't know this, please go back and check the last class on localization. In fact, we talked about Bayes Rule, and we talked about total probability. Please, again, check, whether Bayes Rule and total probability apply to measurements or motions. The answer is the measurement, the product, was using Bayes Rule, and motion was using total probability. In Kalman filters we iterate measurement and motion. This is often called a "measurement update," and this is often called "prediction." In this update we'll use Bayes rule, which is nothing else but a product or a multiplication. In this update we'll use total probability, which is a convolution, or simply an addition. Let's talk first about the measurement cycle and then the prediction cycle, using our great, great, great Gaussians for implementing those steps. Suppose you're localizing another vehicle, and you have a prior distribution that looks as follows. It's a very wide Gaussian with the mean over here. Now, say we get a measurement that tells us something about the localization of the vehicle, and it comes in like this. It has a mean over here called "mu," and this example has a much smaller covariance for the measurement. This is an example where in our prior we were fairly uncertain about a location, but the measurement told us quite a bit as to where the vehicle is. Here's a quiz for you. Will the new mean of the subsequent Gaussian be over here, over here, or over here? Check one of these three boxes. The answer is over here in the middle. It's between the two old means--the mean of the prior and the mean of the measurement. It's slightly further on the measurement side, because the measurement was more certain as to where the vehicle is than the prior. The more certain we are, the more we pull the mean in the direction of the certain answer. Now, here's a question that's really, really hard. When we graph the new Gaussian, I can graph one that's very wide and very peaky. If I were to measure where the peak of the new Gaussian is, this would be a very narrow and skinny Gaussian. This would be one that's width would be in between the two Gaussians. This is one that's even wider than the two original Gaussians. Which one do you believe is the correct posterior after multiplying these two Gaussians? This is an insanely hard question. I'd like you to take your chances here, and I'll explain to you the answer in just a second. Very surprisingly, the resulting Gaussian is more certain than the two component Gaussians. That is, the covariance is smaller than either of the two covariances in isolation. Intuitively speaking, this is the case because we actually gain information. The two Gaussians together have a higher information content than either Gaussian in isolation. It'll look something like this. That is completely not obvious. You might have to take this with faith, but I can actually prove it to you. Suppose we multiply two Gaussians as in Bayes rule-- a prior and a measurement probability. The prior has a mean of mu and a variance of sigma-squared. The measurement has a mean of nu and a covariance of r-squared. Then the new mean, mu prime, is the weighted sum of the old means where mu is weighted by r-squared and nu is weighted by sigma-squared normalized by the sum of the weight factors. The new variance term--I'm going to write sigma-squared prime here for the new one after the update--is given by this equation over here. Let's put this into action. We have a weighted mean over here. Clearly, the prior Guassian has a much higher uncertainty. Therefore sigma-squared is larger. That means that nu is weighted much, much larger than the mu. So the mean will be closer to the nu than the mu, which means that it'll be somewhere over here. Interestingly enough, the variance term is unaffected by the actual means. It just uses the previous variances and comes up with a new one that's even peakier. The result might look like this. This is the Kalman situation for the measurement update step where this is the prior, this is the measurement probability, and this is the posterior. Let's practice these equations with a simple quiz. Here are our equations again. Suppose I use the following Gaussians: [μ = 10, σ^2 = 4, ν = 12, r^2 = 4] These are Gaussians with equal variance but different means that might look as follows. Compute for me the new mean after the update and the new sigma-squared. The answer for the new mean is just the one in the middle, and reason is both weights over here are equivalent, so we take the mean between mu and nu, which is 11. Then with sigma-squared it's 2. If you take 1/4 plus 1/4, then you get 1/2, so 1 over 1/3 equals 2, which means the new variance term is half the size of the previous variance terms. Let's do this again. Suppose our mean is 10 and 13, and the variances are imbalanced--8 and 2-- which corresponds to the following picture. There's a relatively shallow distribution centered on 10 and a much more peaked distribution centered on 13. Compute for me what the resulting mu prime and sigma-squared prime are. With a little bit of math we find that the new mean is 12.4, and the new sigma-squared is 1.6. 12.4 is much closer to 13 than 10, and that's because the Gaussian centered on 13 has a much narrower variance than the one on 10. We find the resulting variance is smaller than each of the two variances over here. In particular, let's start with the variance. 1 over 1/8 plus 1/2 is the same as 1 over 5/8, which results in 8/5 or 1.6. This is just applying this formula over here. For the weighted average we get 2 times 10 plus 8 times 13. Then we normalize by the sum of those two things over here. So this is 124 divided by 10, which gives us 12.4. Let me ask you a different quiz, which from the math, but it tests your intuition. Suppose we have a prior that sits over here and a measurement probability that sits over here--really far away-- and both have the same covariance. Let me first quiz you where the new mean would be. Is it going to be here, here, here, or here? Please check the corresponding check mark. The answer is here. It's in the middle. It's in the straight middle, because these two variances are the same, so we just average the means. Let me ask the hard question now. Will it be a Gaussian like this where the variance is larger, a Guassian with the exact same variance, or an even more peaked Guassian that's more certain than the two original factors in this calculation. Please check exactly one of the three boxes over here. The answer is it's the more peaked Gaussian. That is somewhat counter-intuitive. You'd think if this was your initial measurement probability you really don't know where you are, and you should pick a very right Gaussian. But the truth is our new sigma-squared is obtained independent of the means. It's this formula over here. Now because both means are the same, this resolves to 1 over 1/σ^2. That's the same as σ^2 over 2, which means a new variance squared is half of the old one. That makes it a narrower Gaussian, so the green one here that's the most peaked is indeed the correct answer. This is very counter-intuitive, but now we understand why. I hope you feel comfortable with the fact that we have actually gotten more information about the location, which is manifest by a more focused estimate. Let's now go back and write a program in which we calculate the new mean and the new variance term. I really just want you to write a Python program that implements those equations so that we can test them. I'm giving you a skeleton program, which has a function update, that takes as an input a mean and a variance for the first distribution and a mean and a variance for the second distribution and outputs the new mean and the new variance of the product of those. Here I am testing it with a mean of 10 and a variance of 8 and a mean of 13 and a variance of 2, which was one of our examples. Out should come the answer over here--12.4 and 1.6. Of course, Python tends to not give you the exact output, so there are a couple of zeros over here but ignore those. The answer is effectively 12.4 and 1.6. Can you fill in those gaps? Here's my answer. This is the expression for the mean. This is the one for the variance. I run it, and I get the exact same answer. I run it again for my other example of equal variances and 10 and 12 as means, and miraculously, the correct answer comes out-- 11 for the new mean and 2 for the new variance. If you programmed this correctly, then congratulations. You've just programmed an essential update step in the Kalman filter-- the measurement update step. That's really the difficult step in Kalman filtering. The other one--the prediction step or the motion step--is much, much easier to program. [Thrun] So let's step a step back and look at what we've achieved. We knew there was a measurement update and a motion update, which is also called prediction. And we know that the measurement update is implemented by multiplication, which is the same as Bayes rule, and the motion update is done by total probability or an addition. So we tackled the more complicated case. This is actually the hard part mathematically. We solved this. We gave an exact expression. We even derived it mathematically, and you were able to write a computer program that implements this step of the Kalman filter. I don't want to go into too much depth here. This is a really, really easy step. Let me write it down for you. Suppose you live in a world like this. This is your current best estimate of where you are, and this is your uncertainty. Now say you move to the right side a certain distance and that motion itself has its own set of uncertainty. Then you arrive at a prediction that adds the motion of command to the mean, and it has an increased uncertainty over the initial uncertainty. Intuitively this makes sense. If you move to the right by this distance, in expectation you're exactly where you wish to be but you've lost information because your motion tends to lose information as manifested by this uncertainty over here. The math for this is really, really easy. Your new mean is your old mean plus the motion, often called U. So if you move over 10 meters, this will be 10 meters. And your new sigma square is your old sigma square plus a variance of the motion Gaussian. This is all you need to know. It's just an addition. I won't prove it to you because it's really trivial. But in summary, we have a Gaussian over here, we have a Gaussian for the motion, with U as the mean and R square as its own motion uncertainty, and the resulting Gaussian in the prediction step just adds these 2 things up-- mu plus U and sigma square plus R square. Since this was so simple, let me quiz you. We have a Gaussian before the prediction step which mu equals 8 and sigma square equals 4. We then move to the right a total of 10, with a motion uncertainty of 6. Now describe to me the predictive Gaussian and give me the new mu and the new sigma square. [Thrun] And the answer is just add those up. 8 + 10 = 18 4 + 6 = 10 And that's it. [Thrun] Let's program this. I'm giving you a skeleton code. This is the same update function as before. Now I would like you to do the predict function, which takes our current estimate and its variance and the motion and its uncertainty and computes the new updated prediction, mean, and variance. So for example, if our prior is 10 and 4, our motion is 12 and 4, I would like to get out to 22 and 8 according to the formulas I've just given you. And yes, it's as easy as this. We just add the two means and the two variances. It's amazing, this entire program over here implements a one-dimensional common feature. So now let's put everything together. Let's write a main program that takes these 2 functions, update and predict, and feeds into a sequence of measurements and motions. In the example I've chosen here are the measurements of 5., 6., 7., 9., and 10. The motions are 1., 1., 2., 1., 1. This all would work out really well if the initial estimate was 5, but we're setting it to 0 with a very large uncertainty of 10,000. Let's assume the measurement uncertainty is constant 4, and the motion uncertainty is constant 2. When you run this, your first estimate for position should basically become 5-- 4.99, and the reason is your initial uncertainty is so large, the estimate is dominated by the first measurement. Your uncertainty shrinks to 3.99, which is slightly better than the measurement uncertainty. You then predict that you add 1, but the uncertainty increases to 5.99, which is the motion uncertainty of 2. You update again based on the measurement 6, you get your estimate of 5.99, which is almost 6. You move 1 again. You measure 7. You move 2. You measure 9. You move 1. You measure 10, and you move a final 1. And out comes as the final result, a prediction of 10.99 for the position, which is your 10 position moved by 1, and the uncertainty--residual uncertainty of 4. Can you implement this so you get the exactly same outputs as I've gotten over here? This piece of code implements the entire Kalman filter. It goes through all the measurement elements and quietly assumes there are as many measurements as motions indexed by n. It updates the mu and sigma using this recursive formula over here. If we plug in the nth measurement and the measurement uncertainty, it does the same with the motion, the prediction part over here. It updates the mu and sigma recursively using the nth motion and the motion uncertainty, and it prints all of those out. If I hit the Run button, I find that my first measurement update gets me effectively 5.0. It's 4.98. And that makes sense because we had a huge initial uncertainty, and [inaudible] of 5 with a relatively small measurement uncertainty. And in fact the resulting sigma square term is 3.98, which is better than 4 and 1,000, slightly better than 4. We're slightly more certain than the measurement itself. We now apply the motion of 1. We get to 5.9. Our uncertainty increases by exactly 2, from 3.9 to 5.98. And then the next update comes in at 6, and it gives us a measurement of 5.99 and now a reduced uncertainty of 2.39. And then we go to move to the right again by 1, which makes the prediction 6.99. Uncertainty goes up. We measure 7. We get to 6.99, almost 7. Uncertainty goes down. We move 2 to the right, measure 9, 1 to the right, measure 10, and move 1 again. The final thing is the motion. And if you look at the end result, our estimate is almost exactly 11, which is the result of 10 + 1. And the uncertainty is 4.0 after the motion and 2.0 after the measurement. This code that you just wrote implements a full Kalman filter for 1D. If you look at this, we have an update function that implements what actually is a relatively simple equation, and a prediction function which is an even simpler equation of just addition. And then you apply it to a measurement sequence and a motion sequence with certain uncertainties associated, and this little piece of code over here gives you a full Kalman filter in 1D. I find this really amazing. Let's plug in some other values. Suppose you're really certain about the initial position. It's wrong. It's 0. It should be 5, but it's 0. And now we assume a really small uncertainty. Guess what's going to happen to the final prediction? As I hit the Run button, we find this has an effect on the final estimate. It's not 11. It's only 10.5. And the way this takes place is initially, after our first measurement update, we believe in the position of 0. This is 1.24 to the - 10th, but a really small uncertainty, even smaller than this one over here. We apply our motion update. We add a 1. We have a higher uncertainty. And now when the next measurement comes in, 6, we are now more inclined to believe the measurement because uncertainty is now basically 2 as opposed to 0.001. We update our position to be 2.666, which is now a jump away from 1, and we reduce our uncertainty. Motion comes in, 3.66. Uncertainty goes up. We now are willing to update even more. As you see the 7, we're willing to go to 5.1, but not quite all the way because we feel fairly confident on our wrong prior estimate. And this confidence makes it all the way to the end when we predict 10.5 as opposed to 11 with an uncertainty of 3.98. We've corrected some of it. We were able to drag it into the right direction but not all the way because our false initial belief has such a strong weight in the overall equation. Now we understand a lot about the 1D Kalman filter. You've programmed one. You understand how to incorporate measurements. You understand how to incorporate motion, and you really implement something that's actually really cool, which is a full Kalman filter for the 1D case. Now in reality, we often have many Ds, and then things become more involved, so I'm going to just tell you how things work with an example, and why it's great to estimate in higher dimensional state spaces. Suppose you have a 2-dimensional state space of x and y--like a camera image, or in our case, we might have a car that uses a radar to detect the location of a vehicle over time. Then what the 2D Kalman filter affords you is something really amazing, and here is how it goes. Suppose at time t = 0, you observe the object of interest to be at this coordinate. This might be another car in traffic for the Google self-driving car. One time step later, you see it over here. Another time step later, you see it right over here. Where would you now expect at time t = 3 the object to be? Let me give you 3 different places. Please click at the most likely location. The answer is here. What the Kalman filter does for you, if you do estimation and high dimensional spaces, is to not just go into x and y spaces, but allows you to implicitly figure out what the velocity of the object is, and then use the velocity estimate to make a really good prediction about the future. Now notice the sensor itself only sees position. It never sees the actual velocity. Velocity is inferred from seeing multiple positions. So one of the most amazing things about Kalman filters in tracking applications is it's able to figure out, even though it never directly measures it, the velocity of the object, and from there is able to make predictions about future locations that incorporate velocity. That is just really, really, really great. That's one of the reasons that Kalman filters are such a popular algorithm in artificial intelligence and in control theory at large. To explain how this works, I have to talk about high dimesional gaussians. These are often called multivariate gaussians. The mean is now a vector with 1 element for each of the variance. The variance here is replaced by what's called a co-variance, and it's a matrix with D rows and D columns, if the dimensionality of the estimate is D. The formula is something you have to get used to. I'm writing it out for you, but you never get to see this again. To tell you the truth, even I have to look up the formula for this class, so I don't have it in my head, and please, don't get confused. Let me explain it to you more intuitively. Here's a 2-dimensional space. A 2-dimensional gaussian is defined over that space, and it's possible to draw the contour lines of the gaussian. It might look like this. The mean of this gaussian is this x0, y0 pair, and the co-variance now defines the spread of the gaussian as indicated by these contour lines. A gaussian with a small amount of uncertainty might look like this. It might be possible to have a fairly small uncertainty in 1 dimension, but a huge uncertainty in the other. Huge uncertainty in the x-dimension is small, and the y- dimension is large. When the gaussian is tilted as showed over here, then the uncertainty of x and y is correlated, which means if I get information about x-- it actually sits over here--that would make me believe that y probably sits somewhere over here. That's called correlation. I can explain to you the entire effect of estimating velocity and using it in filtering using gaussians like these, and it becomes really simple. The problem I'm going to choose is a 1-dimensional motion example. Let's assume at t = 1, we see our object over here. A t = 2 right over here. A t = 3 over here. Then you would assume that at t = 4, the object sits over here, and the reason why you would assume this is--even though it's just seen these different discrete locations, you can infer from it there is actually velocity that drives the object to the right side to the point over here. Now how does the Kalman filter address this? This is the true beauty of the Kalman filter. In Kalman filter land, we're going to build a 2-dimensional estimate. 1 for the location, and 1 for the velocity denoted x dot. The velocity can be zero. It can be negative, or it can be positive. If initially I know my location, but not my velocity, then I represent it with a Gaussian that's elevated around the correct location, but really, really broad in the space of velocities. Let's look at the prediction step. In the prediction step, I don't know my velocity, so I can't possibly predict for location. I'm going to assume. But miraculously, they'll be some interesting correlation. So let's for a second, just pick a point on this distribution over here. Let me assume my velocity is 0. Of course, in practice, I don't know the velocity, but let me assume for a moment the velocity is 0. Where would my posterior be after the prediction? Well, we know we started in location 1. The velocity is 0, so my location would likely be here. Now let's change my belief in velocity and pick a different one. Let's say the velocity is 1. Where would my prediction be 1 time step later starting at location 1 and velocity 1? I'll give you 3 choices. Here? Here? Or here? Please pick the one that makes the most sense. The answer is right over here. Why? If a cars starting point is the point over here, for which we know the location is 1, and the velocity is 1, and if we predict 1 time step in the future, then for that prediction, we know the location will be 2, and the velocity might be a little uncertain, but it stays about the same. So we end up with a point over here. Let's do this again. Let me consider a velocity of 2, which means this is our starting point. Let me ask you where you would expect among those choices to be the most plausible prediction to be. Just like before, it'll be over here with a velocity of 2, initial position of 1, we find ourselves in 3. And again, this model assumes that in the absence of more knowledge, the velocity shouldn't really change. When you put all this together, you find that all these possibilites on the Gaussian over here, link to a Gaussian that looks like this. This is a really interesting 2-dimensional Gaussian, which you should really think about. Clearly, if I were to project this Gaussian uncertainty into the space of possible locations, I can't predict a thing. It's impossible to predict where the object is. The reason is, I don't know the velocity. Also, clearly if I project this Gaussian into the space of x dot, it is impossible to say what the velocity is. A single observation or single prediction is insufficient to make that observation. However, what we know is our location is correlated to the velocity. The faster I move, the further on the right is the location. This Gaussian expresses this. If I, for example, figure out that my velocity was 2, then I was able, under this Gaussian, to really nail that my location is 3. That is really remarkable. You still haven't figured out where you are, and you haven't figured out how fast you're moving, but we've learned so much about the relation of these 2 things with this Gaussian. To understand how powerful this is, let's now fold in the second observation at time t = 2. This observation tells us nothing about the velocity and only something about the location. So if I were to draw this as a Gaussian--it's a Gaussian just like this, which says something about the location but not about the velocity. But if we multiply by prior from the prediction step with the measurement probability, then miraculously, I get a Gaussian that sits right over here. This Gaussian now has a really good estimate what my velocity is and a really good estimate where I am. If I take this Gaussian, and predict 1 step forward, then I find myself right over here. That's exactly the effect we have over here. After that I get a Gaussian like this, I predict right over here. Think about this. This is really deep insight about how Kalman filters work. In particular, we've only been able to observe 1 variable. We've been able to measure observation to infer this other variable, and the way we've been able to infer is that there's a set of physical equations which say that my location, after a times step, is my old location plus my velocity. This set of equations has been able to propagate constrains from subsequent measurements back to this unobservable variable, velocity, so we are able to estimate the velocity as well. This is really key to understanding Kalman filter. It is key to understanding a Google self-driving car, estimates and locations of other cars, and is able to make predictions even if it's unable to measure velocity directly. There's a big lesson here. The variables of a Kalman filter--they're often called states because they reflect states of the physical role like where is the other car and the fastest moving. They separate into 2 subsets--the observables, like the momentary location, and the hidden, which in our example is the velocity, which I can never directly observe. But because those 2 things interact, subsequent observations of the observable variables give us information about these hidden variables, so we can also estimate what these hidden variables are. So from multiple observations of the places of the object--the location-- we can estimate how fast it's moving. That is actually true for all of the different filters but because Kalman filters happen to be very efficient to calculate, when we have a problem like this, you tend to often use just a Kalman filter. When we design a Kalman filter, you need effectively 2 things. For the state, you need a state transition function, and that's usually a matrix, so we're now in the world of linear algebra. For the measurements, you need a measurement function. Let me give you those for our example of the one demotion of an object Be known that the new location is the old location + velocity, turns into this matrix. You have a 1 over here and a 1 over here. The new velocity should just be the old velocity, which gives us 0 over here and a 1 over here. If you multiply this matrix by this vector, this is exactly what you're getting. For the measurement, we only observe the first component of place, not velocity, and that uses a matrix like this. This matrix would be called F and this H. The actual update equations for a Kalman filter are involved, and I give them to you, but please, don't memorize them, and I won't prove them for you. Even the proof is very involved. Every time I use them, I just look them up. There's a prediction step where I take my best estimate x, multiply it with a state transition matrix--matrix F, and I add whatever motion I know--u. That gives me my new x. I also have a covariance that characters my uncertainty, and that is updated as follows, where T is the transpose. There's also a measurement update step where we use the measurement z. We compare the measurement with our prediction where H is the measurement function that maps the state to measurements. We call this this the error. The error is mapped into a matrix s, which is obtained by projecting the system uncertainty into the measurement space using the measurement function projection + the matrix R, the characters of measurement noise. This is then mapped into a variable called K, which is often called the Kalman gain, where we invert the matrix s, and then finally, we actually update our estimate and our uncertainty using what ought to be the most cryptic equation that you've seen in a long time. Now I wrote this down so that you have a complete definition, but this is something you should not memorize. If you really wish to understand this math, it happens to be just a generalization of the math I gave you to higher dimensional spaces, but I would recommend just not to worry about this. There's a set of linear algebra equations that implement the Kalman filter and higher dimensions. I have a new, challenging programming assignment for you that will take you a while, but I would like you to implement a multidimensional Kalman filter for the example I've just given you. The matrix class is a class for manipulating matrices that should make it really easy. It has a function that initializes matrices--I'll show you an example in a second. It can set them down to 0. It can compute an identity matrix. It can print out a matrix with show. It overloads operators like addition, subtraction, multiplication, and even computes the transpose of a matrix, and in some more extended code, it can actually invert a matrix using Cholesky factorization. There's a function here called inverse. This matrix class is available. It's a small version of what is found in typical libraries. I want to demonstrate it for you just for a second. You can make a matrix with a command like this with the argument in the parenthesis. It's a 2-dimensional matrix. In this case it's a vertical vector. With the show command, you can print out the result of the vertical vector. You can put the transpose as follows. If you run this, you'll find the horizontal vector. Say you wish to multiply a matrix by a vector, then we can make a 2 x 2 matrix with this initialization over here, a matrix of [12., 8.] and [6., 2.]. We can print this matrix. Here it is: 12, 8, 6, 2. These are these values over here. And we can multiply F and a. So here b = F x a. And if we show the result, we get the vector 280. That's obtained by 10 x 12 + 10 x 8 is 200. 10 x 6 + 10 x 2 is 80. So using my matrix libraries, I set an initial state. This is a tracking in 1D where the state is the position and the velocity. I initialized both with 0 because I don't know the actual location and the velocity. I get an uncertainty matrix where I have a really high uncertainty in the position and a really high uncertainty in the velocity, and they're both uncorrelated. That's the matrix of 1000, 0, 0, 1000. I specify an external motion, but it's 0, 0, so it has no effect, so just ignore this. I build the next state function, which is the one we just discussed, [1., 1] [0, 1.]. That assumes that the velocity is just being added to the position, and the velocity and expectation stays the same. I build a measurement function that extracts just the first of the 2 values, 1 and 0, so I can observe the location but not the velocity. I have a measurement uncertainty. It happens to be 1 in this example. And I have an identity matrix, [1., 0.] [0., 1.]. And then I run a filter with these 3 measurements over here, and what should come out is that by running the filter, I can estimate the velocity and therefore make better predictions. In the filter that I want you to program, I want the measurement update first and then the motion update. Every time we run the filter, I want you to update the measurement first, then the motion. Here is my empty procedure filter that we have to fill in where I go through our measurements, and it builds the measurement update and then the motion update, the prediction, and then I just print out the resulting estimates. I do this a number of times, 3 times in this case. Once we fill this in and I hit the Run button, I get the following output. After my first measurement update, I observed the location 1, which gets copied over essentially to .99 over here. I learn nothing about the velocity, so it's still 0, just the way I initialized it. And then there's an updated uncertainty matrix which now shows what happens to be a strong correlation, 1000, 1000, 1000, 1000. That differs from the initial one only that we filled in the off-diagonal elements. It happens to be exactly what the Kalman filter does. And then I'll observe again the 2. I want the output to now tell me that my next prediction is 3. It's the observation plus the prediction. But now I have a really good estimate on what the velocity is. It's essentially 1, and the reason is my Kalman filter was able to use the Kalman filter equations to find this value. There's a new covariance matrix, and for the third observation followed by the prediction, the prediction is correctly effectively 4, 3.999. The velocity estimate is correctly 1, which is .99999, and I have yet another uncertainty matrix which illustrates I high certainty in the velocity estimate. Notice a relatively high certainty in the position estimate compared to my initial uncertainties. So can you write the algorithm filter that outputs those exact values over here? This is an involved programming assignment. What you have to do is you have to essentially implement the equations I gave you. You have to familiarize yourself with the matrix class and then go and fill in the filter code in accordance to the things that I showed you for the multivariate Kalman filter. [Male] And here is my code. If you've got this correct, then I'm mightily impressed with what you've done because you've taken something that often takes many, many classes to explain to students, and within a single class, you understood the gist of it and you wrote a piece of code that is really non-trivial, that you can reuse many times, and that's the core of the Google self-driving cars' ability to check other vehicles. Here is the line by line implementation of what I've shown you before for the measurement update and the prediction, and you'll find using my matrix class that it implements step after step exactly what I've shown you. A little non-triviality. We have to make a measurement matrix of the nth measurement. If you solve the problem, you have probably something like this. The arrow calculation, the matrix S with a transpose, the Kalman gain K with the inverse, back to my next prediction and my measurement update, and this is the prediction step. You can see it implements exactly what I showed you in these 2 equations over here. Now I know programming with this is involved, and I'm really impressed if you were able to do this. If you've done this, you've achieved something really, really major. You now understand Kalman filters, and you've implemented a multidimensional Kalman filter all on your own using a fairly mechanical matrix class that I wrote for you. You ran it, and it's gotten really good results in which a sequence of position estimates, 1, 2, 3, led you to make a prediction and understand the velocity of the moving object. These are the equations you just implemented. Congratulations. You really understood something fundamental here that I believe is really essential to artificial intelligence and to building self-driving cars. You implemented effectively our method for finding other cars. Let me put this in context. Here's a Google self-driving car. Here's another car. Our Google self-driving car uses radar on the front bumper that measures the distance to vehicles and also gives a noisy estimate of the velocity. And it also uses its lasers, and again, it measures the distance to other cars but no velocities. If you take the same situation from above, here is the Google car. It is localized on a map. Here is another vehicle, and another one. Using radars and lasers, the Google car estimates the distance and the velocity of all these vehicles, and it does so using a Kalman filter. We have feeds and range data from the laser, and it uses state spaces like this one of the relative distance in x and y and the relative velocity in x and y to get state transition matrices of the type I've just shown you to find out what these other cars are, and this is exactly what you've just learned and programmed yourself. I didn't tell you how to extract the location of other cars from radar and laser. There's something called a correspondence problem. Sometimes you don't know which one each car is, and I won't talk in much depth about it. But you understand the gist of the solution now, and you've been able to program it. If you were in a situation like this, you can use range data like laser data and radar data and come up with a rational algorithm that takes momentary measurements of other cars and not just estimates where they are but also how fast they're moving. That's really a tremendous feat. Congratulations on getting this far. If you got this far in my class, I promise you you've just digested the most difficult thing I have to teach you in this entire class. Congratulations. [Next Class] So, in the next class, I'm going to teach you our final state estimation algorithm. It's called particle filters [Particle Filters]. And there's the two first methods we learned. The histogram method and now the Kalman method are really complex and involved. This one is really simple and easy to implement and really powerful. [Easy to implement. Powerful.] We are using it in various places in the Google self-driving car. And it's possibly the most popular method to-date in recent times. And it really overcomes some basic barriers in both of the 2 previous methods. It's also really fun to implement because you can implement it in about 5 lines of code, which is quite amazing. So, please come back when the next class comes and learn how to program a particle filter. So this completes my unit on Kalman filters. You learned actually quite a bit. You learned about Gaussians, how to do measurement updates using multiplication, how to do prediction or state transitions using convolution, and you even implemented your first Kalman filter, which is really super cool. You've implemented it in the context of vehicle tracking, and you used this to estimate a nonobservable velocity for measurement data. Now, next is your homework assignment. I hope you can prove what you've learned and ace it. Then, next week, we're going to move into particle filters, which is an exciting third method for state estimation. So I'll see you for the homework assignment, and then I'll see you in class for particle filters. So before I start today's class I'd like to take you on a trip that I recently did to share some of the things that excite me in my life with all of you. I actually went to Washington, D.C. and the highlight of the day for me was to visit the National Museum of American History. Some of you might know this. In 1998 a team led by Wolfram Burgard and myself put a robotic tour guide into this museum. This robot was in the museum for about two weeks and it led kids and visitors of the museum through the exhibit. It did localization similar to what I taught you before. It used a learned map of the environment. We programmed by hand the specific location of exhibits and was also able to say something. It was also able to smile and to frown. And as you can see in this video, sometimes even kids climbed onto it. But today I'm here to see Stanley. Stanley has been here for a couple of years now in exhibits our robot that won the DARPA Grand Challenge, and it was really fascinating to see the thing that we've built and visited in its own little room in the Smithsonian museum. This exhibit has been made specifically to celebrate Stanley's victory. And in exploring this exhibit again I found that the curators had actually put some program code on the wall for people to understand. Let's zoom in. You can see common. So, at the time you are already working with common faters and in our parameter file, as shown here, the word common occurs many times. So, what I've been teaching you last class about the common fater really had a major role to play in making Stanley the robot win the DARPA Grand Challenge. So these are deep emotional moments for me going back to the past and seeing what we've done and how it's been preserved. But I want to share with everybody here because it’s part of my life and it’s part of what we’ve done in building self-driving cars, and the methods I’m teaching you in this class are essentially the same methods as we used back in Stanley and in Junior when we did the Urban Challenge. [Narrator] So, in this class you will learn about particle filters. In our sequence of algorithms for estimating the state of a system, this is the third one and in many ways is the best one. It's the easiest to program and in most ways is the most flexible. And to understand why I'm saying this let me start with a little quiz that goes back into the first 2 classes. In class 1, we learned about histogram filters, in class 2 about kalman filters, and we even had to prove there. Today, I'll teach you about particle filters, but we can't really know about particle filters quite yet. So, my questions will only pertain to histogram filters and kalman filters. First, I'd like to know whether the state space was discrete or continuous. Please check exactly 1 of those 2 boxes over here, and I understand these are not entirely non-ambiguous questions, but in the spirit of the method please check whichever fits best. When a histogram filter was discrete distribution was defined over a finite set of bins, whereas the common filter had a continuous state space. [Narrator] So, let me ask you a second quiz. In particular I would like to know whether distributions that can be represented may be unimodal or can also be multimodal. So, check unimodal if this is all we can do, whereas if we can have multiple bumps in our probability distribution, check multimodal. [Narrator] And here the histogram filter scores better. Even though it was discrete, it was able to represent multiple bumps, which the kalman filter couldn't, so it's unimodal. If you forget this go back to the past class and look at this. The kalman filter was a single Gaussian which is by definition unimodal, whereas the histogram filter can have bumps over arbitrary grid cells. [Narrator] The next question I wouldn't need to dwell on in the class, but I think it's important. When it comes to scaling in the number of dimensions of the state space, the amount of storage we have to assign. I give you 2 answers. It could be quadratic or exponential. So, we have quadratic exponential--quadratic exponential--and I understand I didn't really discuss this, but go back in your memory to how grids are represented and how Gaussian's are represented, and I promise you 1 of those answers is correct for either of the 2 filters here. [Narrator] The histogram filter's biggest disadvantage is it scales exponentially, and the reason is any grid that is defined over arcade dimensions will end up having exponentially many grid cells in the number of dimensions, which is really unfortunate because we can't really represent high dimensional grids really well. So, it works really well for low dimensional problems like 3 dimensional robot localization problems. The kalman filter in contrast, under certain assumptions, is quadratic. All we represented was a vector, the mean, and the covariance matrix, and the covariance matrix is quadratic. And it turns out all the computation, if your measurements space is a fixed size, ends up to be quadratic without a proof here. So, you just have to take it by faith, but the queer thing is this is a much more efficient method. So, if you have a 15, 20 dimensional state space, the kalman filters will be more efficient than the histogram filters. [Narrator] Let me ask a last question. When applied to robotics do we believe the histogram filter is exact or approximate? Same here. I know we've never talked about this. Please just check the boxes you find most likely, and then move on to see my explanation. [Narrator] While histogram filters tend to be approximate because the world tends not to be discrete. So localization, for example, it's clearly an approximate filter. It turns out kalman filters are also approximate, and it's a much more subtle observation. It turns out kalman filters only are exact for linear systems, whereas the world happens to be nonlinear. Now this goes into a lot of deep math, which I don't want to get into here, but you should understand that both of these filters are not exact. Both of them tend to be just approximations of the correct posterior distribution. [Narrator] Now let's look into particle filters, the subject of today's class, and it's really interesting to see the answers for particle filters. First, the state space for particle filters is usually continuous. So, you can get into the more interesting version of state spaces, but we're not confined to unimodal distributions. We can actually represent arbitrarily multimodal distributions. They are also approximate just like the other 2 filters, and in terms of efficiency the world is still out there. In certain incarnations, they clearly scale exponentially, and it is a mistake to represent particle filters over anything more than say 4 dimensions. But in other domains, mostly in tracking domains, they tend to scale much, much better, and I've not seen a good treatment yet of the complexity in practice for particle filters. However, the key advantage of particle filters is actually none of those things over here. The key advantage, at least in my life, has been they're really easy to program. As you hopefully see today, writing a particle filter is really, really easy. In fact, you will write your own particle filter for a continuous value localization problem, which is in many ways more difficult than any of the problems we talked about before. So, let's dive in and see a particle filter in action. So, here is a floor plan of an environment where a robot is located and it has to perform what's called global localization, which is it has no clue where it is and it has to find out where it is just based on sensor measurements. This provides his range sensors as indicated by the blue stripes those use sonar sensors, which means sound, to range the distance of nearer obstacles, and it has to use these range sensors to determine a good posterior distribution as to where it is. What the robot doesn't know it's starting in the middle of the corridor. In fact, it is completely uncertain as to where it is. Now, the particle filter represents this using particles. Each of these red dots of which there are several thousand here is a discrete guess where the road might be. It's structured as an X coordinate, a Y coordinate, and also a heading direction, and these 3 values together comprise a single guess, but a single guess is not a filter. It is the set of several thousands of such guesses that together comprise an approximate representation for the posterior of the robot. So, let's start the video. In the beginning the particles are uniformly spread, but the particle filter makes them survive in proportion of how consistent 1 of these particles is with a sensor measurement. Very quickly the robot has figured out it's in the corridor, but 2 clouds survive because of the symmetry of the corridor. As the robot then enters 1 of the offices, the symmetry is broken and the correct set of particles survive. Let me play this again. The essence of particle filters is to have these particles guess where the road might be moving, but also have them survive using effectively survival of the fittest so that particles that are more consistent with the measurements are more likely to survive and as a result places of high probability will collect more particles, and therefore be more representative of the robot's posterior belief. Those particles together--those thousands of particles are now clustered in a single location. Those comprise the approximate belief of the robot as it localizes itself. [Narrator] Hi, I'm Kathleen and Sebastian wrote a piece of code for you that I am now going to demonstrate. So, the main class is a class called robot. This robot lives in a 2-dimensional world of size 100 meters X 100 meters. It can see 4 different landmarks that are located at the following coordinates: 20, 20; 80,80; 20,80; 80,20. So, here's how we make such a robot. It's really easy. All you have to do is call a function robot and assign it to a variable my robot. So, now that we can do things with my robot. For example, we can set a position. These 3 values are the X coordinate, the Y coordinate, and the heading in radians, and this command assigns those values to the robot. So, let's print these things out and down here you see the output X=10, Y=10, and heading=0. Next, let's make the robot move. This robot moves 10 meters forward and doesn't turn. So, let's print the resulting position. And here we go, you can see that it's now at 20, 10, and 0. It moved 10 meters forward from 10,10 to 20,10. Now, let's make the robot turn by pi/2 and move 10 meters. So, now the robot is heading in the direction of pi/2, and it moved forward 10 meters in the Y direction, instead of the X direction. So, as you can tell it's really easy to program. The last thing I want to show you is how to generate measurements. There's a really easy command called sense and all it does is give you the distance to the 4 landmarks, 1, 2, 3, and 4. For now this is all you need to know about the class robot that Sebastian has programmed for you. You might want to spend some time familiarizing yourself with the code to see how this is all configured. [Sebastian:] Thank you Kathleen. I really appreciate it. This code has a little bit more stuff than you just talked about. It actually assimilates noise, but the noise filters are all set to 0, and those noise filters are really important for particle filters so you can play with those if you like. In fact, there's a function over here called set noise. It allows you to set them, and then later on we have a function that makes kind of no sense right now, but really important as we implement particle filters called measurement probability, and this accepts a measurement and tells you how plausible this measurement is. It's kind of the key thing for the survival of the fittest rule in particle filters. So, if you look through the codes don't be confused by this function; we will actually use it later. [Narrator] Here's our first programming exercise. I'd like you to make a robot that starts at coordinates 30 and 50, and it heads north, which means its heading direction is pi/2. It then turns clockwise by pi/2, which means you subtract pi/2 from the heading direction, and it moves 15 meters. It then senses, and I want you to print out the sensor measurements. It then turns clockwise by pi/2 again, and moves 10 meters this time and I just want you to print out the sensor measurements after this entire procedure. So, there are 2 print statements for the sensor over here and the sensor measurements over here. So, here's the output I would like your program to generate. After the first motion, the first measurements will be 39 plus something, 46, 39, and 46. And then after the second motion I expect to see 32, 53, 47, 40. Of course, there's lots of decimal-point numbers over here, but these are the numbers I would expect you to output. So, have fun coding it! [Narrator] And here's my solution. I initialized the robot--my robot--using the function robot. I set the coordinates to be 30, 50, and pi/2. I then apply the motion command, assign the result to my robot again with minus pi/2 and 15. I print the measurement values. I move again, and I print the measurement filters again, and when I hit run, this is exactly what I get. [Narrator] Next, I'd like you to play with the noise. Our class robot has built-in noise variables. One is for forward motion. This is the added Gaussian noise variable to the motion you command. The same for turn and the same for the sensor measurements. And as I scroll down I find the function set noise lets me set those values to values other than zero. So, I want you to--into your code--set these values as follows: forward noise equals 5.0, turn noise equals 0.1, and sense noise equals 5.0. So, please fit this into your code. [Narrator] And here is how I would do it. I would just call the function set noise with the parameters as specified for the object my robot, and when I hit run now I get different values like those, or those, or those. In fact, every single time I hit run I get a different set of values. [Narrator] So, now we know about our class robot who can turn and then move straight after the turn, and which it also can sense the distance to 4 designated landmarks, L1, L2, L3, and L4, and these distances comprise the measurement vector of the robot. We told you the robot lives in a world of size 100 x 100, and what this means if this robot falls off 1 end, it disappears on the other. So, it's a cyclic world. So, let's now talk particle filters. [Narrator] The particle filter that you're going to program maintains a set of 1000 random guesses as to where the reward might be. Now, I'm not going to draw 1000 dots here, but let me explain how each of these dots looks like. Each of these dots is a vector which contains an X coordinate, in this case 38.2, a Y coordinate 12.4, and a heading direction of 0.1, which is the angle at which there are points relative to the X axis. So, this one moves forward, it will move slightly upwards. In fact, now a code--every time you call the function robot and assign it say to a particle, here the [i] particle, these elements p[i]x, y, and orientation, which is the same as heading, are initialized at random. So, to make a particle set of 1000 particles what you have to program is a simple piece of code that assigns 1000 of those to a list. So, let's do this; let me set N=1000 for 1000 particles. Here's my initial set of particles; it's going to be an empty list, and I want you to fill in some code after which we have 1000 particles assigned to this vector over here. So, when I print the length of this thing I will get 1000 instead of 0. [Narrator] Here's 1 possible solution if we iterate the following loop 1000 times we create an object called robot, and we print this object to our growing list P, and when we're done we have 1000 particles, and let me do something I might regret, which is let me just print out this entire set of particles with print P, and what I get is 1000 items that look just like this over here. If you have an X, a Y, and a heading direction all generated at random. So, if you look through those you'll find there's a lot those. So, we now have a set of 1000 particles, each of which just looks like one of these dots over here, and each of which has exactly 3 values associated, an X, a Y, and an orientation. [Narrator] I now want you to take each of these particles and simulate robot motion. Depending on the heading direction, this might yield a different direction. So, each of these particles shall first turn by 0.1 and then move by 5 meters. We already implemented something just like this for individual robot motion. Now I'd like you to apply this to the entire particle set. So, please go back to the code and make a new set P that is the result of this specific motion turning by 0.1 and moving forward by 5.0 to all of those particles in P. [Narrator] So, here's one possible solution: reconstruct P2 as a temporary particle set with a later set P equals P2, so this is just a temporary set. We then go through all the particles, again, and here is the tricky line. We append to list P2 the results of our motion of 0.1 and 5.0 applied to the [i] particle chosen from the original particle set. So this applies the move command to each of the particles exactly the same way we applied move to the robot motion before. When we are done we reset P=P2. So, we've done the full recursion of applying our motion update to our full particle set. If you've gotten this far then you got about half of particle filters implemented, and fortunately it's the easy half, but the difficult half isn't that much more difficult. [Narrator] Let me explain how the second half works. Suppose an actual robot sits over here, and it measures these exact distances to the landmarks over here. Obviously, there some measurement noise that would be just more or less an added Gaussian with 0 mean. Meaning there would be a certain chance of being too short or too long and that probability is governed by a Gaussian. So, this process gives us a measurement vector of 4 values of those 4 distances to the landmarks L1 to L4. Now let's consider a particle that hypothesizes the robot coordinates are over here and not over here, and it also hypothesizes a different heading direction. We can then take the measurement vector and apply it to this particle. Obviously this would be a very poor measurement vector for this specific particle over here. In particular, the measurement vector we would've expected looks more like this. That just makes this specific location really unlikely. In fact, the closer our particle to the correct position the more likely will be the set of measurements given that position. And now here comes the big trick in particle filters: the mismatch of the actual measurement and the predicted measurement leads to a so called importance weight that tells us how important that specific particle is. The larger the weight the more important it is. Well, we now have many, many different particles and a specific measurement. Each of these particles will have a different weight. Some look very plausible, others might look very implausible as indicated by the size of the circles over here. We now let these particles survive somewhat at random, but the probability of survival will be proportional to their weights. If something has a very big weight like this guy over here will survive at a higher proportion than someone with a really small weight over here, which means after what's called resampling, which is just a technical term for randomly drawing N new particles from these old ones with replacement in proportion to the importance weight. After that resampling phase, those guys over here very likely to live on, in fact many, many times. Whereas those guys over here likely have died out. That's exactly what happened in our movie in the beginning when we looked at localization in this corridor environment. The particles that are very consistent with the sensor measurement survive with a higher probability, and the ones with lower importance weight tended to die out. So, we get the fact that the particles cluster around regions of higher posterior probability. That is really cool and all we have to do is we have to implement a method for setting importance weights and that is, of course, related to the likelihood of a measurement, as we will find out, and we have to implement a method for resampling that grabs particles in proportion to those weights. So, let's just do this. So, let me add back the robot code. We built a robot, and we make the robot move, and we now get a sensor measurement for that specific robot using the sense function. So, let's just print this out. These are the ranges or distances to the 4 landmarks and by adding your print my robot statement you can also figure out weight importance as 33, 48, 0.5, obviously this is a random output because you randomly initialized the position of the robot. What I want you to program now is a way to assign importance weights to each of the particles in here. I want you to make a list of 1000 elements where each element on the list contains a number. So, this number is proportional to how important that particle is, and to make things easier I coded for you a function in the class robot called the measurement probability. This function accepts a single parameter, the measurement vector, the Z edge as defined, and it calculates as an output how likely this measurement is, and it uses effectively a Gaussian that measures how far away the predicted measurements would be from the actual measurements. You can dive into this code and understand what's going on. There's one last change we have to do to make this code run. We have to actually assume that there is measurement noise. If there is 0 measurement noise, then this function will end up dividing by 0. So, let's put in a clause that specifies what we believe the actual measurement noise is. I'm going to do this not for the robot, but I do this for the particles. In this line of code over here where we create the particles for the first time, I now just initialized these positions remain numbers but also assume a certain amount of noise that goes with each particle, 0.05 for the translational noise, 0.05 for the rotational noise, and 5.0 for the measurement noise in those ranges. So, this is the crucial number over here. So, coming back to what I want you to do, I wish you to construct a list of 1000 elements in W so that each number in this vector reflects the output of the function measurement probe applied to the measurement Z that we receive from the rear robot, such that when I hit print W, I actually get a list of 1000 importance weights. [Narrator] And this can be done in a single line of code. You construct the list W by appending the output of the function measurement prop applied to the [i] particle with the augment of the extra measurement, and as you can see over here most of them look insanely unlikely. So, they have exponents -146, -24. Some of them are more likely--the ones that are closer to the truth like -5. Those are the particles that surely survive, whereas those ones over here with a -126, those are really ready to die off because they are so far away from the truth we really don't need them anymore. So, in the final step of the particle filter algorithm, we just have to sample particles from P with a probability that is proportional to its corresponding W value. Particles in P that have a large value over here should be drawn more frequently than the ones with a small value over here. How hard can that be. [Narrator] And it turns out it's actually harder than you think, but I'm going to show you how to do it, and once you've done it, you can use the exact same code forever for any particle filter. But let me emphasize what resampling actually means. We are given N particles, each of which has 3 values, and there's N of them, and they also now have weights. These are simple floats or continuous values. Let's call big W the sum of all these weights, and let's normalize them just for the consideration of what to do, and it's called the normalized weights alpha. So alpha 1 would be the weight 1 divided by the normalizer W, and so on all the way to alpha N, and obviously it goes without saying that the sum of all alphas is now 1, since we normalized them. What resampling now does is it puts all these particles and then normalized weights into a big bag, and then it draws with replacement N new particles by picking each particle with probability alpha. So, for example, alpha 2 might be large so we're going to pick this one, P2. Alpha 3 might also be large so we pick that one. Alpha 4 might be really small but just by chance you might actually pick it. So, we have P4 over here, and then we might pick alpha 2, again. So, you get 2 versions of P2, perhaps even 3 versions of P2, depending on the probabilities. We have N particles over here. We do this thing N times, which is why I said with replacement we can draw multiple copies of the same particle, and in the end those particles that have a high-normalized weight alpha over here will occur likely more frequently in the new set over here. That's called resampling. So, to make sure you understand this let me ask you a couple of quizzes. Suppose we have 5 particles with the following importance weights: 0.6, 1.2, 2.4, 0.6, and 1.2. If I, in the process of resampling, randomly draw a particle in accordance to the normalized importance weights. What is the probability of drawing P1, P2, P4 and P5? [Narrator] And the answer is 0.1, 0.2, 0.4, 0.1, and 0.2, and to see we just have to normalize those importance weights. The sum of those numbers over here are 6. So, we divide 0.6 by 6. We get 0.1. 1.2 divided by 6 is 0.2. 2.4 divided by 6 is 0.4. Obviously those over here add up to 1. [Narrator] So, let me makes this alpha-wise expressive and ask another question. Is it possible that P1 is never sampled in the resampling step? Yes or no? Please just check one. [Narrator] And the answer is yes, in the random resampling process something with an importance weight of 0.1 is actually quite unlikely to be sampled into the next data set. [Narrator] Let me now ask the same question about P3 which is the particle with the largest importance weight. Please check yes or no. Is it possible that P3 is never sampled in the resampling step? Yes or no? [Narrator] And the answer is yes, again. Even though this importance weight over here is large, it could happen that in each of the 5 resampling steps we pick one of the other 4. [Narrator] So, I'm going to ask you a tricky question and maybe you can calculate this. So, what is the probability of never sampling P3? To answer this question assume we make a new particle set with N=5 new particles where particles are drawn independently and with replacement. [Narrator] And the answer is 0.0777 approximately, and the way to obtain this is for this particle to never to be drawn in the resampling phase. We always have to draw 1 of the following 4 particles. Those together have a probability of 0.6 to be drawn, which contrasts to the 0.4 for P3. So for 5 independent samplings to draw 1 of those 4, we get a total probability of 0.6 to the fifth, which is approximately 0.0777. Put differently, there is about a 7.7% chance that this particle over here is missing, but with almost 93% probability we'd have this particle included. If we hadn't set up P3, gone for P1 over here, which has a much smaller probability of being drawn, then this 0.07 will be as large as 0.59, which is 0.9 to the fifth. Now this means with about 60% chance we will lose particle 1, and only with a 40% chance it will include it. Put differently, the particles with small importance weights will survive at a much lower rate than the ones with larger importance weights, which is exactly what we wish to get from the resampling step. [Narrator] So, what I would like you to do next is to modify our algorithm to take the lists of particles and importance weights to sample N times the replacement and new particles with a sampling probability proportional to the importance weights, or in the code now that we calculated our new particles and the corresponding importance weights, construct a new particle set P3, which we will call P, again, when everything is said and done, so that the particles in P3 are drawn from P according to the importance weight's W. Now to warn you this is more difficult than it looks like. I'm going to show you a trick in a second, so if you fail to do this don't worry. I give you a chance to do it right now, but then I'm going to tell you a little bit more about how to organize this in an efficient way and you get a second chance. So, try it out, see if you can do it, and if you fail look for my advice and then try it again. [Narrator] Now it turns out this is not an easy thing to do and obviously I think it might be to complete all this normalized alphas, but you still have to be able to sample from those. So, in the spectrum of our alphas you might draw a random variable uniformly from the interval 0;1, and then find out the alpha so that all the alphas leading up to it, and some are smaller than beta, but if we add the new alpha to the sum you would get a value larger than beta. Now that's doable. It's inefficient. In the best case you get an N lock and implementation. Let me show you what is commonly done, and I don't take any guaranty that it's entirely unbiased, but there's a very simple trick. [Narrator] So, here's an idea how to make this more efficient, and it turns out empirically it also gives better samples. Let's represent all our particles and importance weight in a big wheel. Each particle occupies a slice that corresponds to its importance weight. Particles with a bigger weight, like W5, occupy more space. Whereas particles with a smaller weight occupy less space. Very initially let's guess a particle index uniformly from the set of all indices. I did note this as a uniform sample at U from the discrete self choices of index 1 all the way to N, and as a caveat in Python, of course, it goes from 0 to N-1. So, say we pick W6. Then, the trick is--then you're going to construct the function better. Then, I initialize the 0 and to which I add--when I construct these particles-- a uniformly drawn continuous value that sits between 0 and 2 times W max, which is the largest of the importance weights in the important set. W5 is the largest, so we're going to add a random value that might be as large as twice W5. Suppose the value we added brings us to here. So, this is the value we actually drew, measured from the beginning of the sixth particle which shows an initialization. I now then iterate the following loop: if the importance weights of the present particle doesn't suffice to reach all the way to beta. So, if W index isn't as big as beta, then I subtract from beta this very value W index and I increment index by 1. So, what have I done? I've moved index to over here, and I removed this part of beta so the point over here is still the same as before. We now get to the point where beta becomes smaller than W index, which is the case in the next situation. Now index=7. Then, index is the index of the particle I pick in my resampling process. So, I picked the particle index; I now iterate I add another uniform value to beta. Say I add this one. This is the value I add, this is the value beta previously had. The same iteration now will make index flow up reducing beta by all the slice over here, which is W7, and then jump over here, and particle 1 is picked. It can easily happen that the uniform value is so small that the same particle is picked twice, and it's easy to see that each particle is now picked in proportion to the total circumference it spans in this wheel of particles. So, this is essentially my implementation for the resampling step. So, I want you--if you can--to implement that specific resampler in Python. [Narrator] So, here's my implementation of the resampling step, and it follows the same logic that I gave you in a diagram. We're creating a new set of particles called P3; it's an empty set in the beginning, and inside this routine, every time I resample, I add a particle from the previous particle set with the index index. So, that's the main loop over here, and at the end I assign P3 back to P. So, that's the resampling step. My very first index is drawn at random. This is a uniform random sampler of all the indices, and then I had this running variable beta that I set to 0.0, and I cash away the max of W just to be slightly faster. You don't have to do this; they come in over here. Doesn't really matter if we have max over here, but then I go and produce exactly N particles, and the way I do this I add to beta a uniform random that is twice as large and maximum in the range as my max weight W. Now, 2 times max weight W will be a very large step, but by adding a random variable that sits between 0 and 1, I have uniformity in 0 and 2 times MW, and then while this beta variable is larger than the weight of the current index, I subtract this weight from my beta value and I increment index by 1 modeler N the total number of particles, and when it's smaller I'm done. I can just take that particle, add it, append it, and repeat. So, this entire procedure over here is somewhat involved if you got that right I'm impressed. I hope you learn something from doing it. It happens to be really easy to program once you know what to do, and every time we write a particle filter you can just reuse it. You never have to think about it again because there's nothing domain specific in this specific procedure over here. So, let's run it; if I run it nothing happens to that empty set. So, let me print out the resulting set of particles. So, now I have a print P over here. Let me run it, and of course, I'm going to get 1000 particles, right? A lot of particles but let's look through them. If you just look at the first value over here, they are all about the same. They are all between 76 and 82. The second one--they're all about 42, 44, 43, 41, 39, 38. So, what you've gotten here is a set of particles that are all co-located . So, instead of having a complete random set of particles, like we had before, the resampling step--we can see this already gives me particles of very similar X and Y positions. Now it turns out the orientations are not very similar. They jump like crazy, and the reason is-- --well, if you think about it, we only have 1 location so far, our distances to landmarks are independent of the orientation. Such as that our orientation plays no role in the protected measurement, and therefore has no roll in the selection. Let me make the point, again; here's our 4 landmarks, and we measure the distances to those. A robot facing this direction has a certain set of distances. A robot facing a different direction, like this one, has the exact same set of distances. Therefore, in our particle future, the heading direction plays no role. Here's a quiz. Will orientation or heading never play a role? And the answers are: Yes, Never-- so we always get a random set of orientations-- or No--eventually they matter? And the correct answer is: of course they will eventually matter. So No is the correct answer, and let me show this to you. Again, assume our 4 landmarks and consider our robot facing to the right, to this direction. We get a certain set of distances that is invariant to the orientation. But now this robot moves, and we get a new set of distances. And now orientation matters. If we assume a different initial orientation, like this one over here-- and that robot moves-- its measurements will be very, very different. So orientation does matter in the second step of particle filtering because the prediction is so different for different orientations. Let's go and program this. So I want you to take this particle filter, and program it to run twice. And here's my answer: this is all initialization over here, so we shouldn't touch it. But from here on, we want to do things twice. So we're going to put a "for" loop here, from "t in range(T):" and then after in, add all the stuff below. I end it all the way to the final statement, but I only want to print the final distribution. Let's run it--and surprisingly, the orientations aren't really that well worked out. If I go down I find, still, failure random orientations here. But if I go to 10 steps forward, which means the robot really moves quite a bit across its environment, and I hit the run button-- I actually get orientations that all look alike. So you can see, they're all about 3.6, 3.8, 3.9, 3.7. You can see the "y" values are all about the same and the "x" values are all about the same. So this is the particle failure working. What I'll do next is to give you another program assignment. Rather than printing out the particles themselves, I want you to print out the overall quality of the solution and to do this, I've programmed for you an "eval" code which takes in as a robot position and a particle set, and it computes the average error of each particle, relative to the robot pos in "x" and "y"--not in the orientation. And the way it does it, it basically compares the "x" of the particle with the "x" of the robot, computes the Euclidian distance of these differences, and averages all of those. So it sums them all up and it averages them, through the number of particles over here, which is upper caps "m". Now, there's some funny stuff over here. The reason is, the world is cyclic and it might be that the robot is at 0.0 or at 99.9. It's about the same values but, according to this calculation over here, they'd be different. So while there's normalization over here, I make sure that the cyclicity of the world doesn't really affect negatively the estimated error I've enclosed in the boundary. You might parse this. I'm adding "world_size" over to the computer model operation and then subtract the same thing over here--I can't even see it. It's just too long a line. Be that as it is, I want you to take the function, "eval", and produce a sequence of performance evaluations. And if we hit the run button, I want you to produce something like this stuff over here: 4.9, 3.6, 2.9, 2.8, 3.1. This is the residual error. Remember, it's a world of size 100 by 100, so this is actually a relatively small error, compared to the world's size. Can you code it so I get, for each iteration, the error number produced by this routine over here? So here's my solution. It's the same command as: print eval(myrobot, p)-- I know it wasn't very hard, but it gets you a kind of fun to play with it. So here's the sequence of numbers I get out. It turns out, we don't always get the same number. Sometimes it doesn't work. Here's the second run. These are small values again-- another one, another one. Here's one that's interesting, so we can look at these errors: 5, 5, 7, 1-- 3, 5, 6, 7, 7, 6. It is a good run, so the error is down to 6, compared to whatever it would be if you didn't do particle filters and had a random set of particles. In fact, to understand this, let me just take the "print eval" command and move it to the very beginning, where we have done no particle filters. So hit run, and what you will find is it goes from 38 to 4, 3, 3 in just one step of particle filtering, which is a drastic reduction of error. Now running many times, there will be cases where it fails, where there's just no particle nearby. Strangely, they don't show up right now. When I was testing it and programming it, I actually got one of those. I had an error of like 15 or 20. Whenever I want to demo something it just doesn't work. Well, this particle filter is just too good. It just gets the answer relatively right really, really quickly. So one thing I want to stress here is that you've just programmed a full particle filter. So you got, from me, kind of a very primitive and rudimentary robot simulator that uses landmarks as a way of doing measurements, and it uses three-dimensional robot coordinates, so it's a little bit better than the stuff we talked about in previous classes-- it's not quite a Google car yet. And you solved the estimation problem. In fact, you solved this problem in 30 lines of code. That is an amazing small amount of code for something that's amazingly powerful. And you can reuse these 30 lines of code in pretty much all problems you might study that require particle filtering; they're very generic. So let me get back to the theory of particle filtering. Let me ask you a few questions. We had measurement updates and motion updates. In the measurement update, the computer posterior over state given the measurement. And it was proportional to, up to normalization, of probability of the measurement, given the state times "p" of the state itself. In the motion update, if you compute a posterior over distribution, 1 times sublayer. and that is the convolution of the transition probability times my prior. Now those formulas--those should look familiar. This is exactly what you implemented. You might not know you implemented this; let me explain to you how you implemented it. This distribution was a set of particles. A thousand particles, together, represented your prior "x". These were importance weights. And technically speaking, the particles with the importance weights are a representation of distribution. But we wanted to get rid of the importance weights so by resampling, we work the importance weights back into the set of particle so the resulting particles-- the ones over here--would represent the correct posterior. You've implemented this. I'm just telling you what the math is behind this. This, you also implemented. This was your set of particles again, and you sampled from the sum by taking a random particle over here and applying the motion model with a noise model to generate a random particle, "x-prime". As a result, you get a new particle set that is the correct distribution after the robot motion. So you recognize the math, and hopefully you understand how your code implements this math. You can prove all kinds of interesting facts about this math. For example, you can prove conversions if the number of particles goes to infinity. It is obviously approximate. Particles are not an exact representation. And it was amazingly easy to program. So when you go over your particle code you realize you implemented a fairly involved piece of math that is actually the same for all the filters we talked about so far. The same math underlies our histogram filter we talked about in Class No. 1. And the same math for Gaussians is the Kalman filter we talked in Class No. 2. So let me ask you an interesting question. Which of the 3 filters did Sebastian use in his Job Talk at Stanford? Histogram Filters, Kalman filters, Particle Filters or None of the above? Check one or all that apply and, of course, you can't really know unless you Google me and look up my Home Page. Then you might find out some evidence. So just take a random guess and I'll tell you the answer in a second. I should say I was hired by Stanford, in 2003, into a tenured Associate Professor position so obviously my Job Talk wasn't that bad. And the answer are those 2. Those, I used in a Job Talk of 2003. But I previously applied at Stanford and actually received an offer, which I turned down, in 1998. And in my earlier work, I didn't understand particle filters, and I did everything with this filter over here. But came 2003, I was a big, big fan of particle filters and my Job Talk talked about a version of those that applied them to the somewhat more involved robot mapping problem. Now fast forward to 2012. We built the Google Car. We're now using multiple methods. We use histogram methods and particle methods. The main difference to what you've learned, so far, is two-fold. The main difference that we've learned so far is the Robot Model. The vehicle is actually modeled as a system with 2 non-steerable wheels and 2 steerable wheels. That's often called a Bicycle Model because half of it--this thing over here-- acts like a bicycle. The big difference is the Sensor Data. Instead of using landmarks, we get this really elaborate road map, and then we take a single snapshot and we match that snapshot into the map. And the better the match, the higher the score. And then on top of it, we have additional sensors, like GPS. We also have Inertial Sensors. I am not going to go into those at all, but the methods I taught you are rich enough to handle these additional sensors. So you've just learned about the jist of the method in which the Google Car is able to find where it is, and where other cars are. When you build a system, you have to try it with these more elaborate methods. But I think it's very much doable. It's very easy to replace the current simple motion model with the slightly more sophisticated, for what's called a Bicycle Model, and it's easy to write a correlation function of the map data and computes normalized correlations of measurement images in a big pixel map. I'll leave this an an exercise so if you want to hack your own car and make it drive itself, have fun. I just want to congratulate you. You've actually, in these 3 classes, learned pretty much as much as any of my Stanford student learns in my Specialized AI classes on robotics, when it comes to robot perception. In fact, you've learned pretty much what there is to know to become a successful practitioner in robotics. In the next class, I will tell you about robot motion--how to make the robot move. So we're going to move beyond the idea of just state estimation--finding out where we are or where the other cars are. And we're going to move to the fascinating topic: how to use all this to decide where to move. So in the next class, I'll talk about a basic AI method called "search"--. in particular, "A* search"--which is an algorithm invented by my colleague, Nils Nilsson, at Stanford and I will apply it to the problem of planning robot trajectories and making it move in real time. So I'll see you in the next class. Let's talk about motion planning. The fundamental problem in motion planning is that a robot might live in a world like this, and it might want to find its way to a goal like this and has to device a plan to get there. This same problem occurs for a self driving car that might live in a city near a highway on a network of streets. It has to find its way around and navigate to its target location. If we zoom in and look at this intersection, and this is my best rendering of a street-light environment. We have also planning problems here. Picture a car coming from here that wishes to go over here. To take a left turn on this intersection over here, this car would have to turn right first, engage in a lane shift and then take the left turn to the goal location. Now, a lane shift over here is a risky proposition. If there's a bit truck parked over here, the space might be insufficient to carry out the lane shift. An alternative plan might be to go straight over here, take the detour around the block, and then go straight to the target location. The process of finding a path from a start location to a goal location is called "planning." For robots, it's often called "robot motion planning. Today I'm going to talk about discrete methods for planning in which the world chopped into small bins. In the next class we're going to talk about continuous motion using those plans. What's the planning problem? We're given a map of the world. We're given a starting location. We're given a goal location. Usually, we're given some sort of a cost function. The simplest way to think of cost is just the time it takes to drive a certain route. The goal is find the minimum cost path. Before we program anything, let me see if I can ask you a couple of questions for minimum cost paths. Suppose we live in a discrete world like this, and this is a world we'll be programming. Let's for simplicity assume that the world is divided into little grid cells. Our initial location is over here facing north or up. This is the vehicle and the little arrow over here indicates where it's facing. I'll call this "Start." We wish to get to this area over here, presumably facing to the left side. Let's assume at each time step I can either move forward or I can turn the vehicle. I'm going to call these actions. Each of those costs me exactly 1 unit of cost. Then what's the total cost I have to endure to move from start to goal? This is a number. Please put a number in here. The answer I want to see is 7 and not 6. The reason why I want to see 7 is it takes 6 steps to go on the shortest path to the goal-- 1, 2, 3, 4, 5, 6. But in this cell over here I have to turn to the left side. That turn also costs me a unit of 1. That's in total 6 straight motions and 1 turn, gives me 7. Let me now change the action model into a different model. We have 3 actions. Here is the first. I can go just forward. The second one is I can turn left and then go forward. The third one is I can turn right and then go forward. In all of these actions, I take a step forward. In the left one, I don't turn at all. The center one over here I turn left and go forward. This one over here I turn right and go forward. Let's say for the time being the cost of each of those is 1. What is now the total cost of the optimal path to the goal? The answer is 6. We apply this action over here 3 times--1, 2, 3. We're at this section over here. Now we apply the left turn and go forward action, which is the center action over here. We end facing over here. That's the fourth. Then we go straight again twice. This action over here--5 and 6. So we get a total of 6, as opposed to 7 when we counted the turning separately. The reason why I change the actions this way is for my next quiz. Suppose we punish left turns. Why would we do this? Well, in real traffic, left turns are harder to do than the right turns. Often you have to wait for oncoming traffic. Let's say in our planning, left turns are more expensive. In fact, I should mention that parcel delivery services that plan for optimal routes of trucks like FedEx and UPS in the States, they actually plan routes that try to avoid left turns during rush hours, because it just takes much longer to do left turns. If they can go right turns, they prefer those. In this example here, let's now say that forward motion will cost you 1. A left turn costs you 10, and a right turn costs you 1. Now, what is the optimal path, and specifically what's the cost of the optimal path? This was a tricky question. It's 15 it turns out. The path stays the same. I haven't punished left turns enough. Let me just try this. If I were to go forward, forward, forward, left, forward, forward, we accrue a total cost of 1, 2, 3, 13, 14, 15. You can arrive at the same 15 by taking the 6 steps it takes to the goal and add the extra 9 penalty it takes just for the turn itself, which is 10 minus 1, and you get 15. Let me look at the possible other path, which would take the loop over here and avoid the left turn altogether--1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16. The path around this loop over here has a cost of 16, which is more than 15. We still prefer the left turn. So for this final version, let me punish left turns even more Now they cost us 20. What is now the total cost to get to the goal? The answer is 16. Now we're going to take the loop over here-- 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11, 12, 13, 14, 15, 16. With a cost of left turns this high, the left turn over here becomes prohibitively expensive, and we'd rather take a detour on the right side and do 3 right turns as opposed to 1 left turn. Let me look at a different maze, and this is not a car example anymore, but it's close to what we're going to program. Suppose we start over here, and our goal is to go into this corner over here. There are multiple blocked cells along the way. In this case, we have a robot that can go up, down, left, or right. How many steps does it take for the robot that starts over here to reach the goal position? Please enter your number over here. The answer is 7. As you can see, the shortest path will lead along here. Then it becomes ambiguous. We could either go right or up. Let's say we randomly go up and hit the goal-- so 1, 2, 3, 4, 5, 6, 7 steps to the goal. Let's look at the path planning problem as a search problem. If you took my AI online class, you know what this is all about, but I want to make sure that everybody can understand what I'm talking about. Let's start with a little grid world of size 6 x 5 where our start location is in the top left corner, our goal in the bottom right corner. I block off a few cells so there is still a safe path to the goal. This could be a search through a city graph, through a parking lot, or through a maze of streets for a mobile robot. Just for simplicity, in this example let's assume the robot is given 4 actions. It can go up, down, left, or right. Also for simplicity, let's assume every action succeeds with absolute certainty. We don't model uncertainty in this example. The path planning or search problem is to find the shortest sequence of actions that leads our robot from the start state to the goal state. Just to check, tell me how many you think these are. How many action are required to go from start to goal? The answer is 11. You go 2 down, 3 to the right makes 5, 1 up makes 6, 2 to the right is 8, 3 down is 11. The big question now is can we write a program that finds the shortest path from start to goal? To do so, let's give the grid cell names. We have 6 columns, named from 0 to 5, and 5 rows, from 0 to 4. The basic idea I'll pursue is that I keep a list of notes that I wish to investigate further or, as we call it in search, expand. Let's call this list "open." In the beginning we only have 1 state on this list at [0, 0]--my initial state. Just to make sure I never pick this state again--I don't want any cycles in my path-- let me just check mark this state with a little red check. I now can test whether this state is my final goal state. Obviously, it's not. I'm not done with planning yet. What I do next is expand this state. I take it off my open list and look at all the successors, of which there are 2 over here--[1, 0] and [0, 1]. Those two are now expanded, so I check them. One last thing I maintain for each of these states on the open list is how many expandages it took to get there. This was 0 over here, and it's 1 for these 2 states in red. That's called my "g-value." When I'm done with planning, this will be the length of the optimal path. Let's now go further and expand one of the two. We always expand the one with the smallest g-value, but these are equivalent. They both have a g-value of one, so it doesn't make a difference. Let me expand the first one. This one over here. This one has 3 neighbors--[0, 0], [1, 1], and [2, 0]. But because [0, 0] is already closed with a check mark, we don't consider it anymore, which gives me [2, 0] and [1, 1], both now with a g-value of 2, and we check those over here. I now pick the node on the open list with the smallest g-value, which happens to be this one over here. There's really no choice. It's the node over here. And this has 2 neighbors--[0, 0] and [1, 1]--but both are already checked. Therefore, there is no expansion that takes place. I only expand if I find an unchecked node. The new open list are these two nodes over here. What's going to happen is my nodes will expand gradually into the free space until I eventually hit the goal node. Without proof, the g-value when I hit the goal node will be exactly the number of steps it takes to go from the start state to the goal node. The secret here for that to be the case lies in the fact that I always expand the node with the smallest g-value. But we won't worry about this. What I want you to do is to implement a piece of code that implements what I just described. To warn you, this is a bit of work. Here is my coding environment. My grid is this one over here. It's the same as the grid over here. You can see the obstacles here and the T-shaped obstacle over here. Our starting location is [0, 0], which is the first one you put on the open list. Our goal happens to be [4, 5], which is the coordinate of the cell over here, starting count of course at [0, 0]. I've also coded for you the 4 potential actions into a single field called delta, so that when you go through the different successors on the list in the search you can just go through these sequentially. The first one goes up by subtracting 1 from this dimension. The second one goes left. The third one goes down. The fourth one goes right. Ignore for now the names of these actions. I will use them later. I want to use the cost function of 1, so each step costs you exactly 1 for now. I'd like you to write a piece of software that outputs triplets of this type where the first value is the g-value and the next two are the coordinates x and y. It then retrieves the element with the smallest g-value from the list, expands it--the grid cells [0, 0] gets expanded to [1, 0] and [0, 1]. The g-value is incremented to 1 in both cases. Then as I scroll down a little bit, now it takes again one of the items with the smallest g-value, breaking ties whichever way you want to break them. There's a tie over here. I just happened to take the second one. Expand this one into a new successor. the only one that's not checked yet in the table is [1, 1], which gets a g-value of 2. Now remove again the element with the smallest g-value, which is now the first one. It's being taken down from the list over here to produce a new open list that's sitting over here. As it goes through this--I'm going to scroll down a little bit more. You can see these different elements being taken. You can see the g-value keep going up--3, 4, and so on, all the way to 7 here. At the very end, when the g-value turns 11, it should expand node [3, 5], which is this one over here, find it's only non-checked neighbor, which is [4, 5]--this guy over here, and add to the list with the g-value of 11. When it then looks at the remaining list and picks the one with the smallest g-value, which is this one over here, it should identify that this is actually the goal state and call the search a success. Now, this is all intermediate debugging output. What I want your code to output just for us to check is just the final triplet of the g-value and the coordinate of the very last item that is being retrieved. This is the path length over here, and this is the coordinate of the goal, which is the same as the one over here. I want you to write the code to only output this one triplet over here. Your code should output to this grid over here--[11, 4, 5], and [4, 5] is the goal coordinates. There's nothing interesting here, but the 11 is the key thing. It takes 11 steps to go from here to here. If I change this, for example, by opening up this grid cell over here. It now takes 2 steps less. I want to see the 9 over here. If instead I force a greater detour, I see now a 15 over here for this maze where you have to go down, left, up again, and down again. If there's no way to reach the goal point, as is the case if I block out this entire area over here. Then I want the program to output "fail"--the single word "fail." Please implement this using the algorithm idea that I've just given you. It's going to be difficult. It's going to take you a while. But if you do this, you're almost where I want you to be to learn about A*. Here's my solution--I defined a function "search," which is the only function I'm going to run in the end. It's like the main routine. To check cells once they're expanded so we don't expand them again, I define an array called "closed" as the same size as my grid, and it has two values, 0 and 1--0 being it's still open, 1 meaning it's being closed. You could also use Booleans. This over here assigns an array of the same size as the field grid. I initialize the starting location as checked and assign the coordinates to x, y, and a g-value of 0. My initial open list is going to be just 1 element of my initial coordinates and the g value of 0. So far what I've done is I've defined a array called closed of the same size. All the check marks are not there except for the ones in the left corner, and this is my starting location in my open list right over here with a g-value of 0. Inside my code I use two flags--one is found, which will be true when the goal position is found, and one is resign, which will be true if I don't find a goal position and I've explored everything. The second one will be the case when my open list just turns empty without finding the goal. That's really important for the case where I can't find a path to the goal. Those print commands were the ones I used to debugging. You can look at them. They print out the existing open list. Nothing else. But here is the code. I repeat the following while I haven't found a path to the goal and I haven't proven that the problem is unsolvable. Both found and resign are false. If my open list is empty, there's nothing to expand, then resign is true, and I print "fail." This is one of the 2 terminating conditions. You can convince yourself there's no path from S to G. You'll expand every node on the left side of the barrier until we finally run out of nodes to expand at which point the open list will be empty, and our search failed. If there is still elements in the open list, the else case comes into place. Here is how I remove the element with the smallest g-value. I use the list sort function, which sorts elements in increasing order from the smallest g-value up. Now I want to pop the element with the smallest number. Unfortunately pop pops at the end, so I'll just reverse the list and then pop the element with the smallest g-value from that list. There's a little bit of a trick here. It's not very elegant. It's also not very efficient, but it does the job for now. That here gets me the element with the smallest g-value. For that it's important that the g-value comes first in each of the triplets. That's why I put it first, right before the x and the y. I then assign the 3 values to x, y, and g, which is my expansion. Again, g is the first, x and y are the second and third. Now I'm in the position to test whether I reach the goal. If x is the goal 0 and y is the goal 1, I'm done. I call found equals True. I print out this triplet, and that gives me the triplet over here. This "print next" over here is this triplet and that's the one I was looking for, asking you about printing exactly this solution over here. Now, if I'm not done yet, then here's the interesting case. That's the meat of what I'm programming. I'm going through all the possible actions. There are 4 of them. Delta is an array of 4 different actions. I apply the action to x and y with this addition over here by applying the corresponding delta vector to construct x2 and y2. If x2 falls into the grid and y2 falls into the grid and [x2, y2] is not yet checked, which is tested by this field called "closed," and the grid cell is navigable--there is no obstacle here. If all these things are correct, then I found an expansion that I now add to the open list. I increment the cost from g to g2 by adding 1. In this case, cost is 1. Then I append the new [g2, x2, y2] to my open list, and I check the coordinate [x2, y2] so I never expand it again. That is the recursion. Put differently, when I drew down this element over here, for example, I looked at possible ways the robot could move. In my software, this means the robot has to stay inside the grid, and the grid cell has to be unoccupied, which is this test over here. I also check whether there was already a check mark by the cell, which is this test over here. It is always true. I added the new element to my open list with the new g-value incremented and the new coordinates. That is exactly happening over here. I increment the g-value, and I add it with the new coordinates. This is the key of a search algorithm. The only remaining thing now is that I call the search routine that prints me out this thing over here. In the next programming quiz, I would like you to print out a table called expand, which does not exist right now. What expand is, is a table of the same size as grid that maintains at what step each node was expanded. So the very first node over here was expanded times 0. The second node to expand was this one over here: 1, 2, 3, 4, 5, 6, 7. In this table, every node that has never been expanded including all the obstacle nodes should have the value of -1. Like these guys over here - these are obstacles. And when a node is expanded, it should get a unique number that is incremented from expansion to expansion and counts from 0, in this case, all the way to 22 for reaching the goal stated. To give you a second example of how the quotes should work, let me block off the goal by adding 1 over here so there's an entire items that block the left side from the right side. Now the switch fails, and in the expansion list you find that all nodes on the right side have never been expanded. You get 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. A little warning; this is not unique. Depending on how you break ties you might expand in a different order, so I don't expect your table to always look exactly the same way as this one over here. So for example, you might have 0, 2 over here, and 1 over here but what should be the case is when there is a full blockage the right side should just never expand. And here is my solution. Analogous to the closed table, I make an expand table of exactly the same size but initialized it with -1. I introduced a counter in the procedure that counts the expansion. And then finally, there's a simple statement over here which implements all I need. When I expand a note - the else statement - I set the expand index of the expanding note to count, and I add 1 to the counter. This constructs the table the way I want it. When later on at the very end, I print out this table using this command over here, I get the table down here. So your implementation should show something just like this. Now I have a really challenging piece of software for you. I would like you to augment this to print something entirely different, which is the final solution. This is nothing to do with expand, and you have to implement a new delta structure for this similar to expand. Here is the output I would like to see. There is an arrow to the right, which is the optimal action to take in this cell over here. Again, this is ambiguous. There might be a different optimal action that is equally good, but my software picked the one to the right. Here I want to go down. This little v over here is an arrow down. An arrow to the right again, an arrow to the right, an arrow to the right, an arrow to the right, down, down, down. In the end we find a star, which indicates the location of the goal. Let me modify the maze. I'm closing up the wall over here, opening the wall down here, run it. Here is my policy. You can see in the grid this is the only way to make it to the grid over here. You should write for me a piece of software that outputs this specific thing over here. Part of this is the delta name that I kind of brushed over before. These four symbols over here are the ones being used to indicate arrows to the top, left, down, and right. They correspond to the four actions over here: go up, left, down, and right. So use those over here to print out the table over here. It's very, very nontrivial to write this, as you will find out. In the end it's not much code, but you have to carefully think about how to cache actions and how to assign them to this table over here. So take a while; do it. It's challenging. If you fail, not a big deal. You can completely understand how the lecture works and not sure of the code once you hit the submit button, and you move on. So here is my solution. I make a field called action of the same size as the grid, where I memorize for each cell what action it took to get there. So for example, if in the goal cell over here, it took an action of go down to get there from the previous cell then this cell over here would have the action index for the action down. That's little a little bit tricky, but it turns out to be really easy to program. In my node expansion routine, where I go from x to x2, which we talked about before, I now add just a single command for the successive state x2 and y2. I memorize the action it took to get there. Notice I don't associate it with x and y, the from state. The reason is in the from state I'm trying out many different actions, and I don't yet know which one succeeds. When I hit the 2 state and expand it for the first time then this is going to be the expansion that's part of the optimal path. So I associate the action with the successive state not with the originating state over here. Very subtle, very important. If you got this right you know exactly what I'm talking about. Now I have a field that memorizes for all these states over here the action that it took to get in there, but I don't have this wonderful representation as I have over here. This will be compiled into a field called policy or plan, which I initialize with blanks, but it is the same size grid as the field over here, which I eventually print out down here. In that field I set the location of the goal explicitly to be the star, resetting over here. Then I go from the goal backwards. I iterate from the goal location, x and y, now in backwards order all the way to the start. Do this as long as x and y haven't become my initial location yet. I apply the inverse action. So I find the originating state by taking my current state and subtracting the action exactly the same way I added it before using my action field as finding out what action was actually being used. In doing so the first time I do this, x and y was the goal state, and x2 y2 become the state before. I happen to know in the goal state that the action was a down action. If I apply the negative of it I go up and find myself over here. I then mark the policy field for the originating state to be the special symbol associated with this specific action over here. Then I recourse. I set x and y to the state x2 y2, and I then go a step further. In doing so I will reverse the path step by step, print the associated action, and get exactly this state over here. Very tricky, but look this is an advanced artificial intelligence class, you might as well program something really tricky. It took me a while to program it myself, but I finally got it right too. Now I want to come with you to the absolute meat of this class, which is called A-star. A-star was invented by Nels Nelson at Stanford many years ago, and is a variant of the search algorithm that's more efficient than expanding every node. If you've gotten so far, and you understand the mechanism for searching by gradually expanding nodes in the open list, A-star is almost the same thing but not quite. To illustrate A-star I'm going to use the same grid as before but with a different obstacle configuration. This is oine way A-star performs really well. Obviously we are forced to go down to here, but in here we still have to search for the optimal path for the goal. Here is the same in problem code; you can see all the ones over here. Start set is over here, goal set is over here. If I run this code and give you my expand list, the ones you programmed before, you'll find that the expansion goes down from here, but then it expands into the open space. Diagonally it expands into the open space and until it finally hits the goal node 15. This took 16 expansions to get to this point. Let me now switch on A-star and run the code again. What we now find it only takes 10 expansions to get to this point, zero to nine over here. So it expands down to four, but then it expands straight toward the goal never touching this area over here, somehow magically knowing that up here the path to the goal will be longer than going straight. Now I didn't cheat. I didn't tell it that there's a straight path over here. So let me put an obstacle right here next to the goal and run A-star again. What you'll find it does expand up to seven over here but then moves to the second line over here, expands up here, and then hits the goal again. So it kind of does the minimum amount of work necessary to make maximum progress to the goal. That's A-star, and now we look into A-star in more detail. A-star uses a so called heuristic function, which is a function that has to be set up. If its all zeros then A-star resorts back to the search algorithm already implemented. If we call the heuristic function h, then for each cell it results into a value. So let me give you some values. Here is one: Its number of steps it takes to the goal if there was no obstacle. Clearly the number I'm putting in right now , 1, 2, 3, 4, 5, and so on, are not reflective of the actual distance to the goal because they don't consider the obstacles. In a world without obstacles the heuristic function that I'm giving you would actually measure the distance to the goal. So the heuristic function has to be an optimistic guess how far we are from the goal. So put differently, for any cell x y the heuristic function has to be an optimistic guess, which means a smaller equal to the actual goal distance from the coordinate x and y. Now that sounds a little bit ad hoc, but very often you can give good heuristic functions really easily like in this case over here. If we just know that the agent can move left, right, up, or down, it's really easy to say what is the number of steps it would take the agent with no obstacles to get to the goal location, and that's this table over here. That is easily generated automatically. Now in reality this is an underestimate. If obstacles, for example, look like this then from here it takes you more than 9 steps to get to the goal. It takes you 13 steps to over the hump over here. Therein lies the beauty of the heuristic function. It doesn't have to be accurate. If it was accurate you probably already solved the planning problem. There has to be a function that helps you understand where to search next in the case of ties, and it has to be just so that it underestimates or at best equals the true distance from the goal. Many, many problems have functions like these in our self-driving car. We use a function just like this; in fact the function I was just showing you, we are using in our software for free-form navigation. It boils down much to the number of which cell steps but for the Euclidean distance to a target location. I hope you understand how heuristic function might look like. It has many, many value heuristic function including setting everything to zero, which would not really help me. So let's work with this one heuristic function. Here is the heuristic function in the code. You can see the same heuristic function. The key modification now for our search algorithm is really, really simple. We again have an open list, and we add our state, we write down the g-value, but we also write down the g-value plus the heuristic value. G-value here is zero; heuristic value is 9. So the sum of the two is 9, and I call this the f-value. This is the cumulative g-value plus the heuristic value as looked up in the table over here. If I now expand I remove the element with the lowest f-value and not the lowest g-value. That's all there is to A-star. Let me give you an example. Say we went to the open list all the way down here. That is we expanded all these states over here, and this is the one present here on the open list. Our g-value will be 5. Our heuristic will be 4, and the sum is 9 as before. Let's now expand this node. We get to this one over here, the g-value increases to 6. G plus heuristic is still 9. Now let's expand it more, and there's now two options finally: This state over here and this state over here. The one up here is called 3 2, the one on the right is called 4 3. The g-value over here in both cases is 7, but when we add the h-value we get a difference. Up here we find the h-value to be 4. We kind of moved a little bit away from the goal according to the heuristic. That gives us a total of 11. Whereas for the feed over here we find the h-value to be 2. Here is the first time that A-star makes an actual difference. It has a preference to expand this node over here over the node over here. To see why the f-value, the sum of g and h, over here is 9 but over here is 11. What this reflects is that, according to the heuristic, this guy is actually 2 steps closer to the goal than this guy over here. This guy, according to the heuristic, may be 2 steps away from the goal, and the guy over here is at least 4 steps away. A-star now will expand this node over here because its f-value is 9 versus 11. So let's do this. In expanding this node we find there is two valid neighbors: the guy up here and the guy on the right. The first guy's coordinate are 3 3. The second guy is 4 4. As before we increment the g-value by one. It was eight in both cases. Now we add the heuristic to the g-value, which for the first one over here is 3; Whereas for the one on the right we get one as the heuristic. That's the result of expanding the node over here. Here is our new open list, and again we have a preference. On the open list are these three states, and we prefer the one on the right because its f-value is smaller than the other two f-values. The one over here is 9; the ones over here have an f-value of 11. So once again we expand, and in the expansion will be the goal state, and then we find the goal set and we're done without ever expanding anything in the maze up here. That feels like magic, but the key thing here is by providing additional information, the so called heuristic function, we can guide the search. When we have an impasse we can pick a node that looks closer to the goal state. As a result we will likely make more progress towards the goal. So with the heuristic function I've given you of simply the minimum number of steps it takes to get to the goal in the absence of obstacles, you can now construct an algorithm that implements a star by maintaining not just the G-values but also the F-values, which is G plus the heuristic. Out should come an expand table that looks exactly like this that are signs -1 not just to the obstacles but everything over here that has not to be expanded according to the heuristic. That's your task for the next quiz. It turns out the actual implementation is really minimal compared to what you already implemented. With this modification you've implemented A-Star, which is one of the most powerful search algorithms that they use for the present day to drive self-driving cars through unstructured environments. The very first thing we do is we expand elements in the open list to not just contain g as before but also f. I also included h over here. That isn't necessary, but I did it anyhow. So now we have five tubelets where g is defined as before. H is the heuristic value of the cell x y, and f is the sum of the two. The reason why I put f left is I need this for my sort trick so that I can sort according to f when I sort the list. So notice this has become two elements longer, and by moving f to the left side I've implemented that the element I remove will be the one with the lowest f-value not the lowest g-value. As I go further down and expand the node as happened in these lines over here, I now need to modify the index into the next structure a little bit. X is now element number three, which is technically the fourth element in the list when we start indexing with zero. Y is element number four. G is element number one. F and h, I don't need to pop here because I compute them from scratch in just a minute. So as I go further down where I expand a node from the list and compute of all possible actions what the successive state is and test whether these are legal states to expand. I now, as before, increment g by the cross function but here two new lines of code. First I compute the heuristic function for the new expanded node. That's very straight forward. I call it the h2. Then the next line of code I compute the new sum of the g-value and the h-value. I use those five things: the new f-value, the new g-value, the new h-value, and the x and y of the expanded nodes to append to the open list. So new here is most importantly the f-value but also the h-value. That's all there is to implementing A-star. So all I've done is I've just changed the logic according to which I remove nodes from the stack to pick the one that has the minimum f-value as opposed to the minimum g-value, and I have A-star. So let me run it. This is for the maze we looked at before. Let me move the open spot to the top over here and put a wall back here. It turns in this case A-star is not so efficient and the area over here it has no preference to go either way. It will finally find the go node. That, however, changes when I put a big obstacle horizontally over here, at which point it's really interesting to see A-star cannot decide whether the horizontal path is best or the vertical path. So it alternately pops nodes from either one of those. The moment its over here the same trick applies as before. It doesn't expand anything in the center anymore and goes straight to the goal and reaches the goal over here. That would not happen without A-star. In fact, the way to rework back to the old search is to give it an empty heuristic function. So here is a definition of the heuristic function initializes h always zero everywhere instead of the heuristic function over here, which I won't use for a second and just call it heuristic old, and this is the current heuristic function. If I run it with a heuristic function of all zeros I get back my original search algorithm. You can see this search algorithm explores into the interior a little bit, and the result expands more nodes than the A-star. This might look very insignificant, but if you get to very large environments, it can make a huge difference especially if there is a huge dead end somewhere that can't reach the goal. Then A-star performs much, much more efficiently than the simple search. So here is an actual implementation from the DARPA Urban Challenge. The Stanford trial car trying to find a way through a maze. As you can see the maze is changing as the car moves. This reflects of the fact that the car uses sensors to see obstacles, and obstacles are sometimes included. The car can only see them when they are nearby. What's really remarkable here is that the car is able to plan really complex maneuvers to the goal. At any point in time we can see its best guess toward an open path to the goal. The orange trees are A-star search trees. They aren't exactly grid trees. Our car moves differently from a grid-based robot. It can turn at different angles, and each of these little steps is a different turning angle combined with a different forward motion. Leaving this aside you get these amazing trees that find paths all the way to the goal using A-star. This implimentation is so fast that it can plan these paths in less than 10 msec for any location in this maze. It was faster than any other driving team that I know of at the DARPA Grand Challenge or the DARPA Urban Challenge. The planning is repeated every time the robot cancels the previous plan. You can see additional adjustments at place at times. As you go through this video you can see how A-star planning with a simple Euclidean distance heuristic is able to find a path to the goal. When you implement this yourself the big difference or grid implementation is a different motion model. You have to implement a robot that is able to turn and you have to write on the math of what it is able to turn and go forward. This robot also can be reworked, so going backwards is a distinct different action. Other than that it is essentially the same A-star algorithm you just implemented. So if you want to build a self-driving car you now understand to make a really complex, nice search algorithm to find a path to a goal. So this is a scene where DARPA trapped our car using a barrier that went all across the street. So the only way for the car to navigate this was to take a multi poled u-turn, and it had to plan this all by itself using A-star planning. The car pulls up to the barrier, realizes there's no path to go, and invokes its A-star planner and comes up with a turn-around maneuver, that is not particularly elegant, but it's super effective. The car was able in this competition by itself to turn around using A-star, find the optimal plan to do so, and move on. Otherwise it would have been stuck forever behind this obstacle. In this final video I'll show you a parking situation where the car has to back into a parking space between two other cars, and you can see how the obstacles are visible, how these other cars are visible, and how our vehicle, Jr, navigates an actual parking lot. Again this is using A-star. It finds its way optimally into this parking spot, backs in, and backs out again all by itself. The planning time for each of these A-star runs is less than 10 msec, and the car was able to competently do this. Even during the advance it had no clue where the obstacles were and where the parking spot was. That is A-star for robot path planning, and what you've implemented yourself is the core of it. Again, if you want to turn it into a real robotic motion algorithm you have to change the motion model. You have to see the next class I'm teaching, when I go into continuous models and I'm going to show you how to turn this into a continuous path. I now want to teach you an alternative method for planning. This alternative method has a number of advantages and a number of disadvantages. It's called dynamic programming, and just like A-star, it's going to find you the shortest path. You give it a map of the environment as in A-star, one or more goal positions-- let's assume just one goal position. What it outputs is the best path from any possible starting location. This planning technique is not just limited to a single start location, but to any start location. Why would we worry about this? Let me give you an example. Suppose you are the Google self-driving car in an environment just like this. You're in this little street over here, and you're asked to turn right, but your goal is right over here. As before, there are two different lanes over here--a left turn lane and a straight lane. If you reach the straight lane, the only way to get to the goal is to go around the block over here and proceed in this direction. You've seen this example before. Now, the point I want to make is a different one. That is, your attempt to do a lane shift over here might fail. Why would it fail? Well, it could be there's a big, big truck in this lane over here, and as you go into the right lane when you're waiting for the truck to disappear, there are these people behind you that honk their horns. You really don't want to wait for the truck to disappear. That means the environment is stochastic. The outcomes of actions are non-deterministic. In our planning so far we ignored this, but in reality that's the case. In reality, you might find yourself--wow, I'm over here. How did that happen? Well, it's happened because the world is stochastic, and this truck over here-- this stupid truck---didn't let you in. What that means is you need a plan not just for the most likely position but you might need a plan for other positions as well. What dynamic programming gives you is a plan for every position. If we redraw this environment as a grid with a goal location and certain obstacles, they dynamic programming gives you an optimal action to do at every single grid cell. As you can see, each grid cell now has a label. That label is often called policy, and policy is a function that maps the grid cell into an action with the action in this case as a move left, move down, move right, or move up. Now, we will compute a policy using dynamic programming. That is, given a grid world like this and a goal state like that, we will write software that will output for each of the grid cells what the best thing is to do should the robot find itself there. That requires a different algorithm than A-star. It happens to be a more computation involved algorithm. As I said before, it's called dynamic programming for robot path planning. Let's look at a very simple piece of code that implements this planning algorithm. We have a grid here as before with 0s and 1s. You're familiar with it. The start location on the top left, the goal location on the bottom right. We can set up arbitrary obstacles like a wall over here and a wall over here that forces the robot into kind of an S-curve around the corner. If I run this code with this table, what I get is a table that looks like this. This for each of the states that is a non-wall state. It tells me what's the optimal thing to do. So over here it says go south. Here is says go right. Here is says go up. Here is says go right again and go south. Realize that even states I'm not likely to every reach, like the one over here and the on over here, have an optimal policy and action associated, because there is really no start state. There is just a goal state over here. The specification of the initial state has no bearing on this result. How can we compute this efficiently? Let me make a simple example of a world like this with an obstacle over here. Say our goal state is the one in the corner over here. Rather than telling you how to compute the optimal policy, which assigns an action to each of these cells, let me instead teach you about "value." A "value function" associates to each grid cell the length of the shortest path to the goal. For the goal, obviously, it is 0. For each adjacent cell to the goal, it's obviously 1. For the guys over here, 2, 3, 4, 5, 6, and 7. This is recursively calculated by taking the optimal neighbor x-prime, y-prime, considering its value, and by adding the costs it takes to get there, which in our example will be plus 1. By applying this update equation recursively, we can attain this value function over here. Once we have this value function, we find that the optimal control action is obtained by minimizing the value, which is a hill-climbing type of action. Let me give you a quiz. In this world here with the goal location over here, I'd like to understand what is the value of the cell in the bottom right. The answer is 15, because it takes 15 steps on the shortest path to go from here to here-- 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, and 15. What is the value of this cell over here? The answer is 3--1, 2, 3. In fact, when you draw in all the values, you can see how the value propagates in the structure and so on. Let's now implement something that calculates the value function. Here's our familiar grid again. We have a vertical obstacle over here, a T-shaped obstacle over here. Our goal location is in the bottom right corner. When I run it, I get this table over here, which is a little bit hard to read, because I set the value for each obstacle in the grid to be 99. So all the 99s over here correspond to actually obstacles in the grid. From there on you see that the value of the goal location is 0-- 1, 2, 3, 4, 5 for these two states over here, 6, and so on. You should implement a function that takes this table as an input and computes this table over here, which is unambiguous. You should be able to output this exact table over here. In fact, if I change the configuration--for example, open up a path over here-- then your function should compute a very different value function where now we can see the value propagating straight to this line over here, which wasn't the case before. Before you implement this, as before I'm giving you the delta table with different actions--up, left, down, and right. I also give you something we'll be using in a later quiz and the cost for the step is supposed to be 1. Here's my implementation, which should be relatively straight forward. We have a value function that is the same size as my world, and I initialize with 99 everywhere. This has to be evaluated as large enough it doesn't conflict with any actual value. I now update the value function a number times--I don't know how often-- but as long as change something, I update it. Therefore, I introduced the variable "change," which I set to True in the beginning. While change is the case, I update, but I neatly set change to False. The only way to come back to True is that I actually changed something. Now I go through all the grid cells in a fixed order. It happens to be not very efficient, but certainly gets the job done. I first check if the grid cell I'm considering is the goal. Here is a typical case where I check for change. If the value is presently correctly set to 0, I don't do anything. If it's larger than 0, such as 99, then I set it down to 0, and I've just changed something. Therefore, I set the change flag back to True. If it's not a goal cell, then here is my full update function. I go through all the actions. I project a potential next state upon executing an action by adding the corresponding delta to the x and y. That gives me x2 and y2. I test whether x2 and y2 are legitimate states. For that they have to be inside the grid. I check whether the numbers are larger than 0 and smaller than the dimension of the grid. And it has to be an action that action navigable grid cell. Therefore, I check that the coordinates in the grid has a 0. If that's the case, I can propagate back the value. My new value is the value of this future grid cell plus the cost per step, which happens to be 1. Now, if this value is better than the value I have already, which means it is smaller, then I assign this new value to my original grid cell x and y, plus of course the cost step. Then I know I've changed something. Therefore I set change to "True," and the procedure repeats. The only thing missing at the very end when I'm done, I print out the value function using these commands over here. I should warn you this is not very efficient. The reason why it is not efficient is that value slowly propagates from the end towards the beginning. But leaving this concern aside, it actually computes the correct value function. There are ways to make it more efficient. It's also interesting to see what happens if I cut off any path to the goal. The the resulting value function will retain 99s for most of the state variables-- exactly those where there is no valid path to the goal. In this next quiz I'd like you to extend your software to print out the optimal policy. That's happened over here. If we look at the grid, there's an obstacle over here. There's a T-shaped obstacle over here. The goal is in the bottom right corner. Obviously to get the goal, over here you want to go down as indicated by these v's over here--down, down, down, down, down. Up here you'll want to go either right or down. Down here, in this little dead end, you want to go left, left, then up, and then right again into this passage over here, up again over here, right, and down, down, down. The optimal policy is ambiguous. Sometimes there are multiple optimal actions. For example, up here you go right or down. For other places like the ones over here, it's not ambiguous. There's only one optimal thing to do, which is going down. I want you to write code that outputs this policy, which is in many ways very similar to the path output by A-star. In dynamic programming, this happens to be really easy to program-- even easier than in A-star. Here is how I did it. I defined a field called "policy" of the same size as my grid, initialized it with lots of spaces. Now in my dynamic programming procedure check for whether we have reached the goal state, and we have. Then I set the corresponding element to star, using a single new command that just sets policy [x, y] to star. Finally, in my big update loop where I assign an improved value to a grid cell [x, y] based on its successor, I assign to the policy the character that corresponds to the action that led to that update over here. Put differently, as we look for a better value we look into all possible directions by looping over all actions. If one of those succeeds, I just memorize in my policy function what that action was with a command over here. If I finally know output this, then I get this field over here. Let's now have some fun and apply this to an actual car problem. The one I'll using is a bit simplified as always, but it does relate to real world path planning as is done, for example, by Google Maps. Suppose we have a car down here. This car now has its state an x, a y, and an orientation, theta. By orientation for simplicity is chosen from 4 possible directions--up, down, left, and right. As I quiz you in the beginning, I'd like to get to the location over here, facing left. Realize that now the state space is 3-dimensional, just like in our localization example. I now would like to implement a dynamic programming planner that gives me the optimal path for going from here to here and that let's me play with cost functions. There are three principle actions. One is move in which the car just goes 1 grid cell forward in its present orientation. It doesn't turn at all. That could be applied anywhere in the maze in any direction. One is turn left and then move. This car in this position in the cell over here could chose the turn left and move, which makes it move over here. The last one is turn right and move, in which case it would, from this cell over here, turn over here and head in this direction. Here's our world again. You can see there is a street over here that's navigable, one over here that's navigable. You see the loop on the right side. Remember that now this state space is 3-dimensional, not 2- dimensional. Our goal is to move to cell [2, 0], which is the one over here. Our initial state is up here, and the initial state has not just a position of [4, 3] but also an orientation of 0. It's a 3-dimensional state. Here are my orientations--0, 1, 2, and 3. The first one makes the robot go up, the second go left, third one go down, and the fourth one go right. Here are the names associated with it---up, left, down, and right. This thing here is interesting. As actions, we have 3 actions. We can add to the index orientation -1, 0, or 1. If we add -1 we jump 1 up in the cyclic array over here, which is the same as doing a right turn. For example, if you go from go left to go up, that the same as turning right. If we add +1, that's the same as turning left. If we leave the orientation unchanged, then we go straight, which is indicated by this hash symbol over here. These actions come with different costs. Right now the left turn costs me 2, going straight costs me 1, and going right costs me 1 as well, which, as we all know, makes the left turn the preferred solution over here. Indeed, as I run it, you can see how the car turns left over here to the goal location. If I were to increase the cost for the left action to 20, then my solution changes. You can see the car dashes straight ahead over here, turns right over here, right over here, right over here, and then goes straight to the goal location. That software I want you to implement. There is one more hint. The value function itself is 3-dimensional, and here is the code that I've been using. Not necessarily the most efficient, but it has inside 4 identical arrays of the size of the grid concatenated into a megagrid and initialized all by a very large value--999 in this case. You need functions just like these, and it turns out this makes it more difficult to write the code. This is our last quiz in this lecture. Our last programming assignment, and you might spend some time. It took me a while to program it myself to get an output just like this over here. Here is my solution, I have the value function initialized. It has lots of 999s. The policy is a similar function in 3D. Then I have a function called policy2d, which is the one I'm later going to print. That's the same in 2D. Scrolling down, my update function is exactly the same as before for dynamic programming While change exists go through [x, y]'s and all orientations of which there are 4, so it's now a deeper loop. If you found the goal location, then update the value, and if there's an actual update, set "change" to True and also mark it as the goal location. Otherwise, if our grid cell is navigable at all, let's go through the 3 different actions and here's a tricky part how to make the action work but it works beautifully. We go through the 3 different actions. When we tag the ith action, we add the corresponding orientation change to our orientation modulo 4. It's a cyclic buffer, so this might subtract 1. Keeping it the same will add 1 to orientation. Then we apply the corresponding new motion model to x and y to obtain x2 and y2. Then over here is our model of a car that steers first and then moves. Scrolling down further, if we arrived at a valid grid cell in that it's still inside the grid and it's not an obstacle, then like before we add to the value the value of this new grid cell plus the cost of the corresponding action. This is non-uniform, depending on what action we pick now. This improves over the existing value. We set this value to be the new value, and we mark change as True. We also memorize the action name as before. This is all effectively the same code as we had before when we did dynamic programming in a 2-dimensional world. It gets us the value function, and it gets us the policy action. However, I printed out a 2-dimensional table, not a 3-dimensional table. To get to the 2-dimensional table, I now need to be sensitive of my initial state. Otherwise, it actually turns out to be undefined. Let me set the initial state to be x, y, and orientation. All I do now is run the policy. With the very first state, I copy over the policy form the 3-dimensional table into the 2-dimensional one, which will be this hash mark over here. While I haven't reached the goal state quite yet as indicated by checking for the star in my policy table. Now, my policy table has a hash mark R and L, but otherwise is the same as before. If it's a hash mark, we just keep our orientation the way it is. If it's R, I turn to the right. L is turn to the left. I apply my forward motion, and I then update my new x and y coordinates to be the corresponding after the motion, and I update my orientation to be o2. Finally, I copy the 3-dimensional symbol for my policy straight into the 2-dimensional array. This is the array that I finally print. The key insight here is to go from the 3-dimensional full policy to a 2-dimensional array I had to run the policy. That's something you would have done to get back this table over here. That's somewhat nontrivial. I didn't tell you this, but I hope you figured it out. But everything else is the same dynamic programming loop that you've seen before. A visualization of our Standford racing car, Junior, in action, applying that exact same algorithm for actual driving. You can see here that a right turn is being executed, followed by a lane shift. These are discrete actions, and the car actually performs those to reach a goal location at the orange circle. But if we make a lane shift prohibitively expensive, just as we made a left turn expensive before, then the car chooses a different path. It goes straight, then takes a left turn, which right now isn't that expensive. It takes a left turn and a left turn again to find itself in the lane where the goal location is located. That is a result of modifying cost functions in our dynamic programming algorithm in the same 3D regime we just studied and that you just programmed. Your program could effectively drive this car, and by changing these cost function, as you can see over here, the car would be able to attain its goal in the optimal way, relative to the cost that you have given him. Congratulations. You made it through my first motion class. Today we assumed that the world is discrete, and we looked at 2 planning algorithms--A-star, which uses a heuristic to find a path, and dynamic programming, which finds an entire policy that has a plan for every location. We implemented both of them. In fact, in the policy case, even in 3D, which is quite an achievement. I want to congratulate you to get here. You really now understand two of the main paradigms by which our robots make motion decisions, and they are are also some very major paradigms for artificial intelligence in general. In the next class we talk about how to turn this into actual robot motion. We'll talk about continuous state spaces, and we'll talk about what's called "control," in which we make a robot move. I'll see you next week. Welcome to Unit 5. Today we talk about actual robot motion. In the previous unit we talked about how to find paths . Now we'll talk about how to turn these paths into actual motion commands. In particular, we'll talk about generating smooth paths. Then we'll talk about control. In particular, a method called PID control. As usual, you get a chance to program all these wonderful things. You might be remember that our planning took place in a discrete world, and the type of plans we found look very much like this, perhaps in response to obstacles over here. Now, a path like this has lots of disadvantages. You don't want to a robot to go straight, take a 90-degree turn and go straight again. For one, a car can't even do this, but for another this will force the robot to move really slowly around the corner over here. A much better path would look like this. This is a smoother path. In the extreme case, you might even generate a path just like this. The question becomes can we modify the blue path to look more like the red or the green path? Suppose your car like a robot right over here wished to get to right over here. Which path would you prefer? The blue path, the red one, or the green one? Just check one of the three. This is a question where I check your intuition. This is not a mathematically precise question. The answer is subtle. I would actually prefer the red path. The reason is as the robot is facing the right side over here, it can't take a 45-degree turn onto the green path. It should slowly move around the corner, go down here, and slowly move to the right. The greet path might be shorter, but it requires two 45-degree instantaeous turns, which are unfeasible for the robot. To some extent, we'd like to find a smooth path that is executable by the robot. In the path planning class, we specified paths as a sequence of points in a 2D grid just like this over here. For the smoothing purposes, we will call each point xi. This is a sequence that goes from x0 to xn-1, and each of the x's is really a 2-dimensional coordinate, but that should be immaterial to the smoothing. You could do this in 1D, 2D, or 3D. Here's the smoothing algorithm. Initially we create variable yi that are the same as all the xi's. Remember that these are the non-smooth locations that the planner has found. Then we optimize two criteria. The first is minimizing this expression over here, and the second looks as follows. In the first one we minimize the error of the ith original point with the ith smooth point, and the second we minimize the distance between consecutive smooth points, both to the square. Here's a little quiz. If we only apply the first criteria--forget about the second one--will we get the original path, and smooth path, or no path. Please check exactly one of those boxes. The answer is if we just optimize this one over here, we get the original path, and the reason is obvious. It's already zero for the original setting. Further minimization doesn't make it any smaller. It's a quadratic error. The minimization has no effect. We just get the original path. Now let me ask the exact same question just for the second criterion. Forget about the first one. What happens if you only optimize this criterion over here. Do we get the original path, smooth path, or no path. The answer here is you get no path. This criterion alone asks that all the y's are as similar as possible. If it's minimized, then all the y's are the same, which means you only get a single point, and we get no path at all. Obviously these two criteria are in conflict to each other. In reality, what we do is minimize both, and you do that with some sort of a weight, alpha, which you can play with in the code. The stronger alpha, the smoother the path. The smaller alpha, the more we retain the original points. Here's my next question. Suppose we optimize both at the same time with an appropriate alpha? What do we get? An original path, a smooth path, or no path? The answer is we get a smooth path. To see why that's the case, let me just simulate the optimization. Suppose we're given a solution to the planning problem like this, and you run the optimization algorithm. Consider a place like this. By shifting this point into that direction, and perhaps shifting the other points in other directions, we can decrease the second error term, both for this pair of points, and this pair of points. However, we do this at the expense of the first error term, since we're now shifting the point away from the original x. Depending on the weight of these error terms, we might arrive with like the following. This new path suffers an error of the first type that we moved the points away from the original points, but it drastically reduces the inter-point distance as in this error term over here. If you insist that the original points are not changed, then just exclude those from the optimization. In fact, in our exercise, we will not consider those points Y0 will always been the same as x0, and yn-1 will always be the same as xn-1, assuming we have n points starting in x0. The optimization is only applied to the intermediary points. How can we optimize these two terms over here? The idea is to use gradient descent. That is, every time step we take a small step in the direction of minimizing the error over here. Here's the expression for the first objective. When we iterate, we assigned to yi recursively the old yi, but we subtract a term that's proportional to the deviation of yi to xi, weighted by a weight function alpha. That's not exactly the same alpha as before. We set this alpha over here to 0.5. For the second term, we could implement this as follows where we retain the old y variable, but we move a little bit in the direction of yi+1 and away from yi. But an even better implementation looks as follows. This is combining the step on the left and the step on the right. Realizing that each yi occurs twice in this optimization term, one here and one here, we can now go and implement this in a single update rule where we wish yi to be as close to yi-1 and simultaneously be as close as yi+1 by optimizing this combined term. Think about this as little bit, but that's what I want you to implement. We're going to set beta to 0.1. Now let's go and implement this. As a last hint, I don't want you to apply this optimization for the first or the last node in the sequence. I want those to be unchanged, as we'll see in a second. Here's the code I'll be giving you. There's a path in a 5 x 5 grid, starting at [0, 0] to [4, 4]. If you look very carefully, it goes to the right at first, then straight down, then to the right again. This is exactly the path we discussed so far and looks like this graphically. I now want you to implement the function "smooth," which takes as an input the path, our two weighting factors, and a small tolerance variable, which I'll explain to you in a second, and it creates the new path, which are the y's in our equations so far from the old path. This is a deep copy over here. Then below the line, I want you to implement the smoother. What the smoother does is iteratively applies the two equations I just gave you to all nodes except for the first and the last, and it does so until the total change observed in the update step becomes smaller than tolerance, at which point we consider the smoother to have converged. Here is my command. I compute a new path as a result of the function smooth. In your test, you should uncomment the "newpath" smooth routine and the print routine that outputs my result, "Thank to EnTeer," a student who posted a much better way to output matrices on the discussion forum. I'm going to use his or her code. Thank you so much. Here's the result. After hitting "Run," I have the original path over here--zeros all the way to [4, 4]. The two initial and the end position, should be the same as before, so please don't modify them, but in between we get these interpolation positions over here. If you look at this, my original path didn't vary the x's at all for the first three steps whereas this one goes from [0, 0] to 0.17, so it got closer. It went down a little bit. Also, it went less to the right side than my original path. We went all the way to 2 over here to 1.8 over here. What this means is that our new points lie a little bit like this. As you go through the list over here, you'll find that our new points really smooth out the path to something more like that. Hi, I'm Andy, the assistant instructor for this class, and I just wanted to provide some clarification about how you're going to use gradient descent to minimize these functions. First, I'd like to point out that there was a slight error in the path used in Sebastian's code. That [4,4] that he was using should actually be replaced with a [4,2]. Also, those minus signs that you saw here should actually be replaced with plus signs if we want this gradient descent implementation to converge. Finally, even though these yi's and xi's are two-dimensional vectors, when you implement your code, it may be easier to just iterate over each individual entry. So, for example, these xi's would be these values here. One way to solve this is to define a variable called change and set it equal to tolerance, where tolerance is a parameter in our function, and you can see its default value is very, very small. And, as long as this change is greater than or equal to tolerance, we're going to initialize change to zero, and iterate over not quite every entry in the path. You can see here that we want to exclude the first and last entries. And so, for each entry, we're going to set an aux variable equal to the new path value at that entry, we're going to increment newpath, where these are the equations from gradient descent, we're going to increment change by however much this newpath has changed in this step, and we're going to keep doing this and doing this until change becomes less than tolerance, at which point we can return our new path. And we see if we run this, we go from this right-angle, jerky path to this nice, smooth path taking us from [0,0] to [4,4]. Here is a tricky quiz question. Say we alpha, our data weight to zero, and better, our smoothing weight, to 0.1, and we run this to completion. What do we get? Your original path? The straight line connecting the initial the final location? Or will everything collapse to a single point? You can try this out in the RDE before answering this quiz. The answer is a straight line, and the reason is really subtle. It's because we're not modifying the first and last point. Otherwise, it would be a single point. I just hit the run button, and if you look at these coordinates, they go in 0.5 increments from 0, 1, 2, 3, 4, and the same is true with this coordinate over here. This is obviously the coordinates of a straight line as a result of our smoother applied with a smoothing weight of 0.1 and no data weight. If we change this to no smoothing weight, we obviously get the original path. If we look at the right side as well as the left side, it's not identical. When you implement your algorithm, please test this on those settings, and we will give you a different path in our testing to see if your algorithm is implemented correctly. Congratulations! You just learned how to produce a smooth path. There are a few caveats. If you apply this algorithm in a robot world like this where an A-star planner might give you something like this, the smooth path might then lead straight through the obstacle, which you don't want. There are ways to accommodate this, which I'll just hint at. One is to use dynamic programming with a stochastic action function. You've learned this in the previous homework assignment. That way we stay away from the obstacle. The second one is to introduce a term that propels you away from obstacles. I won't go into any depth here, but in your optimization you could have a term that pushes you away from obstacles by maximizing the distance between the nearest obstacle and data point. When you toss that in, you get a path that might look more like this that is still smooth but maximizes your clearance from obstacles. We will revisit this in the homework assignment. Let's now talk about the second part of this lesson called PID control. PID control is a vast field in control, and many, many classes can be taught about this one subject matter. What I'll do is I'll give you the very basics, and I'll let you implement the very basics. I promise it'll be fun. You'll be able to drive a car around, and the Google car to the present day uses a version of this exact same controller that is, of course, much more tuned the specifics of our car. But you get to see some of the essence of what it means to control a car. Here is the problem. Consider the following car with a steerable front axle and 2 non-steerable wheels in the back. Say we wished this car to drive along this line, which is the output of our smoother we just discussed. Let's assume the car has a fixed forward velocity, but you have the ability to set the steering angle of the car. How would you do this? You would keep the steering constant? You would use random steering commands? Or you could set the steering angle in proportion to what's known as the "crosstrack error," which is the lateral distance between the vehicle and the so-called reference trajectory. The third possibility is steer in proportion to the this crosstrack error or CTE. Choose one of those that you think is best suited to control the car. And yes, you'll steer in proportion to the crosstrack error, which means the larger the error, the more you're willing to turn towards the target trajectory. You can see that this works. As you get closer to trajectory, your steering will be slower and slower. You will reach the trajectory. Clearly the other two answers are really bad. A constant steering will put you in a circle and not in a straight line. Random steering, if you ever implement this, is a really bad idea. Believe me. We accidentally did this once. It's a really bad idea. What you just learned is called a "P-controller" where P stands for proportional. Here is a really trick question by which I want to test your intuition-- one that doesn't have a unique answer, but it has a best answer. Suppose you do what I just said. You steer in proportion to the crosstrack error. That is, your steering angle is proportional by some factor of tau to the crosstrack error. What will happen with the car? It never quite reaches the reference trajectory? It overshoots? Or either can happen? My answer is it actually overshoots. The problem is no matter how small this constant is over here, It will eventually turns its wheels quite a bit towards it's trajectory. Then it'll move towards a trajectory more and more, and when it hits it, it's wheels will be straight, but the robot itself will still be oriented a little bit downwards, so it's forced to overshoot. What this means is that applied to a car, a P-controller will act like this. It'll slightly overshoot, and that could be okay. The overshooting is very small. But it'll never really converge. It'll be what's called "marginally stable" or often just "stable" in the literature. I want you to implement such a controller. Here is the code I've prepared for you. There is a class robot with which you're familiar. It has an "init." You can set the position using the function "set" as before. There are steering_noise and distance_noise. You're familiar with this. There is also something called "drift," which you won't use right now, but later on it'll become handy. There is your move command, all the way we've implemented before. I've improved a little bit the print out of the coordinates using floats. I want you to implement the run command, which takes as input the control parameter that governs the proportional response of the steering angle to the crosstrack error. The robot has an initial position of 0, 1, and 0, a speed of 1, and I wanted to simulate it for 100 steps. Here is what I envision to happen. Your robot is initially off the the x axis by 1. I want it to drive along the x axis. The y value is the same as the cross track error. By turning, inversely proportional to the y value, using a parameter tau that sets the response strength of the proportional controller. I want the robot to turn towards the x axis, drive in that direction, overshoot, turn around, and drive back. To do this, simulate the world for a 800 steps, and use a proportionality term that sets my steering angle alpha in proportion to the crosstrack error y. Enter your code here, and when you're done with it, and you run it with the coefficient 0.1, here's the output that I want you to produce. It's 100 lines. You can see the robot position starting 1 off in y. It then reduces y over time to go into negative territory. On the right side you see this corresponding steering orientation, and you can see as you move on the y coming back into positive territory, and you can see how the robot overshoots slowly around the reference trajectory of the x axis. Please go implement this. Here is my implementation. It's really simple. I execute the following loop 100 times. I set the crosstrack error to my robot y. The steering angle becomes minus my control parameter times the crosstrack error. I call the move command with a steering angle and the given speed. I print as an output my robot along with the steering direction. This simple routine just does it. Let's now modify the parameter to 0.3, and here is my quiz. If you modify the control parameter from 0.1 to 0.3, what happens? It oscillates faster? It oscillates slower? Or nothing? Please check exactly one of those. As we'll see, they'll oscillate faster. You have to spend some time with this, but for the larger value of 0.3, we reach a negative value in y already here, which means we just crossed the line. This is just 13 steps in whereas if we were back to 0.1, then our step 13 would still be 0.6 off. So clearly the control oscillation is much slower, and we compensate much less The basic next question is is there a way to void the overshoot? It would be nice if we could do this, because driving in an oscillating car is no fun. In fact, it makes you really seasick, believe me. I've been in this car for months on end when we prepared for the Darpa Grand Challenge. The trick is called "PD-control." In PD-control my steering alpha is no just related to the crosstrack error by virtue by virtue of the gain parameter tau p, but also to the temporal derivative of the crosstrack error. What this means is that when the car has turned enough to reduce the crosstrack error, it won't just go shooting for the x axis, but it will notice that it's already reducing the error. The error is becoming smaller over time. It counter steers. It steers up again. This will allow it to gracefully approach our target trajectory, assuming appropriate settings of our differential gain--tau d versus the proportional gain tau p. How do you compute this derivative over here? Well, at time t this is the same as the crosstrack error at time t minus the crosstrack error at time t minus 1 divided by the time span between t and t minus 1. In our code, we assume delta t equals 1, so so we can omit this. The difference of the current crosscheck error and the previous one is this term over here. We now control not just in proportion to the error itself but also to this difference of the error using a second constant tau d. Let's implement this. Now I give the run command two parameters--param1 and param2. I want you to implement a controller that varies the steering direction proportionally according to parameter 1, and differentially proportionally to parameter 2. Again, run for 100 time steps and see what happens. When I run my new controller with proportionality parameter of 0.2 and the differentiation one is 3.0. Then, I get a sequence of y values that converge much more gently to 0. Miraculously, as time goes on, they really go down to 0 and stay at 0, which we didn't achieve for the proportional controller. Please write that routine so we can test it. Here is my solution. I build a variable called "diffcrosstrackerror, which is in my differential, that is set to the momentary crosstrack error minus the previous one which I the very first time initialize to the present one. Then in the steering, I don't just steer proportionately to the crosstrack error, but also proportionately to the differential crosstrack error times the parameter 2. When I put this in and I run it, I will get exactly the output that I showed you. Let's talk about a problem that often occurs in robotics called a "systematic bias." It goes as follows. When you ordered your car, you believed the front wheels were 100% aligned, but your mechanic made a mistake, and he aligned the wheels a little bit at an angle. Now, for people that isn't a big concern. When we notice this we just steer a little bit stronger. But let's try this out with out our proportional controller. I'm now adding a line that sets the steering drift to be 10 degrees, which in radians is this expression over here, using setsteeringdrift command. I now want you to run my proportional controller with parameter 0.2, and for now we're going to set the differential controller to zero. When you do this, what happens? It works just as before or it causes a new, big crosstrack error? Go try it out. Of course it causes a new error. If I go to my output for 100 steps, I find that the y value is between 0.7 and 0.9. It really a lot of error. Put differently, the robot oscillates a bit like this with a fairly constant new offset error due to this bias. Even though the bias was in steering, it manifests itself as an increased crosstrack error in the y direction. Here's a question for you that will require some thought, and you can try it out before answering this. Can the differential term solve this problem? Yes or no? The correct answer is no. Let us try this out. Let's enter a 3.0 for the differential term, run everything, and the y error is still large. It converges now, 0.87, but it's still really, really large. What's the intuition? If you drive a car and your normal steering mode leads you to a trajectory far away from the goal, then what I submit you do is you notice over a long period of time you can't get closer. So you start steering more and more the more time goes by to the right to compensate this bias. As a result, when you drive you steer the car this way. To do so, you need a sustained situation of large error. That's measured by the integral or the sum of the crosstrack errors over time. Let's make a new controller where steering is proportional to the crosstrack errors before. It's equally proportional to the differential of the crosstrack error, but now it's also proportional to what's called the integral or the sum of all the crosstrack errors you ever observed. This term is interesting. If we have a constant crosstrack error of, say, 0.8 and the sum will increase by 0.8 for each time unit, it'll become larger and larger and larger, and eventually it'll correct the robot's motion. This is called the PID controller. This is the P or the proportional term, the D or the differential term, and the i for integral. P-I-D. Let's implement this, and the integrated crosstrack error is the sum of all crosstrack errors you ever observed. Let's implement this in our code. I give you an integral factor of 0.004. Let's not worry why I picked those. They're actually wisely chosen, as you will see in a minute. But let's run our code and now modify our code to also allow for this parameter over here. Here is my solution. I implement a variable int crosstrack error outside my main loop then initialize with zero. I then add to the int crosstrack error my local crosstrack_error. Then I have a controller that steers in proportion to the intcrosstrackerror. When I hit run, I find that my y variable slowly converges all the way down to 0 or 0.05. I get even faster conversions when I set this parameter to 0.01, looking down you can see a little overshoot, but my controller converges to 0.0 fairly quickly and then tends to stay close to 0.0. This PID controller is kind of the best solution for the control problem at hand. You just implemented one. Now, here's the big question for you. How can we find good control gains where control gains are these parameters tau p, d, and i. Now, this is my favorite part of this class. Every one of my students has made it through it, and every one of my students is puzzled why I insist on this, but when they implement it they get to love what I'm just about to show you. The answer is to called "twiddle." Twiddle is my favorite algorithm that I have used in my entire life. Some people call it "coordinate ascent" to make it sound a little more sophisticated, but I just called it twiddle, because it really gets to the heart of what's happening. In Twiddle, we're trying to optimize for a set of parameters. To do so, our function run() must return a goodness. This goodness value might be the average crosstrack error. Say I wanted to implement Twiddle so as to minimize the average crosstrack error. If that's the case, then the output of run depends on the three parameters. Here's how Twiddle works. Build a parameter vector of our 3-target parameters, and initialize it with zero. Also, build a vector of potential changes that you want to probe and initialize them for now with 1. Then you can run our command run( ) with our parameters, and whatever it outputs is our best error so far. Now we wish to modify p as to make the error smaller. That's where Twiddle comes in. It's a really smart algorithm, I believe. We sequentially go through these parameters. Obviously, you shouldn't write 3. You should write len of p. First we tried to increase p by our probing value, compute a new error for this new modified p. If this new error is better than our best error, then we do two things. First, we set best_err to err, and we even modify our dp to a slightly larger value by multiplying it with 1.1. Otherwise, we try the other way. We subtract dp from p--and we have to do it twice now because we added it before. Then we do the same thing again as over here. I'm not going to write it out. We check whether the error is better than our best error, we retain it, and we multiply dp by 1.1. But if both of those fail--this one over here and this one over here-- we set p[ i ] back to the original value, and we decrease our probing thing over here, say, by multiplying it with 0.9. That's the core of Twiddle, and what it really does is for each coordinate in isolation it moves our parameter down a little bit by this value over here. If it then finds a better solution, it retains it, and it even increments the probing interval. If it fails to find a better solution, it goes back to the original and decreases our probing interval. We do this entire thing so long as the sum of the dp's is larger than the threshold. Somewhere in here we say while some of dp is larger than 0.00001. It's hard to read, but I hope you can follow it. This is Twiddle. Let me put this into pictures. We have three parameters--0, 0, 0. Then in the first iteration, we bump one of the parameters up and see if it improves the error. If that's the case, we retain it. Then we go to the second parameter. We bump it up. It might not work. We bump it down and maybe retain that one, and so on. Now, as we keep bumping up, we might find that neither bumping up nor bumping down helps. What we do instead is we retain the original solution but make our probing interval smaller than before by a factor of 0.9. In doing so, we can zoom in more and more into a detailed parameter until it finally converges. It's local hill climber, but it happens to be really, really efficient. So to implement this I’ve modified the RAN procedure to input a parameter vector of three parameters. And for reasons that will be obvious later, I have a print flag that I default to false. I have the same initial parameters as before, speed and arrow cross stick arrow – into the cross stick arrow, I set my fifth parameter and to make it a little bit more obvious how what the effect of parameter selection has on my arrow. I also set the total number equations to N times 2 and when I count the total cross stick arrow, I only counted from step N 1, so I give the algorithm a chance to convert to zero for N steps, they don’t count the cross stick arrow, but like to know how the cross stick arrow evolves quite dramatically from step 101 on to 200. If the print flag is set, I set the output here in degrees and not in variance and I return my average arrow before value. So I wanted to write the routine twiddle and the routine should find the optimal parameters and return them to me. So I want you to implement the twiddle with a tolerance and threshold of 0.001. In one twiddle, it shows the parameters over time and the cross stick arrow. And this cross stick arrow very quickly goes to zero. In fact after a few iterations, 107 in my implementation we get a cross stick arrow of 3.611 to the minus 17th. And here is the typical control one, you see my X, my Y, my orientation, my drift is constant, that’s my constant drift parameters and the beginning of the arrows are 195 on average. But after a short amount of time, you find that my Y arrow goes down to 10 to the minus 6 and stays there, which means our controller is really, really good in tracking our desired location. In our final control error between time step 100 and 200 is 3.611. So I wanted to implement twiddle, we might change something on the vowel set up, but when you run it, it will be such that if you want a full PED controller and find the optimal parameters that the final control error, will be really, really small and that’s how we’ll checking. Here is my implementation of twiddle. This is a routine that you can keep this way for many, many different applications. All it requires is a way to evaluate something that depends on the parameters and gives you a single arrow that you would like to minimize. We have three parameters in total, I set the parameters themselves to zero, but the deltas to one and it’s just the counter, its unimportant. If the sum of D params is still larger than our tolerance which we initially have as 0.001 and they go through all the parameter sequentially I increment that by D params, find out what the arrow is if the arrow is better than our best arrow, which I initialize with the initial arrow and I keep the best arrow and I even increment D params. Otherwise, I try the other direction. One, find out the arrow, if that succeeds, I keep it, I increment D params. And here is my last case, I didn’t succeed, so going to set it back to the old parameter vector and decrease my D params by 0.9. I increase my counter, here is my little print out command for debugging and I will turn the parameter vector. So it will be comingout of the print vectors over here and play with this a little more . If I want twiddle, compute the best parameters and then calculate the error using these parameters and print out the parameters along with the arrow, I get a parameter vector and I get an arrow that’s basically zero. Now let’s switch off the integral term. And I can do this with a little trick. I just set D params number two, which is the final one to 0.0 as if I’ve already learned the integral term. When I run this, I get a zero integral term, but the arrow that’s somewhat larger than the final arrow, the desired. And that’s because the integral term is really required to drive the arrow, down to zero. Let’s also remove the D term and see what happens and the result is a really large arrow, 0.55. That large arrow, sustains even if I remove the over drift by commenting it out. You still get an arrow, of 0.10 if it does have a proportional controller, whereas if I add in the differential parameter again, by removing the D param 1 command the 0.103 goes down to 5.7 to the minus 11, which is practically zero. So you can see the importance of the D term for driving the arrow, down to zero, in the case without drift and for the integral term in cases for vowels with a systematic bias. You play a little bit more with this code for the homework assignment. But this is it for now. Let me summarize. You learned two really important things that we use every day in robot programming and they were somewhat complementary. First we talked about smoothing which took a discrete path and turned it into continuous smooth path. And then we turned about how the robot can follow this smooth path, using a control mechanism called PID. With this, you can actually implement quite a bit of robotics, with no matter what robot you have, apply a planner, apply a smoother, apply a controller and it will do just fine. In fact, this is about the level at which our DARPA Robot Challenge car won the DARPA Robot Challenge . The Google car itself has a little bit more juice in it. It considers the case that the steering wheel itself has inertia and that the steering wheel was driven by something called torque , not by position, but leaving this aside, you really got a gist out of what’s happening in robotic driving. And now we’re able to really control an actual robot. Congratulations, that is really, really cool. [Sebastian:] Welcome to Unit 6 in CS373: Programming a Robotic Car. This is the final unit focused on putting it all together. Let's now review some of the things you've learned. You learned about localization, tracking, planning, and control. And you also learned about probabilities, so-called filters, you learned about A*, dynamic programming (DP), and about PID control. This is a lot of stuff, and you implemented quite a bit. So today, we take all those pieces, and we put them together to build a true, robotic, self-driving car. Let's start with localizing. We learned about how maps are being used to localized a moving robot. Here is a laser map being produced--in this case, Stanford's car, and we used particles to localize this robot by matching momentary measurements with the previously acquired map. In doing so we were able to localize Junior in the Darpa Urban Challenge with centimeter precision. A similar method is being used by the Google Street View car, and it is one of the secrets sources of the street-view car. We heavily rely on a previously built map and a type of localization method that you implemented to localize the robot and make it follow known lanes. Here is a localization quiz. We learned about the 3 filters, Kalman filters, histogram filters in our first class, and particle filters. Check the corresponding box if the attribute applies. If any of those is multimodal in their distributions, please check the box, exponential in the computation complexity relative to the number of dimensions, and useful in the context of robotics. Please check none, any, or all of those boxes. Of course, we learned that the Kalman filter is unimodal, just a single bump, whereas histograms and particles can have multiple bumps. The Kalman filter was more efficient than the histogram and the particles that both require exponentially many units in the number of dimensions in the worst case. And yes, my god, they are all extremely useful--super, super useful in case you didn't get this from the class. We also learned about planning--in particular, breadth-first planning, A-star planning, and dynamic programming planning that all address slightly different use cases. Those are being used to plan the trajectory of the Google Street-View car and the Darpa Grand Challenge cars. In fact, you implemented a good number of those, which is quite amazing. Here is my quiz for planning. You learned about breadth first, A-star, dynamic programming, and for the sake of this comparison I will also include the smoother, which is more like plan refinement. Check any or all of those boxes if the corresponding planner acts in a continuous space, if it finds an optimal solution, if it's a universal plan-- that means the solution, once found, can be applied to arbitrary states-- or if the solution is local, which means given an approximate initial solution, it cannot really do anything more than just locally refine the solution. Smoothing was the only planner that actually really worked in a continuous domain. Everything else was very discrete. Our breadth-first, A-star, and dynamic programming all find the optimal solution. Dynamic programming is the only one that's universal, and smoothing is the only local operation whereas those over here all optimize a global planning problem. Then we also learned about control. You implemented your first controller. Control is what makes things like the following experiment possible. Here's my quiz. You learned about PID control and, specifically, the P term, the I term, and the D term. Which of the terms is most associated with the idea of avoiding overshoot, minimizing error relative to a reference trajectory, and compensating drift. I want you to check exactly one check box for P, one for I, and one for D. The primary function of P is to minimize the error. We steer in proportion to the error. The D term avoids the overshoot, if used wisely. Systematic drift and biases are best addressed by the I term. That's the correct answer, which I'm sure you got right at this point. Now let's put them all together into a single piece of software. Upfront it took me about a whole day to do this. I'm not going to ask you to do it all yourself, because it's going to cost you probably at least an hour if it takes me a day. But I still want to be able to take all the lessons that we did together into a single system. I'm going to help you a little bit--bits and pieces-- but up front here is the environment that I wrote for you, which is very much derived from the environment we studied in the past. We have a class robot that has certain kinds of noise characteristics you can find over here. As I scroll down, you can see the familiar init function, the position-setting function, the set_noise function, and then we have two checking functions-- whether we have a collision with the world called "grid," which I will show you in a minute. and we have a check_goal function to see if we reached a goal according to a certain distance threshold. The move function should be very familiar at this point. It applies noise to the motion command. It the same code that you originally wrote. Then we have a very simple sense function, which measures the robot's (x, y) location, similar to a GPS on a car but with substantial measurement noise. Corresponding to this sense function we have a measurement probability function that you might want to use in your filter, and it evaluates the probability of a measurement relative to the ground truth coordinates of the robot using Gaussians. Armed with all this here is the problem. I'm going to give you a grid. Here is an example grid. Let me draw this for you. This specific one happens to be of dimensions 6 and 5, and there are a number of blocked off cells like this. If we look carefully into the code, you'll find information about the initial starting location on the left upper corner and the goal location, which is the bottom right corner. In putting everything together, you're going to build a robotic car using our bicycle model that'll drive through the free space through the continuous free space on something close to the shortest path all the way into the goal. Here is my solution that I implemented and that you will get to see for the most part towards the end of this class. I am starting up over here. These are my obstacles. They implement as circles through the center of these grid cells. It's not exactly correct but good enough for my implementation. Here are multiple runs using the same code, and you can see they're far from optimal. They are non-optimal because there is control noise, and there is also measurement noise. But they all make it safely through free space into the corner where the goal objective is. If we look at them in detail, like this solution over here. You'll find that the spacing of the circles is somewhat variable. You'll find that there's little corners over here that are either the result of control noise or of measurement noise or of my somewhat deficient implementation. You'll also find the control set points that are the smooth points of my A-star planner as shown here in green. In the version that I implemented for you, the controller does something very, very different. It actually chooses as the control objective to head straight to the goal, using the atan2 function, executes the action at a speed of 0.1, and then reports a collision whenever the robot is moving. Just looking down to the output where we see the robot's coordinates along with the orientation there are very frequent collisions that the robot undergoes in its attempt to reach the goal, which it eventually does, but you can see 2 big regions of collisions until the goal is finally reached. I spent a couple hours porting all the code over into this new format, and I want to spare you all this editing work so in the final code that is a little bit incomplete still, we have a grid. We've got a function called "main." Main then runs a path planner, A-star, smooths it, and then runs the controller, as in "run." Then the controller even implemented for you our particle filter that you're familiar with. There's nothing new here. You're going to get the exact same code from class that you programmed yourself. Then I go through a loop where I compute a crosstrack error, apply my only PD controller--here is no I term here-- and I run my particle filter as before to estimate where the robot is. What I would like you to do is to implement the crosstrack error function, and I want you to use as an input the estimate, not the actual robot position, but the best estimate, which you can get by running filter.get_position. Now here is the difficulty, and I can tell you confidently it took me more than an hour to solve this problem myself just for this class. Our path now is a sequence of linear pieces. When our robot drives along, it has a certain crosstrack error, but as the robot state project beyond the end of a line segment, as is happening here, we have to change the corresponding line segments to be the next one. I addition, to calculating the assigned error relative to an arbitrary line segment, not just the y-axis, we also have to detect when the robot steps beyond the end of a line segment and switch over to the next one. Now, suppose this is our line segment. The path is given by the coordinates of the beginning point, p1, and the end point, p2, both of which are (x, y) coordinates, which you get straight in the path. Suppose our robot's position is something like this where it has its own (x, y) estimate that comes out of the particle filter in your case, and it has it's own orientation, theta. Then both the cross track error as well as how far it has progressed along the line segment--call this "U"--can be calculated using a dot product. Specifically, let's call this vector over here delta x and delta y as defined in x2 minus x1, and y2 minus y1--this vector over here. Let's call this vector over here our Rx, which is x minus x1, and Ry. Then U, the ratio of how far we've progressed along this segment is given by the dot product Rx times delta x plus Ry times delta y divided over the sum of squares delta x times delta x plus delta y times delta y. Why? Well, this normalizes the vector length to 1, and this is the dot product of this vector over here and the green vector, which happens to define the distance. If this is larger than 1, we know we've left the segment and it's time to move onto the next one. Finally, the crosstrack error--the red one over here--is given by a similar but not identical dot product of Ry times delta-- notice we are now multiplying a y with an x-- minus--instead of plus--our x times delta y with the exact same normalizer as down here. You can see the normalizer over here. What I want you to implement are these pieces of math over here. When you run your controller, you will find that I setup for you a variable called "index" that's the index into your path. When U exceeds 1, we should increment this index to make sure it never goes beyond what's legal in path length. The crosstrack error should be computed relative to the current index and is, of course, the assigned error using the exact same dot product I've shown you. The last thing I want to tell you is what the path is. I want you to use the following path. The path is called "S" path. It is given the run function as one of the parameters over here. You can see it up here. S path index is the indexth element of this path and 0 stands for x and 1 stands for y. Please fill in the missing code over here. I should tell you, when you run our controller with the missing code included, you get actually a valid, nice path that mostly doesn't collide. Occasionally it does, because of randomness in the system, but it should be mostly collision free. For this example, it will require about 130 or so robot steps. Just so that you see a typical answer, here is a random run. You read this as follows--true means the robot actually found the goal, zero means zero collisions, and it took 137 steps. Let me run it again, and here is another outcome. The robot didn't collide and reached the goal in 145 steps. I should warn you that sometimes I do get collisions here, and it's because our obstacle surfaces are relatively large. The noise in the system makes it hard to navigate. But most of the time we should be able to get to the goal without difficulties if we implement this piece of code correctly. Just to warn you, it took me quite a while to work this out. Here is my code. I compute the dx and dy the way I told you by using the spath of index i + 1 minus the same at index. My rx and ry, called drx and dry over here, are the robot estimates as obtained by the filter minus the path. Then I apply the exact same two equations that I gave you for the progress U and the crosstrack error cte as shown over here. Of course, if I advance too much I add 1 to the index. You could have done this before computing the crosstrack error, but I chose to do it afterwards. So I add the missing bracket, and when I run it I get sometimes a collision. There are two collisions here but I still reach the goal in 140 steps. Let me run it again, and now I reach the goal without collision. In the final question, I'd like to explore something. I don't have a good answer for this, but I'd like you to play with those parameters over here-- the data weight, the weight smoother, the control parameters p_gain and d_gain. Play with them, try to find a setting that gives me fewer collisions on average than my current parameters and maybe reaches the goal even faster. I should warn you these are about the best variables I could find, but I didn't really apply twiddle to this. I did more of an approximate investigation of what a good parameter might be. When you apply twiddle and try this, you will find that it's hard to apply because your function might never return, so you have to build in the time somehow. It's fun playing with those to see if you can find a better solution than what I gave you. If you do so, don't expect the correct answer from me. I didn't implement it myself. But I want to give you the opportunity to play with those parameters and see what the effects are on this solution. This finishes the lecture part of CS373, my basic introduction to robotic AI. Even though we haven't done the final exam yet, I want to congratulate you for getting this far. The fact you got this far means you are really likely a very amazing student. You put a lot of work into this--I know this. It took me a lot of work to make those classes, but it probably took you even more to digest them. I hope you learned a lot and had a lot of fun and you feel empowered to program robots better than before. What I taught you, I believe, is the very basic set of methods that any roboticist should know about programming any robot intelligently. All of those robots have tracking problems, state estimation problems, planning problems, and control problems--be it a surgical robot, be it a robot maid, or your intelligent future household robot , or even a flying robot, such as an autonomous plane. I want to thank you for being with us so far. The rest of this unit contains extra information on SLAM. Otherwise, I'll just see you on the final. Hi, students. I am back to teach you a bit about SLAM. There was a request--a popular request, actually, in email and the discussion forum. SLAM is a method for mapping that's short for "simultaneous localization and mapping." Some of the this might show up in the final exam, so do pay attention. Mapping is all about building maps of the environment. You might remember in the localization classes we assumed the map was given. One of the big passions in my life has been to understand how to make a robot make these maps like this map here, which is a 3D map of an abandoned underground coal mine in Pennsylvania near Carnegie-Mellon University. Over the past 10 years or so, I have worked on a number of different methods for buildings maps that are quite sophisticated, like this particle filter method over here that you can see. All these methods have in common that we build a model of the environment while also addressing the fact that the robot itself accrues uncertainty while it moves. When, in this example here, the loop is being closed, you can see how our mapping technology is able to accommodate this and find a consistent map despite the fact that the robot drifted a little along the way. The key insight in building maps is the robot itself might lose track of where it is by virtue of its motion uncertainty. You accommodate this in localization by using an existing map, but now we don't have an existing map. We're building a map. That's where SLAM comes into play. SLAM doesn't stand for "slamming" a robot. What it really means is "simultaneous localization and mapping." This is a big, big, big research field. Most of my AI book is about this technology, and today I want to show you my favorite method called "graph SLAM," which is also by far the easiest method to understand. We will reduce the mapping problem to a couple of very intuitive additions into a big matrix and a vector, and that's it. Here is a quick quiz. When mapping an environment with a mobile robot, uncertainty in robot motion forces us to also perform localization. I'm going to give you two possible answers--yes and no. The answer is yes. In nearly all cases of mapping, we have robot uncertainty in motion. That uncertainty might grow over time. We need to address this; otherwise the map looks really bad. Let me give an example . Suppose a robot drives down a corridor, and it senses surrounding walls. If this robot has a drift problem and because of uncertainty it its motion, it actually believes it drives a trajectory like this. Then the surrounding map would look very much like that. Now, these might be indistinguishable at first glance, but if this robot ever comes back to the same place, then it has an opportunity to correct all this. A good SLAM technique is able to understand not just the fact that the environment is uncertain but also the robot itself runs on an uncertain trajectory. That makes it hard. Let me tell you about my favorite method of all, called "Graph SLAM." This is one of many methods for SLAM, and it's the one that is by far the easiest to explain. Let's assume we have a robot, and let's call arbitrarily the initial location x equals zero and y equals zero. For this example, we just assume the road has a perfect compass, and we don't care about heading direction just to keep things simple. Let's assume the robot moves to the right in x-direction by 10, so it's now over here. In a perfect world, we would know that x1, the location after motion, is the same as x0+10 and y1 is the same as y0. But we learned from our various robotic Kalman filter lessons and others that the location is actually uncertain. Rather than assuming in our (x, y) coordinate system the robot moved to the right by 10 exactly, we know that the actual location is a Gaussian centered around (10, 0), but it's possible the robot is somewhere else. Remember we worked out the math for this Gaussian? Here's how it looks just for the x variable. Rather than setting x1 to x0 plus 10, we try to express the Gaussian that peaks when these two things are the same. If we subtract from x1 x0 and 10, put this into a squared format and turn this into a Gaussian, we get a probability distribution that relates x1 and x0. We can do the same for y. Since there is no change in y, according to our motion, all we ask is that y1 and y0 are as close together as possible. The product of these two Gaussians is now our constraint. We wish to maximize the likelihood of the position x1 given the position x0 is (0, 0). What Graph SLAM does is defining our probabilities using a sequence of such constraints. Say we have a robot that moves in some space, and each location is now characterized by a vector x0 and a vector x1, vector x2, vector x3. Often they are 3-dimensional vectors. What Graph SLAM collects is initial location, which is a (0, 0, 0) initially-- although here it looks a little bit different-- then, really importantly, lots of relative constraints that relate each robot pose to the previous robot pose. We call them relative motion constraints. You can think of those as rubber bands. In expectation, this rubber band will be exactly the motion the robot sensed or commanded, but in reality, it might have to bend it a little bit to make the map more consistent. Speaking about maps, let's use landmarks as an example. Suppose there is a landmark out here, and the landmark is being seen from the robot with some relative measurement--z0, z1. Perhaps I didn't see it it during time 2, but this is z3. All these are also relative constraints very much like the ones before. Again, they are captured by Gaussians, and we get relative measurement contraints. One such constraint is every time the robot sees a landmark. Graph SLAM collects thosee constraints, and as we'll see, they're insanely easy to collect, and it just relaxes the set of rubber bands to find the most likely configuration of robot path along with the location of landmarks. That is the mapping process. Let me ask you a quick quiz that'll take thinking. Suppose we have six robot poses--that is, one initial and five motions. We have eight measurements of landmarks that we've seen. These might be multiple landmarks. Sometimes the robot saw more than one. The question now is how many total constraints do we have if we count each of these constraints as exactly one constraint. The answer is 14. There is 1 initial location constraint, 5 motions, which adds up to 6, and 8 landmarks constraints. That's the gist of what we're going to implement. The key insight now is that this is insanely simple to do. What we do is we make a matrix and also a vector. We label the matrix, which is quadratic, with all the poses and all the landmarks. Here we assume the landmarks are distinguishable. Every time we make an observation, say between two poses, they become little additions, locally, in the 4 elements in the matrix defined over those poses. For example, if the robot moves from x0 to x1, and we therefore believe x1 should be the same as x0, say, plus 5, the way we enter this into the matrix is in two ways. First, 1 x0 and -1 x1--add it together should be -5. So we look at the equation here--x0 minus x1 equals -5. These are added into the matrix that starts with 0 everywhere, and it's a constraint that relates x0 and x1 by -5. It's that simple. Secondly, we do the same with x1 as positive, so we add 1 over here. For that, x1 minus x0 equals +5, so you put 5 over here and a -1 over here. Put differently, the motion constraint that relates x0 to x1 by the motion of 5 has modified incrementally by adding values the matrix for L elements that fall between x0 and x1. We basically wrote that constraint twice. In both cases, we made sure the diagonal element was positive, and then we wrote the correspondant off-diagonal element as a negative value, and we added the corresponding value on the right side. Let me ask you a question. Suppose we know we go from x1 to x2 and whereas the motion over here was +5, say, now it's -4, so we're moving back in the opposite direction. What would be the new values for the matrix over here? I'll give you a hint. They only affect values that occur in the region between x1 and x2 and over here. Remember, these are additive. Here is the answer. Let me just re-transform this as is done over here-- x1 minus x2 is now +4, and x2 minus x1 is -4. I have to add +1 over here, -1 over there, +1 in this diagonal element over here, and -1 over here. Let me just do this. These numbers added in transform the first number over here to 2. We get a -1 for the off-diagonal elements, and then 1 over here. Now we add 4 to the 5, which gives us a 9, and a -4 to the 0 gives -4. This is where we are now. Let me do another quiz. Suppose that at x1 we see landmark L0 at a distance of 9. This gives me a relative constraint between x1, right over here, and landmark 0, which is over here. Just like before, these link two things together relatively--the x1's and the L0. Now this doesn't look like a submatrix, but it is. It's spread a little bit apart. But I want you to modify those 4 values in the matrix and those 2 values in the vector to accommodate that we believe that the occasion of L0 is 9 greater than the robot position x1. Here is the answer. Obviously x1 minus L0 is -9, because L0 minus x1 is a measurement of +9. Let's add this in. We add 1 over here on the main diagonal--1 and 1. We subtract 1 off the main diagonal just like before--a simple pattern. Then a 9 over here goes back to 0 but to 9 over here. I hope this makes perfect sense to you. Let's summarize what we've learned in the form of a little quiz on Graph SLAM. Check this box if Graph SLAM seem to be all about local constraints. They require multiplications--if that's true check the box over here. The require additions--check this box if this is correct or none of the above. The answer is obviously they are all about local constraints. That's the entire point. Every motions ties together two locations, and every measurement ties together one location with a landmark. Multiplication is just the wrong thing here, so they're all about additions. These are the correct things to check. I want to add one last thing here--the initial robot location. If we define x0 to be 0, which is the origin of the map, then what this means is we add 1 over here and 0 over here. The reason why is this constraint is that x0 is 0. Let's take a robot moving around and let's say it sees a landmark from the first pose x0 and from the third pose x2 but not from the second pose. I want you, in this matrix over here, to check mark all the fields that are being modified by Graph SLAM-- it's a binary check--and the same for the vector without putting actual numbers in. So go into this matrix and ask yourself which fields will be 0, the ones untouched, and which ones will not be 0 that are the ones we modified. The answer is our initial constraint would touch this guy over here. The one to second motion touches these things over here. The second to third, these guys, and then the landmark observation over here puts something between x0 and the landmark that sits here, here, here, here again, and here. This observation over here puts something between x2 and the landmark-- these guys over here and the guys over here. That means we have only the following places that are still 0. This means there is no direct constraint between x2 and x0. That is, there is no direct motion information between these guys, and there is no direct constraint between x1 and L, which is this guy is missing over here. Let me do the same quiz again. Now we have two landmarks, and the picture I'm giving you is a robot with three total positions. There's a landmark here and a landmark here, and say this landmark is being seen in these two positions, and this landmark these two poses, but landmark L1 is not seen at x2, and landmark L2 is not seen at x0. Of the 30 fields over here how many of them will never be touched? Please put your answer over here. The correct answer is 6 for the following missing links: this guy here gives me two values in the matrix. This guy here another two. And this link here is also missing. That's another two. So 6 values are missing. Let's prove it to ourselves. Moving from x0 to x1 fills up this area. From x1 to x2, this area. Seeing landmark L0 from x0 and x1 means we fill these guys over here. and the main diagonal there. Seeing the other landmark from x1 and x2 means we fill these guys over here. Let's count the ones that are still open, and here are the ones. My answer was actually wrong. It's 8. I overlooked there is no direct link from L1 to L2 either. My apologies that I gave you the wrong number, but it proves to you this is actually not an easy question. It's harder than I thought. The reason is there is also no direct link that constrains L1 and L2 directly. Landmarks can't see, so they can't put a direct link between any two landmarks. Or put differently, in this part over here our matrix will always be a diagonal matrix. The last thing I want to tell you before we go into programming is why this makes any sense. Suppose you fill the matrix, which I call omega [Ω], and the vector, which I give the Greek name of xi [ξ]. My apologies to my non-Greek students here. You Greek students should be very proud. You'll always have a special place in my mathematical heart. Then I can find the best solution for all the landmark positions or the world positions by a very simple mathematical trick that is completely counterintuitive. I invert the omega, I right multiply with xi, and out comes a vector mu [µ], which gives me the best estimates for all the robot locations and the landmark locations. Now, that is quite amazing, because all it means in Graph SLAM is that you keep adding numbers to these matrices every time you see a constraint. When you're done with it, you run a very simple procedure and out comes the best places for your robot. Let's go and try it. I'm now going to ask you to program this. I'm giving you my matrix class, so you can do this easily. What I'm asking you to do is to build a 3 x 3 matrix and, of course, a 3 x 1 vector about which you shall state that our initial location is -3. X1 in exportation is obtained by adding 5 to x0 and x2 is obtained by adding 3 to x1. In exportation what we should get out when we run the mu equals omega minus 1 times xi trick is that x0 becomes -3, x1 becomes 2, and x2 becomes 5. Diving straight into our programming environment, I'm giving you a matrix class--you might want to take a moment to look over it. It's a little bit augmented to what I've given you before, and I fixed a bug with the inversion code, which is quite essential. If I run it, I construct an omega matrix piece-by-piece-- that's the one that you should come out with-- a xi vector, and then I run and print out, using the "show" command, the result of omega to the -1 times c. You can see -3, 2, and 5 are the correct results that result from the omega matrix and the xi vector. What I want you to do is write code that incrementally step-by-step constructs the omega vector and the xi function and then returns to me those results over here. There is an empty function in your code that accepts as parameter the initial position, -3, and the two motion values, 5 and 3. Here is my result. I construct an omega matrix of size 3 x 3, and initially I set the top left corner to 1. Then the vector xi, I set very first value to init. Everything else is 0. Now come the important additions for the first move and the second move. Both times I do exactly what I told you before. For the two involved variables, I add a +1 on the main diagonal and a -1 off-diagonal. Same over here. Then I subtract move and add it 1 row later, and the same with move2 and move2 over here. Look very carefully. This is exactly what I told you about. I'm going to draw this graphically. I begin with a matrix like this. I then add this guy and then this guy. As far as the vector is concerned, I start with this, add this guy, and finally this guy. Then these two together are being combined down here where I compute the inverse of omega multiplied with xi. That gives me the vector of res, and res is being output using those "show" command and returned from the procedure. Now let's add the landmark. Let's say the landmark is being seen at all time steps. Let's say in the very first time the difference between position and landmark is 10. Obviously this is a 1-dimensional example and not 2-dimensional as the picture suggests over here. Then it's 5, and then it's 2. Now, what's the landmark position? You can work this out in your head. It's a single number. Please enter it here. The answer is 7. Obviously, -3 plus 10 is 7, -3 plus 5 plus 5 is 7, -3 plus 5 plus 3 plus 2 is 7. All of those work out to be 7. We have a fully consistent situation. The landmark seems to be consistently seen. There seems to be no noise whatsoever. Now I want you to extend your routine to accommodate the landmark Specifically, I want you to use a function that I coded for you that is very useful that is called "expand." You can run omega.expand, xi.expand to take a 3 x 3 matrix or vector and move it to a 4 x 4 vector that you actually need when you have to include the landmark itself. Give that a try and see if you can modify the code to now have additional input parameters of measurement 0, 1, and 2. In particular, here is our new doit routine. It now has as input parameters my 2 motion commands and the 3 measurement commands for the 3 different poses. Here is the code that you produced before. That's my version of it where we have the initial 3 x 3 matrix. Then using the expansion command you can now increase those to a 4 x 4 matrix and a 4 x 1 vector. When you run it what comes out is this result over here-- -3, 2, 5, 7. I want you to do this where -3 and 2 and 5 is the robot path, and 7, as before, is the landmark location. Please code this and realize that I can modify the input to doit just fine, and your code should not just produce this one vector, but it should implement the right math. Here is my answer. Here is the expand command. It takes the omega vector and turns it in to a 4 x 4 and assigns the existing coordinate to 0, 1, and 2 and expands to make xi vector of 4 x 1 where it uses the previous dimensions of 0, 1, 2, and 0. That turns out to make a larger matrix and larger vector. Now, I go and add in the measurement constraints. In all of those, I have to relate to the last coordinate, which is my measurement coordinate from the first, the second, and the third pose. And I have to subtract -z0, 1, and 2 from the corresponding robot poses and add them all up back to the last pose. If you implement this correctly, then you get a omega and xi that, once you implement this solution equation, gets you this solution over here. Here is a really tricky quiz. Let's look at the robot motion again. Say I change the last measurement from 2 down to 1. You might remember the robot poses were -3, 2, and 5. Before that modification the landmark position was 7, but a 1 doesn't really add up. A 1 suggests I might be at a different distance to the 7 than the 5 over here that comes from this side. Here is my quiz. First, I want you to know if I make this modification what is the effect on x2? Will the estimate be smaller than 5? That is, we shrink the robot path a little bit? Will it be exactly equal to 5 like before? Or will it be larger than 5? Check exactly 1 of the three boxes. I also want to know what is the effect on x0. Will it be smaller than -3, equals -3, or larger than -3. This is a completely nontrivial quiz. It takes really some thinking. Invest in thinking, and you can even go back and try it out. I'm trying it out and see what happens. Before it was a -3, 2, 5, 7, and now the first position is completely unchanged. I'll explain to you in a second why. The third one went from 5 to 5.5, and the landmark went down from 7 to 6.8. Graphically, this guy becomes larger than 5, and this landmark even shifts a little bit to the left to make these two things closer together than they were before when they had a separation distance of 2. Now this picture doesn't really explain it well, because it's a 2D picture, but in 1D that's exactly what's happening. Also interesting is the initial position is unchanged. These are the correct answers. Now, I would be blown away if you guessed them correctly. The reason why the initial position doesn't change is the only information we have about the absolute coordinate location is the very first initial position anchor that we said was to be -3. None of the relative rubber bands change the fact that we need this guy to be -3. A relative change between these 2 things over here means the rubber band is different, but it's a relative thing. This is the only absolute constraint we put in. Clearly the absolute location of the first position doesn't change. The reason why becomes larger than 5 is--well, think about rubber bands. Our landmark is around 7. We believe to be at position 5 in the noise-free case. We just put a tighter rubber band between them. It's not 2 anymore; it's now 1. That means we are inclined to move the landmark and this position closer together. That's exactly what happens. If you go to this solution over here, the final position becomes 5.5. It's now 5.5. The landmark becomes 6.875 instead of 7. Now, this is the case where the rubber bands don't add up. This is one of the places where Graph Slam is just magical. Before everything added up, but we have cycles in these structures. These cycles might not added up, because we have noise and motion, noise and measurements What our method does by computing this thing--omega to -1 times xi-- we find the best solution to the relaxation of those rubber bands. That, to me, is sheer magic. I'm going to give you a glimpse as to why it works. Suppose we have two robot positions, x0 and x1, and we know they're 10 apart with some Gaussian noise, and we know the Gaussian noise in exportation moves the right robot position 10 off the left robot position, but there is some uncertainty. When we talked about Kalman filters, we talked about Gaussians, and this uncertainty might look at follows: There is a constant exponential, and the expression that x1 minus x0 should relax to 10 but might deviate from it. This Gaussian constraint over here characterizes the constraint between x1 and x0 and wishes them to be exactly 10 apart. The Gaussian is maximum where this equation is fulfilled, but if the residual is not equal to 0, there is still a probability associated with it. Let's now model a second motion. Say x2 is 5 apart. We now get an even bigger Gaussian relative to the very first one, but the local constraint over here reads just like the constraint over there. Let me just write it down. X2 minus x1 minus 5 squared over sigma-squared. Now, the total probability of this entire thing over here is the product of these two things. If we want to maximize the product, we can play a number of interesting tricks. First, the constant has no bearing on the maximum, just on the absolute value. If we want to find the best values for x0 and x1 and so on, we can drop the constant. Secondly, we can drop the exponential if we're willing to turn the product into an addition. Remember, we added things in omega and in sigma. That's why. Finally, we can actually drop the -1/2. It turns out that also plays no role in the maximization of this expression. It turns out what you added where constraints just like these, and you even added them at a certain strength of 1 over sigma-squared. In particular, if you really believe that a constraint is true, you should add a larger value in this matrix over here, and on the right side you should multiply the right constraint with an even larger value. Put differently, take an expression like this and multiply in the sigma-squared you get something of this nature over here where 1 over sigma regulates how confident you are. For a small sigma, 1 over sigma becomes large. So 5 is much larger than 1. That means you have much more confidence. Let's go back to the code and modify the code so the last measurement has a really high confidence. I want you to multiply the last measurement between x2 and our landmark with a factor of 5 in your code. Hard code it. That is, go in somewhere over here where the last measurement is being applied and do the trick that I just showed you and see what the outcome is. When I do this, I get -3, 2.1, 5.714, and 6.821 as the answers. You'll see in this final result the final robot position of 5.714 and the landmark position of 8.821 are really close to 1 in difference, which was the measurement, because you know believe this measurement over-proportionally over other measurements and motions. Here is my answer. In the omega, I replace all the 1's by 5's so we add 5, -5, -5, and 5 over here. I also multiplied the measurement by 5. If you forget this, you get a very kooky answer. You have to adjust these things over here in the same proportion as the guys over here with a 5. That gives you the result that I stated. So now we've learned all about Linear GraphSLAM, and that's quite a bit--and it's really simple. Every time there's a constraint-- Initial Position, Motion or Measurement-- we take this constraint and add something to Omega, Xi. And what we add is the constraint itself, but it's up multiplied by a strength factor. There's nothing else but 1 over sigma-- the uncertainty in Motion or in Measurements. And then when we're done with this adding-- we simply calculate this guy and out comes our best possible PATH-- and along with the MAP of all the landmarks. Isn't that something? Isn't that really cool? So let's dive in and have you program your own real robot example. This is a fairly complicated generalization of what we just saw. I'm giving you an environment where you can specify the number of landmarks that exist, the number of time steps you want the robot to run, the world_size, the measurement_range--that is the range at which a robot might be able to see a landmark-- if it's further away than this--it just won't see it; a motion_noise, a measurement_noise, and a distance parameter. The distance specifies how fast a robot moves in each step. And then I'm giving you a routine which makes the data. It takes all these parameters and it outputs a data field that contains a sequence of motions and a sequence of measurements. The code comments on the exact format of what data looks like. Now I want you to program the function, SLAM, that inputs the data and various important parameters and it outputs my result--a sequence of estimated poses, the robot PATH, and estimated landmark positions. This is really challenging to program. It's based on the math I just gave you. The robot coordinates are now x and y coordinates. The measurements are differences in x and y-- so you have to duplicate things for x and things for y. I, myself, put them all into one big matrix, but you could have them in 2 separate matrices, if you so wish. You have to apply everything we learned so far, including the weights of one with our measurerment_noise and one with our motion_noise. These happen to be equivalent, in this case--but they might be different. And then you have to run SLAM and return back to me a result data structure. I'm also supplying you with the print_result routine so you can go in and see how the result has to look like. There's an example routine--that doesn't work-- that outputs all the correct formats, but it tries not to implement the estimate that I want you to estimate. You have to bring this to life and turn this into an amazing SLAM routine so that when you run it, you get the same results that I do for the examples here, where there's an estimated PATH and estimated landmark positions. There's one last thing I wanted to know-- is I assume the initial robot position is going to be in the center of the world. So it's the real-world set of 100 and it's going to be 50/50--or here it's printed as 49.999, but this is the same as 50. So you have to put in a constraint that sets the initial robot pose to the center of the world. So here is my solution: I've takan all the input parameters, and the very first thing is I've set the dimension of the matrix and the vector: the length of the Path, plus the number of Landmarks-- times 2--because I'm modeling x and y for each of those, in the same data structures. I then create a matrix for Omega and a vector for Xi, give it the appropriate dimensions, and subsequently I introduce the constraint that the initial positions have to be world-size/2.0, with a strength value of 1.0, That tells it this has no bearing on the final solutions because it's the only absolute constraint. But you can see--I add 1.0 over here in the main diagonal: 1 for x and 1 for y-- and then now add the same thing over here. It's important to understand how I set up the data structure. There's our positions--and let me just, for a second, call them "S". And there's our landmarks. Each of those have an x-component and a y-component. So in doing this, I'm taking this matrix and I'm setting it up, not by a matrix of Path length plus the number of landmarks, but each of those becomes a 2 by 2 matrix, where I explicitly retain the x and the y value. So the dimension here is 2 times N--the Path length-- plus the number of landmarks. And the 2 is the result of modeling x and y: xy, xy, xy. That's really important for my solution. You might have done this differently--you might have said: I'm going to build 1 matrix for x and 1 matrix for y. and then each of those becomes just a single value, which is closer to the way we discussed it in class. And that's fine, in this case. In general, it has disadvantages in that it cannot correlate X and Ys. So for a real robot that has real rotations, this doesn't work. My solution is better, but for this specific example this would have been perfectly fine. Coming back to my example, I now process the data. I go through all the data items and my Path index is now the data item, times 2-- which is the xy thing. I extract my measurements from the data-- my motion from the data-- using this command over here; and then I go through all the measurements, of which they are my multiple ones. I find the index in my matrix of the measurement, which is the Path plus the measurement index, times 2--because there are X and Ys again. And then the next routine just implements the simple addition with the measurement_noise as the inverse weighting factor. So it adds: 1, 1, -1, -1 to the corresponding elements in the submatrix, and in the vector, it adds the measurement-- all divided by the strength of the noise variable. If you look at this carefully, you'll take a minute to digest it and what was the use in the auxilliary variable, b, to account for the effect of this x and y. So b goes from zero to 1. And these are all the combinations. You have to stare at them to make sure they are all correct, but I can promise you--they're actually all correct. Motion is handled very much the same way: I extract the Motion command, I add among, the main diagonal-- between the 2 variables that are being tied together--a "+1", and then I add, in the off-diagonal elements,"-1". So again, you have to stare at this very carefully to see they're all correct. And then I add the Motion itself to the vector, Xi. That's what I had to implement. I then solve, as before, and return the solution. And that's exactly what's being printed out down here. I have to say, we got this correct--I'm mightily impressed. You understood a lot about Mapping and you solved a really hard programming problem. I'm responding to some of you online who asked for challenging programming problems. This is a challenging piece of code to write. It took myself a number of hours to write, and that would be wonderful if you got it right. So congratulations. I'm impressed you made it so far. You really learned a lot about SLAM. You learned about the MAP, which is a sequence of coordinates, and you learned about Localization-- and "L" and "M" are really important letters here. The Simultaneous and the And are not that essential. We put all of those into a big Matrix, Omega, and a vector, Xi. And every time, we got some relative information between poses. We carved out some stuff in here or we measured something. We added some stuff in here, and over here. These were all just additions and, as we now understand, those implement the straightforward constraints that come from the motion--the measurements. And then the key thing was that you could solve, with a simple piece of equation, for the Path and the Map at the same time. That was quite an amazing achievement. So you've implemented this, you've implemented your first SLAM algorithm. That was way beyond anything I ever taught students at Stanford in a single class. Congratulations for doing this. So this is the last, and final, class. I'm going to output a challenge to everybody, briefly, where you can write a Robot program that puts most of the stuff we talked about together. But, for now, congratulations for making it so far. That is really impressive. So welcome to our very first homework assignment. This is number 1. [Homework Assignment #1] Just to recap, in the class we learned about localization. [Localization] We learned about about histogram filters. [Histogram filters] And we programmed some in Python. [Python] So the homework assignment will cover this plus some very basic probability. [Probability] [Question 1] In the first question, I'm going to ask you some very basic probability questions. We have random variable X and the probability of 0.2. What's the probability of the complement? We have 2 random variables, X and Y, whose probability individually is 0.2. And X and Y are independent. [X, Y independent] What's the probability of the joint X, Y? And we have the variable P(X) with probability of 0.2, and we have 2 conditionals, P(Y|X) and P(Y|¬X), both 0.6. What's the probability of Y? Here you have to apply total probability. [Question 1] The correct answer in the first case is 0.8. This is just 1 minus 0.2. If X and Y are independent, then we just take the product of those 2 things, which is 0.04. And in the last case, it turns out X and Y are independent but just by a coincidence because the probability of Y is independent of what X says, and therefore, the outcome is 0.6. Put differently no what matter value X assumes, whether it's X or not X, Y is always 0.6 probability so it must be that P(Y) is 0.6. You can actually compute this using total probability where P(Y) equals P(Y|X) times P(X) plus P(Y|¬X) times (P¬X). When you plug in the numbers, you get 0.6 times 0.2 over here; 0.6 times 0.8 over here, the complement of 0.2. And if you regroup this, or you put the 0.2 and the 0.8 together into one, you end up with 0.6. Let me ask you a localization question. You remember a robot operating in a plane environment has usually 3 coordinates. It has an x-coordinate, a y-coordinate, and a heading direction--often called orientation. Now, flying robots have more coordinates. If you can orient the robot fully in free space then you have an x, y, and z, and you also happen to have 3 rotation angles-- often called roll, pitch, and yaw. If you built a localization system for robots with higher dimensional state spaces, I wonder how the memory used will scale for our histogram-based localization method. Does memory scale linearly, quadratically, exponentially, or none of the above in the number of state variables used in localization? Again, for a robot operating on a plane, there will be three of them. So the number of state variables will be three. If you were to look at a flying robot where you have x, y, z, roll, pitch, and yaw, You would get six such variables, and I wonder how the memory use of the basic histogram localization scales. Please check exactly one of those four boxes over here. The answer is exponential. Suppose we resolve each variable at a granularity of 20 different values, so there's 20 different values for x and 20 for y and 20 for θ. Then the joint table over all of those will be 20^N where N is the number of state dimensions. That's an exponential expression. There is unfortunately no easy way around it. The biggest disadvantage of the grid-based localization method or the histogram method is that the scale of memory is exponential, which means it's not applicable to even problems with 6 dimensions, because you can't really allocate memory for 6 dimensions. I'm now going to quiz you on Bayes Rule. Say you own a house, and you know that the house might catch fire in your absence, but the probability of it catching fire--"f" over here--is small. It's a 10th of a percent--0.001. Let's say every afternoon you talk to your neighbor, and every afternoon you ask your neighbor, "Does my house burn?" Of course,you a little bit paranoid if you do this, but for the sake of the argument, let's just assume you do this every afternoon. This afternoon he comes back and says, "Yes, it burns," so B. You happen to know that the neighbor is not very truthful. In fact, every time you ask him a question, you know there is a 0.1 chance--a 10% chance--the neighbor will just produce a lie and a 0.9 chance the neighbor actually speaks the truth. So you ask him exactly one question--"Does my house burn?" He says, "Yes, it burns," but you know that the probability of this being a lie is 0.1. So in applying Bayes Rule I like to first compute the non-normalized posterior--p bar-- bar now stands for non-normalized--of fire given the neighbor just called. The same for the opposite event of no fire given the neighbor just said, yes, it burns. After you've done this, I'd like you to compute the normalized values that have to add up to 1. Please enter all 4 values for this homework assignment. Here are my answers. The prior for fire is 0.001 times the probability that the neighbor now correctly said, yes, it burns, which is 0.9. He lies with a probability of 0.1, so the complement is 0.9. This gives us 0.0009. For the complement, the prior of no fire, is 0.999, but now the neighbor would have lied, which multiplies with 0.1, which gives us 0.0999. Now, these two values don't add up to 1. The normalizer will be 1 over these two things, which is about 9.92. Multiplying these with their normalizer gives us approximately 0.0089 and 0.991. So the answer your neighbor gave you--yes, it burns-- raised your probability from 0.001 to 0.0089. It's still small, but it's significantly larger. In fact, it's approximately 0.9 times as large as the initial probability. The reason why that is the case is it relates to the 0.9 probability of speaking the truth. It's not exactly 0.9 because of normalization, but it's approximately 0.9. This is our first programming assignment. In class, we localized the robot in a 1D world with a number of grid cells where each grid cell could have a different color, and the measurement vector was a sequence of observations. Our world was cyclic--if you fell off one end you would continue at the other end. In this assignment, I want you to do the same for 2D roles of arbitrary dimension. Just as before, begin with a uniform distribution as you always do in global localization. Then we have a number of motion commands--[0, 0] is no move, [0, 1] means you move to the right, [0, -1] means move left, [1, 0] makes you move down, not up, and [-1, 0] makes you move up. Again, the world shall be cyclic. If you fall off one end, like over here, we continue at the other end, like the one over here. Here is a simple example of the type code I give to you as a specification of the problem, and then you have to compute what my code computes but that it can't see right now. The world in this specific instance is a 3 x 3 matrix--3 row and 3 columns. It has only 2 possible colors, green or red, and this specific world has only a single red at the center location over here. We have a motion vector and a measurement vector. We start with a motion. This one says stay in place, and this was says we're going to observe red. Additionally, I give you two more variables called "sensor_right" and "p_move." Sensor_right is the probability that the sensor measurement is correct. In this specific instance, I set it to zero, which means the sensor value is always correct. P-move tells you at what probability the motion is executed correctly. Right now it's 1.0. It's always correct. If it's a smaller value, then the motion might fail, and when it fails, our robot won't move at all. Let's execute this. Here we didn't move, we observed red, and we had a noise-free sensor. As a result, we get a matrix that says zero everywhere except it's a 1 at the center location that has a red color. Let's modify the world. Let's make this grid cell red over here as well. Then now let's just rerun the code. What we get is a matrix just like the previous one, but now we have winning grid cells, both of which have a 0.5 probability so that all the probabilities add up to 1. Let's now model a noisy sensor and set sensor_right to 0.8. Your code should now computer the following: a 0.06 for almost all grid cells except the two winning ones, which come in at 0.26 and 0.26 each. Check that your code does this. Let's now bring in some motion. After not moving at all, I assume we're going to move 1 to the right, and we always have to have as many measurements as motions. So let me add a second measurement. Let's say we sense red again. Intuitively, this lands us in the square over here. Why? Well, we didn't move in the beginning, we saw red-- there's two possibilities--but now we move again to the right side. We see red again. That makes this cell over here the most likely. Let's just check, and as predicted, almost all cells have a probability of 0.03. Some have 0.13, but the one over here has a probability of 0.533. If we set our sensor probability to 1.0, that is no sensor noise, we get back this array over here, which assigns all probability to the rightmost cell. Finally, I want to show you what happens if you modify the move variable. Say our motion succeeds only with 0.5 probability and with the remaining 0.5 we remained at the same location. This doesn't affect the first motion command, because success and failure is the same thing here. We don't move. But with this one over here there is a 50% chance of moving and a 50% chance of staying at the same location. Let's run the code. Here is our posterior probability.1 The cell on the right still wins, but now with a smaller total probability of just 0.46. If we now assume perfect sensors, by setting sensor_right to 1.0 we get this thing over here. We have 0.66 chance associated with the right cell over here and a 0.33 with the possibility that we moved this specific red here twice by just not moving in between. Check your code to make sure it gives you the exact same operative result. Finally, I want your code to execute input as complex as this one. This is a 4 x 5 world--4 rows and 5 columns, all with reds or greens. There's only two colors. There's a sequence of measurements of 5 elements and, correspondingly, a sequence of motions of 5 elements. All the measurements are green. The motions don't move at all, move right, move down, move down, and move right again. Then there are certain sensor probability and motion probabilities that I set at will. I set it to 0.7 and 0.8 over here. Now, if we look at the sequence, green, green, green, green, green, we first don't move at all, then move right, down, down, and right, you find that the most likely match in this world is we first sense this green over here. We then moved right to this green. We moved then down to this green, further down to this green, and right to this green. This would be the cell with the largest posterior probability. It is the 3rd row and the 4th column. Let's run it. And here is the result. It's somewhat illegible, and I apologize for the poor formatting of my Python routine. But if you look at these probabilities--0.011, 0.024, and so on-- you'll find that indeed the largest element is the one over here--0.3535. And it's our 3rd row and our 4th column gives me this large probability. I want your code to produce numbers just like those, and we'll check that you got the code correctly. In summary, read your colors, build a probability distribution of the same dimensions-- in this case 4 x 5-- Initialize distribution, execute a motion first, then measurement, motion, measurement, motion, measurement, motion, measurement, and so on. You can safely assume that the measurement vector is of the same length as the motion vector, using the measurement correctness probability and motion success probability, and then compute an output of just the final distribution. If you've done this, you'll succeed. Here is my solution to the programming assignment, and it's quite straightforward given the class, but I'm really proud if you go it correct, because it enabled you to program your own localization algorithm very similar to the way we do it at the Google self-driving car. First, I did two simple bookkeeping assignments. I assigned a value to sensor_wrong as 1.0 minus the probability of sensor_right and a probability of staying--that is, a motion failure-- as 1.0 minus the probability of p_move. Let me scroll down very slowly. Let me first go to my main routine. I actually put a little check where the length of the measurements vector is the same as the motions vector, and it would give me an error message if not. Of course, this wasn't necessary for you. I just did it because I want my software to look nice. Then here is my initialization of my probability table. I compute my initial uniform distribution by calculating the size of the array-- the number of rows times the number columns-- and then dividing 1.0 over the product of those to be my initial distribution value. This thing over here just builds up an array of the size of my colors array but initializes it with the value of "pinit." These two lines over here give me an initial uniform distribution, and then I iterate. I go through the number of measurements, which is the same as number of motions. I move first using the "move" command of which I provide my current distribution and my motions command to obtain a new distribution. Then I do the same with the sensing command. I take my current distribution, the world itself, and the measurement vector to obtain a new probability distribution. When I've done this as many time as I have measurements and motions, I output the final distribution. So much for the main routine. I now have to specify what move is and what sense is. Let me start with sense. This is my sense routine. It goes from here to down here. As an input, I have a probability distribution and my world map-- they're both of the same size--and a specific measurement, which is either red or green. I construct and cite my new posterior distribution. I initialize this with zeros, and I set the same size as my vector p. In the inner loop, I now iterate over all elements in my grid cell. I compute whether the measurement matches the color in the cell, in which case I call it a hit. Now my non-normalized posterior is the prior times this big sum over here. It uses sensor_right if the measurement was correct and sensor_wrong if the measurement was incorrect. Finally, I add up all the values in aux--I do this with the variable "s." Down here, I can normalize aux to have a total probability of 1, and then I return it. The "move" command takes as an input a distribution and a motion vector. It constructs the next distribution just like before as aux variable and sets it to zero. Now I go through each grid cell and for each cell, I collect possible cells that the robot might have come from. With probability p_move, it actually moved, in which case its prior coordinate would've been i minus the motion command. That's because you go backwards in time. This is a truncation, indicating we have a cyclic array, and we do the same with j--let me scroll very carefully. It's j minus the motion command, again in a cyclic fashion. But it might've been we didn't move, in which case you just use the probability of that specific cell multiplied by the probability of staying. Now, this line together gives me the correct probability for the variable i and j. I don't have to normalize, because it's not Bayes Rule. I just return the corresponding posterior distribution. I also have a little routine called "show" that goes through the entire probability field and computes out all these probability vectors of p that makes it slightly better formatted than just printing p in a single command. If I run my software with a specific word over here, it initializes p as 1/20 because there are 20 grid cells, then runs 5 times through the motion command and the measurement command, updates those, and then shows me the final result, which I already explained, which is the array over here. If you got this correct, then you've done something quite amazing. You've programmed the core of Google's self-driving car localization methods. In Google's case, the world isn't as simple as just red and green. In Google's case, these are carefully assembled 2D surface models of the road surface. But that doesn't affect what we've programed here. It makes the measurement function slightly more involved. The fact is that the thing we programmed here captures the key of the probabilistic inference necessary to localize the Google care. If you programmed this, you just have to replace the simple matching of a measurement of green with an image matching of an entire imagery record with imagery map. I leave this as an exercise, because I can't do this in this Python environment here. But I congratulate you that you really managed to do something quite amazing, which is build an amazing piece of localization software. Congratulations. You made it through homework assignment number 1. You learned about Monte Carlo robot localization with a technique that I often call histogram filters. You've implemented it successfully and learned a lot about statistics. This is all just a single class. Congratulations. That's really awesome. Now, next week we talk about common filters and tracking other cards in traffic, and you're going to implement our common filter, so I'll see you in the next class. [Homework Assignment #2] This is homework assignment #2 in CS 373, and it's all about Kalman filters. [Kalman Filters] [Measurement Update] So question #1 is measurement update. Let's start with 2 Gaussians and say they have identical variances. Let's multiple them. We know that the resulting Gaussian's mean will be just at the center between those 2 Gaussians. I wonder if the variance Σ² is now smaller, larger, or the same as either of the individual Gaussian. Please check one of the three. And the correct answer is smaller. We are more certain afterwards. You get a more peaky Gaussian whose standard deviation is smaller. This is the correct answer. Say we have a prior of a Gaussian with a mean mu and covariance sigma squared, and our measurement has exactly the same mean and same covariance. Suppose we multiply both and get a new mean, which is the same as the old mean, and we get a new covarience, sigma squared, which as you all know, corresponds to a more peaked Gaussian. I want you to express nu squared as a multiple of sigma squared. Just put the answer here as a real number. The answer is a ½ or 0.5 To see, let us multiply these two gaussians over here-- which the exponent becomes an addition-- we can now rewrite this as follows-- when we bring the factor 2, for these two terms in to the numerator as our 2 sigma square, and from that we see that the new variance is 0.5 as large as the old one when applied to the squares. So, as a result, you would have this equation over here. I have another Gaussian question for you-- this is called a heavytail Gaussian. So, as you go to +/- infinity, the Gaussian levels off at some finite value alpha, as opposed to zero. My question is-- "Is it possible to represent this function as a Gaussian?" This is the formula for the Gaussian again. Particularly, can you find a Mu and a sigma square for which this exact function over here is obtained? Please just check one of these two boxes. The correct answer here is no. Suppose we let "x" go to infinity, then x-Mu2 for any fixed Mu would go to infinity. So if x with a -infinity would go to zero-- because it is a constant-- Therefore, we know that in the limit of x goes to infinity-- this expression must be zero. However, in this graph--it stays at alpha, and doesn't go to zero-- So, therefore, there can't be a valid Mu at sigma square. If a deep improbability, you know that the area in the Gaussian has to integrate into one, and this area diverges, it is actually infinite in size, so it's not even a valid execution. My next question pertains to the tracking problem that we talked about in class. In class we addressed a 1-dimensional tracking problem where we estimated the location of the system and its velocity. I'd like now to generalize this to a 2-dimensional problem. We'll be given coordinants x and y, and the object we're tracking moves in 2-dimensional space, and we wish to use a Kalman filter to understand where the object is, what its velocity is, and even be able to predict a future location based on the estimate of the position and its velocity. So the only difference, class, is that our object now moves in a 2-dimensional space, where as in class, it moved in a 1-dimensional space. So my first question is what's the dimension of the state vector in the Kalman filter? [Dimension of the state vector?] In the class, it was this kind of state vector. Now, we have a new one. How many dimensions or how many variables are there? And the answer is 4. So rather than X and Ẋ, we have X and Y and Ẋ and Ẏ as our state vectors. And there's 4 variables Now comes the tricky question. In the Kalman filter program that we studied, the 2-D Kalman filter, we had a matrix F. And for delta T equals 0.1, our F matrix, the state transition matrix, had a main diagonal of 1, which means in exportation our location stays the same and our velocity stays the same. And we also knew that the velocity affected our state in the following way. And you could now place 0.1 instead of the delta T to get this specific F matrix. Now I want to know from you for this new 2-D case with a 4-dimensional state vector what is the new F? It is a 4 by 4 matrix, so I want you to fill in all of those values. Again please assume that delta T is 0.1, and don't write delta T, write 0.1. And the answer is the main diagonal remains one. This expresses the effect that in the absence of any velocity, we expect the x-coordinate and the y-coordinate not to change. We also don't expect the velocities to change. But the x-velocity will impact the x-position through a 0.1 over here. The third coordinate is the velocity, and it affects the first coordinate. The same over here. All the other values should be zero. So this is the 4-dimensionalization of this 2-dimensional state-transition matrix over here. Let's now come to our programming exercise I want you to program exactly what we just talked about. We're given a two-dimensional world where we observe in 2D measurements of a moving object with an unknown but fixed velocity. Using a state vector of this type, I'd like you to implement the Kalman filter. Now, this Kalman filter now has 4 state variables whereas the one we used before had 2 state variables. I will give you the entire code for the Kalman filter, but I want you to set up the state vector x, the motion u, P, F, H, R, and I, which are all those variables that define the Kalman filter. Start with the assignment that we had in 2D and make it work in 4D. Here is exactly the same matrix class that I wrote you before. Here is the Kalman filter procedure. We'll go through our measurements and apply the Kalman filter equations. I should point out there's a slight difference to the code I gave you previously where I insert a zeta transpose. It makes it a little bit easier to work with multidimensional measurements than how I had it before. But you don't have to pay attention to this. It's just fixed. There was a kind of a bug before. As I scroll down, the output of the Kalman filter routine will be an x and a P. In our example, the measurements will be a sequence of measurements in two-dimensional spaces now--in x and y. Look at the x's 5, 6, 7, 8, 9, and 10. The y's go 10, 8, 6, 4, and 2. You can imagine what the regularity is and what the velocity is. We assume a dt of 0.1. That means when it goes from 5 to 6 the velocity is actually 10, not 1. We won't tell the system, but we will tell the system our initial x,y location, which is 4. That goes nicely into 5, 6, 7, and it's 12. That blends nicely into 10, 8, and 6. Our initial state vector I have already given you, which is the initial x and y. and 0, 0 for the two unknown velocities. The motion vector, just for completeness, will just be 0, 0, 0, 0. We have no external motion. That's a bit confusing, because there is actually motion in the system itself, but this will be more like an external change of the motion as if someone hit the object with an external force. So it's 0, 0, 0, 0--please don't change it. P is the initial uncertainty, and I want you to initialize it so that the uncertainty for the x,y coordinates is zero, but the covariance term for the velocities is 1000, indicating that we really don't know the initial velocity. We just know the initial position. I want you to plug in the f matrix. I want you to design an H matrix that's a projection matrix from 4-dimensional state space to 2 dimensions, reflecting the fact that we can only observe the first two state variables--x and y-- but not the velocities. I want you to define the measurement uncertainty matrix, which now is a 2 x 2 that has 0.1 as the main diagonal as measurement noise. This is an identity matrix over here. Once you design all those, you should get the following output. So when I run this, I get as an output for my 4-dimensional example the x coordinates 10 and 0. This makes sense given that these sequences over here has a final measurement 10 and 0--5, 6, 7, 8, 9, 10--10, 8, 6, 4, 2, 0. The interesting thing that I want your program to produce is the velocities. They are approximately 10, which makes sense given out delta-t of 0.1 gives us per time step a 10 divided by 10 equals 1 increment over here. The second velocity is -20 multiplied by 0.1 gives us a -2. You can see it over here--10, 8, 6, 4, and 2. I also want you to output the covariance matrix, which has certain elements that are still 0, like these guys over here. We find that along the main diagonal our uncertainty has shrunk substantially. It's 0.03 for the coordinate estimates and 0.1 for the velocity estimates. Remember, this number over here was 1000 before. Here is a second example where we have an initial coordinate of -4 and 8. We can see the measurements 1, 6, 11, 16. It seems the increments are in x direction 5. In the y direction they are -4--8, 4, 0, -4, -8. If I now run this, I get for my x vector approximately 16--the number over here, approximately -8--the number over here. These are velocities--50 and -40 in approximation, which multiplied with 0.1 is our plus 5 and our -4. Here is yet another example. Initial state 1 and 19. You can see the first coordinate doesn't change at all. You should get a velocity of 0. The second coordinate goes 19, 17, 15, 13, and 11. Running it gives us 1--unchanged--and 11. Velocities are 0 and -20 for the decrements of -2. Going into the covariance, we see values along the main diagonals-- 0.05, 0.05, 0.33, and 0.33 for the velocities. There are certain off-diagonal elements. Make sure those all match what your code produces. I can now change some of these measurements to make a noisy measurement. One way to do this is to set an oscillating measurement between 2 and 0. Remember that this Kalman filter assumes a fixed velocity. There is no way to explain these measurements with a fixed velocity, so there has to be measurement noise. We can run the Kalman filter again. Your filter should output the following values: 0.7 for the current state, 11 as before, and here are our two velocity estimates. It actually believes there is a slight velocity of -0.66 in the x direction where we had noisy sensor input. The covariance matrix would look exactly as before, because it's not affected by the measurements themselves. Your job is to fill in these various matrices. Good luck. Here's my solution for the programming assignment. There are many different ways to structure this, but that's what I've done. I have a dt equals 0.1. I set my initial state vector to be the initial x and y coordinates, and for the two velocities I set them both to 0. My u vector, as I said in the statement of the problem, is zero everywhere, so just ignore it. Interesting is my P matrix that measures the uncertainty. I set the uncertainty initially for the locations to be zero. These are the two main diagonal elements over here, and the uncertainty for the velocity is to be really high--it's 1000. So this is my initial uncertainty matrix. That guarantees that I can really estimate the velocity based on data, and I believe the initial state estimates are correct. Our F matrix is a 4-dimensional generalization of the F matrix we had before where we have 1 along the main diagonal. This one says that the position is retained in expectation and the velocity is retained, and we have two dt's over here. The x dot, which is my third state vector influences the x by a factor of dt for each time stamp. The same is true for y dot. These are the places where our velocities impact our position estimate. As I scroll down, the H matrix is a 4 x 2 projection matrix where we project out the x dimension and the y dimension without any velocities. For the measurement uncertainty I assume that each measurement has uncertainty covariance of 0.1, and these are along the main diagonal of the 2 x 2 measurement noise uncertainty matrix. This is obviously how a 4-dimensional identity matrix looks over here. If I run this for my first example where the measurements are 5, 6, 7, 8, 9, and 10, and the second dimension is 10, 8, 6, 4, and 2 and you can't see it but it's 0 over here. In my output, I correctly get the estimate of 10 for my x and 0 for my y. Velocity is 10 and -20. As we had before, because dt equals 0.1, a step from 5 to 6 within a 10th of a time unit requires velocity of 10 and from 10 to 8 one of 20. These numbers are correct. But we want to look at the covariance matrix. It's hard to read anything off it other than we are fairly certain as to what our location is, and we have a fairly good estimate of what our velocities are. Our covariance of velocity uncertainty is 0.1, and this is down from 1000, which was the initial value in these uncertainties. Going to the second example, I now commented away the first example and put in place a second example. If I run it again, here we see the first dimension go for -4, 1, 6, 11, 16. The second dimension 8, 4, 0, -4, -8. These are the exact same values over here--16 and -8. For velocities I get 50 and -40, which are exactly the correct velocities. Finally, for our third example, where the first coordinate doesn't change at all, we get the correct 1 over here and velocity of 0. Second coordinate goes from 19, 17 all the way to 11. We get 11 over here and velocity of -20. This is the implementation I wanted you to do. If you implemented this thing over here, you got it right and congratulations--you implemented a fairly nontrivial Kalman filter in stages, but through this class, we now have code that allows you to run Kalman filters on complicated problems, and I hope you really got an understanding how the Kalman filter works. So congratulations. You just made it through the Kalman filter class and the second homework assignment. You've implemented Kalman filters, you learned a lot about Gaussians, and you wrote your first vehicle tracking software. Congratulations. That's actually really, really cool. Next week, we'll talk about particle filters as yet another method for state estimation. It is very interesting and very fascinating. We're going to implement our first. See you next week in class. [Andy:] On the forums, we saw a lot of confusion about homeworks 2.5 and 2.6. Specifically, questions about what was going on with the linear algebra. I want to talk about some of that today, and I want to do it by comparing the 2D case -- and that's the case we talked about in lecture-- with the 4D case, which is what you're asked about on the homework. So in the 2-dimensional case, I want to first talk about this f matrix that Sebastian was calling the "state transition matrix." The idea behind this matrix was that we wanted to take some old beliefs, some old state, which in the 2-dimensional case was represented by x and ẋ where ẋ is our velocity and x is our position, and from that we want to extract our predict some new state, which was called x-prime and ẋ-prime. The question was what do we fill in here to get the proper values for x-prime and ẋ-prime. Let's think. What should our position be--our predicted position after some time has elapsed? Well, we want to include our old position, right? Lets first write out these formulas. We expect that x-prime will be composed of our old position plus whatever motion was occurring due to the velocity. That is going to be dt--the time elapsed--times our velocity. This is just velocity times time, which tells us how much our position has changed. Now, in matrix terms how do we express that? We're talking about x-prime, so that means we're going to think about this top row here. We want to keep x, which means a 1 goes here. We want to multiply ẋ by dt, so that means dt goes here. Just like that we figured out the first row of our F matrix. I'll label it here--F. Now what about the second row? Let's do a similar thing for ẋ prime to figure out where we should go in the second row. After our prediction, we said that we're just going to assume that the velocity hasn't changed. Velocity after equals velocity before. That means we don't want to have anything to do with this x, meaning a 0 goes here. We want everything to do with this ẋ--we want to keep this--so we put a 1 here. Okay. This kind of gives us some intuition for how this works in 2 dimensions. Let's see if we can generalize to 4. Now, again, we're going to some new state, and we're doing that by multiplying a state-transition matrix by some old belief. But now instead of x and ẋ, we also have y coordinates. So we have x, y, ẋ, and ẏ. Here we're going to, of course, get x, y, ẋ, ẏ, and all of those are prime, because they indicate after our prediction. Now, I'm not going to fill in this 4 x 4 matrix for you, but I think using similar reasoning to what we did in the 2-dimensional case, you can come up with what these formulas should be, and from that fill in this matrix appropriately, remembering that this entry corresponds to the first row, this entry the second, and so on. Good luck. Now I want to talk about the H matrix. This is a matrix that takes a state, and when it multiplies by that state, spits out a measurement. Remember, we can only directly measure position and velocity, so that's all we want the H matrix to keep. Again, I want to talk about the 2D lecture case and the 4D homework case. Hopefully, by comparing them, we'll be able to build some intuition, and you'll be able to answer the homework. What was the goal of the H matrix? The goal of the H matrix was to take some state-- in the 2D case, our state was represented as an x and an ẋ-- multiply some matrix by that state in such a way that we extract a measurement. In the 2D case the measurement was just x--just the x coordinate. We can think of this as a 1 x 1 vector or a 1 x 1 matrix. The matrix we use to do that was this one. That was our H matrix--1, 0--because 1 times x gives us the x, and 0 times ẋ gives us the nothing--exactly what we want. But now let's talk about the dimensionality of these matrices and how this multiplication yielded just this number x. So we can think of x here as a 1 x 1 matrix. We got that matrix by multiplying this one, which is a 1 x 2-- one row by two columns--with this, which is two rows by one column. What we see here is that this 1 actually came from right here, and this 1 came from right here. These 2s we can think of as canceling out, in a way, giving us this 1 x 1 matrix. Now, let's see if we can generalize that to the 4-dimensional case as presented in the homework. In the 4-dimensional case our state is now given by x, y, ẋ, ẏ. We're going to have some H matrix. I don't know anything about it yet, but I'm just going to put this there for now as a placeholder. We want to get a measurement from that. What should this measurement be? It's not just going to be x, because now our position includes both x and y. So it's going to be a column vector--x and y. Again, let's think. What's going on with the dimensionality here? Here we have a 2 x 1 matrix, and that came from this matrix, which I said we don't know anything about yet-- I'll just say a question mark by question mark-- and this matrix, which is four rows by one column. Now, can you use the intuition we built up here for how the dimensionality of matrices works with this to fill in the question marks? Once you figure out the number of rows and the number of columns in this H matrix, figuring out where to put your 1s and 0s will be a little bit easier. I wish you luck. Welcome to homework assignment number 3. This is all about particle filters. All ask you three questions. Then we have a fairly involved programming assignment. Here is my first question. Consider the following world with 4 states--A, B, C, and D. Suppose we wish to initialize our estimate with uniformly drawn particles. My question is what is the probability that cell A is empty? I'm asking this for different values of N-- N equals 1, 4, and 10. For N equals 1 it is 0.75 with 3/4 chance that particles will find themselves in B, D, or C, and there's only a 1/4 chance that a particle is in A. For N equals 4, it's 0.316, which is the same as 0.75^4. For each particle we have a 75% chance to be outside A, and 0.75^4 are the chances of 4 particles are outside A. As we move on, we get 0.75^10 with a 0.0563. Question 2--consider the same world as before with 4 states. Now we're facing a situation in which there are 5 particles in A, 3 in B, 3 in C, and 1 in D. We will now take a motion step. This is not about measurements, just motion. Our robot moves with a 50% probability horizontally. For example, if it was in D, it would move to C. It moves with a 50% chance vertically but never diagonally, and it never stays in the same cell. After 1 motion step, how many particles do we expect in cell A, B, C, and D? Answer the same for the asymptote. If we move infinitely often, what is the distribution that we expect this to converge to? I somewhat sloppily wrote "after infinite steps." This is really what does it converge to in the end. The correct answer of the 1 step is 3, 3, 3, and 3. For example, for cell A we can only get particles from B or C. Those will come to A with a 50% chance each. So 6 times 0.5 is 3 for A. Each cell has exactly 6 particles in its total neighbors, so therefore, each of those cells gets a 3. Asymptotically, this won't change. It's again 3, 3, 3, and 3. I apologize it is a little bit misleading here that all these answers are the same number, but that's what this example gives us. In question 3 I'd like to quiz your intuition. Suppose we run a particle filter with N equals 1 particle. What will happen? It'll work just fine, perhaps with a slightly larger error. It ignores robot measurements. It ignores robot motion. It likely fails. Or none of the above. You can check multiple boxes on the left. The correct answer are it ignores measurements and will likely fail. It ignores measurements, because the measurement sets the weighting factor in this resampling process, but with only 1 particle the same particle will be resampled no matter what. So the chances of the particle to be resampled is 1. Therefore, it's independent of the actual measurement. The robot truly ignores the measurements. Because it ignores the measurements, it'll likely fail, so works fine is incorrect. For ignoring robot motion--that's not the case. Even a single particle can be sampled forward. So these are the two correct answers over here. Here's our programming assignment. In class you already programmed a particle filter for a really simplistic robot that was able to measure ranges to landmarks and moved pretty much like a trashbin. Now I'd like replace it with a more interesting robot that's more realistic. In particular, I'd like you to use a car. Here's a car. A car tends to have fixed tires and two steerable in the front. Suppose the location of our car in a coordinate system is given by its x-coordinate and its y-coordinate-- I'm picking the halfway point on the rear axle as the reference point-- and by its heading dire theta. The state will be x, y, and the orientation theta. Then this car also has a steering angle that is called the alpha. The question is how is the state effected by driving a certain distance d with a fixed steering angle, assuming the initial state is x, y, and theta. It turns out to answer this, I also need to know the length of the vehicle, which I will just call L for length that is a constant throughout out consideration. This is usually called a bicycle model. Obviously, it suffices to look at one pair of tires because the other one-- at least in approximation--runs pretty much parallel. If we look at the robot locally where we have a steering angle, alpha, robot length L, and we're driving the rear tire forward by distance D, then the robot will turn to the left, and it's turning angle, beta, is proportional to the distance that the rear tire moves forward divided by the length of the vehicle times the tangent of the steering angle. Let's now compute the changes of x, y, and theta in the local coordinate system. Realize the turning radius R of this robot is simply the distance that we drive forward divided by beta. It's relatively simple math, which I don't want to explain in detail. This means that the robot is turning around a point over here at cx and cy. After the turn, the vehicle is located somewhere over here. In global coordinates, here is the way we describe this. Cx is the x coordinate of the robot x minus--now we go to the left-- the sine of the robot orientation before motion times radius R. Similarly, cy is this expression over here--y plus cosine of the orientation times R. Then after motion, we can go back from cx to cy to a new state over here simply by adding in the turning radius beta. That is, our new x is cx plus sine of theta plus beta times radius. Notice how this parallels this guy over here, except for two changes. What we previously subtracted we're now adding, and we're adding beta to the argument of the sine. The same with y, and the orientation is just increased by beta--modulo 2π. This all works if the robot is actually turning. If the robot were to go straight, then R would become infinity by this division over here. For small betas--that's smaller than 0.001--we can approximate this all as straight motion. Our new x is the old x times our driven distance pointed in the cosine of the heading direction. Similarly for y we go in sine of heading direction, and the heading direction stays the same. You could add beta, which is basically zero, to be slight more precise or you could just use theta. It doesn't really matter. In this programming assignment, I'd like you to implement this piece of math over here in our particle filter. To make sure we increment it correctly, I will give you some example data that you can check. In our first part, I've prepared a lot of software for you, basically copying the old particle filter software over, and removing the motion and the measurement model for now. In this I just want you to practice the motion model. We assume a length of the road of 20. We initialize the road with this length parameter, and for this first iteration we assume no steering noise and no distance noise. I set the robot to (0.0, 0.0, 0.0) in the beginning, and then I cache away a number of motions, The way to read those is this robot is moving by 10 total with the steering angle of zero. Then it moves another 10 with a steering angle of pi divided by 6. Then it moves 20, again with a steering angle of zero. A pi over 6 is a left steering, so the robot should change its coordinates in the beginning just in the x direction, because it's facing an x direction over here. Then turn left a little bit, go forward, and go straight again. Scrolling down a little bit, I also give you the code to run the robot. We've created the robot here. You print the initial coordinate. Then for each of the motions in this list over here, we apply the myrobot equals myrobot.move command, and we print out the successive command. If you get this right, these are the numbers I would like to see. Initially, the robot position is 0, 0, 0, 0. That's just the one over here. It's out first print command. It then moves forward in the x direction by 10. The orientation stays 0 and so does y, because there's no steering. Now we steer. This affects x. It doesn't quite move 10. It only moves 9.86. In the y direction it only moves 1.433, and its new orientation is 0.2886. Then we move straight again, and now the x coordinate becomes 0.3903, y coordinate becomes 7.12, orientation 0.28. Your code should output exactly the same values also over here. Just to give you a second test--this is a sequence of 10 motions where the robot moves 10 forward and always turns right but an angle of 0.2 in radians. We look at the outputs we get the following array. You can see that the orientation starting at zero, which is the same as 2π, decreases all the way to 5.26, and you can also see that the robot starts running in a circle whereas initially we add almost 10 to the x direction and almost nothing in the y direction. As you come down here, we subtract quite a bit in the y direction, because now the robot is going in a circle. You should look at these numbers over here and see if your code matches these exact numbers that my code outputs. Here's a function "move" as a class function of the class robot that implements where I get my motion vector, and the motion vector is defined to be steering first, then distance. I have a few error checks here to make sure the steering doesn't exceed the max steering angle, and the same is true for distance. I want it to be non-negative. As I go down, I now implement the motion model. Let's just look a little bit more. I make a new robot copy as in my sample code in class. I copy all the narrowing parameters--length, bearing noise, steering noise, and distance noise. Nothing surprising here. Here I'll apply the noise, which you don't need it for the first implementation, but later on as we go on, you need it to actually add noise. I just add Gaussian noise with the corresponding steering noise and distance noise parameters. If I set the mean of the Gaussian to be the steering command and the distance command then this adds noise. I could have equally written steering plus random.gauss, zero, comma, and then the noise parameter. As I go down further, here is my execution of motion. My turning angle, I called "turn." This is the tangents of the noisy steering times the distance moved divided by the robot length. As in my explanation of this question, I'm going to branch and see if my turn is significant enough. It's smaller than tolerance, and tolerance was set about to 0.001. Then I just model a straight motion. I get my new robot coordinates by the old robot coordinates, moving in the orientation of the robot--cosine for x and sine for y. I increase my orientation by turn, which is likely essentially zero. In case I go beyond 0 or 2π, I do the modal operation here just to make sure my angles are nicely between 0 and 2π. The more interesting case--as we go down in this program you can see that I now calculate the radius as the noise distance divided by turning. Then I find the center of the circle around which I'm turning, using the exact same math I just gave you. I now first change the orientation to be the new orientation by adding turn to the old orientation, modal, or 2π. Then I plug the new orientation into the sine and cosine argument, multiply by radius, add to the center of the circle to get my result. This routine over here gives me exactly what I wanted. Now I want you to implement a measurement model, using the function sense, that is more characteristic of what's often in the literature on robotics. Say we have a robot and we have a landmark, then the robot can measure their bearing or angle to this landmark relative to its own local coordinate system. Whereas before we measured ranges or distances, now we measure bearings or angles. We assume in the world there are 4 landmarks-- L1, L2, L3, and L4. All of those are distinguishable. The measurement vector is a set of 4 bearings that correspond to those four different landmarks. When you implement this, I recommend you use the function arctan2, which is the generalization of arctangent that takes as input delta y and then delta x. Arctan 2 would give you the orientation of a vector in global coordinates. We then have to adjust for the fact that it's relative to the robot's local coordinates, which is done by subtracting the orientation of the robot. This should give you the implementation of a bearing to a landmark. With this implementation I add a variable called "bearing_noise," which you probably already used because it was already referenced before. I set it to 0 just so that we have no noise, and you can just your code. We initialize the robot coordinates as 30 and 20. Motions are now irrelevant. But as I go down, I now give the following two lines of code. I print the coordinates as before, and I print the measurements. The robot is at 30/20, and the bearings for these landmarks will be 6.00, 3.72, 1.92, and 0.85. My question for you is can you implement the software the measures those bearings. If I changed the initial orientation of the robot to be pi over 5, I now get my new robot coordinates over here, and my measurement vector outputs me very different values. That's because this robot is now rotated and therefore all the bearings to the landmarks do change. It's 5.3, 3.1, 1.3, and 0.22. Implement a measurement function that gives me exactly those values. Here's my implementation of sense, the measurement model, as a function in the class robot. I produce a vector called Z, which I return in the end, which has 4 bearings. Then I go through all my landmarks, and you already have the landmarks in your code-- --there's 4 of them--and I use the atan2 function, which is the mathematical function for computing the angle. It takes the y value as the first argument, and the x value as the second argument. This is the local angle of a landmark relative to the robot coordinate. Because the robot has a global heading direction, I need to subtract this to get my bearing value. If I were to add noise, which is a flag over here, then I just produce a random noise adding variable. This is something you shouldn't have implemented, but you need later as you implement the noise. Of course, I make sure that the bearing is normalized between 0 and 2π. I append them to the list and return it. Now, in our final programming exercise, I want you to put everything together and build a particle filter. I'm supplying you with code that has as gaps pretty much the 2 functions you just programmed--move and sense, and some additional code that I copied from class-- the particle filter code that you're familiar with and also code that helps you test your routines so you can make sure they're correct. The key new thing you have to do is you have to work on the noise. There is now bearing noise and steering noise and distance noise. The code that you wrote didn't have any of those. I want you now to modify your procedures to accommodate this noise-- steering noise, distance noise, and bearing noise-- and all of it should be Gaussian. Let's go all the way to the end. There are two test cases. The first test case, which are uncommented so we can run it. What this is is it creates a sequence of robot motions. At each of these time steps the robot turns a little bit and moves forward. It also has 8 measurements, which are the bearings to the those 4 different landmarks. If I go up a little bit in the code, then you'll find that the ground true final position was 93, 75, and 5.2. When I run it, it runs the routine particle filter with those motions and those measurements as an input. It produces an estimate, which is 94, 71, and 5.2, which isn't exactly the same as up here, but it's close. This is a particle filter working. I'm supplying quite a few functions for you. You should look around. One is called particle filters. That's exactly the same code we used in class and constructed together. I just copied it over, so if you look through this you'll find, hopefully, no surprise here. I'm also supplying you the measurement probability function, which is part of implementation. Lets just go there. Here is the measurement probability function. There is something non-trivial here. I compute the predicted measurements, and then I compute a Gaussian that measures the distance between the measurements passed into the routine and the predicted measurements computer over here. That's all happening down here. Here's my Gaussian function with the exponential. Then I return my Gaussian error. There should be no surprise here. What's important is a little modification to the sense function that we haven't seen before. I can now give the sense function a parameter, and I give it the parameter 0. It switches off the noise model, so you will need the noise model for forward simulation of the robot, but you don't need it for computing the probability of the measurement. It augments your sense function to have a flag that if it's set to 0 it switches off the noise modeling and gets you the predicted best possible measurements. What you have to do is you have to find the part in the code that says "only add/modify code below here." You have to copy over your move function and then work in, as it says in the instructions, the steering noise and the distance noise and it's Gaussian--I hope you know how to do this. Then you also have to plug in the sense function, and you also have to plug in bearing noise and make sure there's a flag that allows you to switch off the bearing noise. It should be an optional flag, so it has to have a default value of the bearing noise being on. Otherwise your code won't run. Here is how we will test your code. If you go to test case number 2, then I wrote a few extra functions for you that allow you to test your particle filters on many, many instances just like the ones we were using for testing that are all randomly generated. Let me just go through that code line-by-line. Our test case will be 8 steps long. There is the same motions vector we had before of a slight turn on the circle. "Generategroundtruth" gives us a sequence of measurements and a robot position that we can split as follows, using a robot simulation. Then you run your particle filter over here, and the function "check_output" down here compares the final robot position, the ground truth position, with your particle filter position, estimated position, from here and gives us a single flag whether this is all correct. Let me just do this. We generate a robot that finished with final location 20, -29, and this orientation over here. The particle filter came up with 22, -31, and 0.14, which is close to the original. My code check said "True." Let me run it again. Different values--still true. Run it again. Different values--still true. Now, it could happen that the code check says "False." I just ran it 20 times, and it said true for me every single time. But I've seen it say "False." The reason is it's a randomized algorithm. It's a particle filter. It might actually not have a particle at the right place. So when we test your routine, we're going to code our own code check, check_output. We have our own function for this, but we're going to run it multiple times. If you get it wrong once it's not a big deal. In summary, you will have comment out all the test cases again. All you have to do is supply the missing function. You can test the correctness yourself. You can basically grade yourself with this test case over here, but when you submit it, have those commented out, because we have our own test software. All we're going to test is whether your particle filter gives us a good estimate when we choose randomly the initial position of the robots, measurements, motions, and so on. So to implement the full particle filter, the only thing is really missing is the measurement_prob function. And that's a little bit more involved because I have to really compare what the exact measurement would be for any ove, overt particle. And what I sensed and compute the probability correspondence between the correct measurements, and what I sensed over here. To do this, I calculate predicted measurements using the sense function. Here comes a little flag that I defined. If I set it to 0, than the sense function acts noise free. Which is what I want, it could be the measurement model. But even we you left this out, you're going to get a fine answer on my opinion. But that makes it a little bit more accurate. So that allows me to compute the exact bearings of the landmarks for my particle. And then I can compare these correct bearings called predicted measurements with the ones that I received. Now do this, down here, in the compute errors routine. Where I go through each measurement and in two steps, I calculated the error in bearing. First, it's the absolute difference between my measurement that I observed, minus the predicted measurement, and there's an i at the end over here. Let's see if you can see this. Right there. And this difference might fall outside minus pi plus pi. So this line over here just brings it back to the smallest possible value in this cyclic space of 0 to 2 pi. So adding pi, adding more load 2 times pi and I subtract pi again. So this gives me a value between minus pi and plus pi. I then pluck this error_bearing into a Gaussian. And here is my Gaussian where I squared it, I divide it by my bearing-noise squared, complete the exponential, and use my normalizer to strictly speaking of, don't really need for the implementation, I can safely omit it because weights are self-normalized. But I left it in, so it's actually really a Gaussian. And I take this Gaussian value and multiply it up into my error function. So for each of the measurements, I multiply in one Gaussian. And the final Gaussian is my importance whether I return in this function over here. So this is not easy to implement. I hope you got it right. Scrolling further down in my code, I now implement the particle field as follows. It uses a thousand particles. And this is exactly the same routine we had before, where we generate our initial particles. Here, I set the noise for these particles, to be bearing_noise, steering_noise and distance_noise. I don't comment out the measurement generation step, I just take the input. And then as I go further down, I just run my particle theta. This is the exact same code you're familiar with. There's a motion update, there's a measurement update, and there's a resampling step over here. All those are the same as before. And at the very end I just print the result of get_position. So if I do this for my example, here is the position I get. And I guess for, I forgot to uncommon the Robot coordinate over here. But if you look at the values over here, 7.0 is about the same as 8, 49 is about the same as 48, and 4.31 is about the same as 4.35. So this particle filter, clearly does a pretty job in estimating the forward position In homework 3.4, you're asked to simulate circular robotic motion. We gave you some equations to help you along in your simulations. I want to give you those formulas again and explain where some of them came from. The first equation I want to talk about is this one. The radius of curvature is equal to the length of the vehicle over the tangent of alpha where alpha is our steering angle. Let me write that up here. So where does this equation come from. To derive it, the key realization is that the front and rear tire do not travel along the same circle. Here's my rear tire, and here's my front tire. They are, of course, separated by a distance that we called "L." Let's draw the circles that these tires travel along. Well, this rear tire is actually going to travel along a smaller inner circle while this tire is going to travel along a larger outer circle. Since we defined our radius of curvature as the distance from the back tire to the center, Let's label this r, and we can see that the line connecting these tires defines an axis, and here we have our steering angle, alpha, from here. Now we can do a little bit of geometry. Let's make a right triangle. Well, if this angle here is alpha, then this much be a 90 degree angle, because a radius intersecting with a tangent line always forms a right angle. That means that that this angle here must be equal to 90 degrees minus alpha, which means this angle, since this is a right triangle must be alpha. Well, we're almost there. The tangent of this angle is equal to the opposite side, which is the length, over the adjacent side, which is the radius. So tangent of alpha is equal to L over r. We manipulate this equation a little bit, and we find that the radius of curvature is equal to the length of the vehicle over the tangent of the steering angle. For homework 3.4, we also give you equations for the position of our robot. Let's start at the center coordinates of our circle, because our robot is undergoing circular motion, and we can look at the (x, y) coordinates of our robot before the motion. Now, the question is given these (x, y) coordinates and the robot's orientation angle theta, how can we find cx and cy. Well, we just define a right triangle, and we see that this angle is theta. We can show this angle must also be theta. This length is R, so that means this horizontal distance is R times the sine of theta, and the vertical distance is R times the cosine of theta. That means that cx is just equal to the initial x position minus this extra distance, R times the sine of theta, and cy is equal to the initial y position plus this extra distance, R times the cosine of theta. Okay. Now let's let our car advance by some turning angle beta. This angle is beta, and here is our robot car. Let's call these coordinates x-prime, y-prime. How can I get an equation for x-prime and y-prime. Well, we can see that this total angle here is equal to beta plus theta, and just as we defined a right triangle before, we can define another right triangle where this line is going to be R times the sine of beta plus theta, and this line will be R times the cosine of beta plus theta. So working from our center point, x-prime is going to be equal to cx plus this extra distance, which is R times the sine of beta plus theta, and the y-prime we can see will be cy minus the extra distance of R times the cosine of beta plus theta. Theta prime, of course, will just be equal to our old theta plus the turning angle. We can't forget to make the mod 2π. Good work. Welcome to homework assignment #4 in CS373. To remind you, we covered A-star and dynamic programming in class. Let's start with an A-star question. We learned in class that we can use heuristics, and a heuristic is a admissible if the heuristic value is no larger than the actual cost it takes to get to the goal. In a maze, we know that the number of steps in the maze is a good heuristic, because obstacles will make the path only longer, not shorter. Consider a maze of size 3 x 3 and say there's a single goal state, and the cost of moving is 1. Is the function shown here admissible or not? Please check one of the two radio buttons. The answer is "yes." In this part of the state space, we are obviously entering values that are too small, but the heuristic is admissible if h(x) is lower than the cost-to-goal. It would have been inadmissible if the values over here were much larger than the values we want them to be. Here's another heuristic function for a 3 x 3 maze. Assume again the cost is 1, and it's our sole goal state in the corner over here. Tell me is this admissible? Please check one of the two buttons here. The answer is "no," because these numbers over here are too large. Why is this bad? You can see that in a maze like this you would expand node over here before we touch any of the nodes over here. If those are an optimal path, then we might miss them and pick what might end up being a suboptimal path even though in this specific case all the paths seem to be equally good. Let me ask a more general question now. What may happen if h, the heuristic, is not admissible? A-star finds the optimal path always. A-star may find a suboptimal path in some cases. A-star may fail to find a path even if one exists. Or none of the above. In answering this question, it is important to say this is a statement that always applies, and these statements suggest they just apply sometimes under certain circumstances. There was only one correct answer, which is this one over here. The inadmissibility of the heuristic might lead down an exploration of the state space that leads to the goal on a suboptimal path. Let me demonstrate this to you. Here is a world. Suppose our start step is here. Our goal is here. The heuristic is super large for those states over here and smaller around here. The nodes will be expanded following the zeros around here and around here, and the first time the goal is set occurs when we reach the point over here. We never discover the shortcut straight from S to G because of the inadmissible heuristic. That renders point 2 correct, and as a result A-star will not always find the optimal path as I have just proven. Now, it turns out A-star will find the path if one exists. It might find a suboptimal path, but eventually it expands all the nodes that can be reached. If the goal state is among them, it'll succeed to find a path. Therefore this answer here was wrong. Of course, none of the above is also a false answer. I now what to quiz you about dynamic programming. Consider the following 3 x 3 world with a goal state in the corner over here. Say the value of the goal state is defined to be zero. The cost of motion is 1. Quickly fill in the values for all the other cells over here, assuming you can either move straight or diagonally. In both cases the cost of motion is 1. The answer is 1 over here, 2 here, 3 here, 4 here. This one also is resolves to 2 and 3. [Sebastian:] Let me talk about dynamic programming with stochastic actions. At the end of this assignment, you'll be able to program this. The motivation to study this is as follows: suppose we have a robot, and we have an obstacle and a goal state. Then this robot might be tempted to scratch the wall over here and head towards the goal. In practice this is a bad idea, because robots that get really close to obstacles are frightening. In fact, if their actions aren't quite perfect they might actually hit the obstacle, which is no good. In practice, what we tend to do is we go more like this. We maintain a certain clearance to obstacles. Now we're going to modify our dynamic programming planner to do just that-- to find paths that are a little bit safer and lead a little bit away from obstacles. The way we will do this is we will model actions as stochastic, and we have not done this before, but you'll learn how to do it. Take the action of going north from the from the cell over here. In a deterministic action, it obviously succeeds, unless of course we run into a wall. In the stochastic case, it succeeds with a certain probability--say 50%-- whereas with 25% chance, it might accidentally go left or right even though we commanded it to go up. This is a stochastic action, and it's exactly the type of action I want you to program later on. If, for example, there was a wall over here and the robot decided to go up, there'd be a 25% chance it would hit the wall, which is not good. So staying away from the wall is a good idea in a stochastic situation. I will now show you how the backup works without asking or quizzes. Suppose we study the action of going up over here, and we have a 50% of success. Suppose at the time we do the backup, the value over here is 10, 20 over here, and 40 on the right. Then the value of the state for the action "go up" would be obtained as follows. It's a 50% chance times 10, 25% times 20, and 25% chance times 40 plus 1, which is our motion cost, which together gives me 5 plus 5 plus 10 plus 1 equals 21. That is the backed up value over here for the action go up, and you can do the same with go right, go left, go down. The interesting case in your implementation is the one for an action that either runs off the grid or into a wall. Let's study the case of going north from this cell over here where there is a 25% chance of falling off the grid. Then the value for the cell and the action of going north is 50% times 20, which is 10, 25% chance times 40, which is also 10, and then we have this case where we fall off the wall. Let's define the penalty for falling off the grid or for hitting an obstacle as 100. So we add 25% chance times 100 plus 1, 1, hence the new value is 10 plus 10 plus 25 plus, which is 46. I want your update to produce for the cell over here 46 for this specific action of going up. Of course, the value might be smaller because there might be a better action like the action of going right. But for this specific calculation you should receive 46. Your programming assignment looks as follows. I will give you a grid. In this case it's a grid that's entirely unoccupied. As before, I give you a goal state and a set of actions. I want you to program dynamic programming all the way to the end, so when I run it, the values I get are 0 for the goal state, 37 for the 2 adjacent states, and 44, 16, and 63. If I go right to the goal, there is a 50% chance of hitting the goal, in which case my cost is just one, which is the cost of motion. There is 25% chance of running into the wall, which costs me 100 plus, of course, 1 for the robot motion, and there's a 25% chance I find myself down here, which costs us 44. If I put this into mathematics, we get 50% chance for reaching the goal, 25% for hitting a wall, which costs me 100, 25% for going south, and the value here is 44.77 plus the cost of motion is 1. If you add this all up, you get exactly the 37.193 that this value over here constitutes. After conversions for this grid, I want the value function to look just like this. Here is the corresponding policy. If I modify the grid to insert an obstacle over here, then I get a value function like this. One thousand is my initial value for each cell. It's unmodified for the obstacle cell, and these are the values I receive-- 94, 86, 73, and 44. You can check whether these values are correct. Let me pick a slightly bigger grid with a goal position in the top right corner and two obstacles down here--size 4 x 4. Here is my output for the value function. I can read those values. They happen to be 1000 for the two obstacles. What's remarkable is the policy. When I'm close to the wall, you can see that I'm being pulled away from those wall elements to stay in free space. I will eventually reach the goal, but I never am willing to drive in a way that crashes me into a wall if it's not avoidable. What you have to do with the code we provide is to modify the dynamic programming routine that assumes a deterministic action with the one for a stochastic action. Specifically, what this routine should do is it should incorporate the probability of successful action and the collision costs. For example, if I modify the probability of success into a deterministic function, then my value function will look as follows. It's just a number of steps in the usual way--1, 2, 3--as before, and the policy drives me straight to the goal as shown over here. In your final submission, please exclude any printouts so we can modify the grid and these values for success probabiilty and collision cost, and we can see what your code generates. In the initial value function, please initialize your value function with 1000. Here is my solution. As I go through all different actions a, as before, I now create a new inner loop of going through different action outcomes. This lists is (-1, 0, 1), and I set the actual outcome to the adjacent action in the action list. You might remember the action list is a list of different outcomes. By incrementing it by 1 or decrementing it by 1, I can pick a slightly different action in that list. Of course, I have to do the modulo 4 on the right side. Then the limitation is similar to before. I project the outcome into new coordinates--x2 and y2. Now I need to assign the probability with this outcome where if they modify a 0, we take the success probability. If it's not 0, we take 1 minus that divided by 2, because there are 2 possible undesired outcomes. Then the test proceeds by checking whether this is a legal grid cell, it's inside the grid, and the grid value is 0. Then like before, I add the value of the grid cell by now multiplying by the probability of that specific action outcome. Otherwise, I do the same for the collision cost. Finally, I take my cumulative value of v2, which I initialized with the cost of motion. You can't see this right here, but it's filled up. I update my value function just like before. You can see the quote over here. This is what you should have programmed. The key difference to our example in class is the inner loop over here where I go over different possible action outcomes, compute the actual action outcome, and then do the probabilistic addition of these outcomes rather than just studying one outcome. Welcome to homework 5. In the first problem, you're going to build some intuition for what happens to a robot's path when one of the these parameters, either toe p, toe d, or toe i, is set equal to 0, so if we don't include that term in our steering angle equation. In each of these 4 cases on the left, the black horizontal line is the intended track for the robot, while the pink represents its actual motion. In each of these cases, the robot has a 10 degree positive drift and starts with a cross check error of 1. For each of these 4 cases, you should go through and decide which of these radio buttons is the correct selection. So for example, if you thought that this was appropriate robot motion and all the parameters were tuned correctly, you would select no problem. Keep in mind that each column should be selected exactly 1 time. Good luck! This was actually a really tricky question. So let's, first, mark down the answers, and now we can talk about why. So these were the answers and let's think about why. We have some really interesting behavior over in some of these graphs, and it's nice to think about what's causing that behavior. So let's start with number 2. Pretty straight forward. The robot does exactly what we want. It starts off away from the track, then it approaches it, and stays right on the track. That's exactly what we want PID to do, so this is no problem. Number 3 looks similar to number 2 but not quite because now you can see the line that we're approaching is not actually the track. That's because we're reaching the steady state where the robot's upward drift is being balanced by its downward tendency caused by this term, and though we reach some sort of steady state, it's not the steady state we want, which would be right on the track. The key to understanding number 4 was realizing that the robot starts by moving upwards. This is the only case for the robot started going up, and what's happening here is the drift is dominating since the toe p is equal to 0, there is no initial term that steers the robot towards the goal, and without that, the drift is going to take over. Of course, as we accumulate error that gets corrected for and we steer down, but eventually we get into this sort of oscillating behavior. The oscillations are not as frequent, however, as they are in case number 1. If we think of this differential CTE term as a sort of smoothing term, which damps out these wild oscillations. Then we can see how, if tow d is equal to 0, we would expect these sort of growing oscillations, and this behavior is characteristic of any PID controller where the differential term is too small. Welcome to homework assignment #5. Today I only have programming assignments. One on smoothing and one on control. In both cases, we are trying to drive a car on a racetrack as fast as we can. Okay? You'll see in a minute what I mean by this. But think of the car going on an oval just like that. So in this first exercise, you're going to go through a sequence of improvements of the code smooth.PI. I'm giving you a path that is very much a box path. It starts at 0,0, cranks up the estimation all the way to 6, then increases the y all the way to 3. It goes down on x all the way down to 0, and then uses the y all the way down to almost 0. If I draw this, the way this looks like-- we start at 0,0, all the way to 6,0, then we go up here to 6,3, and you go left to 0,3, and down to 0,1. This is a cyclic course so we're smoothing before what's for an open path. I'd now like you to modify smoothing, so it can smooth this path over here. So I want you to run the function smooth with path as an input-- the weight_data of 0.1 and the smoothing parameter of 0.1 and tolerance of 0.000001. When you run it, what it should produce is a path like this, that when you plot it, it looks a little bit like an oval. You can look at these numbers carefully. It ranks the corner points more into the interior. I want to do it in a way that is cyclic, so that the last point connects smoothly to the initial point over here. So please modify the code just to be a cyclic smoother as opposed to before it wasn't a cyclic smoother, and make it so each point get uprooted. You don't keep 2 of the points fixed. So here's my solution. I modified the code in trivial ways. I go through all the i's as opposed to leaving out the first and last node, and because we are cyclic, I now have to add the model of division by the length of the path in this smoothing parameter. That's it. So it's a very small modification. I don't have a graphical interface, but what the points look like is very much like an oval, like this, after the smoothing. It actually put all endmarks a little bit, which is a bit of a concern, which we address in a minute. [Thrun] So in this next assignment, I'd like you to fix certain data points. Just as much as you fixed the data points originally in our path, the beginning and end point, now I want to fix you 4 points. These are the corner points in our data points. In our original data, it's the one over here, the one over here, and I want those to be fixed so they can't move. And the way I indicated this in the code is by giving you a fix array that looks like this where each of these numbers corresponds to exactly 1 data point. There's as many elements in fix as there is in path. And then when I call smooth, I add fix as a parameter. So when you change the procedure and run it, I can tell you you should be surprised by the output in that when I run my procedure it copies over the initial path into the output. We'll talk about this in a second, but please go ahead and change your procedure so that your output ends up to be giving you something very much like this. The modification is simple. As I go through my data points, I only apply the update if it's not fixed. So if the fix flag is not set, they go in here. This is all indented one compared to before. It's a single line change. Let me tell you why this doesn't work. Consider a few data points like these. If you fix this data point over here, then this guy is perfectly happy with his 2 neighbors-- this is a smooth path-- and so is this guy over here. The only data point that's unhappy is the one over here because it's not part of a very smooth path, but this one is never being updated. So as a result, all the other ones stay where they are. They already have the absolute minimum for smoothness and for the data fit. No updates take change, and we don't have the desired result. We don't get a nice round curve the way we wish to get it. To get this, we are going to modify our rule a little bit now, and that's the interesting part. For a node like this with coordinates xi and yi, we add to it with a very small constant gamma-- in fact, it will be half our weight smoothing constant in a minute-- 2 times the previous guy-- and of course that's cyclic, so you have to make sure this is really cyclic-- minus the guy 2 steps away and minus our node. Why this makes sense we see when we go into the opposite direction. Here we add a small gamma, 2 times this guy over here minus this guy here minus our original data point. And behind this is a certain desire. We want the vector over here to be the same as the vector over here. This vector is this difference over here of xi + 1 over yi + 1 - xi over yi. That is this vector over here. And the vector over here is similarly the difference of the point i + 2 minus the same for i + 1. If you set these to be equal and bring yi to the right side, then if you modify yi in proportion to this expression over here, we reduce the error. In fact, the expression in the brackets is the same as this one, called (A), minus this one, called (B). So we're looking at the mismatch between these 2 vectors and use it to adjust the xi factor over here. We do those sequentially--this one first, this one second. You might call them a little bit sloppy because the first update already inferences the second update. But who cares? We're just going to add lines of code that achieve this over here, and when you do this, please make sure that your update understands the fact that these indices are cyclic. So I did this in my code and I'm running it, and out come the following numbers that you can read over here. These numbers are indeed a cycle. So if we were to plot them--and this is the original data-- these are the points we would constant, then the new data lies in over very much like this. You can plot them. Unfortunately, I can't plot them in our environment here, but you could see this. Now we are not shifting the racetrack inbound anymore. Please go to your code and modify the function smooth by doing these new constraints. For the update strength, I suggest you use half of weight_smooth for each update. So in total, you add another 0.1 as update strength. And here is my solution. These are two update lines. They’re basically identical except one goes in one direction and the other direction. Let’s look at the first; we have an update weight over here. We add two times our new constraint with the index I minus 1. This is the subtract part. We subtract I minus 2 and we subtract the original on the right side over here. It’s exactly the math I’ve given you. Same over here. If you won this math , you get this new smooth curve. I should warn you, this can be unstable, it can give you results that are very poor you have to adjust these parameters very carefully and this is one of many ways to smooth. There is many other ways to smooth, but the nice thing that you achieve here is that you get actually smooth two points that they are holding fixed, which is actually really, really useful. [Thrun] So here's my second piece of software assignments in controls. In the class we talked about how to make a car follow a straight line. We used the line x = 0 where the crossing arrow was defined as the y difference between this axis. We just went off the y value. So now I want to make a more interesting course, a cyclic course, a racecourse. Here's my racecourse. It has the radius r, which you can set. The way I'd like to define this racecourse is through a half cycle where we use r, same half cycle over here, and the stretches in between I want to be 2r long. So for example, if this radius equals 25 meters, then this would be 50 meters. The whole thing would be 100 meters, and this measurement would also be 50 meters. I want you to program it such that the initial car is stationed right over here, pointed upwards, and then it drives onto the racecourse like this, all the way around infinitely often. The key in doing this is going to be to set a function. I have already modified for you the function run from our control class to use that crosstrack_error with the parameter radius, which we're going to set to 25, but I can pick a different value in my testing. And then I have modified a little bit the update over here to maintain the differential and the integral crosstrack_error, and here is our steering control law that you are familiar with. Instead of twiddle I'm just going to give you parameters, 10, 15, and 0. Those work fine for me, and they're actually the result of running twiddle without the integral part. And then when I run it, I get my output. And here it is. The crosstrack_error by and large is very small. You can see the steering tends to be on the negative side. Here we're steering at the first turn, here we're on the straightaway. You can see this by the numbers. Here we go into a turn again, so the turning becomes larger. In all of this you find the crosstrack_error to be relatively small, about 0.1 or so--not very much--and this continues and continues. The final crosstrack_error for the second half of the race is 0.005. So you want this number to be really, really small. The tricky part is when you code this up and code the function cte, you need a different branch for this area here, for this area over here, this area over here, this area over here. Keep in mind that the robot is going to go in a cycle. So it's going to traverse this one in the opposite direction with this one over here. So good luck coding up the correct crosstrack_error function. [Thrun] And here is my solution. We have 4 cases. On the left side, if I'm on the left side of the racecourse, as defined that x is smaller to radius, then my crosstrack_error is defined by the distance to the circle, centered at radius comma radius minus the radius itself. So this is going to be 0 if I'm exactly on the circle. If I'm more than 3 times over to the right side, I get a circle again. It looks like the one before, but now the center of the circle is a little bit further to the right by 3 times radius as opposed to 1 radius. The rest is identical to the line over here, so I'm subtracting the same radius on the right side. You can't quite see it but it's there. Interesting are the straightaways. So if my y value is large on the upper part of the diagram-- in fact, it's larger than radius--then my crosstrack_error is the y coordinate times 2 times the radius, which is the height of the racetrack. If I'm down at the bottom, it's just the y axis, but really important is the minus sign because I'm moving in the opposite direction. So for your code, to run this correctly you would have gotten everything in this routine right. So that finishes my homework assignment. Congratulations. You were able to make a car drive on a racetrack. That is actually quite a significant component of making cars drive. The PID control methodology that you learned today and the smoothing methodology are really, really essential not just in controlling self-driving cars but in a great number of other control setups. Thank you for taking the class so far. You learned a lot already. You learned about localization, about planning and control, and we almost learned talking about how to build a self-driving car. So we saw a lot of confusion in the forums about what was going on with this gradient descent. The equations that were given to you were presented as a black box, and I'd like to open up that black box and see what's going on inside. For those of you who looked at Wikipedia to try to understand what's going on with gradient descent, the first formula you would have encountered was this one. This equation describes gradient descent. Now this is a little intimidating at first. What do all of these parameters mean? b = a - gamma x the gradient of the function of valued at a. Let's figure out what's going on here. Before we get into gradient descent, let's first answer the question, what is a gradient? To help me answer this question, I'm going to get the help of my blindfolded hill climber. Here's our blindfolded hill climber, and he's climbing this rounded hill. He wants to get to the top, and he wants to do it in as few steps as possible. What is his method going to be? Well, as long as the hill is steep, he knows he can afford to take a really big step because he's still not really close to the top. As the hill smoothes out and becomes more level, he's going to want to take smaller and smaller steps because he's going to be afraid of overshooting the top and winding up on the other side of the hill. How can we describe this mathematically? The way we're going to do that is by using the gradient, and when we do that, our climber is going to get to the top of the hill. Let's look at this hill from a top-down view, instead of from the side. I'm going to draw these isocline, so these lines all represent lines of the same altitude. We can see they come together. They're really close together in the beginning and become further and further apart as we go inside, and that's because the hill is getting less and less steep. So at any point, the gradient is a vector. If I took the gradient at, let's say, this point. I would get a vector pointing in the direction of steepest descent, and the length of that vector would depend on exactly how steep the hill is. So at this point, which maybe corresponds to somewhere down here on the hill, the hill is quite steep, so our vector is quite long, and it points directly uphill. If I was over here and taking the gradient, I would still be pointing uphill, but the vector wouldn't be quite as long now because now we're up here, and the slope isn't so great, and with reduced slope comes reduced gradient and a reduced step size for our blindfolded hill climber. Gradient descent is very similar to blindfolded hill climbing. Now instead of climbing up a hill, we're climbing into a valley and trying to find the bottom of it because we want to minimize a function. That minimization is encoded in this - sign so instead of adding the gradient, we're going to subtract it. Let's talk about what each of these variables mean. A is our current position. If we were the hill climber, it would be right here where he starts off. B is going to be his next position so this is how the hill climber decides where to go next. This gradient term tells us the direction of steepest ascent. This - sign flips that around. It says okay, let's actually find the direction of steepest descent. This gamma is just a waiting factor. That's all this formula is doing. It's saying, let's take a step down the hill and use that as our new equation-- our new position. Sorry. Now to elaborate on this, we're going to need to use a little bit of calculus, so if you don't know calculus, don't be intimidated. Try your best to follow along, and if you can't, that's okay too. I think this captures the essence of what's going on with gradient descent. First, we have to figure out what exactly is the function we're trying to minimize. For the purposes of the lecture, the function we're trying to minimize was a function of yi, and it was equal to some waiting alpha x xi - yi squared, + some waiting beta x yi - yi +1 squared, so the next y coordinate, and we also do the same for the previous y coordinate, so yi - 1. We want to minimize this, and we're going to use gradient descent. Well, gradient descent says that, we should just iterate over this process until we get to a sufficiently shallow slope that we're confident are in the bottom. We've really found a minimum. So at each step, we're going to follow this. B, our new location becomes yi prime. That's going to equal our old location, yi, and then - this gradient, and the gradient here is just going to be the derivative with respect to yi. If you don't know calculus again, don't worry about it, and if you do, this is a fairly simple derivative. I'm going to writie it simplified. I'm not going to show you all the intermediate steps. But once we do this out, we find that yi prime = yi + 2 alpha x (xi - yi) + 2B(yi + 1 + yi - 1 - 2yi). This almost, almost looks like the equation that you were given in class. There's a little problem, and that's these 2s. I'm going to do something where I just erase them. You may be saying, hey, you're not allowed to do that, but let's just pretend I'd originally called these parameters alpha over 2 and beta over 2, then everything will work out just fine. That's gradient descent. That's where this occasion comes from in our update step. This would be simultaneous update. Another good way of using gradient descent. It's not how we implement it in lectures, but that's okay. I hope this was helpful. Good luck! [Narrator] Hi class, this is homework assignment #6 on SLAM, and we're going to practice some more questions about the graph SLAM algorithm, and I will ask you something new about how to make graph SLAM more efficient towards the end of this homework assignment. There's 3 questions and 1 new thing, which comes with additional questions and a programming assignment. Let's dive in. In this example, picture a robot that starts at initial position 5. This robot which lives in 1D, even though I'm drawing this in 2D, sees a landmark with a relative measurement of 2. It then moves to coordinate X1 by taking a step of 7. It now sees a different landmark, L1, with a measurement value of 4. Finally, it moves again with a motion 2, and now it sees the same second landmark, not the original one,at a value of 2. Obviously, when I work it out and solve this, you find that the best coordinates over here are 12 and 14, with landmark coordinates of 7 and 16, but none of the numbers I just told you will be inserted in the question I'm about to ask. Picture the matrix omega and the vector C. I want you to add all these constraints into omega with a caveat that for the initial constraint I assume the strength is just 1, so enter it the way you're used to. Now let's assume the motion update has a sigma of 1, but the measurement update--the sigma for measurements has a value of 0.5, and remember we weigh those updates with 1 over sigma, which in the measurement case would therefore be 2. I want you to fill in the values for sigma and for C for this specific example. I gave you some of the sigma values and some of the C values, and I want you to fill in the missing values over here. You can check whether you got them correct by verifying that I got those numbers correct, and you can also solve for omega minus 1 times C, and out should come the right positions shown in the diagram over here. So here is the answer, I feed in all the missing values for you. And let’s try to check a few. For example, let’s take the elements related to X1. We move from X0 to X1 which added one of the remainder indiscernible . We move from X1 to X2, which added another one, so we have two, but we also had one observation in X1, we incremented by two because of the measurement string, 0.5. One over that is 2 so when we added the measurement end up with 4. Let’s pick this off tagging element over here. This was only affected when moving from X1 to X2 and it was a minus 1. This element over here was affected when we sensed L1 from X1 and again the off tagging element is negative. So we added minus 2 over here. It’s quite involved, as you can see. Let’s take a number on the right side, the minus 2 over here. This is adding up a re-contribution to X2. The first one occurred when we moved into X2 from X1 at which point we added two . But in X2 we also measure length by L1 with a measurement value of two. We have to multiply that for a two to arrive at four and subtract it, so we have made minus four, you get the minus two. If you work out at all these examples, using the math we explained in class and working in the effective measurements get a default effect of two for every value you add, you arrive at exactly this matrix over here. I hope you got a good number of numbers right that was not an easy task. [Narrator] I now want to ask you the really challenging question over here, and it goes as follows: one of the weaknesses of slam is as we move along and map a world by seeing these landmarks, the matrix omega which is the big thing here and of course the vector C grow linearly with the length of the path. Now this means if you go to your local supermarket and buy a robot, and that robot lives for a long time, say a year or a decade, then this thing here, the path will be a really, really large even though the environment in which it might operate, the map might be of a fixed size, and that means a robot programmed by graph SLAM will eventually stop working because it gets slower and slower. Now, we all know computer operating systems that have that property. The older they are the slower they are, but we're not talking about how to fix operating systems. We're just talking about how to fix SLAM. The crucial idea that I want to tell you about and I want you to implement from scratch in our software is the following: we can actually reduce the map we maintain to one that only contains the most recent position in the path, and all of the stuff over here can be safely erased when we build our map. You don't know how to do it, but let me tell you. Suppose we have a robot position and we have a matrix omega that only contains the information pertaining to 1 position. If the position is 1 dimensional, it's just 1 row and 1 column. Whereas, you might have many different entries for the map. Suppose the robot moves to a new position; let's call it XT+1. Then, we'd do exactly the following: we'd grow the matrix in the vector by using the expand function that you're already familiar with such that we now have space for our new position. The new area we added are these rows over here. It's a single row of numbers of a position 1 dimensional. It's 2 rows of number of a position that is 2 dimensional, and it's this column, again, is just a single column if our position is 1 dimensional; otherwise, it is 2 column, and we initialize those all by 0, and then we can apply our regular motion update, that as you know adds 1 or some number like 1 to the main diagonal and -1s off diagonal, and the same on the right side for the vector C. That is just a motion update but that runs the risk that our path increases. Now we go back to a form like this. We make X2+1 survive. Simplified speaking, you might think about doing this by just cutting out the new sub-matrix that starts over here, and the sub-vector that starts over here; however, if you do this as you can easily verify, that sub-matrix doesn't give you the correct answer, and here is where the meat is. We now cut out 3 sub-matrices or values from the full matrix on the right side. One sits here, that one I will call A. One is over here; it's a single element for a 1D robot, but is a 2 x 2 matrix for a robot in 2D coordinates, and one that I'll call C, and it's obvious to see that this thing over here that you want to cut out is called A-1. These values carry a lot of importance. We can't just erase them, but we can forward them back into the surviving matrix by the following simple operation: we take the surviving matrix to be called omega prime and the surviving vector to be called X prime, and you can get omega prime and X prime by using the function take that is in your own matrix library. You have to look into how to make take take exactly those elements over here, and then if we modify X prime and C prime with the following piece of math. We subtract from omega prime A transpose times B to the -1, the inverse, times A. If we implement this correctly this gives you a matrix of the same size as sigma prime, and that's what you subtract to arrive at our reduced sigma. Similarly you do the same for C. You subtract A prime minus B to the -1 times C. This tends to be the same as Gaussian where we technically do away or we call it integrate away the variable X1, and I don't want to go into detail why that's correct. My book has a multi-page proof of that simple equation. I just want to give you intuition here, which is these values do carry importance, and to get rid of those you have to redistribute them into the remaining variables and that over here happens to be the math. It's A transpose times B times A that you subtract from the remaining omega prime, and the same for C over here. When you do this, you are now left with a matrix of the same original dimension because we first added the post and then we subtracted one, and as you can see when you do this many, many times the final dimension of the matrix is only determined based on the size of the map plus, well, a single entry which in the 1D case is 1 row and 1 column. In the 2D case is 2 of those corresponding to the robot position. That means SLAM scales to really large environments because we can do the trick every single time a robot moves. You asked for a challenging program assignment. I promise you, you will be busy with it for awhile. I'm giving you now my piece of code in which I implemented SLAM for you. You're familiar with this. You have all of that, and then I run SLAM with 3 landmarks, 3 time steps, a world size of 100, and a measurement range of 100. I make data at random as you're familiar with and you'll complete the result, and the result in this case contains a vector of omega C and U concatenated. Then what I might get out looks as follows: there's 3 landmarks. There's a sequence of estimated positions leading up to the actual robot position. They're both correct, and then there's estimates for where landmarks are. Every time I run it, I get a different answer because my landmarks and my world is different every time. Now I also implemented and that's your task now a function called online SLAM. It does exactly what I told you to do. It resizes the matrix every time a new motion occurs and then goes back to the original size, and I've printed out here as an example the information matrix omega and in the vector C that I obtained, and I also printed out the final result. In the final result, we get exactly the same estimated pose as for the full SLAM algorithm which is 86.0 and 33.7. I go down, these are exactly the same number for the estimated pose; that's how you can verify this. The same is true for the estimated landmark. Those coordinates are identical despite the fact that I reduce the size of omega and C, but when I print omega and C, we find that the dimensionality is reduced. It's an 8 x 8 matrix omega, and the number 8 comes because there's 6 coordinates for the landmarks and 1 final robot pose. That is substantially smaller than the matrix I would obtain for the full SLAM case, and the same is true for the information vector of size 8; here's an example. What you are asked to do is to fill the entire online SLAM routine and to do this, every time you get a new pose, you want to expand to grow the matrix by inserting something right behind the existing pose. You then run take to take out the sub-matrix. You also calculate A, B, and for the information vector C, and then as before your reduced omega is obtained with this equation, and your reduced C is obtained with this equation. If you make no mistake, then you get exactly the same area as you had before, and you can test your routine and arbitrary maps and arbitrary data sets, and it'll just be fine. So, good luck; this is a wonderful programming assignment because it gives you the first really scalable SLAM algorithm and when you implement it, it's actually a major achievement. I can tell you it took the scientific feat of SLAM easily 15 years to really discover this form, and ever since what was really complex and involved lots of common failures, and I can tell you lots of headaches, became amazingly easy. So, implement it and you can call yourself a robotic mapper. So here is my solution to the programming assignment where I asked you to program an online version of online SLAM. Let me run it and compare it to the offline SLAM. When I run it, I get random landmarks and a random initial robot pose. My offline solution gives me this long path over here and estimated landmarks, and the remarkable thing here is that my online version that I coded gives me the same final pose and the same landmarks without retaining this huge matrix for the path before. So how did I do this? Here is my online SLAM routine. In large parts it looks exactly like my offline SLAM routine. I do it in 2D – I have a measurement and motion. Step, here is my measurement update. I have to get all these indices right so you can stare at them for a while, but they’re all correct here, there is a plus one and minus one over here. And here is the first nontrivial thing. My matrix so far has one robot pose and one entry for each landmark, but now I need to add space for the next robot pose. And the way I do this is, I make an expansion list using the expand command. And this expansion list retains the original robot pose, which is coded 0 and 1, these are two-dimensional poses, and indices for the landmarks. So, I’m squeezing in two new rows and two new columns for the next robot pose. That’s happening in this code over here. With this squeezed in, I can now do the update. The update is being applied exactly at these two new rows and columns that I put in. And here is the math I gave you for factorization applied to this problem, where I go and compute the intermediate matrices A, B and C that I explained in class. And then I use the take command to kick out the very first row and column, the first two of them, to remove the old robot pose, using the exact same logic that I gave you in class. So you can look at this, this actually implements online SLAM. Welcome to the project phase in this class. We hope you can now leverage what you've learned so far into solving more complicated problems. The problem we've created for you is not an official one. It's called the Runaway Robot Project, and the scope of this project is to chase a robot that is trying to escape. The very first version has full observability, and your task is to find out whether or what is the new status information about his heading direction. So, dive in, read the instruction on top of the exercise, and try to solve the problem. Congratulations to solving the first challenge. That's really, really great. Now it's going to get a little bit more difficult. This time we're going to introduce noise in your measurements, which means you can't really see the X Y coordinates exactly anymore. You see them approximately. Now this is a challenging problem, and the reason is, everything in class that we've talked about so far was linear, but here the motion model is non linear. So when you apply a thing like a common filter or a particle filter, you will be in for a surprise. So give it a try. It might take longer than you think, but I hope you can solve this challenge. Congratulations. You just mastered the second challenge of finding out where the robot is, with measurement noise. Now, we're going to turn the attention to motion, to control. Your mission, should you choose to accept this, is to capture the robot. And to make it a little bit easier, we made you twice as fast, and took away the measurement problem so you can seize the state again. Good luck. So let's turn to the most difficult of all those challenges so far. Which is chasing the robot. And this time, your going to be exactly the same speed as the robot. So running behind it, will not serve you well. Good Luck. Congratulations. You're done doing all four challenges. We have one optional challenge, of putting everything together. Measurement uncertainty and trying to chase an escaping robot with you being just as fast as the robot. There's a reason why I'm not counting this, we're making it optional. It turns out when I tried to program it, it was actually really hard. Was hard for our [UNKNOWN], so it is a real challenge. You might want to try it but we didn't want to make it mandatory. But, try it out and see how far you get. Hey congratulations, good job. I hope you won't dream of robots running away from you, and I hope you learned something interesting. [Sebastian Thrun - Professor CS373, Andy Brown - Course Manager CS373] [Andy:] Welcome to our first office hours. We've seen a lot of really good discussion in the forums. Now we have Professor Thrun here with me--I'm Andy the moderator in the CS373 forum. We're going to divide the questions into the course content questions, and we're going to after that talk about some other questions-- things that are specific to Sebastian's work. First course content question that a lot of people seemed to have questions about was when we're talking about the sensing, you introduce these multipliers called p_hit and p_miss, and you assign values 0.6 and 0.2. People were confused about where those numbers came from. [Sebastian:] This is something I just put in ad hoc when I explained it, and later on they've become the measurement probabilities, but I didn't want to start talking about probabilities without programming them first. P_hit would be what's the probability of, say, being next to a door and really seeing it, and p_miss will be the probability of being next to a door and not seeing it. There is usually actually four of those. There are four measurement combinations-- the probability of seeing the right measurement under there is a door and if there is no door, the probability of seeing the wrong measurement under these two assumptions, but two of them are sufficient for the math. [Andy:] In a related question about your own work, how do you assign these? You said it was done ad hoc in this little example, but in really designing a robot car, how do you decide on what these values are going to be? Is it done experimentally? Is it based on the parameters of the sensors? [Sebastian:] That's where a lot of meat comes in, and it's certainly meat I didn't really cover in class. I do cover it in my book. It depends a lot on the sensor. So if we use a camera, I use a different model than if we use a range finder. We use a lot of laser range finders. If I was using a laser range finder--they measure distances-- and it's not phit/pmiss anymore, it's the probability of measuring a certain distance in a certain location. The very first approximation is physics. You can actually derive what the centrifugal noise in the sensor and characterize those. But when you just use physics, you often have a very poor ??? on the actual uncertainty, so when tuning those parameters, you often do this very experimentally. We go in and try out certain parameters to see which ones work and then often soften them to make them fit the math. [Andy:] The next question a lot of people had was does this method we're using depend on the robot already having a map of its environment? [Sebastian:] Localization does depend on having a map of the environment. Everything I taught so far assumes there is a map. There is an entire field called simultaneous localization and mapping-- or for short, SLAM--that addresses the general problem of acquiring a map at the same time . I haven't covered this in class. [Andy:] Those were two of the most heavily voted course content questions. In a real localizer--I guess you sort of addresses this--but what are we looking for? We're not looking for doors. We're looking for--? [Sebastian:] In the Google self-driving car, which is a good example, we actually have a map of the ground that is not that dissimilar from the way Google Earth project images on the ground. Then if you think about the robot having a laser camera, but a camera, that produces local maps, and these local maps have some of the same features like lane markings, as our global map. And then we probability match question is one where we take the local map, superimpose it over the global map, and see where it matches the best. It's a search usually in x and y space. It's also a little search on orientation space, because you might have your orientation slightly wrong. The likelihood function that replaces our p_hit and p_miss is then usually a correlation function that says how well does the local map that you see right now match the global map. When you shift it to the right position, it usually matches much, much better, and as a result you get a much larger probability. [Andy:] Some people were worrying about whether changing road conditions, whether weather or lighting conditions, things like that, would greatly affect the usability of the robot car. [Sebastian:] That's a great question, and we discuss this every day. Turns out that in certain weather conditions the car is relatively robust right now. Rain, for example, changes the total appearance of the street, and we can adjust this with a single constant factor that adjusts the total brightness of the laser map. Other things like a complete snow-covered street we have no solution right now it turns out. What we're doing right now is we just don't drive in snow. We live in California. There's not much snow here. When there is snow, we just tell the driver not to use the system. That's something we have to work on. I think as we move into things like snow and very massive changes, we have to reference other features. Maybe there trees around or other structures or rocks. Typically, snow comes in the mountains so maybe there's mountainous structure. And we have to toss some of that information into the system. [Andy:] People are getting very excited in the forums. They want to actually build their own robotic car. Some questions have been how much is it going to cost? If I can't do it because the cost is too high, can I get some Legos and build a robotic car that way? [Sebastian:] There are a lot of kits you can buy. I love Lego Mindstorm, is which a wonderful way to experiment with these things. There are a lot of low-cost robot platforms you can buy. I don't really recommend to hack into your car. The reason is the moment you start snipping wires, your car might actually become unsafe. If you do work with a car, make sure you never, ever drive this car manually again or you know what you're doing. We've actually been really tapping into cars, and we've done things like decoding and understanding how you use an electric steering booster to turn this in to steering motor so that the car would be able to steer on computer command not just on human command. But it's a lot of work, and it's actually very risky, so don't do it yourself. Rather go and buy yourself a Lego Mindstorm or something similar and play with it. You can actually study a lot of the same stuff using a Lego kit. If you make a Lego robot move from the kitchen to the living room, that's actually really impressive. [Andy:] One last question is for the people who are especially enthusiastic about robotics and AI. How do you get involved in this field? What can someone do after taking this course? [Sebastian:] Take this class. As you know, we've been handing the CVs of our top students onto various companies like Amazon and Twitter and Facebook and Google. That's one way to get involved. That doesn't work for everybody. The way I got involved is, for one, I had a lot of toys that you could program. I had a computer. I had little things to move around. There was a construction set in Germany which I could play with as a kid. Then I really started becoming a scientist. I started publishing papers. I started solving problems that the literature left open-- look into those and see if I could make a contribution. At the age of about 22, I had my first big, big paper that was at a major conference. That kind of started me to get into the field and got me really hooked. The nice thing is I think there are many, many problems around you that require the same technology. There's tracking problems, estimation problems all over the place, right? So if you can't afford a robot, why not just by a web camera and put it in your kitchen and have it, for example, track whether your cooking is good or bad? That's something that everybody can do with the same technology. Like today, most cooks occasionally boil water over and it makes your stove wet. Can you build a system that prevents this? That's a robotic task that uses the same technology. If you do this and do this really, really well, you might start a business. You might find other people to work with you. You might show it to a professor, and if they got excited about it, they might publish it at a conference. There are many, many ways into the system, honestly. Just be creative about it. [Andy:] It sounds like you're emphasizing you have to think like a roboticist. That's, I think, what this class is going to help to do. [Sebastian:] Yeah, what I'm trying to do is make people think like roboticists. I'm extremely excited that in this class you get to program it yourself. That's a new thing. When I was a student, I didn't have that possibility. In our previous class you couldn't do it. We are not at the point where you can drive your own robot around yet. I hope to get to this point where we have a good robot simulator so you can play with this. But we have enough stuff you can just buy. Most importantly a camera--like a camera on a phone. You can do localization on a hand-held phone in a room using images. If you do a really fantastic job, you can start a company and sell it to one of the major players. It's an open problem to the present day how to do this really well. There's a lot of technology available today that allows you to think like a roboticist and still solve the interesting problems--with this class, I hope. [Andy:] All right. Well, thank you very much. [Sebastian:] Okay. Thank you. [CS 373: Programming a Robotic Car - Taught by Professor Sebastian Thrun] [Udacity - udacity.com] Welcome to the second office hours. I have Sebastian here and myself. We're going to do this the same way we did last time. First we're going to talk about some content, and then we're going to go into some applications, referring mostly to Kalman filters but also talking a little bit about Stanley and Junior. Okay, what's the first question? The first question--both of the content questions actually have to do with linear algebra. When we were talking about this state transition matrix f, we have this equation x' equals f times x plus U. You said that this U is the motion, but the motion seemed to be embedded in this f matrix in that the velocity was taken from the state with the f matrix. What exactly is going on with U? U allows you to apply a choice like when we just said the position is a function of velocity that's describing physics but doesn't give you a choice. When you want to insert a choice into the system, like for example, you might want to change accelerations and that affects velocity and affects state. That choice is expressed with the vector U We didn't really use it because we were tracking things. We didn't know how they were being actuated, so we set U to zero in our example. But if you have control over the system you're trying to track and you can insert like a motion yourself, then you use the vector U. Interesting--just one other piece of intuition we're hoping to get is a lot of the matrices we talked about either didn't have off-diagonal elements or we at least didn't initialize any off-diagonal elements. Specifically, in the covariance matrix and the r matrix, can you give us some intuition about what those off-diagonal elements would represent? We actually ran into an example that did have off-diagonal elements. We didn't initialize it, but then the velocity became correlated with the position, and we realized the faster we moved the further we are to the right. That was expressed by an off-diagonal element. They turn into correlations, so the largest these elements are, the more 2 variables are actually correlated. The more knowledge about one variable correlates itself with another variable. Now, let's get into some of the applications that the students seem especially interested in. Specifically, this r matrix. Where does it come from? How do we get it in real life? What are we doing with our sensors? So the noise matrices express how noisy your sensor is, and at first approximation you'd say let's just measure what the variation of the measurement is and then plug it in. But because these filters, a very subtle thing, assume conditional independence, they assume that noise is independent from one type to the next whereas in reality it isn't. Typically you start with a very large value, and you look a the result, and if the result looks good to you, you leave it that way. Unfortunately, there is no good science for it. Awhile back, together with Andrew Ng, I published a paper on how to learn the noise matrix, but that goes way beyond this class. You mentioned that Stanley and Junior both use laser and radar. Does each one offer something unique or are the both just doing the same thing? Either way, how do we incorporate measurements from 2 sources into our Kalman filter? These are two questions. First of all, laser and radar at first approximation do the same thing. They measure how far things are away. Radar also measures how fast they're moving for the Doppler effect, but their characteristics are very different. They measure different things. There are certain things that can only be measured by laser, others just by radar. In fact, laser tends to have much higher spatial resolution, but events become foggy. The wavelength of light tends not to be as good as a radar wavelength. So they're somewhat complementary. To incorporate both, Bayes rule allows you to incorporate sensor measurements one after another. If you have a laser and a radar, you just multiply both of them in, and that's just fine. Okay, great. The top-rated question, actually, was about the programming languages used in Stanley and Junior. Was it Python? Was it something else? &gt;&gt;It was not Python, honestly. &gt;&gt;Okay. At the time, we started out with C, but then C++ became very popular. I was popular. And it was the better choice, so almost all the code is written in C++/ How do you make that decision when you're starting a project this big? Is there debate? There is always debate. The beauty of C++ is it is very efficient in execution, and when you use it right it can be very powerful. If you don't abuse it. It has way too many things built in, but some of the things like the classes are just really, really good. Then you hire people, and you work with students, and some of them like Java, so they write their code in Java, and others like C++ or Python or Ruby on Rails. Then you just bring all the stuff together. Okay--thinking of Stanley and Junior, what were the major hardware and software differences between the two vehicles? Stanley had about 6 embedded processors. Junior had 2 PCs with quad cores so there was a more integrated system. The biggest difference in the hardware was really the sensors. Stanley had very, very cheap and simple laser-range finders as the main sensor. We did have radars. We didn't use them much. Whereas Junior had a much more ranging sensor suite. Junior could look in all directions--we call this "surround sensing"-- whereas Stanley could only look straight ahead. So that thing we saw spinning on the top of Stanley, that was the laser-range finder? What you saw spinning on the top of Stanley was actually on Junior. That was on Junior--okay. Yeah, you never saw anything on Stanley, because there's nothing spinning there. That was on Junior, and that was a laser-range finder. Right, and that one spins because it looks in all directions. That was important for city driving because what's behind you actually matters in cities whereas if you drive in desert terrain there is no traffic. You don't have to look back. That's a good segue into the next question, which is what's the next big challenge? We've done deserts. We've done urban driving. The next challenge is it'll take over our cars. Basically get this technology into every single car and make sure driving is safe. Every person has a special button that says, like I explained, my little chauffeur button. I want to just drive automatically, and then I'm just going to get home without having to pay attention. Finally, how comfortable is it? Do these cars drive similar to the way you or I would drive? When we first started out, I would say the driving is effective but not elegant. You would get in the car, and you'd know exactly what I mean. The steering wheel would go like this all the time, and it would make a lot of noise. It was pretty clear you were inside a robot. On the outside it looked pretty great, but on the inside it didn't. But as things moved on, if you get into a Google car right now, you won't be able to distinguish from a human driver. It's really rock solid. The steering wheel stays like this, but it turns it confidentally drags it around, moves the right direction, comes back. It's actually come a long way. To get from what you said to where we are now--was it low-pass filtering? We will have a class on control, and the control techniques ended up to be very sophisticated but also very, very good. All the motion of the steering wheel are all related to inaccuracies. They came from multiple sources. Some of it was that we weren't processing our GPS date good enough yet and our map data. Some of it was the map resolution--like if you have a 10-15 cm grid cell and your estimate jumps from 1 grid cell to the next or your particle filter jumps a little bit around. They might not look dramatic on a screen, but if you turned to steering motion in your steering wheel, it goes by 2 or 3 or 4 degrees. That's really bad. We had a sensor that tracks that angle of the steering wheel and the spatial resolution was about a degree. That means you couldn't quite know what the angle was, so you would drive a little blind--like up to a degree. A degree of steering wheel doesn't sound like much, but it's actually a lot. You can try this out. If you drive a car and you only move it by a degree, you feel a noticeable effect. You'd find out after a while we'll actually pulling in this direction. Let's drag it back. And all that stuff we kind of fixed. So we'll learn about that in, I think, Unit 5? PIT controllers? &gt;&gt;Yep. Excellent. I can't wait. &gt;&gt; All right. &gt;&gt;Thanks a lot. &gt;&gt;All right. Take care. Hello and welcome to the third office hours. Let's jump right in. All right. Many students, myself included, had some questions about the resampling wheel. Specifically, when you draw a random number between 0 and twice W max, where did that number, twice W max, come from? I have the same question. I made it up. I wanted to make sure that these wheel can jump over entire particles that wouldn't large enough, but if you make it really large then you have to search a lot. I figured if it's 2^n it's going to be fine. Does this bias the sample in any way, choosing this number. I actually don't know. I think there is certainly a correlation between adjacent particles in the selection of particles. They're not independently drawn. I just don't know what the effect on the particle filter will be, and I wish I did. The next question, or two questions I should say, come from George. He wants to know if there are any rules of thumb that we should keep in mind when we're choosing what filter to use for a given situation. When do we use the particle filter? When do we use the Kalman filter? Absolutely, yes. The particle filter is the easiest to implement, but the complexity scales exponentially with the number of dimensions. That's usually a problem, because if you have a high-dimensional space, you just can't apply it. The Kalman filter is the only filter that does scale exponentially, so it's very nice. If you have like a 15-dimensional space, you will usually use a particle filter, but the problem with the Kalman filter is it's unimodal, so you can't really have multiple hypotheses. There are extensions of the Kalman filter that does this called mixed rough Kalman filter, multihypothesis Kalman filter that can do this. They address some of these problems. The histogram filter is applicable in situations similar to the particle filter where you have a global uncertainty, and it's more systematic. In the particle filter, if you loose track of the correct hypothesis, you might never regain it. In a grid-based filter you have a chance of regaining it. Grids are easily supported in many programming frameworks. Sometimes there are better ones to use, but they also have a generic limitation in the accuracy, which is related to the resolution of the grid. My recommendation is if you have a multimodal distribution use particle filters if you can. If it's really a continuous space with a unimodal distribution use Kalman filters. Okay. That's a great segue this question about switching between filters on the fly. For example, we have our particle filters that converges to a unimodal distribution. Then can we switch to the Kalman filter? It isn't done much that people switch. The reason is when you switch between filters, you end up getting moments of increased uncertainty. You can see this when you buy commercial GPS receivers. They tend to run multiple Kalman filters, it turns out, 3D depending on whether it's 2D or navigation. When they are switched, the behavior becomes a little bit iffy, and often that is bad for robots, because they they a little bit because this is where it says this thing versus the other thing. They're not quite consistent. There are ways to combine multiple filters types. The most common one is called the Rao-Blackwellized filter--Rao-Blackwellized-- after Rao and Blackwell. What they found is that in a particle domain, sometimes if we nail certain dimensions with particles, everything else conditional on the particle becomes Gaussian or unimodal. Then you can exploit the efficiency of a Kalman filter that is now attached to individual particles. I'm not going to go into depth here, but there's an entire field of Rao-Blackwellized filters that sometimes can estimate in the spaces of hundreds of dimensions. Great. Thank you. That actually predicted George's next question, so we'll move on. Drew wanted to know about what happens to a particle filter when the motion modal moves a particle into an invalid place. For an example, in that corridor demonstration you gave, what if a particle gets moved into a wall? Well, thanks Drew. Great question. The obvious answer is you killed that particle. The way to think about this is in the measurement model you've got to have this kind of implicit sensor that says, "I'm just sitting inside a wall." The truth is the robot never sits inside a wall, so that sensor would always say with absolute certainty, "I'm not sitting in the middle of a wall." This kind of hypothetical sensor would justify that the weight of that particle that's in the middle of the wall would get weight 0, so you just kill it. Okay. A couple more questions from Drew. What about dynamically adjusting our big end, our number of particles when we want to trade off between computational cost versus the accuracy of our filter? Dynamically setting the number of particles has been done quite a bit, and it's a good idea under certain circumstances. Obviously, the fewer particles you have the faster you can compute. If you're tracking really well and all the particles are centered in one location, there isn't really a need to have as many around as when you're globally uncertain. They way to set the number of particles is often done by looking at the total non-normalized importance weights. If all your importance weights are really large, then you're probably doing a great job tracking, and you don't need that many particles. Whereas if your importance weights are all very small, then chances are you're doing a really lousy job tracking, and you need more particles. That isn't perfect. An unlikely measurement can cause weights to be small, but a good heuristic would be to say, let's particle sample until our non-normalized importance weights reach a certain threshold, and then let's stop sampling. Now truth telling, many of the systems we're dealing with are real-time systems, and you can't really afford using many particles sometimes and few other times, because there's a fixed amount of time in which you want to do your estimates. Then yes, it's more tricky, but in principle using few particles when you track well is a very viable solution. Thank you. Taka also had a question. He wants to know how to distinguish between moving landmarks and non-moving landmarks. The maps that we dealt with in class were all static, and the landmarks were fixed. How does the Google car distinguish between these moving vehicles, moving people, and these static landmarks? That's a wonderful question. First of all, the Google car assumes that the ground map, like the surface map of the street, is basically fixed. If lane markers would move along a little bit, then the Google car would probably get confused-- a little secret here--so please don't repaint the lane markers when the Google car comes by. That's treated differently than stuff that sticks out of the ground, and even that is used as a landmark. What we do is we have a probabilistic threshold that says what's the chance of this thing being mobile or not? We do this by establishing correspondence, which means we take measurements a time step earlier and measurements a time step later, see which has the most likely correspondence between these two measurements and then see if there's a motion vector in between. Sometimes we can explain it away by just noise, but for cars and people and so on, there is very often a very clear and strong motion in which case we assume this thing is moving and tracking. It also turns out that the way we build our prior maps is sometimes we drive by multiple times. We do differencing, and then we have most captures are static things in our maps. We happen to know that in the middle of a street, there tend to be no static things. There tends to be just moving things. You can bias the estimate toward saying, well, in the middle of the street what we'll see is likely not static. Tossing this all together gives us a fairly good tracker. Thanks a lot. That's all we had for this time. We'll see you all next week. But I want to make a comment. I hear that many people really positively received the homework assignment on particle filters, which is great. It took me a while to make it. On the discussion forum there is a posting of a graphical version of it, and I downloaded it. It looks really great. I couldn't get the curser keys to work on my computer, but it's a really great basis to visualize. I think particle filters are really hard to understand without visualization, so I'm really sorry that our current programming environment doesn't provide visualization. I hope in the next round, we'll fix that. I'm pretty sure we'll fix this. So please play with it. The other thing I noticed in the discussion boards, and I wanted to just call for feedback, and the same for the Facebook group, that people want harder homework assignments. I'm not sure that's true universally, so if you have an opinion, why don't you just post it. I'd like to get a sense of it. I'm thinking of making an assignment where we toss everything togethera and really build like a mini version of an actual car. That's going to be really involved, so let me know what you feel like. This course right now, I would argue, is really Stanford caliber. What you guys are doing, and you girls are doing, is really at a quality that I would expect my best Standford students to do. The type of things you implement, certainly, is at the same pace I would teach at Stanford and possibly faster. But if I go to a general build-a-robot example, then I'm going to exceed beyond Stanford base. It's up to you. Let us know. All right. Keep us posted. I'll be reading the forums. I'm sure Sebastian will too. Thank you very much and see you next week. All right. Hello, and welcome to the fourth Office Hours. We've had a lot of really good discussion in the forums and some great questions. Let's start with a question from Michael. He asks, "In the last office hours, you mentioned something about the possibility of a project at the end of the class. What are your thoughts on the students building a robotic car?" I just recorded the final class, and I actually spent some time putting everything together into a simulation of a car at the level we've been operating--not a real car. Honestly, it took me a whole day to put this all together. It's substantial work, because at the surface these pieces fit together nicely, but when you put them together, you realize that what the planner generates is not quite isn't quite exactly what the controller really wants. Tuning those parameters was actually a challenge, but what I've done is for the final class, I'm giving you the environment first and saying solve the problem of driving a car from A to B. If you're really so inclined, you can go out and really try it, but as the class goes on I give you most of my implementation, leaving bits and pieces out. Then you can go and plug those bits and pieces and and hopefully make it work. Unfortunately, I can't really give you a real car. That's the real fun thing. As we'll see in the final example, the methods that we discussed are the core methods. They give you a good path, but they won't give you a path good enough for actual driving. There's a lot of work that goes beyond this class that could be done to make things smoother and more elegant. Perfect. Tangled asked, "In the Unit 4.14 video, you mentioned something about the other cars honking." He wants to know if Junior can here. Does he have a microphone sensor that affects his actions, and can he honk himself? [Sebastian:] None of our cars hear, and none of our cars honk. I know in certain parts of the world that's a recipe for not driving confidently and safely in traffic, but we chose not to make audio communication part of that experiment. George asked, "While it's possible to constantly re-plan if enough computational power is available, A-star assumes a static and deterministic world." Can you comment on some of the possible pitfalls of using A-star? And what are some of the alternatives to this algorithm? Those are great and really deep questions, to which I've dedicated a number or research years, it turns out, and I've graduated a number of PhD students on that very topic. A-star gives you a taste of how cool planning can be, but there are many, many opens problems. For example, A-star cannot deal with branching outcomes where you flip a coin, and you have to pursue both outcomes. It just doesn't work. It cannot deal with information gathering. Sometimes in planning what you want to do is take a specific action just for the sake of a reducing uncertainty. For example, if I am going to a grocery store, and I don't know whether I have money with me, I might just check, do I have my money here, and this checking action has no other bearing than informing me that my money is in my pocket. That is completely not doable with A-star now. The dynamic programming stuff that I discussed has that flavor. You can actually pursue multiple outcomes and bring them together. It is computationally very inefficient, so you have to basically go over the entire state space. The fact that we wrote the universal plans is just a side effect of pursuing every possible combination. The moment the state space gets very large, dynamic programming doesn't scale. It's a big, open issue. Now, if you go to something like Google Maps, what you'll find is that you can do instant path planning between two points. What Google has been doing is pre-caching a lot of the principal sub-plans, so the remaining planning problem of finding the shortest path becomes mostly a table lookup. They've done this in a way that's actually optimal. They can prove it's optimal for path planning. It's really an amazing achievement that's way beyond A-star. The planning field is much more interesting than the lecture alluded. I just want to get your juices flowing to have you use one. Now, we did use A-star in our world, and the way we addressed re-planning in the current environment is that we just plan really, really fast. We had a version of A-star that was super fast. If we saw something new, it would just re-plan, and that was just fine for us. You mentioned something about dynamic programming just now, and a student had a very good question about that. How large is the planning policy for Junior? In Junior's case and the Google self-driving car cases, we don't do that much planning. We basically plan only within the visual horizon of the vehicle horizon of the vehicle itself. We don't address the problem of how to get from here to Paris or what streets to take We assume that's solved by Google Maps. We assume a fixed route in all our driving, but once the fixed route is speced out, all the planning that takes place is within the visual range of the robot. They are typically 3-dimensional state spaces. They're typically x, y, and orientation. Now, in some of our experiments we also use velocity as a state variable, because sometimes you want to take a little detour so you can straighten and be faster, but they are relatively low-dimensional spaces. Okay. Another question from George was about prohibitively expensive left turns. Are the costs of actions--are these a dynamic variable that's constantly being updated or are they static within the car? The way we've handled the cost of action is we have a version that looks at the global picture--like go from A to B and find the cost there-- then zooming back to the local picture. And the local picture variable is to actually consider cars right now at this moment whereas in the global picture we assume a static world, and we make a fixed cost for left turns. Now, as we go to the local picture, we do a lot of local roll outs, local look-ahead plans--not dissimilar from A-star-- and then fill in the actual cost of left turns and replace them with the assume cost. That's sometimes needs a different action. There situations where we are planning to do a lane shift but we can't because it's just too expensive right now. We'd rather endure being stuck behind a slower car until the lane shift is cleared. There's an interplay between two levels of planning going on. A student wants to know about multi-goal environments. An obvious example of a multi-goal environment would be a parking lot. I want to get to an empty parking spot. That's a great question I should've asked you the students, because it has a really nice and obvious answer, which is--here's the answer. In A-star, you can certainly designate multiple states to be goal states. Nothing prohibits this. You just have to make sure your heuristic is admissible. It takes a value that is underestimating or at least estimating correctly the distance to any of those goal states. Then it's just fine. The same is true for dynamic programming. You can certainly have multiple goal states there. Nothing in the formulation is a problem. Talking about heuristic functions, a student had a question, and I think a lot of people are wondering, since this is really where A-star gets its speed in the choice of the heuristic function, are they deduced algorithmically or are they just done with intuition or trial and error? How do we decide? That's a fantastic question. That's actually a really deep question as well. In the car domain, we actually calculate heuristic function, and we have two heuristic functions we kind of superimpose. Both of them get at the gist of the problem, but are much easier to compute. Suppose we have an environment with obstacles, and your planning space is 3-dimensional--your x, your y, and your orientation. One way to do your heuristic is to ignore the rotational degree of freedom and assume you're moving a disk in any direction. Obviously, moving a robot in any direction is easier than being constrained by the filters or the car. This becomes now a 2-dimensional problem, and by going from 3 dimensions to 2 dimensions, we can solve the entire problem in no time. We plan in 2D and use the 2D planning result as a heuristic for the 3D planner. The reason why it works is because 2D planning is so much faster than 3D planning. That's on that is basically ignoring the physical constraints of the car and going to a simpler car model which can move in any direction any time. The second heuristic we're using looks at the 3D problem but without obstacles. We can pre-plan the optimal action of a 3D robot that has turning constraints in the 3D space--x, y, and orientation--to any goal location in advance-- we do this once and it's true forever--without any obstacles. What this gets us is that sometimes to get in a certain target orientation, you have to take a certain turn, and that turn has to be taken no matter what, and obstacles will only make it worse. If you compute this obstacle-free heuristic, you sort of get an underestimate that gets only worse with obstacles. You have an admissible heuristic. We toss both of their heuristics in as heuristics in A-star, and we get a speed-up of a planner easily of a factor of 10^10 in practice. That's very interesting. Great. So it's actually--I'm giving this as an example. I don't want you to really implement those right now, but that shows you how deep this question really is. If you can come up with good ways of cheating and solving the problem by lessening constraint and do it much faster, that tends to be a great heuristic for A-star. All right. Thanks a lot. We had a lot of great questions. I'm sure we'll have a lot next week. Can you give us a little preview of what's coming in Units 5 and 6? Yes, first I want to thank you for these questions. I really appreciate how deep they are, and how much you connect with the material. I think that's great. I honestly personally much prefer them over questions such as, "When can I buy my next self-driving car?" I think is really deep material I'm trying to teach you, and I love the fact that many of you are diving in and really deeply, because that's what I want to enable you to do, to apply it in a deep way. So keep these questions coming. In Unit 5 and 6 I'll dive into control. Control is now the art of turning these abstract, discrete, and chucky paths into actual steering motion. It's a continuous thing, because your steering is continuous, and time is continuous. We're going to go away from the discrete world and into the continuous world. In Unit 6 I try to toss everything together. In Unit 6 we program, hopefully, a robotic car with my help where we go all the way to localization at the same time and path planning at the same time and control and hopefully generate some actual trajectories by our simulated car. Can't wait. I'm sure the students feel the same way. Thanks a lot. See you in class. Welcome to the 5th office hours. We had a lot of great questions on twiddle and parameter optimization. Let's get started. The first question comes from George. He wants to know, first, is twiddle really all that good? Does it really give better results than other methods you might use for selecting parameters? Well, George, that's a great question. It's good in only one dimension, which is it's really easy to implement. Sometimes people overlook this. This is an entire literature on search parameter optimization. In a lot of cases where twiddle would fail other methods would succeed. You can write a PhD thesis about this, but the nice thing is twiddle takes no time to implement, and I've never seen a piece of code that couldn't be made better with a twiddle around it. That's why I explain it. I think it's just an easy method. I literally use it all the time. Another question from George about twiddle is local minima. Are they a problem with twiddle? It seems like it's pretty easy to get caught in one. That would be a fantastic final exam question. Sorry for taking it out of the final exam. Of course, it's susceptible to local minima. It's essentially a hill-climbing technique. It might on occasion jump over because of the step size, but by and large we're going down the hill. It's also can get stuck on ridges and other phenomena that might occur in certain optimization spaces. It really good if you have a good guess for the initial parameters. It's pretty horrible if you want to search systematically. Then you have to do things like grid search or a kin of particle filters as randomized methods for searching as well. Matt wants to know how you can smooth a vehicle's path and still guarantee that it's going to be collision free. It seems like sometimes that new path will run into an obstacles you had previously avoided. Matt, you're absolutely right and that's regrettable for the obstacle and the car. It turns out that in practice the smoothing techniques that we use are a little bit more sophisticated. That is, it's not just points in x-y space, but it's successive state vectors for cars. The parameters from one to the next are the steering commands for the car, and then you can actually often much better guarantee that you stay away from obstacles. There is still this issue of how to--obstacles are repellent, and you have to put into your optimization terms a constraint that expels you away from these obstacles. To be honest, this becomes a bit of a black art how to tune these parameters. I had a PhD student work on very narrow openings with smoothing where we used the distance between obstacles as one way to scale the pressure that obstacles would assert on the path to pull away from themselves. The car would be able to go through very narrow passages but in very wide passages wouldn't go straight to the middle of it. It's a bit of a black art, and I'm really trying to scratch the surface here. All right. Lauren wants to know--in the unit we had this PID exercise where we were driving the car and trying to get it to align with this straight line at y equals zero. He found that when halfway through his run he switched the target to y equals 2 instead after the car had already gotten going along y equals zero pretty well, this went sort of haywire. Do you have any idea what may have happened? It seems like in reality a moving target is a pretty common thing to have in a car. Don't do that. Don't do that with your own car. Okay? Your car will go haywire. It turns out in a lot of controls there is something called the "basin of attraction." It says a controller, when you look at it mathematically, is only guaranteed to succeed, we called this "stable," if it's initial set point, the distance and the angle and so on from the reference trajectory, is confined in a specific area. If you were to analyze our controller, which happens to be a nice stable controller so you can actually do this, you'll probably find that the set point 2 off is outside the basin of attraction, specifically if we have an integral term. That would be particularly bad. There are other ways to derail it. You could have the car face the wrong direction, and I believe the car would produce something that you wouldn't expect. When you design the controller practically and theoretically it's useful to guarantee to yourself or convince yourself that you're within this basin of attraction where the controller actually works. I love the fact that you're running this experiment, because that's the kind of stuff that's really interesting to run. I think it's much more interesting to break things than have things work, because we can understand where the boundaries are. And you found one, and that's really, really great. I didn't even talk about this in class. Thank you. Next question comes from Omar. He's a high school student, and he's taking this course, and he's going to be applying to colleges soon. So Omar-- &gt;&gt;No, go on. Any advice. Go for it. Omar, I'm your friend. &gt;&gt;That's it? &gt;&gt;No, go ahead with the question. He wants to know if you have any advice for projects he could tackle, hopefully, that would show college admissions committees how much he's learned in this class. Omar, I'm your friend. Keep doing what you're doing. You have your life ahead of you, which is wonderful. You're asking the right question. You're taking my class. You are audacious to take this class. That's fantastic. All the high school students out there--you guys rock. In the Darpa Grand Challenge, one of the top competitors was a team from southern California, a high school. What was the name? Palo Verdes, I believe. It was mostly girls, it turns out, and they were all under the age of 17, and they actually beat MIT out of the prelims in the Grand Challenge. I was insanely impressed, and these students to the present day are actually my heroes. What can you do? There is a FIRST. My friend Dean came and started the competition. It's a great way to be creative. There is Lego Mindstorms that you can play with and have a lot of fun with and be creative about it. There are all kinds of mathematics/robots olympiads where people do hard problems, hard programming problems and so on. The only thing I take issue with is I think we pay way too much attention to the admissions process and way too little attention to just having fun and learn something. I have never worried about admission. I ended up at Stanford because I couldn't decide and move on, so I got older and older and eventually became a professor. I think the focus on having fun and exploring something new and learning something new to me is one that I wish you could maybe remember when you do this. Because that's really the most important thing in life in my opinion. That's good advice, I think. Thank you. MJL talked about his own experiences driving a car and how his decisions on the road don't just depend on where his car is and where he wants to go but also on how fast he's going. That's something we haven't talked about much, how to incorporate speed into robot decisions. Coming from Germany we don't really worry about that--just kidding. Absolutely correct, and there are multiple aspects to speed. You bring up one--that is, if you take the same turning radius at a higher speed, your lateral acceleration goes up--your sideways acceleration. Eventually your car will spin, flip, slide, or similar, but it won't work anymore. So in our work on the Google car and the Stanford car, we put actually a limit on the level of acceleration and made that the thing. It means on highways our turning radius is significantly larger than, say, on a parking lot. That's true for any human driver as well. The second part we didn't talk much about is selection of speed. I didn't because the control methods are not quite as interesting as the steering control methods. Basically, you can imagine that you set the speed so that you can always come to a safe stop. Every obstacle in your way, every pedestrian on the side, every car in front of you puts a limit on the speed, as does, of course, the legal speed limit. In the controller is one that uses the throttle and the gas to gradually pull back, in a PID way actually, to the set speed that's the maximum legal speed or drives at the speed given all these other constraints. The lateral acceleration does impose on the maximum speed. When we know we have to drive a turn, we can compute where we do the acceleration based on different speeds and then back propagate those into the equation for speed. That gives us a limit on speed as well. Our car naturally slows down before turning. Does that maximum lateral acceleration depend on things like road condition, which I imagine it would? A slippery road would, of course, have an effect. &gt;&gt;It should. It should. I hope so. &gt;&gt;I think it presently doesn't. It will in the future. We have a bit of a difficult time right now determining what the exact road conditions are, partially because we don't have access to the stability management systems on cars themselves, but it absolutely should, I agree. Laurent asked another question about twiddle. This one's about correlated parameters. It seems like with twiddle we're adjusting one parameter at a time. What if we have a situation where we need to change both of these parameters or two of the three or three parameters to really get a better minimum? That's a fantastic question that a case where twiddle will just fail you. If you have a small diagonal ridge and two parameters to co-evolve, it'll just fail. There are fixes. Again, there is an entire literature. The easiest fixes, if you have happen to know the direction, the can twiddle in that direction. There is something called adaptive samplers that actually learn about this direction dynamically and build gradients to follow. There are all kinds of other methods that look at the surface shape and model the surface shape and model the surface shape and find the optimal direction to go. Again, twiddle was only given because it's so simple, and I hope you found it to be very effective. It was partially given because I couldn't figure out what the right control parameters are, so I just implemented this twiddle thing. But yes, there are many, many ways to improve over twiddle. Phillip points out that there's going to have to be some sort of time lag between when you make your commands and when the car actually responds to them. How do you account for that time lag? Phillip, that's a great question. This is what a lot of our time goes into. This is a really, really, really hard question. The time lag basically kills everything. If you take your car and control software, say, and you make a controller and then you add, say, a 10th of a unit of time delay in there. Just try it out and pretend that you're always acting on the previous sensor measurement. You'll be amazed and how badly the car will drive, because it would constantly over steer and under steer. The best method we have right now is to use a predictive controller, which is we have a model of the car. We know at what time the control command was issued, when the sensor measurement was taken, how much time the processing took, and we kind of predict what the measurement would be at the time the processing is finished and then use this as an input. If you know about time delays in the vehicle itself to execute commands, you add this on. The second thing we do is actually model the car more detailed than I pretended. In the way I taught it it felt like we could set the steering wheel to any set point instantaneously but a physical steering wheel has inertia. Instead of modeling the set point, we have a secondary controller that understands how much torque it takes to turn a steering wheel and uses that as its information. The better your model the more accurate your control. It takes some work to get it right. Thank you. The last question comes from Martin. He wants to know what happens when the physical model that we're basing all of our decisions on changes? If the car gets a flat tire, or something along those lines, what happens? What happens to people? People are actually quite amazing. You can do the test yourself. Pick up a box. You have no clue how heavy it is--someone puts a brick inside or not. You lift it, You can completely adjust the dynamics of your body-- the posture of your body--to be able to walk with it. If find this unbelievably amazing that we have this kind of intuition how to handle things, and we really reconfigure our body. Cars can do this to some extent. They can do it specifically if they have a model of the change. If they understand which way, say, a blown tire affects their dynamics. There has been a lot of work on this. You can run multiple predictive models--one with a blown tire and one without a blown tire-- and then when the one with the blown tire comes along, then this model all of a sudden becomes the more plausible to explain what's happening. That's kind of what people also do in a very, very fast way. They understand, well, I tried something that didn't quite work. Then they correct for it. PID controller is somewhat robust, like when we put the drift in the tire, there was a change like this. But yes, a blown tire will very quickly leverage the PID controller out of the basin of attraction for stable control. As a result it could really happen that this thing flips. In reality the number of things that can happen to a car are quite enormous-- from the mattress on the highway all the way to the blown tire to the defective computer, which also effects the car. I wouldn't claim we have solved this problem. We are really working on this at Google right now through these different situations to make the situation as robust as possible. For me it's still an open problem how to control a car well and also notice that a tire is blown. Thanks for asking this question. Thanks a lot for all the good questions. Thank you for helping to answer them. Next week we're doing SLAM, I think. We don't know what slamming is. &gt;&gt;Whatever slamming is. Who's being slammed? &gt;&gt;I don't know. I think-- Slam, slam, slam, slam, slam. We're doing SLAM. Come see the SLAM lecture. I want to say a word that--we've started with many tens of thousands of students. There's still a huge number of students left, but when you're at this point and ask this level of question, you're a serious person. I totally appreciate this. This isn't an easy course. In fact, this course is really hard. Thanks for asking such insightful questions. I'm going to go back to the discussion forum and answer some more questions there. I guess I'll see you in class. &gt;&gt;See you next week. Hello, and welcome to the sixth office hours. This will be the last office hours. Let's get started. The first question comes from MJL. He wants to know how Graph SLAM deals with non-distinguishable landmarks. That's called the correspondence problem. It actually turns out to be the harder of the two problems. I gave you the problem of how to solve for known correspondence, which becomes a continuance optimization problem, but there is also a discrete problem here, which is does this lane marker correspond with this guy over here or to that guy over here? There are solutions. One of the best ones is called "ransack" where you randomly assign correspondences for three or four landmarks, solve the set of equations and then look for the best nearby matches and then fill them in. You repeat this so you can get rid of the local minima. In general, if your robot is very accurate, then just picking the nearest landmark is a good heuristic. But if you go through a huge cycle, you might just not connect, and there might be multiple ways to connect, and then you can use this ransack method to try multiple choices. &gt;&gt;Okay. Thanks. The next question comes from Marcello. He wants to know in a real environment we don't know the number of landmarks. In the unit it seems like we were always pretending like we knew. In, for example, the mind mapping video that you showed, how do we deal with encountering all these new landmarks? Do we not get the same problem that we had with out position measurements where this matrix just becomes unwieldy? Marcello, that's a great question, and there are many different answers I can give you. One is if we really had point landmarks the way I described them, like tree trucks or corners of rooms, you can just grow the matrix using the functions I provided to you. For every new landmark you just put a new row and a new column there. That's basically it. However, the more interesting version of your question is what is a landmark? In our mine mapping, for example, we don't make distinct features like ??? landmark. Instead what we've been doing is taking little small local maps that are defined over 5 meters of robot motion. We make these maps the basic entities. Now, it turns out there aren't any landmarks in the traditional sense, but they still have a location and you can still match them and see how they fit together. In matching them, you can make a constraint that's very much like the graph-like constraint that is like range and bearing or x and y distance. You can toss them into the same set of equations. That works really like a charm. Great. Sounds sort of like when you're stitching together those panoramic photos. Yeah. There is an entire field of people who stitch together these local images into global ones, including image stitching, panoramic stitching, and so on. They use very different methods like the 3D reconstruction of the boat Titanic that sank a hundred years ago that is done with local stitches that is being stitched together this way. If you go back all the way to Gauss 200 years ago when he invented the squares method it was basically his attempt to map the land and get these local constraints together of measurement points to make a reasonable map of our land. That's what this all dates back to. In fact, he basically invented Graph SLAM 200 years ago. He just didn't know about it. The next question comes from Emil, and he is talking about how we found all these great ways of helping our robot make intelligent decisions. But there has to be some situations where we've just hard-coded some things into, like, the Google self-driving car. Can you talk about any of those exceptions that have been hard coded in? My aspiration usually is to have no exceptions and try to come up with a single framework. The reason is in my experience every time we have an if, then, else rule where you say this is the exception and this one isn't at that border you introduce brittleness. You often get the wrong behavior because most exceptions aren't clear-cut. When I talked to my students at Stanford or my team at Google, I try to get rid of any possible if, then, else statements. Now, having said this of course there are exceptions. There are motorcycles. There are cars. It turns out cars stay in lanes, but in California--probably like the way it is in India-- motorcycles tend to be like between lanes and sneak between cars. We sometimes have special rules for this. For example, we have rules that say certain crosswalks have more likely pedestrians than other crosswalks. Our car will behave differently for these crosswalks and react differently to people on the side of the street. There is a good number of those exceptions. The truth is the real world is messy. You have to really go through those one after another and try to address them. Have said this, from the mathematic perspective the fewer you have in your code the better. All right. The next question comes from Quartz. He wants to know how we use SLAM in a changing environment, and whether there is a way to forget previous landmarks and acquire new ones. Quartz is a great name. It must be non-American keyword, I guess. That's what it looks like. Changing environments--there's a very simple fix to it, which is as we added uncertainty to the robot position in our system of equations, you can do exactly the same track for landmark positions. It's a little bit cumbersome, but you could say let's make a landmark position at time T and one at time T plus 1, and the one at T plus 1 has added uncertainty, very much like the way the robot moved, but there is no motion, say. The you marginalize out the estimate of time T using the same math we talked about, and you get a new set of constraints that basically encompasses a landmark with more uncertainty. You can even toss in things like estimates of where the landmark moves by putting the velocity of the landmarks into the graph SLAM formulas and it actually carries through, it turns out. I just didn't do this. That's one way to deal with dynamic environments. The next one comes from MJL again. As a robot moves, its location uncertainty is going to increase. Is there any way to pull out this uncertainty from our omega matrix? Yes, there is. It turns out the omega matrix is the inverse covariance matrix. I didn't really talk much about this, but it is really the inverse covariance matrix. If you just invert that matrix, you get the full covariance for all of the landmarks and the robot. If you care about, say, just one landmark, you just take the corresponding columns and rows of the matrix. The next one comes from George. He wants to know what are some interesting trends in SLAM research these days. Do you think there is any good problems that you could recommend a young researcher getting involved with? Tons of good problems. There are some really fantastic working coming out of University of Washington by Steve Seitz on taking very large photo collections and tossing them into a big optimizer to build a 3D map. This is a project that is now at various universities. People go to Flickr or other online photo sites--Picassa-- and they extract those photos that tourists have taken and establish correspondence, say, for buildings and so on, and then reconstruct how this object looked in dense reconstruction. That's in computer vision--often called structure from motion--and we call it SLAM in robotics. Anything having to do with dense data, like with going to Google maps, for example, and get oblique shots and ground shots from street view and aerial shots from an aerial probe or from a satellite and rectify this into a 3D model. These are basically unsolved problems. There is some really good progress, but at the moment you have bridges and underpasses and occlusion. There is a ton of opportunity. You can actually take your cell phone camera or your camera and just go through your environment and try to make a full model of your kitchen. That in itself is very, very hard. Then if you add the fact that the world might be dynamic, like objects might be deformable, like this piece of paper over here. If you want to model this in 3D, that's even harder. That's a fantastic research area that will be with us for the next, at least, 2 decades. The last question comes from Katembe. He wants to know about using landmark correlation. For example in our model of omega we assumed that 2 landmarks couldn't see each other, but in reality sometime we have some correlation between landmarks. For example, street lights come at regular intervals. Do we ever implement this, and if we did would it help? It would absolutely help. Any information you can bring to bear is good. This question is like the same question as having a multi-robot situation where multiple robots interact and move around, and they can see each other. In seeing each other, you insert a constraint that is between these two robots. If you had landmarks that either could sense each other-- maybe you have wifi beacons, and they can sense each other's strength and ability or if you have external knowledge like the street view, this is super informative. The formalism falls apart when there is negative information. For example, there shall be no street lamp within a 100 foot radius of the current street lamp. The reason is these negative constraints are very hard to express in a graph-like fashion. Positive information, like this one is this far away from this guy, is much easier to express. But, yeah, in general any information you can bring to bear will help you solve these really hard problems. All right. That was the last question. Since this is the last office hours I want to say a big thank you to you, Sebastian. Thank you for you. &gt;&gt;And a huge thank you to the students. It's been really amazing working in the forums and helping you out with this course for the last 7 weeks. I know I've learned a lot, and I hope you guys have too. And I learned a lot. I really love your questions. I love you interact. I love hanging out in the forum in the evenings. I hope you guys learned a lot, and I hope these office hours and the course enrich your lives and empower you to do things you've never done before.