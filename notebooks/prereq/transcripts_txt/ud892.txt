Now that you've learned all about editing, building, and optimizing your workflow, I have just a final thing to teach you. To forget everything you just learned, and just use a scaffolding tool. Dude, you can't be serious. Okay, all right. Not entirely. This was all important stuff to learn and understand. But, if you set up plenty of projects going forward, you might want to automate the automation. Okay, here's how it works. Scaffolding is a way of creating a starting point structure for your project based on a couple of assumptions that you control. If say, all you do is create single page movie promotions during your work hours, your gulp file might look very similar every time. In that case, writing it from scratch over and over is inefficient. And it's very likely that you've already done the most basic form of scaffolding plenty of times, copy and paste. To be fair, that's a perfectly valid way of doing things. But it's not the most elegant in the world of web development. Let's take a look at some premade scaffolding tools. You'll find links to all three in the instructor notes. If you're just starting out, the HTML5 Boilerplate is a nice scaffolding starting point. All it does is give you a decent basis for your CSS, HTML and JavaScript. It makes sure your HTML is well formed, that fall backs are provided and that your CSS is normalized. On top, it ships convenient stuff like JQuery. It has a simple build script to combine and minify files, but doesn't come with Gulp or any other advanced build tool. You already passed that stage. For an intermediate solution, check out the Web Starter Kit. It's an opinionated starting point that we've built at Google that already includes a built configuration with live editing and so on, but it won't fit every project out there. And finally, Yeoman for the advanced users, to which I'm counting you now that you've reached almost the end of our course. Yeoman is the most flexible solution of the bunch. Use it together with so called generators that fit the job. After calling Yeoman with the generator, in this case the web app one, Yeoman asks you a few questions about what you need, then creates an empty project template based on your preferences. It's basically a much fancier version of copy and paste. Okay, why talk about this now, at the end of the course? There's no point in letting a scaffolding tool generate a project and build template that you have no idea how to use and modify. Now that you know all the basics, you'll be much more confident trying out various Yeoman tasks. And now we've arrived at the end of our quest to greater productivity. You've learned how to set up your editor to code like a pro. Got familiar with Gulp. Dramatically improved your workflow with live editing. Set up safety nets for easier teamwork and better quality control. And optimized the crap out of your site to ship it to your users. That was a lot, I can't believe we've done all of that in such a short time. Be sure to grab the summary notes with all the links to relevant downloads and projects, and happy tooling. Hi, I'm Paul Bakaus and I'm a Developer Advocate at Google. In my previous role as Studio CTO at Zinga, it's my full time job to have my team tool up to be as productive and happy as possible. And I'm James Williams. A Course Developer here at Udacity, where I make Android and web courses. In the next few lessons, we're going to teach how do you use tools. Okay, not these tools, but these tools, to help you become a better and more efficient web developer whether you're just starting out, or have been around for a while and want to freshen up. So, how are we going to do this? You're going to stay here and translate all the tooling mumbo jumbo into English, and explain the ideas and concepts. Well, then you have to make sure that all the material stays in long term memory by quizzing our students and so on. Now, here's a quick disclaimer. I promise you come out here supercharged and more productive than ever. It's going to involve a lot of fiddling with the terminal, a lot of keyboard shortcuts and general groundwork. Or guard work, I should say, but we'll get to that later. But we'll try to spicy it up as much as possible. All right, let's talk about what you'll learn in this course. We'll kick it off with setting up your developing environment, to achieve that perfect flow. Finally, let's get you a coworker that you can delegate tasks to, your built process. And then we'll dive into all the things our fellow is good at. Expressive life coding, preventing yourself from disasters, and saving a lot of time and effort with automatic optimizations. So let's get going. While optimizing your work is generally good, you can definitely go overboard and over-optimize. Before we teach you some good ways to optimize for the web, let's see if you can identify which optimizations are beneficial. Which of the following optimizations are worth doing considering both the time and risk? A 10% speed up in launching your editor, that takes one hour. A post-process that tests your app on a mobile screen that takes a day to implement. An editor plugin that autocloses your HTML tags that takes 30 minutes to implement. Or a script that generates images for hi-res devices and takes four hours. Check your answers here. Most editors launch in less than four our five seconds. So you'll be making the launch, at most, a half second faster. But chances are, you're only starting your editor once per day. So to get back that hour that you put in, you probably have to start and stop your editor for 20 years or so. Even though it takes a day to implement, testing your app manually on mobile devices is a much more time-intensive and cumbersome process that really doesn't scale well. So that day is well-invested. As a front end developer, you'll be writing lots and lots of HTML. And most HTML tags need to be closed. Even though it seems like a micro-optimization, this time that you save will accumulate really, really quickly. Generating multi-resolution imagery is a perfect job for go tool. Even if it takes you have a day to set up a costumer script, it'll be worth it. You're still here, fantastic. I know you want to get to the juicy bits, but I need to bother your once again with a bunch of general advice, so you don't shoot yourself in the foot. Not all tools are created equal, and like you learned in the quiz, not all optimizations are worth doing. Let's go through a few common scenarios that set you up for failure. Chances are you probably can't. And even if you can, it won't be worth it. To give you a web dev example, hundreds of engineering years went into tiny js libraries like jQuery, including some of my time actually. And yet, countless developers have tried to replicate it with a 90% similar feature set, only to fail hard, again and again. Even popular clones like Zepto, couldn't keep up when they realized that jQuery fixed a lot more cross-browser maintenance than people thought. Even in modern browsers. Now we're talking idealistic versus pragmatic. Yes, that 20% speed increase will be ideal. But sticking to a tool that is well supported by the community, is pragmatic. Gulp might be a little slower than someone else's new tool. But you'll find answers for your questions on stack overflow, plugins for virtually anything and can rest assured, that the tool is going to be maintained by its creators. Don't just trust your instincts, but consider the long-term value for your project. So, it's neatly contained. All in package, huh? Well, be careful with those. If a tool promises to do it all, and stays away from offering any sort of connection points, say, in API or modules, run. No, seriously. Life is too short to buy into an ecosystem that you can't escape from, especially if the success of your site depends on it. And finally, you should be careful with any micro optimizations that are just not worth it. If you are doing an optimization of your work floor that takes, say four hours, and cuts a second away from a task you perform once during the day, you need to do it for 40 years to justify the investment. And in 40 years, you could be sitting in an intergalactic flying car, exploring the outer realms of space instead. Now, on to the course. Little help? Little help? [SOUND] In this lesson, you'll learn how to set up your development environment. Namely, your editor. Wait, wait, wait, wait. I thought you told me I could build a web tack with just Notepad and a browser. Well, yeah, I mean in theory you can. But in theory I could use chopsticks to eat this 16 ounce steak, but in practice there are tools that make the experience much less frustrating. If done just the right way, you can achieve what most developers dream of. The state of mind called flow. Check out this awesome multitool. Consider the possibilities. If I have this baby with me, I could eat a steak on the go wherever I want to. On second thought, it's going to be way more pleasant if I just use this fantastic knife here. Sure, it doesn't have all the bells and whistles, but it's really good at its main job, being a knife. And now we're talking IDE versus editor. An IDE, short for Integrated Development Environment. Is an attempt to create a development productivity Swiss army knife that can do it all. Editing, debugging, building, compiling and even, optimizing. But, IDEs come at the expense of tools that do one thing really well. That's not to say that IDEs never work. Far from it. For app ecosystems, such as iOS or Android, the vendor offers official support, and controls both the programming language, and platform. Which is why why so many native developers use Xcode, Android Studio, or Visual Studio. But on the web, we already have to live with fragmentation. Different browsers, devices and standards. We could try to pretend that CSS, HTML and JavaScript come in a beautiful tightly controlled bundle and build an idea around it. Or we could do what most web developers decide to do. Embrace the chaos and use individual tools best for each job while ensuring that they function well together. Choosing the right editor can be tricky due to the overwhelming options and religious developer battles. It's almost as bad as the ongoing battle of taps versus basis, or semicolon haters and lovers. I'm making it super easy for you. In this course, we'll stick to Sublime text three. So out of all the choices out there, why are we going with Sublime? Well it's simple. Sublime is four to five times more popular than the competition. It's extendable, fast, and very well supported by our community. For free alternatives check out GitHub's Atom editor. It builds on a similar plugin architecture. It's the new kid on the block, and it's getting better every day. In the end, I won't be offended if you decide to use another editor. As long as you stay away from Notepad. Just make sure it's fast enough, can be extended with functionality, and is well supported and stable. Chances are the relationship between you and your will be a long one, and like with any good relationship, you need to work on it and invest early on. If you don't, you'll frustrate each other and things get complicated. In the first couple of days of using it, try on lots of built-in shortcuts and features, even if they seem overkill. Try them all out and observe yourself. Which shortcuts stick? Which do you have to force yourself to use? You'll get a pretty good feel for what matters. But let James give you a quick little glimpse of the little tricks he likes best. >From now on, I will use a few shortcuts here and there to jump around the editor and get different things done. I'm doing this on a a Mac, thus I'm going to use the CMD key a lot, but most shortcuts work just as well on Windows by substituting CMD with CTRL. Searching and finding matters a lot to me. Hitting CMD+T will bring up the fuzzy file finder. The search box forgives many spelling mistakes or omissions so jumping from file to file gets lightning fast. Most developers prefer this method over manually searching for a previously open tab as this involves zero mouse input and no thinking about the tab position. It's a perfect example of an action that looks more complex than clicking on a tab, but is actually faster and simpler. Typing an @ symbol into that box, or directly typing CMD+R, will bring up the symbol search menu. This allows you to quickly locate stuff in the current file, and it works extremely well for CSS or JavaScript functions. Now I know I use this color somewhere else in this CSS file and I want to quickly locate it. So I have it already selected and on my keyboard I'm going to hit CMD+ALT+G and it finds the next instance of it. Sublime Text has a very powerful tap completion feature. In a nutshell, tab completion allows you to type a whole lot less. And typing less usually results in your finishing faster. So if I go into my HTML file and type in img and hit Tab, it expands it out to the full image tag. Notice how not only it created an image tag with the most important attributes for us, it also place the cursor in the right position for us to enter information in. Many editors support multiple selection these days. That's the ability to have multiple selections in the same document that aren't next to each other. But most of the time, folks show that off with column selection, which can be done by holding the ALT key and dragging your cursor around. It sure looks cool but the use cases for it are so rare that I almost never use it. So let's see how we can do this the Sublime way. So I have this word color right here selected and have pressed CMD+D. And it shows us the next instance of that word, without clearing the selection of the first. And it doesn't stop there. CMD+CTRL+G selects all instances of the word throughout the whole file. You can think of it as a global selection. So, this makes it very easy for us to quickly refactor. Remember how I told you that your editor is like that steak knife that was built to fulfill a specific purpose? Well, that wasn't the entire truth. It's true that most editors have figured out how to do a great job with basic editing, but almost all modern editors can be extended with functionality. In Sublime's case, all of it comes from a healthy community of developers, like you. They were missing something and built it. And indeed, right now your Sublime is a genetic text editor. It doesn't care about you being a web developer. In the next step you customize it with the help of James, so it becomes a front-end development editor. Sublime doesn't have a built in plug in repository, so the first thing we'll need to do is install package control. Its the only time we'll manually need to install a plugin. We'll take this scary looking block of text from the notes and paste it into Sublime's terminal. And with that it's fetching the plug in and installing all of the dependencies. Let's test whether it worked. Type Command+Shift+P to bring up Sublime's command palette. This drop down menu is useful in many ways. Because it contains all the actions Sublime can do. I generally use it to discover stuff I didn't know about, like this toggle comment action and to execute actions that don't have shortcuts or have crazy long ones. But right now, type in P-A-C, and you'll see many new commands associated with package control. The one we really care about is install package. This one right here. This is the list of all of the packages in the repository. So now that we're prepared, let's install a few of our favorite plugins. This list of plugins is also in the notes for you to quickly copy and paste. Through package controls and stall command, please install Emmet, Sidebar Enhancements, Color Picker and Color Highlighter. Some plugins require restarting the editor. So to make sure to do that after you have installed everything. Emmet dramatically extends and improves Sublime's built in text snippets. Its craziest feature is the ability to use CSS selectors to create new HTML markup. I know that sounds a bit confusing, but let me tell you what I mean. Let's say I need a list with four items in my index.html file. I can do that by typing the CS selector shown here. Then pressing Tab expands the selector into the HTML we specify. Emmet comes with a lot more built in magic that you should definitely check out in the link in the instructor's notes. The Sidebar Enhancements plugin has a pretty descriptive and spot on name. It mostly extends the right-click menu of Sublime's usually pretty bare-bones sidebar with all of the stuff you would expect. Deleting files, new folders and so on. But also, some nifty stuff such as copying content as a data URI. Now, these last two, Color Picker and Color Highlighter work together to make editing and working with colors in CSS, much easier. Pressing Cmd+Shift+C opens the system color picker and allows you to quickly choose a color. And when the cursor is placed in or around an existing color, picking a new one will update it. The highlighter outlines or underlines colors it finds in your CSS with a preview so you always have an idea what you're looking at. Which of these shortcuts converts text to lowercase? For a quick hint, check the command palette. Check the command palette if you get stuck. The correct choice was CMD + K, CMD + L. Remember to use the command pallet to look up any commands that you want to learn the shortcuts for. And with that, we're already at the end of lesson one. I encourage you to not just stick with the plugins and commands that we just told you, but try to get even more familiar with the editor. Take your time and try out lots of plugins from the package control website. Or see how Sublime feels with codes you have already written. This hammer here is a fantastic tool to build stuff. But in order for it to work, it needs my immediate attention of energy. I can't just turn it on and let it do its thing. In today's affordable tool shed, we've only recently got a few additions that are different. These modern high end tools like this 3D print- [COUGH] We don't have the budget for that. Right. Modern tools like our 3D printer, that's out for maintenance, can be given a task that it then performs autonomously until it's done. In today's life of a web developer, we're seeing a similar trend, a large collection of tools that you feed tasks and that then automate and improve many aspects of your workflow. Let's dive into the world of build tools. Build tools aren't a new thing, but web test specific build tools only became popular a few years ago. The most simple build tool is just a shell, or bash script. These scripts, with the .sh extension, are just a series of terminal commands or functions, and can be executed from the command line. But working with the Penzi graphs, or only updating files that have changed are tricky tasks with just a shell. And this is where the original build system, make, comes in, adding file management, and more. After make came build tools such as Ant, Maven, and Gradle, all especially popular in the world of Java. All of them have declarative tasks. But you need to use XML to drive them, or in the case of Gradle, a programming language that you might not understand. But web developers have a burning hate for everything XML, with the only exception of HML as an accepted flavor. And so came a new set of modern web development focused build tools. Tools that would use a language that web developers are already familiar with, JavaScript. Out of the JavaScript are node JS based build tools, two have become very popular, Grunt and Gulp. But why are these the popular ones and what should you look for in a build tool? I thought you'd never ask. A build tool should be fast in execution as there's need for speed when iterating on a website or app. We're used to changing a line or reloading a page afterwards to see the changes instantly and disrupting that flow is a no go. It should also be supported by a healthy community that exchanges plugins that add functionality and answers questions on sites such as Stack Overflow. With popular build tools such as Grunt, chances are there's already an answer to your problem, and somebody else has had it before. Next, it's very important that even if it comes with a lot of bells and whistles you can extend it with more and custom functionality as you see fit. And finally, the tool should at best already solve a few common problems out of the box for it to be useful. Grunt, the currently most popular tool, fulfills many of these requirements. It has a strong community and a healthy plugin ecosystem. But Gulp, the new popular kid on the block, has two significant advantages over Grunt. It's built for speed and can execute tasks in parallel, plus converts open files into super fast streams internally. Gulp's task, use code over configuration, which means that you can just use normal JavaScript, and extend or modify tasks that don't work for you. You might have already guessed it, we're sticking with Gulp for the rest of this course. But if your team or company chooses Grunt or a different system instead, fear not. Usually the concept of what work is done in all of them is quite similar, and many times the plugin exists for both. What are the core qualities of a good build tool? Is it fast execution and build time, a vibrant plugin community, modular and sharable tasks, or a concise and short API? Check your answers here. Things need to be fast when you're interreading on a website or app. So if you change a line and reload the page, you really want to see those changes instantly. And disrupting that flow is a no-go. So make sure your actual build time never goes beyond a few seconds. Build tools are pretty useless by themselves. You either need to write a lot of custom code or rely on plugins. So make sure to pick one that has the plugins you need. Individually contained task allow you to easily enable and disable certain steps of your build process, and they're important prerequisite. There is no point in saving bytes when writing server-side code. The important bit is that you understand what the configuration of your build system does, not how short the notation is. So what's so special about Gulp? I teased it a little already, but let's have a closer look. The main difference between Grunt and Gulp is that Grunt focuses on configuration, while Gulp focuses on code. But what does that even mean in practice? Have a look at this Grunt configuration file. It's not important that you understand everything it does right away. We'll get to that later. It uses a JavaScript config object, to configure certain tasks, such as concat or uglify. To change or extend one of these tasks, you would have to modify the plugins themselves. Consider the same functionality in this gulp config file instead. It looks more like son of JavaScript, and that's a good thing. At any given point, you can intervene, and type your files into another function before moving on. The second big argument is all about speed. Grunt executes tasks in sequence. One after each other. Where Gulp, by default, executes tasks in parallel, and finishes when all have finished. But that's not all that makes Gulp usually come up much faster. Gulp comes with the concept of streams, that cause much less IO, or file system access. And James will tell you a bit more about how streams work in just a bit. Head to the Gulp installation instructions and instructor notes, and install Gulp on your system. If you haven't done so already, you need to install Node.js and NPM before that, and those instructions are also in the note. If you need a little more help figuring it out, the next note will have more detailed instructions. Every Gulp project starts with a Gulp file. This file sits in the root directory of your project and defines all the task that you should execute when running Gulp. Right here, is the most basic Gulp file that you going to have. Since a Gulp file is essentially a no js script, we first require Gulp as a dependency. Then we setup a default task. Now this function right here doesn't do anything, but of course you'd have it do something. So now it's your turn. I want you to replace this comment do something useful here with your own console.log message. And when you're done, you can run it by executing gulp at the command line. Before we move on and create more complex tasks, let's talk a bit about the concept of streams in Gulp. Other built systems, like Grunt, have tabbed their copier files to a temporary place where they make some change on them. As a result, every task incurs a penalty for I/O in file system operations. Gulp on the other hand, converts your input files into an in memory stream. So the I/O is only done initially, and at the very end of all tasks. This is what gives Gulp such a great speed increase in many situations. We'll start with a common task for many web developers. That task is to make CSS suck less, and boy, does CSS suck sometimes. Prefixes, obscure syntax, the inability to nest or use variables, all make working with CSS a pretty unfortunate part of being a web developer. But it doesn't have to be. When we write our style sheet in Sass, a super set of CSS that gets rid of many of CSS annoyances, and compile it to pure CSS. If you've never used Sass before, check out the link in the notes to learn more about it. Then, instead of prefixing CSS properties for every browser manually, let's automate it with auto prefixer. Both Sass and Auto-prefixer have existing gulp plugins that we can use. So, let's install them as project dependencies. Execute npm install gulp-sass in your terminal, which will install the plugin so, we can use it in our gulp file. Now we have to change the folder structure slightly to account for the generated CSS output files. Create a new folder in your project directory and call it sass. Then move the files main.css and extra.css from the CSS folder into the sass folder. And rename the extension to .XCSS. We just turned them into Sass files that soon got complied into CSS files without a new task. It is now an empty CSS directory. Okay, before we can use the gulp-sass plugin, we need to make it available to gulp. This works exactly as expected. Just required as dependency right of the Gulp. Moving on, we create a new task and named it styles, which is what the first argument is for. The second function as previously is executed when a task is called. Now, we just need to tell a gulp what files we want it to work with. For that, gulp has a special command called source on the gulp object. The line I just added looks for files with the .css extension in a sass folder at any potential sub-directories. Now that we have the files, we piped into Sass. The pipe function on the stream of files, we just created, takes the destination that the plug provides, so we call sass right here. The good news is that we have converted our files from Sass to proper CSS now, the bad news is that we haven't saved them anywhere. So finally, we use pipe again, and use the gulp.dest function to specify our final destination, the CSS folder. This is all we have to do to make it work. But by default, the whole bit with stop and error out when Sass discovers an error. That's not always what you want. Very often, it's better to finish and actually that you see what error happened. Many plugins such like gulp-sass emit events for this scenario. Listening to the error event on a Sass object and inserting the sass log error function here, changes the default behavior. Instead of killing the whole build, it tells Gulp to simply log the error and go on as usual. This looks much better. Now, lets remove the CSS error and try running Gulp in the terminal. See how the CSS files came back into your CSS folder. These are generated from the original Sass files. Now will be a great time to get familiar with Sass if you haven't already. Just to give you an example, I can now go ahead and nest my CSS to clean it up. First mission accomplished. You now know the basics of finding, installing and enabling a plugin. I've provided you a link in the notes, but the search for autoprefixer plus gulp on Google would have reveled the MPM plugin right away, and installation process is the same. MPM installer gulp-autoprefixer in the terminal. And in a requirement of the gulp file. Here's the key part. The autoprefixer object is just another receiver of a pipe dream. Pipe stream. I was just testing if you're listening closely. And since we already have a pipe coming from Sass, we simply add a new line right before it arrives at its destination. Like so. Almost there. We finally insert a configuration object to specify the browser's option of autoprefixer, which tells autoprefixer for which browser versions to prefix. In this case, we'll just instruct it to use the last two versions of every popular browser. And as step two of three, see the filter property used on the image in main.scss? Remove the prefix, then rerun gulp styles in the terminal. And here's the generated CSS file with all the right prefixes. Excellent. It's starting to look like we actually got some work done, right? I told you all the Gulp work would pay off. In fact, I have a final piece that would make a lot more sense now that we have our first task written down. Gulp comes with a killer feature called watch. Think of it as an automatic trigger that waits for something to change. Previously I needed to notify James every time I wanted him to do some task. Now I can rely on the watcher doing it automatically. Consider what you currently have to do. If you make any changes to your sas file right you need to switch to the terminal, re enter gob styles, execute it, switch to the browser and reload. A proper watcher lets you never switch to terminal again. Write the code to set the default task to watch for changes in SCSS files. The function gulp.watch takes the same kind of first argument as grunt.source and that's the path and the type of file. So we can copy that from the styles test that we did a little bit earlier to say watch all of these SCS files. The second argument can either be a callback like in the task function or an array with a set of tasks. In this case, we want to execute the styles task when a sass file is changed, so this is what we'll do. And this wraps up our second lesson. Reflect for a moment on what you all learned. I've explained why build tools matter, and what they're good at. Such is the core concept and usefulness of gulp. And I walked you through creating some tasks to make sure it all stays in your head. Take a break, you deserve it. Not too big of a break, we still have work to do. So Paul, what actually happens when you connect your bill process to the browser? [SOUND] Magic. I'm going to need you to be a bit more specific. All right, all right then, let me tell you about the magical world of live editing. Say your job is to construct shelves, just imagine how much your life would suck without a power drill. Sure, a manual screwdriver does the job, and works, if you're just constructing a shelf for yourself once a year. But a power drill will save you lots of time and energy in the long run, and leave you great optimization. Unless your plan is to get Popeye's arms, of course. In the world of web development, you'll see a very similar pattern. You make a change to your code, save, rebuild if you need to, switch to the browser and reload the page. These micro context switches might seem harmless, but you're doing this thousands of times each day. If you now ask yourself, well, surely there's a better way then? You'd be right. Live Editing solves exactly that problem. It works by having a watcher in your editor or build process that's connected to a similar watcher in the browser. When you save a file, these two communicate, and the website is reloaded or patched on the fly without you having to intervene. That means fewer contact switches as you can now have your editor right next to your browser window and see changes happen as you code. In the next hands on step, James is going to be teaching you how to set up Live Editing in three different ways. Live on every keystroke in Sublime, on every save via Gulp, and skipping the editor all together to move your entire workflow into the browser. In the end, we prefer to build two versions, and James is going to tell you why. Which of these are advantages of live editing? Fewer context switches. Less clicks and keystrokes when changing code, quicker preview of changes, or faster build times? Check your answers here. With your editor and your browser output side by side, you'll need to do a lot less context switching. Usually you'll need to reload the page manually and trigger the build process again after you've made some changes to the source code. If your build system is doing this for you, you can save a lot of time. While the actual time of the rebuild or reload of your page doesn't change, the reload is done automatically. So, by the time you focus on the output, you've saved a couple of seconds of contact switching. Even though it might feel faster due to the automatic reloads, the build process actually isn't sped up. It's your brain. One way of setting up live editing is within your browser. Some editors, like Brackets, come with live editing built in. In Brackets, you press a button that will launch a new instance of your browser with your updates already live. Sublime Text doesn't have it built in, but the Takana plugin gets pretty close. However, it supports CSS and SCSS live editing, but not HTML. Chrome DevTools has a relatively little known feature called Workspaces that allows you to ditch the editor altogether and work directly in the browser. You can make changes to your CSS right in the Styles panel, and have them persist. The same goes for JavaScript. There's a link in the instructor's notes listing all the steps you need to take to set up Workspaces on your computer. The fundamental flaw with live editing in your editor is that it isn't aware of your build process. So be mindful of that. Browsersync allows us to have live editing that is assisted by our build tool, improving upon the two methods we showed you before. For this to work, we can reuse something we've already learned about, and that's the watch task that currently watches our Sass for changes and compiles them into CSS. Browsersync works by creating or proxying a local web server which serves and tracks your files for changes. Best of all, it's free, open source, and is compatible with most Node.JS-based build tools including Gulp. So let's go ahead and set it up. And there you have it. A seemingly complex chain of events, running nicely together to dramatically accelerate your work flow. Keep going for awhile and modify the styles of your sample page to get a feel for how this impacts your productivity. Afterwards, we'll see you in the next lesson. Now that we've set up all of our tools, bells, and whistles, we can use them to automate a few alarms and safety nets that will make our lives much easier. I had to learn it the hard way and, hopefully, you don't have to. In this lesson, you will learn how to prevent cross-browser issues in your CSS, catch JavaScript errors before they happen, and more. When I need to build something using materials I shouldn't be breathing in, I naturally put on a dust mask and safety goggles. Just like that, our tools can save us web developers from disaster. Now, of course, you won't accidentally amputate a finger from bad JavaScript, but you can make any of countless mistakes that can result in your site being unusable for a lot of users. Here's the thing. Tools aren't perfect either, but their never tired, never lack concentration, and don't have an ego. By utilizing your build and editor tools, you can quite heavily improve the quality output of your coding. Let's talk about the numerous ways to get there. Linting is a way to automatically check your Java script code for errors. And it can be done at various stages during development via your editor, your build process or your pre-commit hook in version control. There's not always a right or wrong way in linting. A lot of it is heavily opinionated. So you should choose the configuration that fits your coding style and project. There's also the difference of code style linting versus syntax or structural linting. Syntax or structural linting is what most people refer to when they say linting. These rules check for JavaScript anti-patterns, such as unreachable statements or forgetting to do a strict comparison against null. On the other hand, code style linting can complain about things such as variables that aren't properly camel cased, or a particular way of placing braces for a function. So if linting ensures your code looks sexy and checks for all these potential errors, does that mean your code will always run if the linter is happy? Nope. The linter only checks for potential errors. It doesn't actually have any idea what you're trying to accomplish. So now that you're familiar with the concept of linting, lets talk solutions. There are three popular JavaScript linters out there that developers use, JSHint, JSCS, and ESLint. You'll find a link in the notes that details the differences, but to cut it short, we'll stick with ESLint as it supports modern ES6 code, can be extended, and has output that's really easy to understand. Let's see if you remember the various things that linting is good at, and the things that it doesn't help with. How does linting help your code? So how does linting help your code? The first one here is correct. Having a linting configuration for your project helps with code sharing and development with multiple team members. Linting certainly does help with all these and more. Some dead code is easily forgotten and can mess up your app in many obscure ways. And linting will help find many of these patterns. Linting does not help identify slow functions. Linting works purely by analyzing the actual source code. In order to find out how long a function takes to execute, it needs to run in the browser. A linter also doesn't help with this last one. Given the dynamic nature of JavaScript, often the type information is not known. To make the linter most effective, you want to have it run at the earliest possible time. And the earliest possible time is, in your case, after you press the key on your keyboard, making a change in Sublime. In order for this to work, you'll need to install ESlint first via npm. Run npm install eslint in the terminal, and don't forget the g option to install it globally. Great. Now that you've got eslint on your system, you need two different Sublime plugins. SublimeLinter is a framework around linting, but doesn't come with specific language linters. And SublimeLinter-contrib-eslint is the wrapper code that connects ESLint to SublimeLinter. When you're done, restart your editor and come back to this video. Now look at the main.js file in the js folder. Notice that the linter doesn't do anything yet. You might be forgiven for believing that something didn't work during installation. But all is fine. It's just that ESLlint, by default, doesn't do anything unless you configure it. Luckily it's simple to generate a basic configuration. Switch back to your terminal, where you should still be in a sample directory, and run eslint in it. This will bring up this nifty prompt that asks you a few questions, then generates an eslintrc file for you. If you now open that file in Sublime, you'll notice a few style rules from the prompts you just answered. But the key is the extends block at the bottom. This tells eslint to run with its recommended set, and anything you add on top will overwrite or add to it. That's super useful as a starting point. Here's eslint in action. See the foo variable, the red dot next to the line numbers, and how Sublime highlighted it with a red border all around. To know which error occurred, just click on on foo and look at status bar at the bottom of Sublime. In this simple example, it's obvious that we're missing a var. Let's fix it. But now foo is still red, oh but the status changed. We actually fixed a previous error and are now looking at a new one. Foo is now defined but never used in your code. So it's fair that ESLint complains about it. Let's add a return statement to actually use foo in some way. Here we go, just as I stop typing, the red vanished and thus, all errors went away. If you open your gulpfile.js, though, you notice a lot of red around the require blocks, that's because ESLint thinks that files run in the browser. And the browser has no require function. We only want ESLint to turn off no JS warnings in this file, and luckily, you can do that. This special type of comment works just like a configuration, but it's local to the current file. Before we move on to learn how to integrate ESLint into your build, keep in mind that the amazing SublimeLinter plug in also supports dozens of other linters. So if you'd like your CCS, HTML, or even PHP lintered, there's a plug in for that. Check the notes for links on what else you can do with SublimeLinter. If you're collaborating or working on another computer, the other party might not have the linter configured. Since your colleague needs to run the build to work with the site anyway, why not have the build run the linter, and complain when something goes wrong? Install the gulp-eslint package and require it in the head of your gulp file. Turns out the basic example on the gulp-eslint readme works well for us. So I'm just going to copy it and paste it into my gulp file. [BLANK_AUIDO]. As you'll notice, this task looks very familiar. It has the gulp source code, but this time fetches js files, then uses pipes to do a few things with eslint. The first pipe executes eslint and all files matched. The second line outputs the errors to the console, so we actually see what happened. And the third pipe ensures that gulp exits with an error code and fails. Without that line, gulp would show the errors but would proceed with everything else. Now we could execute the task manually. But let's integrate it into our default task. First, add it to the second argument area after styles. So it runs right after running Gulp in the terminal. Then add a new line after gulp.watch to add a new Gulp watch. This time watching the js files and calling lint, instead of styles. The best thing is that we can later reuse this new watcher to do even more great things to our js. And there we go. Try resetting Gulp in the terminal and makes some changes through your js to see linting appear on your terminal after every save. To wrap things up, there's a third almost desperate way of forcing eslint up on your project collaborators. With a pre-commit hook, you can let everyone go crazy on your own file systems. But as soon as they want to commit a change to the shared git repository, eslint will run error out and not let a commit through. In order to learn more about pre-commit hooks, you should check out our source control Udacity course. You've just learned how to protect yourself from syntax and coding style issues. But as you know, this doesn't ensure your code does what it's supposed to do. In order to test the functionality of your code, you should create unit tests. Unit tests are essentially JavaScript functions that pragmatically test an API or aspect of your project. And if this topic is new to you, I encourage you to check out the Java Script testing course here at Udacity. For the purpose of this course, we just assume that you know the basics over learned in layout. And thus we provided a sample test suit as part of our sub lab. Unit test, like Linton, are here to prevent mistakes. Wouldn't it be cool if you could automate running them just like everything else was set up so far with our built. Well there's one issue with that. The unit test for your front end only makes sense if they run in a browser. Thus, running them from the terminal environment won't do any good. But this doesn't mean it's impossible, it's just slightly more tricky. Here's how it works. The key to make Jasmine, the unitest framework used in our [INAUDIBLE] work with our build, is to use a headless browser that we can execute an instruct from the command line because that's what gulp can deal with. Luckily, such a browser exists. It's called PhantomJS, and it's basically a headless version of Webkit. And you don't need to know much more about it right now. Just the fact that the gulp-jasmine-phantom plugin uses that to its advantage to actually run your tests in a real browser environment. Install PhantomJS first using the instructor notes. And then like always, install gulp-jasmine-phantom via npm in your project directory. Then add a require to your gulp file, like so. Now all we need to do is to create a new task called tests. Use gulp.src to find the correct test file we want to run, then use a require with Jasmine. The degradation setting tells Jasmine to use PhantomJS. If it's set to faults, it runs in a node js environment instead which you'd only want if you actually test node js code. And finally the vendor setting needs to point to our JavaScript source files, as this plugin constructs its own specrunner.html with those. Now head back to your terminal and run gulp tests to see Phantom JS and Jasmine in action. All right, what's next? I'm sure you were thinking we now add it to the default task, and then to the JavaScript watcher. And you're half right, that's what we would ideally do. But in an ideal world, we'd have hoverboards and unlimited performance on our computers. Turns out that running complex unit tests, especially in a headless browser, can get really, really slow. So adding into our watch process would kill our life editing workflow. In order to solve this problem, smart people invented continuous integration in the Cloud. Continuous integration is the idea that you're always making sure your code can be integrated with the remote repository.. So across a team, you'll always have a stable build. Now we won't go into detail on CI, as much of it is already covered in the dev ops udacity course. But a key lesson here is that CI in the cloud provides a great place for your time intensive tasks. In particular, your unit tests. A cloud solution like Jenkins will watch the commits going into your repository and trigger any terminal commands you feed it. So if you take the Gulp test task that we've just created and hook it up in the cloud, it means that the test suite will run after every commit. But on a completely different computer, letting you go in coding. If one of these tests now fails, you've got a email and can fix it in your next commit. We'll leave our task as is for now, but do go and check out the dev ops Udacity course when you're finished with ours. And there you have it. With the right safety goggles in place, namely our linters in the unit test suite, you'll feel a lot safer and can iterate our new changes without worrying too much. Linting and continuous integration can help you find errors in your code when they're easily fixable, before they become a catastrophe. Great editing environments, safety nets in place? Check and check. What else could we need? Well, we've optimized your development workflow but that's about it. Now think about all the things that you do or should do after you finishing coding and planning a release. Things like unifying and concatenating your source files and optimizing your images. In today's mobile world, you see hundreds of different device, browser, and screen combinations that use your app or site. In some cases, it's simply impossible to optimize by hand. This is where your build process steps in. We'll use it to optimize things that you can't fix by hand, or simply take too long. And doing so will make your build more powerful, so that it takes a raw source code and polishes it for production. Before we go and optimize it's important to understand that these optimizations are only meant for production. Sure you could execute them all the time, but it would dramatically slow down your iterative build time, and thus make live editing a lot less powerful. Instead I recommend to split your tasks into development and production. Development tasks contain things you really need no matter what. Sass processing, for example, and tasks that only make sense through encoding, for instance, live editing. However, keep in mind that while developing, this means you'll be using a different version of your app than your users. It's going to look and feel mostly the same, but sometimes performance issues or bugs only manifest with a specific optimization technique. So make sure to always test the production version from time to time. In our next couple of instructions, we'll be creating more generated assets. Previously, the only generated files we were producing were the CSS files. But now we need to create a structure to hold production and development files separately. To get started, let's create a disc folder that holds all generated files separate form the source files. This wasn't a problem with sass as we generated the CSS into a separate high level CSS folder. But it would be with JavaScript because we don't want our source and distribution files to co-mingle. The process is pretty simple. We first copy our index.html into the dist folder, then generate our css into dist css, and finally generate our combined JavaScript file into dist JS. Since we've already got our CSS set up, let's change what's needed in our styles task. Here's our styles task in the grunt file. And we can see our destination is set to just the CSS directory. To make it show up under this dist directory here, we just changed the destination to dist/css. And, when we run gulp styles, we can see that the files have been generated and dropped in the correct folder. Next, let's copy the index.html in images to the dist folder. We simply set up two new tasks and call them something like copy-html and copy-images. In the copy-html task, we grab the index.html file with gulp.src and just pipe it to the destination in the dist folder. For the copy-images task, we do the same thing, but instead of copying a file or grabbing a locator to a file, we're grabbing all the files in a specific directory in piping them to our new destination location. You'll want to set up a new gulp.watch next to the others that watches the original index file. You could do the same with the images, but they don't change that often. And don't forget to add the two new tasks to the array that's passed as part of the default task, otherwise the files don't get copied over the first time we run gulp. Just a final fix before we can see our page run again minus the Java Script which will come in a bit. You'll need to modify the browserSync.init function call and make it point to the dist directory, now I've already done this for you. Okay, now it's your turn. Set up the build folder in commands to assemble its contents. Did you notice that browser-sync isn't listening for changes in the index.html file? Do you have an idea how we could fix that? Go ahead and type your code here. If you search for a solution, it's very likely that you found more than one. But here's one that was easiest, shortest and worked the best for me. I just setup another gulp.watch, but this time it isn't watching the original index.html, but the copied one. And every time that the copy operation is done, we can now execute browserSync.reload to reload the whole page. Make sure this works for you. Then we can move on with the JavaScript processing. The first optimizations we do are easy to implement and make our code load faster. We first glue our CSS and JavaScript files together through concatenation, then crunch them with a minifile. In previous lessons, we applied a couple tasks to our CSS. But this time it's mostly about JavaScript. I say mostly because these topics still apply to CSS. It's simply that we're already mostly covered. Sass does both concatenation and minification for you. Manual concatenation isn't necessary, because you can simply include a single Sass file in your HTML. Then use the import directive in your Sass to input other files into your base file. When the Sass compiler processes the Sass into CSS, it will automatically inline those inputs and generate one big CSS file. A minification is just an optional way. Just modify the Sass pipe slightly and add outputStyle: 'compressed' as config, which will produce a nicely compressed file. Onto the world of JavaScript. Concatenation here solves two problems at once. First and foremost, it reduces a number of HTTP requests needed to load your page in production. Which is a big deal, especially if you're on a mobile connection with up to 300 milliseconds of latency per request. Secondly, it's the most basic variant of dependency management. You put all your script into a folder and instead of having to add script blocks to load them one by one into HTML, you simply add a single script block that points to the generated output file, including all of them concatenated. I say most basic variant because it obviously isn't smart enough to detect dependency chains and the required load order. But solving these problems is a much harder task. Lets keep it simple for now. To get started, we should first create a scripts task. However, since we want to do slightly different things in development and production mode. We add another task called scripts-dist, which will be used when we want to distribute our fights for production. In the first step, both of these tasks will look the same. We gulp source the files we need, in this case, our JavaScript files. Then pipe them to the correct destination, in our case, the dist/js folder. Now install the gulp-concat plug-in via npm, and require in the gulp file, then add a new pipe to both tasks with concat all JS before the destination pipe. This plugin takes the files in the stream, and combines them into a single file named whatever you provide as the argument. Try running one of those task in your terminal, to make sure the concatenated all.js ends up in the correct folder. All good. Then we're missing only one last thing. We need to change the references to the individual js files in our index HTML to point to the single js file, at js/all.js. And now your page should run fine again. After concatenation, it's now time for minification to shrink the file size of our JavaScript. The most popular minifier today is which does some heavy but safe optimizations to squeeze every last bit out of your raw source code. And you might have guessed it, plug-in and include it into the gulp-file. Now this is where our scripts and scripts-disk tasks are starting to become slightly different. As JavaScript minification is a very time-intensive stop. Therefore, it makes no sense to do this while live-editing your code. Add the missing pipe to the scripts-dist task right after the concat pipe and call it pipe uglify. Yes, that's all you need and calling this task now will produce nicely concatenated and minified JavaScript. To produce a production ready version of our site, we can skip the whole live editing and watching, and include the scripts distribution task instead. Nothing easier than that. Let's call the new task dist and include the following tasks in that order. Copy html. Copy images. Styles. Lint. And finally scripts-dist. Try running the tasks via gulp scripts-dist. If gulp takes slightly longer, and exists without opening the browser, you're all set, and have a production ready distribution in your dist folder. Before we continue, one word of advice regarding minification. Minification on its own is great, but GZIP is even more affective. GZIP compresses the file before it gets in, and out to the browser, and the browser deflates it. All of this happens transparently in the background and usually only requires a small server configuration change. Read more about GZIPing in the nodes, There's another very worthwhile optimization we can do to our JavaScript, and it's quite similar to how we use Sass instead of CSS. Turns out there is a way of running the very latest spec of JavaScript, ECMASript 6, even though there's no native browser support for many of the features. All we need is a transpiler, which takes one programming language, and converts it into another. Sometimes transpilers stay very close to ECMAScript syntax, adding in a few features here and there. In other cases, they are full implementations of languages you don't typically find in purely front end web development. We'll stick to the former category. Our transpiler of choice is Babel JS. It's very popular, feature rich and well supported by the community. Now of course this step is purely optional. If you're happy with today's JavaScript and don't need all the fanciness, great. But if you're curious to try out arrow functions, generators and classes, now is the perfect time. And sure enough, getting this into our code is as simple as everything else. Just grab the gulp-babel plugin and require it in your gulp file. And in both script task, pipe it after the gulp.src, but before the concatenation. You won't see any difference right away, as you're not actually using any ES6 magic in your current code. So head over to Babel's ES6 learning page and get familiar with some of the concepts. Why is transpiling a good thing? Is it because standardization and implementation across browsers takes a long time, browsers can't support many ES6 advanced features, or that browsers decide to stay with ES5? Check your answers here. It's tempting to say that the browser environment isn't able to support ES6 features, or that browsers decided to stay with ES5, but the real reason is that standardization takes a long time. The developers of each browser will decide on their own schedule which features to implement and when, if ever. Using a transpiler gives you the chance to use tomorrow's features today without ruining the user experience. Images. What would the Internet be without them? Just wait another 100 years, and people will talk about the age of cat images when discussing art of the 21st century. And animated gifs will be the quintessence of historic presentations of the web. In all honesty, though, images don't just spice up the web, they also make websites freaking huge. In fact, the HTTP archive found that 63% of an average website's bytes are images. At Udacity we think responsive design and optimization is really important. Otherwise we wouldn't have made four courses on it. I've linked them all in the instructor's notes. I'll give you a moment to bookmark all the things. For further background on some of the things we'll be working through with images, definitely check out the Responsive Web Images course. Next, we'll chat a little bit about how you can optimize images. How should we compress these types of images? We have, a Hamburger Menu, a Hero Image, and Image Thumbnails. And the choice between Lossy, Lossless and converting to SVG. Check your answers here. Hamburgers are generally small in dimension and have a limited palette. So you can get sufficient improvement by either using a lossy compression algorithm or converting it to SVG. Your hero image needs to have crisp quality to show that it's large and in charge. You should use lossless compression on it. Lastly, image thumbnails don't need to be pixel perfect and we want them to load fast. So applying a lossy compression algorithm works well for our needs. [SOUND] Don't you just love the taste of a freshly optimized website, James? There's nothing better. Indeed. The complex aroma of minified JavaScript poured over crunched images, are one of the subtle flavors of ECMAScript6, just the way I like it. [SOUND]