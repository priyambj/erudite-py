In this lesson we will look into more advanced topics in caching like multilevel caches and various cache optimizations. These are really important to get good overall performance and energy efficiency. So this is a lesson on improving cache performance. There are many methods for improving cache performance, but in general, they can be grouped in three categories. And these three categories have to do with the average memory access time or AMAT, which is equal to hit time + miss rate x miss penalty of a cache. So pretty much if we always hit, we get the hit time. If we miss, we get the miss penalty and how often we miss is the miss rate and the average memory access time as seen by the processor is equal to this. So the methods for improving cache performance can be those that try to reduce the hit time, those that try to reduce the miss rate or trying to reduce the miss penalty. All of these would reduce the average memory access time, which improves the overall performance of the processor when it needs to access the cache. So let's first look at the methods that try to reduce hit time. Some of the methods are pretty obvious, like reduce the cache size. That's going to improve the hit time, but this is bad for the miss rate. So the overall aim at might not actually be improved, because we are going to Improve the hit time, but we're going to increase the miss rate and thus, pay more of those miss penalties that are quite large. So we can do this, but we have to do it carefully to balance the miss rate and the hit time. The next simple method is to use the cache associativity. This is going to make the cache faster, because we need to search in only one or few places for a block on a hit, but this is also bad for miss rate for similar reasons. When we reduce associativity, we are creating more messes. Because blocks is going to start having conflicts and start kicking each other out, even though they might coexist peacefully in a more associative cache. So the less simple methods for a use in hit time involve overlapping the cache hit with another hit. Overlapping the cache hit with the TLB axis. Optimizing the lookups, so that common cases get faster without sacrificing the uncommon case too much and maintaining the replacement state in the cache more quickly. So on hits, we need to update some state that we will need later for replacements and we can make this more efficient. So we will look at four methods that are trying to achieve these four things here. So one way of speeding up hit times is to overlap one hit with another, and we can achieve this, for example, through pipelining the cache. If the cache takes multiple cycles to access, we can have a situation where an access come in cycle N, and it's a hit. And now if another access comes in cycle N+1, and it would be a hit, too, in a non-pipeline cache, this second access has to wait until the first access is done using the cache, and it takes multiple cycles to do that. So in this situation, the hit time, as seen by each access, is the actual hit time of a cache plus the wait time that the access suffers because it cannot access the cache until the previous one is done. And in that situation, pipelining the cache so that we can send in accesses one after the other will improve the overall hit time. Now it may sound straightforward how to pipeline the cache. You just divide it in, let's say, three stages. But how do you split what amounts to basically a read from a large array because that's really what the cache is? Remember that the cache access consists of using the index part of the address to find the set. Reading out the tags and valid bits that correspond to the blocks in that set. Comparing the tags, and checking the valid bits for each of these to see whether it has a hit. Combining these so that we know whether we have an overall hit and where in our set. Once we know where, we read out the data and use the offset to choose the right part of the large cache block. At which point, we have the data that the processor wants, and our cache access is done. So one example of a cache pipeline would be to have this part, reading out the tags and so on from the cache array, be stage one. Determining the hits and beginning the data read would be stage two, and finishing the actual data read all the way to getting the data would be stage three. So as you can see, we can pipeline the cache access, even if we don't know how to actually break down the reading from the cache array, especially if the tags and the valid bits are read before we determine the hit, and we only read the data part of the cache after we determine the hit. In that case, just those two can be separate stages. Usually, the actual cache hit time for level one caches will be one, two or three cycles. One-cycle caches don't need pipelining, and two- and three-cycle caches can be relatively easily pipelined into two or three stages. So usually, level one caches will be pipelined. Next, hit time is affected by having to access the TLB before we access the cache. Remember that the processor starts out with the virtual address. We use some part of that virtual address to index into the TLB and find the frame number. We then combine that with the page offset and now that we have the physical address, we can use it to access our cache and get the data from it. So really, if the TLB takes one cycle and the cache takes one cycle, we need two cycles from when the processor gives us the address until we actually get the data. Because really the TLB access and the cache access have to happen one after the other. A cache that is accessed using a physical address is called a physically accessed cache, a physical cache, and also it is called physically indexed or physically tagged or PIPT cache. The overall hit latency of that cache really is the TLB hit latency and the cache hit latency. We can improve the overall hit latency of the cache using a virtually accessed cache. In that case, the virtual address is what we use to access the cache and get the data. On a cache miss, we would use the virtual address, to tell us what the physical address is, so that we can bring data into our cache. But on cache hits, we can get the data without doing the TLB access at all. So the advantages of this virtually accessed cache over the physically accessed one are that the overall hit time is just the cache hit time with no TLB latency added to it. That's nice and we don't need the TLB access on cache hits, so we can save energy. We like that too. So we can see that the virtually access cache has the hit time that adjust the cache hit time. Because we use the virtual address to access the cache and because of that we get the data even without address translation on a cache hit. So we can only access the TLB on a miss, which saves us TLB misses and saves energy which is also nice. So it looks like the virtual access cache would win every time, so why are we even considering physically accessed caches? Well, first of all, the TLB, in addition to containing the translation for the physical address, also contains permissions that we need to tell us whether we are allowed to read or write or execute certain pages. So even though we don't need a physical address from the TLB in order to get their data, we still need to access the TLB even on cache hits just to get the permissions that tell us whether we are allowed to read, write, or execute that location. So this advantage really doesn't exist in real processors. A bigger problem is that the virtual address is specific to a particular process. So if we were running one process and filled the cache with it's data, once we begin running another process that other process will have virtual addresses that might overlap with the addresses from the previous process. But they should be going to different data. Pretty much in the TLB there will be different translations for that other process, and we should be accessing different actual memory locations. Because our cache only knows about virtual addresses in this case, we now need to flush our cache meaning remove all the data from it every time we do a context switch. Once we are switching to another process, we know that virtual addresses are going to map to different locations, so what we have to do is get rid of everything we have in the cache. This means that we have a burst of cache misses every time we switch processes. Processes are switched once every millisecond or so, so it doesn't sound like a big deal. But keep in mind that the cache can be large, so it takes a lot of misses to bring in the data into it. So this is a disadvantage. So we invented a third type of cache, which is called virtually indexed and physically tagged cache, which is trying to combine the advantages of the two types of cache. It works like this. We start with the virtual address, we break it down into the cache offset, index and tag, but we don't use the tag. We do use the index bits from the virtual address to find the set that we want in the cache. We read the valid bits and the tags here. But meanwhile, we use the page number from the virtual address to access the TLB and get the frame number. So now that we got the physical address, we perform the tag check using the physical address not the virtual one. This is why this is called a virtual index. The index bits come from the virtual address. The tag bits come from the physical address, cache. And from here, it proceeds normally, we know whether we have a hit and so on. So how does this help? Note that the cache array and the TLB access are proceeding in parallel. So if the TLB is fast enough and usually it is because it's small, the hit time will be equal to the cache hit time. It's actually the maximum of the two, which is usually the cache hit time. So this is just like the virtual or virtual index virtually tagged cache and we like that. We get the speed advantage of the virtual caches with this type of cache too. Do we need to flush on a context switch? Well, it turns out no. Because if we switch the process, so that virtual addresses correspond to different physical addresses. What we find in the cache will be checked against the actual physical address. So the virtual address in another process will map maybe to the same set. But once it maps there, the actual tag is not going to match, because these two have different tags. So this is like the physical cache and is nice, but what about aliasing? Recall that aliasing happens, because multiple virtual addresses in the same address space might map to the same physical address. But because we do virtual access to the cache, these addresses might end up in different places in the cache. Thus, right to one will not be seen by the other. It turns out that there are no aliasing problems in the virtually index physical tagged cache, as long as the cache is small enough and it has to do with where are the index bits coming from. So this is very, very nice, because it basically gives us the correctness. The no flush on context switch of the physical caches with the hit time of the virtual caches, but let's figure out what do we need to do in order to not have aliasing. We have seen that virtually accessed caches can overlap the latency of the TLB lookup and the cache lookup. However, virtually accessed caches have a big problem called aliasing. The problem of aliasing occurs because in the virtual address space of the application, we can have one page mapped to some part of the physical address space. For example, we can use the mmap function in Linux and other operating systems to map part of a file so that it appears in a range of memory that we can address. The problem occurs when another page in our virtual address space is also mapped to the same physical addresses. For example, when doing an mmap of the same part of the same file to another address. So now what we have is two virtual addresses will actually refer to the same physical location. To see why this is a problem in a virtually accessed cache, let's look at two specific addresses. So let's say that A and B are 12345000 and ABCDE000, and let's say that we have a 64 kB direct-mapped cache with a 16 byte block size and that is virtually accessed. So let's say that this is our 64 kB cache. For this cache, we have a four bit offset part of the address because of the 16 byte block size. It's a 64 kB cache, so the next 12 bits are going to be the index into this cache. 64 kB divided by 16 bytes is four kilo entries, so we need 2 to the 12th index bits to tell us which index do we have. And the rest of the bits in the addresses are tags. So this is how the aliasing problem occurs. Let's say that the processor writes a value of 16 to A. It breaks down the address, indexes into the cache, using the index of hex 500. Let's say that this is a cache miss. So we go to memory, and from the physical location where this data is mapping to, we fetch a value, and let's say that this value is four. Once we fetch the cache block, we now put 16 there, and the new content of the cache block is 16. Let's say this is a write-back cache, so this 16 just stays there. Now, let's see what happens when we try to read B. We index into the cache using the index for B, which is E00 hex, and because what we are looking for is not here, we go to memory and fetch the value, and this maps with the same physical memory location, so we are going to get four again. And now we're going to read that. Note that this problem is not just a problem on the cache misses. >From now on, whenever we write to A, we will be changing this cache block, and whenever we read from B, we will be reading this cache block, so they never really end up sharing the data as they should. What should've happened is because A and B are really the same data, when we write to A and read B, we should be getting the same data. And this problem results in incorrect execution whenever we have such a mapping. Unfortunately, such mappings are perfectly legal to do in most operating systems, so virtually accessed caches need additional support for this. Every time we write to any virtual location, we would have to do a check for aliases or different versions of the same physical data in the cache and either invalidate them, remove them from the cache, or update them so that they would see the new value. And this is really expensive to do, and it kind of defeats the purpose of virtually accessed caches, which are all about the latency and not needing to do translation. Now, when we write here, we would need to do translation in order to check where else do we have the same mapping. To summarize this, we like virtually accessed caches because they allow us to overlap, tabulate, and see with cache latency, but we cannot use them because of the aliasing problem that seriously complicates their implementation. So lets look at the problem of aliasing in the virtual index physical tag caches. This is a virtual address. And the way we access the cache is these are the offset bits, this will be our index bits, and this will be the tag if we use this to access the cache. In a virtual index cache, we use the index part of the address. >From the virtual address, the rest comes from the physical address. But how do we form the physical address? Well some lower bits of the virtual address will be the page offset. The rest will be the page number. And now remember that the page number gets translated to become the frame number in the physical address. While the page offs just goes and becomes the least significant base of the physically address. And in a virtually indexed, physically tagged cache, our tag comes from here, but our index came from from here, so that we could begin the access quickly. The thing to note is that the index bits are relatively close to the least significant bit of the virtual address, and that the page offset has a number of bits. So for a small cache it can happen that the index bits that we use all come from the page offset, which means they are the same as if we took them from the physical address. So, although we are indexing using the virtual address, in reality, we are actually using the same index that we would be using if we were using the physical address. Which means, there will be no aliasing. How is there no aliasing? Well, because the virtual pages, that have different page numbers that map to the same frame number, can only differ in the page number, but they have to have the same offsets for the same data. And because only the offset matters for the index if the index is low enough here. What happens here is, pretty much, all of the data that can possibly be in the same place in the physical memory has to be in the same set, because the index is determined only from the part that has to be the same. So we have that there is no aliasing if all of the index bits that we use came from the page offset. Because really those are the same index bits that would come from the physical address if we had the physical index cache. We like this very much, but the cache has to be small enough to do this. So for example, if we have a 4 kilobyte page, we will have a 12-bit page offset. If our cache is a 32 byte block, it has a 5-bit block offset here. That means that the index, which still needs to fit in the 12 bits of the page offset, can only have 7 bits, so we can only have 128 sets in this cache, which limits its size. If we have more sets than this, then some of the index bits here come from the page number. And that changes when we have the frame number, so it can easily be that the different these bits here end up mapping to the same frame number, which we don't want because that means that multiple sets in the cache could have the same actual data, which is bad because we cannot find it then. So now that we know how big the virtual index physical tag cache can be to avoid aliasing, let's do a VIPT aliasing avoidance quiz. Let's say we want a cache that is 4-way set associative and has a 16 byte block size, And let's say that the system has an 8kB page size. If we want no aliasing in this cache, its maximum size can be how many kB? Let's look at the solution to our virtual index physically tagged cache, aliasing avoidance quiz. We won't know aliasing, which means the index bits have to come from the page offset. The page, has 2 to the 13th bytes, so the page offset has 13 bits. In the address breakdown for accessing the cache, that means that the index and offsets for the cache need to fit in 13 bits. The 16-byte block size means that the cache block offset is 4 bits. That's how many bits we need to tell us where in the 16 bytes we are, and that means that we have 9 bits for the index. We can use fewer, but that would make the cache even smaller. We cannot use more, because then we will get aliasing. So the maximum cache size will be when it has 2 to the 9th sets. So we have 2 to the 9th sets times 2 to the 4th bytes in each block, times it's a four-way set-associative cache, so we have 2 to the 2nd, four blocks in a set. So this is basically bytes per block, blocks per set, how many sets? And that amounts to 2 to the 15th bytes in the cache, so the answer here is 32 kilobytes, which is 2 to the 15th bytes. So we cannot have a cache that is larger than 32 kilobytes and yet it avoids aliasing with these parameters. Note that really these two have to be equal to the page size because 2 to this number of bits times 2 to this number of bits, has to be equal to 2 to this number of bits. So really, the maximum size of the cache here is the page size times the associativity. And the only way to avoid aliasing in a cache, while making it larger, is to increase associativity. So let's look at the sizes of some actual virtual index, physical intact caches. While doing the previous quiz, we have figured out that the cache size needs to be less then or equal to the associativity of the cache times the page size. So if we look at, for example, the Intel Pentium 4 processor, it has a four-way set associate cache and a page size of four kilobytes and it's level one cache is 16 kilobytes. The more recent Core 2, Hehalem, Sandy Bridge, and Haswell processors from Intel are eight-way set associative. Or course, they still have the four kilobyte page size and surprise, surprise, the level one cache is 32 kilobytes. The upcoming Skylake design from Intel is rumored to be 16-way set associative and you guessed it, it's rumored to have a 64 kilobyte level one cache. For the next metal to reuse hit time, let's revisit the relationship between the associativity and hit time. So if we have high associativity in the cache, we will have fewer conflicts for the set in the cache when multiple blocks go to the same set. Which leads to a lower miss rate. And that, we like. With high associativity we have just seen, we can build larger, virtually, index physically, type caches, which reduces the miss rate, and so we like that too. But high associativity leads to slower hits. The hit time goes up, and that we don't like. On the other hand, if we go all the way to direct mapped caches, the miss rate goes up for these two reasons, and that is bad. But the hit time is reduced, because a direct mapped cache has only place where a block can be, so we can quickly check whether it's there or not. And we like that. So we can see that high associativity in our cache improves the miss rate, but makes the hit time worse. While simple direct mapped caches, sacrifice the miss rate to improve hit time. So now we will see if we can get this and this simultaneously, by kind of cheating associativity. One way of cheating on associativity is called way prediction. We start with a set-associative cache which has a low miss rate but would have a slow hit time. Now what we do, we guess which line in the set is the most likely to hit. So what we do is we use the index bits to determine which set should we be looking at and now instead of reading out all the tags in that line and checking which one hits, we guess which one is the likely to hit and just check that one. So this gives us the access time similar to a direct-mapped cache if we guessed right. If we are wrong, then we got no hit on the line we tried. But the set has other lines, so we try a normal set-associative check, which will have a higher hit time. So the idea is that, overall, our miss rate is that of a set-associative cache. We either find it this way or this way. So the miss rate will be similar to a set-associative cache because even if we didn't find it in a kind of direct-mapped way, we will find it this way. But the hit time, if our guesses are mostly right, is mostly that of a direct-mapped cache. So we have combined the low miss rate of a set-associative cache with most of the time getting the hit time of a direct-mapped cache and sometimes only having to suffer the longer hit time of a normal set-associative cache. Okay, so how often can we expect to guess correctly in a way prediction cache? We can get an idea for this by looking at the smaller direct mapped cache. Basically, let's say we have a two way set associative cache. Each set of this cache has two ways, two lines in it. And by doing way prediction, we are effectively using one of these at first. And only if we don't find it there, we look at the other one. So effectively, a way prediction cache first tries to access what looks like a smaller direct mapped cache. Then it will try the entire set associative cache. So we can get a pretty good estimate for the hit rate and hit latency and so on of a way prediction cache, if we look at the direct mapped cache and the set associative cache. So let's look at the basic 32 kilobyte, 8-way set associative cache. Let's look at its hit rate, hit latency, miss penalty, and the over AMAT. And we will also look at a 32 kilobyte, 8-way set-associative cache with way prediction. The hit rate of this cache, let's say its around 90%. The hit latency might be 2 cycle because it's highly associative. The miss penalty will be 20 cycles, for example. And the overall AMAT would then be 2 for the hit latency, plus 10% of the time we have the miss penalty. And we get 4 here. Now we already said that way prediction behaves like accessing only one of the ways in the cache. With way prediction in a 8-way associative cache, we are really guessing which of the 8 places is going to have the block. So you effectively, with way prediction, our first attempt will be to a 4 kilobyte direct mapped subset of this cache. If we look at the normal 4 kilobyte direct mapped cache, that's going to have, for example, a 70% hit rate, but the 1 cycle hit latency, because it's faster. And the miss penalty will be the same. If we now try to get the AMAT for this basic direct mapped cache, we will have 1 + 0.3, the miss rate, times 20, and this is going to be 1 plus 6 so it gives us 7. So obviously we are better off with a 8 way set associative cache. However, we are paying an extra cycle each time we access it for the hit latency in order to get the low missed penalties here. With the way prediction cache, what we're getting is 70% of the time we're going to find what we're looking for in the 4 kilobyte subset that we are checking first and we will have a 1 cycle latency for that. In the remaining 20%, we're going to check the rest of the cache so our overall hit rate will be 90%. What we don't find here, we will still find here if it's in the 32 kilobyte cache. So we get a 90% hit rate overall. Our hit latency is now 1 or 2. And our miss penalty is 20, so how do we compute the AMAT? Well, what we now have is that 70% of the time, we find what we are looking for in one cycle. The other 30% of the time, whether we have a hit or a miss in the 8-way cache, we have to check it. So the other 30% of the time we're checking all of the ways for which we pay a 2 cycle latency, and then 10% of the time we don't find what we are looking for in all of the ways, so we have to pay the miss penalty. So now we have what looks like our hit time. 70% of the time, it's 1. 30% of the time, we have to use a 2, so we end up having a 1.3 cycle on average hit time. And a miss penalty of the 8-way set associative cache which is still going to be 2. And this is how we improve things, we now get an AMAT of 3.3. Because what we effectively got was the misses of an 8-way set associative cache, but the hit time that is much closer to a direct mapped cache then it is to an 8-way set associative cache. So let's see now if we understood way prediction. We can use way prediction in what kind of a cache? A fully associative cache, an 8-way set-associative cache, a 2-way set-associative cache, and a direct-mapped cache. Check all of those that can benefit from way prediction. Let's look at the answer for our way prediction quiz. We can use way prediction in any cache that has more than one block in a set. A direct-mapped cache has only one block in a set. So we can not use way prediction. Basically, a direct-map cache already knows which one blocked to access without having to guess. In any set-associative, or fully associated cache, we can benefit from way prediction. For example for a twoway set-associative cache, we can guess which on of the two is more likely to have the block. In an eight-way set-associative cache, we're guessing which one of the eight. And in a fully associative cache, we are guessing which block in the whole cache is the most likely to have the block. So the correct answer is, that these should be selected, and this one should not. Next we will look at how the replacement policy for the cache, which chooses, if you remember, which block to kick out when we need more room, interacts with the hit time of the cache. A simple replacement policy, such as random, which randomly picks the block to replace among those that are in the set, has nothing to update on a cache hit. When we have a cache hit, we don't have to do anything in order to be later able to do random. When we have a miss, that's when we start implementing our policy by randomly selecting a block to kick out. This results in faster hits, because there is nothing we need to do for the replacement policy when we have a hit, and that's nice. However the random policy is not very good for miss rate, because we often kick out blocks that we will very soon need to use and we don't like that. In contrast, least recently used or LRU results in a lower miss rate, which we like but we have to run every cache hit update potentially a lot of counters. Even if we don't update them we have to look at them. So there is a lot of activity on every cache hit that results from leaving the state that we'll late use to select the block we will kick up and this spends a lot of power and also results in slower hits. To illustrate what happens on a cache hit using LRU, let's look at a four-way set associative cache whose counters are currently zero, one, two, and three. If the processor accesses this one here and has a hit. The new values of the counters are going to be this one becomes the most recently used And all of the other counters get decremented. As you can see, all the counters have changed and this happens on a cache hit. So a cache hit can result in updating up to n counters, where n is our associativity. Let's look at what happens when one of the more recently used blocks is accessed. Let's say we access not the most recent used but the next one. In that case this becomes the most recent used block. The counters that are larger than this one get decremented so this becomes two and the smaller than this stay the same. So we get this. So only two counters have changed now. But note that even though we have only changed two counters, we had to actually check for each counter whether it's lower than this or not. So even when we access the most recently used block, and the counters don't change at all because this becomes three and counters larger than it get decremented, but there is nothing larger and this, lower than it stayed the same. We still, had to determine that this is the highest counter anyway, so that we don't decrement anything or we had to actually check for each of these counters, whether it's lower. So, one way or the other, even for a hit to the most recently used blog, we have to do something and that again slows down the hits and spends power. So what we want is a replacement policy that retains the miss rate very close to LRU. Meaning we want to smartly replace the blocks so that we can cut the block that is unlikely to be used soon. Or the least likely to be its own, but we want to do less activity on cache hits to do that. So now we will look at one of the so called LRU approximation algorithms, which this one is called NMRU or Not Most Recently Used. And it's trying to approximate the performance of LRU but with less activity on his. So again, NMRU stands for Not Most Recently Used. The way it would work is that we would track which one block in the set is the most recently used one, at any given time. And then, when we need to replace something, we pick a block that is not the block that is the most recently used. So, for example, we can pick randomly among, the other blocks, but we save the most recently used block from eviction from the cache. So how do we track which block is MRU? Let's say we have an N-way set-associative cache and we want to track the MRU block for each set. We need only one MRU pointer per set. We only need to know which block has been the most recently accessed one in the set. So for example, if we have a two way set-associative cache, we only need one bit to tell us which of the two is the most recently accessed. If we have a four way set associative cache we will have a two bit pointer that tells us which of the four has been the most recently accessed and so on. Compare this to N LRU counters that we need percent\g in a four way associative cache, we would need a two bit pointer for NMRU. We would need four two bit LRU counters to maintain LRU. So this policy keeps N times less state for an N way set associative cache, then the true LRU policy. It turns out that this policy works reasonably well. It does have a hit rate that is slightly lower than a true LRU, but if this is what it takes to make our cache go from two cycles hit time to one cycle hit time, It might be worth it. A disadvantage of this policy is that although it prevents the most recently accessed line from being evicted, it doesn't know what the order is among the rest of them. So there is a truly least recently used block among these three for example, but we don't know which one. So we will pick one of these three and might select something that is not the least recently used. So although this works well because the most frequently accessed thing well be saved in the cache, it is slightly weaker than through LRU because of this. Because it doesn't know which one is the really recently used. So what we might want to have is a policy that is still simpler than LRU but keeps more track about what's less recently used than the MRU And one policy, that tries to more closely approximate the PLRU is the so called pseudo PLRU policy. So it's really trying to approximate the PLRU but sometimes it doesn't exactly match it. So let's look at an eight way set associative cache. And with a pseudo LRU policy what we keep is one bit. Per line in the set, so in an eight place set associative cache, we will have a one bit associated with each line as opposed to three bits that we would need for our LRU counting. So how does pseudo LRV work? All of the bits start at zero and what we will do is, every time a line is accessed, we will set its bit to one. So for example, this one becomes one, this one becomes one, then this one becomes one. As long as there is at least one zero bit here, we keep doing this. And if we ever need to replace something, we pick among the blocks whose bits are zero. So all the recently accessed blocks will have the bits at one and thus safe from replacement. Eventually, however, if we have a lot of hits, so for example, in this particular state, we will pick the least recently used block because it is the only one that has a zero bit. Eventually however, all of the bits will be set. So evnetually we will have this become one. And now, we cannot replace anything. So we detect the situation where all the bits have become one. And when we set the last zero bit to one, we zero out the remaining bits. So now, we have state that is similar to what not most recently use would tell us. We only know what was the most recently used block. For the rest of them, we need to pick it randomly if something needs to be placed. But as we start accessing some other blocks, we get a better idea of which ones are the more, and which ones are less recently used. So at any given point in time the pseudo LRV policy is somewhere between the NMRU and the LRU policy. We have seen that when one bit is set we are effectively implementing the NMRU policy. When more than one bit is set. But not, almost all of them were in between and when all but one are set, we're implementing the true LRU policy because we know exactly which one block is the least recently used. So, this policy behaves somewhere in between LRU and NMRU in exchange for using more bits. But it's also very simple because on a hit, all we have to do is set the bit that corresponds to the block. Which can be done very quickly and efficiently. So as far as activity on a cache hit is concerned, both pseudo LRU and NMRU have much less activity than LRU, which helps us with both power and also hit time on the cache. So now that we have seen two policies that try to approximate LRU replacement, let's see if we understood how NMRU works. Suppose we have a fully associative cache with only four lines and it's choosing NMRU replacement. Starts out empty, none of the lines contain any blocks. And suppose the processor accesses blocks in the following order. It accesses something in Block A, then again something in Block A, then something in Block B, then something in Block A, then something in Block C, then something in Block A, then something in Block D, then something in Block A, then something in Block E. A, A, A, A, and finally something in block B. The question for you. What's the smallest number of cache misses we will have on this sequence of accesses, and what's the largest number of cache misses that we might have on these accesses? Let's look at the solution to our not most recently used replacement policy quiz. We have these axises, and now let's see which ones might be hits and which ones might be misses. This one is definitely a miss because we have nothing in the cache. And now we have brought A into the cache. This one is definitely a hit. No we're going to access B, and this is definitely a miss. And we're going to put B in a separate place in the cache from where we put A because A is the most recently used block. So we will put B in one of the other three blocks. And we definitely will put B in a line that doesn't contain A. So we will not take out A because A's the most recently used block. We will choose one of the other three lines to evict to put B there. So far, if this is our cache, we have A and B there. Now we will access A and have a hit, so C is definitely a miss, and at that point A is the most recently used block. And we will find one of these three to evict to put C there. In this type of a situation it looks like B might actually be evicted. But remember that we also have value bits for all these blocks. So when we have a placement policy, typically we will first fill all the lines in the cache set before we move on and start evicting something. So we will put C into one of the empty slots, and now that is the most recently accessed line. A is now a hit, and that is now the most recently accessed line. Now when we access D, it's not in the cache, so we will have a miss. We will again put D in an unused place and that will now be the most recently used one. Then A gets accessed and we have a hit, and at this point A's the most recently used block. Now, we will have a miss on E because it's not in the cache. And now the question is, what will be evicted? We'll definitely not evict A, so we know that the next four accesses to A will be all hits. E will be replacing one of these three blocks. So if we replace C or D, then here we will have a hit. If we, however, select B for replacement, and put E there, then B here will have a miss. So we have a hit or a miss here. In total, we have one, two, three, four, five, or six misses. So this is the final answer to this quiz. Now, let's go back to our cache optimizations, which are about reducing the AMAT. We have seen that AMAT can be reduced by reducing the hit time. And we have seen several techniques that do this. The AMAT can also be reduced by reducing the miss rate. And we will now look at some techniques that try to do that. So, in our work on reducing the miss rate, we really need to consider what are the causes of misses. So, what can cause misses. And the causes of misses are what we call the 3Cs. You should remember this because very often, we will refer to the 3Cs. And you need to know that the 3Cs are compulsory misses, capacity misses and conflict misses. A compulsory miss is a miss that occurs when a block is accessed for the first time. It's called compulsory because you have to have this miss. So even if we have an infinite cache that starts out empty, we would still have the compulsory misses, but we would not have any of the other two kinds. So a compulsory miss is a miss you have to have simply because for the first time you have to bring a block into the cache. Capacity misses are misses that we have on blocks that were in the cache, but were evicted because we didn't have enough room in the cache because the cache is of a finite size. So these are misses that we would not have in an infinite cache but capacity misses would be misses even in a fully-associative cache of a given size. So for example, if we have an eight kilobyte cache that is let's say direct map, we have a capacity miss. If that block would not be found in the cache, even if it was fully-associative eight kilobyte cache. And finally, conflict misses occur because of conflicts within a set. So the block was evicted not because the cache didn't have enough space in it, but because it had limited associativity. So a conflict miss is a miss that would not be a miss if the cache was fully associative and of the same size. So now some of the obvious techniques for using the miss rate will target some of these. So for example, a larger cache would help us reduce a number of capacity misses. A larger associativity would help us reduce the conflict misses. A better replacement policy would also help us with conflict misses. But all of those techniques also affect the hit time. So now let's look at some techniques that are trying to reduce the miss rate without necessarily increasing the hit time in return. So the first technique for reducing the miss rate is using larger cache blocks. They help the miss rate because more words are brought in when we have a miss so subsequent axises that might access those additional words that we brought will not be misses. And they would be misses if we had the smaller cache block. So this does reduce the miss rate, but only when spatial locality is good. But when spatial locality is poor, the miss rate will increase, because now what happens is we bring in the word we need, and along with it, now, we bring more words that we didn't need, so effectively our cache now looks like a lower capacity cache because it has more junk in it. So, we end up suffering more of those capacity misses. So, if we plot the miss rate and how it changes as the block size changes. For a small cache we might get something like this. Where the miss rate drops as we increase the block size, but then starts increasing as we exhaust the amount of spatial locality that the cache can support. But then it starts to grow as we start to suffer from the miss rate increase because there is not enough special locality. For a 4KB cash, this might happen for example with the block size of 64. Interestingly however, for larger caches, we will start out with the lower miss rate to begin with and it will continue to drop up to larger block sizes. Eventually, it's going to start to increase. But here we might be looking at for example, 256 byte block size. This effect has to do with, how much junk are we putting in the cache, and how much room there is in the cache. So some blocks will have more locality than others. And then in a small cache, those that don't have enough spatial locality are creating some junk. And those that do have a lot of spatial locality are being placed in the cache, but there is not enough room for them because of all the junk. When we have a larger cache, for example 256 KB, then what happens is those blocks that don't have enough locality put some junk in the cache, but the cache is bigger. So it can also contain those blocks that do have spatial locality, so the cache tolerates a slightly bigger block size. Overall, we can reduce the miss rate, especially in a larger cache if we increase the block size. So now that we have seen that the increasing block size can result in reduction in the miss rate. Let's see if we understood what's going on. So when we increase the block size, and the locality is good. So the miss rate goes down, the question for you is which types of the misses are reduced? The compulsory misses, the capacity misses, or the conflict misses? Let's look at the answer to our quiz about the relationship between the block size and the miss rate. The question was, which of the three Cs are really reduced when the miss rate goes down? Obviously, some misses have been eliminated. The question is which type suffered the elimination. Let's first look at the compulsory misses. If we have a small block size, and this is our memory, we'll bring in this block and suffer a miss for the first time. We bring in another block and suffer a miss. Now let's say we bring this block and suffer a miss. If the cache line size was twice of what it is now, then this second one would not have been a miss because when we fetch the first one, this second one would have also been in the same cache block. So as you can see, the increase in the block size, assuming there is enough locality, will result in a reduction in compulsory misses. Another way to look at this is to note that we get the compulsory miss when we access the block for the very first time. Well, if you have larger blocks, there are fewer blocks, so there are fewer blocks to access for the very first time, and that will guarantee a reduction in compulsory misses, assuming there is any spatial locality. Now let's look at capacity misses. Let's say that the program is accessing a relatively large array that doesn't fit in the cache, and then, we go back and access it again. On the second set of accesses, we're having capacity misses. The first set of accesses were compulsory misses because these blocks were never in the cache. Because they don't fit in the cache, by the time we get to the end of the array, the beginning of the array is kicked out of the cache, and now we're suffering capacity misses. These blocks will be in that infinite cache, they are not in the cache because it was not large enough. But how many misses we will have for this array depends on the block size. WIth smaller blocks, the size array will be occupying more blocks. WIth larger blocks, we will have fewer blocks. The number of misses will be equal to the number of blocks. So as you can see, if we increase the block size, and our miss rate goes down because there is plenty of spatial locality, we are also reducing the number of capacity misses. And finally, if we look at the conflict misses, we can construct a similar example where two blocks are kicking each other out, and because, each time we fetch a larger block, if the block size goes up, we will be having few conflict misses. So the increase in block sizes can improve all types of misses. That is, it can reduce the number for all three types of misses. When we increase the block size, the compulsory misses are almost always going to go down. The capacity misses are going to go down, as long as there is enough locality, and the conflict misses are likely to stay similar, but might go down. Another technique that uses a similar idea to what we did when we increased the block size to reduce the number of misses, is called pre-fetching. The way prefetching works is that we guess which blocks will be accessed soon in the future, before they're actually accessed, and then we bring those blocks into the cache ahead of time, before they actually will be accessed. So with no prefetching, if time goes to time goes to the right, we might have an access here. Let's say load word A, and then we check quickly whether it's in our cache, and if it is not, we will go to memory and spend a lot of time waiting for it until it comes back, so here we can supply the data to the processor. So this is what happens with no prefetching. With prefetching, assuming that the load would happen here, sometime before the load, we have to guess that A will be accessed and request it from memory. Now, what was memory latency here, will be here, and at this point, the block is in the cache, so when we try to load it, we have a hit. As you can see, with prefetching, we are guessing what will be accessed and start fetching it from memory into the cache. If we guess correctly, instead of a miss, we get a hit. If we guessed incorrectly, we brought something into the cache that replaced something that might have been useful. So with prefetching we have, the good guesses will eliminate misses, but bad guesses lead to what is called cache pollution because we are bringing stuff that is useless into the cache. If it doesn't get access, that means that maybe it would never have been brought to the cache in the first place. And because of cache pollution, because we kick out something that might have been useful with useless data, not only that we didn't eliminate the miss here, because we guessed wrongly, we also might have created another miss because we kicked out something that later might be accessed. And that we don't like. So overall prefetching is about trying to make good guesses so that we eliminate misses while trying to eliminate bad guesses because we don't want to create additional misses. So one way of implementing prefetching is to just add prefetch instructions to the instruction set and let the compiler of the programmer figure out when to request prefetches. So let's say we have a program that adds up the elements of a large array into a sum. This program will access these elements one at a time. So it has a lot of spatial locality, not much temporal locality. And it will have a lot of misses. With prefetching instructions, this program will become something like this. We still have the original program, but in this loop, in addition to accessing the appropriate element of the array in iteration, we will be prefetching an element that we will access in the future. Pdist here should be at least 1, so that we are prefetching the next element, but you might want to prefetch the next, next or even more advanced elements. So here, one of the trickier questions is what should be the pdist? How far in advance to prefetch? If pdist is too small, for example, if it's only one, we end up having something like this. Our axis is here. We issue our prefetch in the previous iteration for this element. The memory latency might be this, which means that when we do our axis, the data still hasn't arrived from memory. We will still have to wait for it. Although, we will have to wait for it less, than if there was no prefetch. So, here we prefetch too late, and the access happens before the data arrives from memory, so we still have a miss. Although it's a slightly less expensive one, in terms on how long we have to wait. However, if PDIST is too large, let's say that the access is here, and we prefetch this element way ahead of time. It arrives from memory here, sits in the cache, and because other elements are being accessed in this loop all the time, this might be eventually kicked out. And in this case, we say that we've prefetched too early. So as you can see, it is not easy to guess how far in advance to prefetch. Furthermore, if you're writing a program and changing it to do prefetching, it might be that in one generation of hardware, the correct pdist might be, let's say 20. And then the process for example becomes faster, but the memory doesn't. And now, the processor does more iterations while the memory is responding so we need to increase the distance. So basically kind of coding this pdist into the program becomes tricky because the correct value for it, so it's not too small or too large, changes as hardware changes. So prefetch instructions are very useful, but they're not easy to get right. So, let's see if we know how to add some prefetch instructions. Suppose we have a program like this where i goes from 0 to 1000 and j goes from 0 to 1000 and in every iteration of this inner loop we add to the ith element of the a array the element of the b matrix. So what is really happening here, is that in the first iteration of the outer loop, we will compute a[0], then a[1], and so on, and we do that by basically adding up all the appropriate elements from the b array. Let's say that the elements of the a and b arrays are both floating point numbers, that they're each 8 bytes in size. Let's say that the cache size is 16kB, fully associative, with LRU replacement. Let's say that a single iteration of this inner loop takes 10 cycles if there are no misses, and that the miss penalty is 200 cycles, because the memory latency is 200 cycles. The question for you is, if we insert a prefetch instruction here so that we can prefetch one of the next a elements, what should be the distance here or check this, if we simply should not be doing this. Here, for prefetching elements of B in this loop, what is the distance at which we should be prefetching? Or should we just not be doing this? Let's look at the solution to our prefetch instructions quiz. Let's look first at this interloop and the prefetch distance for fetching elements of the b array. We were told that each iteration takes ten cycles if there are no misses. And we want to eliminate misses. So if we do prefetching correctly, there will be ten cycles per iteration here. The miss penalty and the memory latency are 200 cycles, which means that we should be issuing a prefetch for a b element, and 200 cycles before we reach it, corresponds to 20 iterations. So we will prefetch twenty iterations ahead of time, and we will do it, because if we do it this way, the prefetch will arrive just in time for b to get its value. Now we need to figure out what to prefetch here, and whether we should be doing the prefetching. We should not be doing the prefetching if, by the time we reach the axis to the element of a, know that this should be i plus 1, or something larger, this eliminates that element from the cache. So the question is, how much cache stuff does each iteration of this outer loop bring in? And the answer is, it brings 1000 eight byte elements here, and an element of a. So if we prefetch with a distance of 1, what we get is that our prefetch is done after 200 cycles. There are 10,000 cycles here, so it's in time to be accessing the next iteration of the outer loop. The question's just does this element get kicked out before we can use it? And the answer is no, because 8 times this 1,000, ends up being 8,000. The cache is 16 kilobytes. So basically if we prefetch here the element of the a array that we prefetched will still be there when we eventually get to the next iteration of the outer loop where we get to use it. So, the answer is we should prefetch with i plus 1, and we should not be selecting this. Note that we should not be prefetching with i plus 20 here. If we do that, there are 20 loops in between when we prefetch and when we actually use it, and each of these loops is bringing in 8,000 bytes worth of b. So if we prefetch that early, then 20 times 8 is larger than our cache, and the element we prefetch will be kicked out from the cache. So that would be an example of premature prefetching. Another example of prefetching would be the so-called Hardware Prefetching. Where the program doesn't change, and the hardware itself, either the processor or the cache is trying to guess what will be accessed soon. There are a number of harder prefetchers that work reasonably well. For example, many current processors include what's called a Stream Buffer prefetcher. Other popular types of prefetchers include a Stride Prefetcher and what is called a Correlating Prefetcher. All of these try to guess what is going to be accessed next based on some sort of locality property. So a Stream Buffer is sequential in nature. It's trying to see whether once we access a blog and then the next block that follows it. We are likely to access the next next block and so on. So it's just trying to fetch several blocks in advance in order to not be too late. A stride based prefetcher is monitoring accesses to see if memory accesses we are doing, are at the fixed distance from each other. So if their addresses differ by the same amount. It's going to guess, that we will just continue doing that. And it starts prefetching with the distance of D times something. And it determines how many of these in advance. Such that it's just in time. A Correlating Prefetcher is trying to predict sequences of accesses that are not sequential or at the same distance from each other. The way it does this is when we see A and B accessed, it remembers in some sort of a table that, when A comes then B comes. Then when C comes it remembers that after B comes C and so on. And then later on if we see A, it's going to look here and issue prefetch for B. If we do access B it's going to look here and issue a prefetch for C and so on. So it can prefetch an arbitrary sequence of accesses as long as that sequence repeats. So it's very good for example for linklist which are not necessarily sequential in memory and are not at fixed strides, but if you access the first note, then the second or third note and you do it again, it's going to prefetch correctly. The final type of optimization for reducing the miss rate that we will look at is a so called loop interchange which is one of the compiler optimizations that can be done to transform the code into code that has better locality. So suppose that we are initializing a matrix like this by having an outer loop and an inner loop and then we initialize the element and then loop like this. The layout of this matrix in memory if it's a C array is such that it begins with a[0][0]. The next element in memory will be a[0][1], and so on. Eventually we have the last element of this row. And only now we will have a[1][0] and so on. So now let's look at how this loop accesses these elements. i and j are both 0 at first. So we will access this element. Next, j will be incremented, so we will access this element, and then this element, and so on. We will access all of the rows of the matrix before we come back, and start accessing the second element in each row. So the problem is, when this is accessed, we fetch an entire cache block worth of stuff into the cache just to use this one element. Then we're going to fetch a block here to use this one element. And by the time we reach the end of the matrix and look back here, we might have run out of space in the cache. And the block that we would now use has been kicked out of the cache. So we fetch it again, use this one element, and so on. So we have one miss per access here. So a good compiler will detect that the order of these loops doesn't match the layout in memory and it's going to perform what is called a loop interchange which amounts to simply sloping the two loops. So now the inner loop is the one that moves to the next element in memory. So once we fetch a block from memory we end up using the whole block. Only then we move to the next block and so on. Once we are done with the row, we move on to the next next row, so now we are nicely sequentially accessing this matrix. Not only does it improve locality because now we end up accessing the entire cache block at the time, but also it makes the access nicely sequential so one of the prefetchers can do a good job on it. So this works really well, it dramatically improves the hit rate that we're getting, but it is not always possible. You cannot just take any two nested loops and transform form them this way. The compiler has to prove that this code and this code are equivalent, which it does by proving that there are no dependencies between iterations of these loops. So going back to our choices for reducing the AMAT, we have seen that we can reduce the hit time. We have now seen that there are techniques that reduce the miss rate. And the final set of techniques are those that reduce the miss penalty. So when we miss, we don't suffer as much as before. So the first technique for using the missed penalty is to overlap multiple misses. If time goes in this direction, our processor does a lot of activity multiple instructions per cycle. And at some point it does a load for example. And now that load tries to be found in the cache. And if not, we're going to go to memory and it eventually comes back. Now if you have a fancy out of order processor, it doesn't stop here and wait. What it does is it finds other instructions to do. So even after we start fetching the data from memory, the processor is continuing. But eventually it starts running out of things to do. And probably before the load comes back from memory, the processor runs out of resources. Remember that it cannot commit this load. So eventually for example, it will fill up the ROB or maybe even sooner than that it will run out of reservation stations, if a lot of things depend on this load. So some part of this missed latency is going to be directly added to the execution time, but some part of it is actually overlapping with the processor activity. During the time, between, trying to execute this load, and running out of things to do, this processor might actually issue another load that will be a cache miss. So, if we have what is called a blocking cache, then this load cannot be done until the first load is finished and only at this point this load can really be tried in the cache. We realize it's a miss, we suffer the miss latency, and meanwhile because the processor can commit these instructions here, the processor can overlap some of this miss latency with some other activity. We can also have a non-blocking cache and a non-blocking cache can support things like hit under a miss, meaning while we are having a cache miss, hits to other blocks in the cache that are sent by the processor will be serviced and returned to the processor with data. And also we can have what is called a miss under a miss, in which case while we are having a miss, we can send another request to memory. So let's look at what that looks like. So our processor is happily chugging along, it has this first load that suffers a miss. We check in the cache, we wait for it to come back from memory, we continue working, and eventually run out of things to do because they depend on this first load. But the idea is that now this load here that also suffers a miss will have its own check in the cache and when we realize it's a miss we will send it to memory. So it will come back here. Now what we have is when the first load comes back from memory, there is a burst of activity. It starts drying out because we are still waiting for the second load. But then the second load comes back and we are very quickly back to a normal operation. So now, as you can see, the inactivity in the processor used to be this and this and now it's just this and maybe a small amount here. So by overlapping the miss time of the two loads, we have almost cut the penalty to on performance to half of what it was because before we had to wait twice this. Now we really wait once and maybe some little more. If we manage to find three or four rows that overlap. Then a blocking cacher will pay the penalty three or four times. Here we might be paying one penalty plus a little bit more. The property that the processor is exploiting here is called memory level parallelism. Here, the memory never gets more than one access at a time. Here, the memory gets accesses to process them in parallel. So, of course, our memory needs to be able to do this. But if it can't, then a non-blocking cache that can do miss under miss, can dramatically cut down on the cost of misses, instead of seeing the full miss latency being added to the memory access time. We are really seeing that the penalties of the two misses overlap. So really we paid a penalty once and we get 1, 2, 15 misses in exchange. So what do we have to have in our cache in order to support a miss-under-miss operation? Before the cache would simply block when it has a miss and not do anything until the miss comes. So it doesn't really need much support for that. With miss-under-miss we have a miss but accesses keep coming to the cache. If there hits, we handle them normally by just finding those blocks and returning to the processor. If there are misses, we have to do more. Now we need to have the so called Miss Status Handling Registers or MSHRs. What they do is they remember what we requested from memory. So they keep information about the misses that we currently have in progress. When we have a miss, we have to check the MSHRs to see if there is any match. Basically if we have a miss, we want to know whether it's a new miss or something that we didn't find in the cache but it has already been ordered from memory because of a previous access that was a miss. So if it's not a match, that means it's a miss to a different block. In that case, we allocate a new MSHR. In the MSHR, we remember which instruction in the processor to wake up when the data comes back. And that's it. If there is a match, that means that we try to find data from a block and that block had a previous miss that was already sent to memory but didn't come back yet. So really, if we did things one at a time this would not be a miss. So we call this a miss and this is sometimes called half-miss, because this is a true miss, that we would have, even if we did blocking caches. This is a miss that would be a hit if we did blocking, but because we allowed the processor to check the cache, before the previous misses came back, some of the new accesses are now not finding the data in the cache but the data is already on the way back to the processor. Now because the request for this block has already been sent to memory, we shouldn't send it again. Instead, we simply add that instruction to the MSHR and that's it. When the data finally comes back from memory, we find the MSHR that corresponds to that request, and we wake up all the instructions that were added to this MSHR. The first one, and all of the subsequent ones that also wanted the data. Keep in mind that the miss and the half-miss to the same block, need not necessarily be to the same word. Actually it's quite common that, for example, you access the first word and then very soon you access the second word in a block. The first word had a miss, the second word will have a half-miss. And after waking up the instructions, we release the MSHR so it can be used by another miss. So how many MSHRs do we want to have? It turns out there is a huge benefit even if you only have two, so it can handle two different blocks in progress at the same time. Four is even better, and there are benefits to be gained even if you have 16 or even 32 MSHRs. So we want to have a few tens of MSHRs if we can. This is because memory latencies are relatively long. So if we can keep sending requests to memory during that time, we can be achieving the memory level parallelism of 16 or even 32. Cutting down dramatically on what the overall performance penalty of cache misses ends up being. So, let's see if we learned, how miss-under-miss and MSHR's work. What kind of application, gets no benefit from miss-under-miss support? You need to select all correct answers. An application that always hits in the cache, an application that has a miss every one thousand instructions, an application that has a miss every 10 instructions, and an application that has a miss every 2 instructions. Let's look at the answer to our quiz. What kind of application gets no benefit from miss-under-miss support? One that always sits in the cache gets no benefit. It never has misses so it cannot benefit from miss-under-miss support. An application that misses every 1000 instructions will have a miss, and the processor will run out of ROB entries because it has fewer than 1000 until it finds the next miss. So that application will suffer the first miss, and only then the processor can continue and find the next miss, so we never actually need to do miss-under-miss. So, this application will not benefit. An application that has a miss every ten instructions will have a miss and will find the next miss, so it can benefit from a miss-under-miss support. So, this should not be checked. And then, an application that has a miss every two instructions, of course, will also benefit from a miss-under-miss, so we don't check this either. So, back to our overall reducing the AMAT. We've seen some techniques to reduce the hit time. We've seen techniques to reduce the miss rate. We've seen a technique for reducing the miss penalty which is to overlap multiple misses. And we will now see a technique called multi-level caches, or having a cache hierarchy instead of just one cache. With this technique, a miss in the first cache of the processor accesses, which is called now a level one cache, will just go to another cache instead of memory. So now, the miss penalty from our level one cache is not equal to the memory latency. The miss penalty in level one cache now is the hit time of the level two cache plus when the level two cache misses, then it's going to be level two miss rate times the level two miss penalty. And this level two miss penalty can be the actual memory latency but we can have a level three cache and if we miss there, a level four cache and so on. So let's look at the AMAT equation when we have a cache hierarchy. The AMAT is still equal to L1 hit time plus the L1 miss rate times the L1 miss penalty, but it used to stop here and the L1 miss penalty would simply be the memory access time. But now we have that the L1 miss penalty is the L2 hit time plus the L2 miss rate times the L2 miss penalty and the L2 miss penalty is the L3 hit time plus the level 3 miss rate times the level 3 miss penalty and so on until at some point, we have a cache whose miss penalty is equal to the main memory latencies because there is no more caches beneath it. This cache whose misses go directly to memory is called the last level cache, or LLC. So it used to be that the L1 cache was the only cache, so L1 cache was the LLC. Then level 2 cache's were introduced so level 2 miss penalty was the main number latency because level 2 went directly to memory and that was the last level cache and then level 3 cache's were introduced so they became the LLC and so on. So now that we have seen the concept of multi level caches, let's see how L2 and L1 compare. So which of these statements are true? The L1 capacity is less than L2 capacity. The L1 latency is less than L2 latency. The number of accesses that go to L1 is is less than the number of accesses that go to L2. And the L1 associativity has to be equal to L2 associativity. So again, check all of the statements that should be true. Let's look at the answer to our quiz. We would ask which of these are true. L1 capacity is less than L2 capacity. This should be true because the L2 cache needs to have hits for things that were L1 misses. So if L2 has the same or lower capacity than L1. The not many things that miss in L1 will actually be hit in L2, and we want the L2 to still have a lot of hits so that things don't go to even slower caches or even to memory. The L1 latency, should it be less than L2 latency? And the answer is yes. We look first in L1 cache not because it has a better chance of finding the data than the L2 cache, but because it has a lower hit latency. The number of accesses in L1 cache, is it lower than the number of accesses in L2 cache? This is not true. All of the accesses, load and stores, go to L1 cache but only those that miss in L1 cache go to L2 cache. So, the L2 cache usually has a lot fewer accesses than L1 and finally, L1 associativity needs to be equal to L2 associativity. This is potentially true, but usually it isn't. Why? Well because the L1 cache needs to have a low hit latency and need not have the capacity of the L2 cache. Similarly for associativity the L1 cache because it needs to be fast can have a lower associativity where as the L2 cache can be slower so we can maybe afford a higher associativities. So typically that's what happens, L1 has a lower associativity than L2. So this is also not true. Let's look at a single cache that is 16 kilobytes in size. Let's look also at a single cache that is 128 kilobytes in size. Let's look at no cache, simply we will just access the main memory, and let's look at the hierarchy that has this kind of cache as the first level cache, and this kind of cache as the second level cache. We will look at the the hit time for this, the hit rate for all of this, and the overall AMAT. So a 16 kilobyte cache might have a hit time of 2 cycles, a hit rate of 90%, so it gives us an overall AMAT of 2 plus 10% of the time we have the memory latency. Let's say the memory latency is 100 cycles. In that case, we end up with an overall AMAT of 12, 2 from the hit time and 10 on average from the miss penalties. Now let's look at the larger cache. This cache might have a hit time of 10 cycles, but a hit rate of maybe 97.5%. This is a bit high for such a cache, but let's say it is. The AMAT when this is used alone would be 10 plus the miss rate times the miss penalty, and we end up with 12.5, which is a little bit worse than with a smaller cache. So having a larger cache alone increases the hit time, improves the hit rate. So it may or may not improve the AMAT, but either way the AMAT is not going to improve a lot. Of course, having a cache is still lots better than not having a cache. Not having a cache means that our memory has a hit time of 100 cycles, basically the memory latency. It hits 100% of the time. So AMAT ends up being 100 plus 0 times the penalty, because it never misses. But we still end up with 100 cycles per axis, which is way too big. So obviously, having a cache is better than not having it. So now let's look at the cache hierarchy. The hit time is different depending on which of the levels we hit in. The hit time is still going to be two cycles for a level 1 hit. For a level 2 hit, however, we first check in level 1. So we pay 2 cycles to check. Then we access L2 and have a hit. So overall now, it's going to be 12 cycles for a level 2 hit. The hit rate is going to be 90% for L1. It's still the same cache as if it was alone, and of all the axises that go from level 1 to level 2, we will have some hits because the L2 cache has this hit rate when working alone. Of all the processor axises, it would have a hit on this many, but of all the processor axises, those that hit in L1 probably would also have been hits in L2. So this cache really has a 75% hit rate of all the things that go into it. It would have this hit rate if it was alone, but the L1 cache is filtering all of the easy to hit axises, so the L2 only gets 75%. Basically it only gets to see kind of the worst 10% of the axises, and for those it's only having a three quarters hit rate. But the AMAT is now going to be two for L1 hits plus 10% of the time we have an L1 miss. When we do, we have 10 cycles to axis L2, plus 25% of those also end up accessing memory. So when we compute what this is, it's 10 plus 25. So the overall miss penalty for our level 1 miss is just 35 cycles. It's much better than what it used to be when the L1 was working alone. And we finally end up with 2 plus 3.5, which gives us an overall AMAT of 5.5 cycles. This is much better than either of the two caches working alone. And it's much better because really we are having most of the axises hit using the hit latency of the fast cache. Some axises have a higher hit latency in the second cache, and fewer of them made this huge memory latency. So this is why cache hierarchies work better than individual caches, and why we have them. It is not enough to simply have a large cache, if it's also slow. In that case, you can combine their good properties so that most of the time we get this hit time. But overall, as far as the memory axises is concerned, we really have this hit rate in the combined cache. So we have observed that when the cache that we use as the L2 cache is used alone, it has a higher hit rate. Let's look back at our example. Here, where we see that 128kB cache, when used alone, will have a relatively high hit rate. But when we use it after the level 1 cache has already been accessed, it only gets a 75% hit rate. So it looks like the L2 cache has a lower hit rate than the L1 cache, which can be misleading. So this particular type of hit rate we call local hit rate for this cache. A local hit rate is a hit rate that the cache actually observes. Of all the accesses to that cache, this many are hits. But note that all the accesses to a level 2, level 3, etc., cache, are not all the accesses that the processor makes. Those accesses that have a lot of locality get to be all one hit, and never reach these caches. So, somehow, their local hit rate is lower because they never see the easy accesses. If such a cache was alone, it would see this, which we call a global hit rate. Essentially if we care only about how many accesses that the processor make go beyond a level 2 cache, then we would really be seeing this. But note that this hit rate really includes the hit rate of the level 1 cache and what are the actual hits in the level 2 cache. So when we talk about the cache size and how bigger caches behave better and so on as far as hit rate is concerned, usually, we think about the global hit rate, because the local hit rate heavily depends on what do you have as the level 1 cache. If it's a level 3 cache then its local hit rate would depend on what is the level 1 hit rate, what is the level 2 hit rate, and only then we even get to access the level 3 cache. Its global hit rate is going to be similar to our hit rate of that type of a cache if it was used alone. So let's define the global and local hit rate. The global hit rate can be defined as simply 1 minus the global miss rate. And the global miss rate can be defined as the number of misses in this cache, divided by the number of all memory references that the processor makes, not just those that reach this cache. In contrast, the local hit rate and the local miss rate can be defined in terms of normal cache behavior local to this cache. So then local hit rate, for example, is the number of hits divided by the number of accesses to this cache. And remember that not all of the memory references a processor makes actually reach this cache. The local miss rate simply can be the number of misses divided by the number of accesses and they, of course, still add up to 1. Another popular metric of how often the cache hits that tries to capture the behavior of caches that are not the level 1 cache, is misses per 1000 instructions, or MPKI, where K stands for kilo instructions. It is very similar to the global miss rate, except that it doesn't normalize the misses with the number of just memory accesses. It normalizes with the number of instructions. So let's see if we can compute the global and local miss rate. Suppose we have a level 1 cache that has a 90% hit rate. What is the local miss rate for this cache, and what is the global miss rate for this cache? And then let's suppose that we also have a level 2 cache, where we have a hit for half of the L1 misses, then the local miss rate of this cache is what, and the global miss rate of this cache is what? Let's look at the solution to our global and local miss rate quiz. For our Level 1 cash, when we say that it has a 90% hit rate, we don't have to specify whether it's a global or local hit rate because it's the same. Its local miss rate is the number of misses divided by the number of accesses to this cache. Its global miss rate is the number of misses divided by all the accesses that the processor makes, but because all these accesses go to this cache, they are the same and they are both 10%. because that's what percentage of all processor accesses misses in this cache, and that is also the percentage of things that reach this cache, that miss in it. For the Level 2 cache, things are different. It hits for 50% of L1 misses. That means that it misses for half of the L1 misses. So its local miss rate is 50%. Of all the things that reach this cache, and those are only L1 misses, half of them hit and half of them miss. But its global miss rate is only 5%, because only 10% of the accesses get to this cache, and half of that misses. So when we express the misses in terms of all the accesses that the processor made, only 5% of all processor accesses end up missing in the L2 Cache. In addition to different miss rates depending on whether we're looking globally or locally at the cache in a multi-level cache, we have what is called an inclusion property. The inclusion property has to do with assuming that the block is in level one cache. Is it that it may or may not be in the level two cache, or, it has to be in the L2 cache? And it is also possible to make a system in which if a block is in L1, it cannot also be in L2. This is often called exclusion. This is what we usually call inclusion and this doesn't have a name. It's neither exclusion nor inclusion. So if we just have a normal cache hierarchy where processor accesses go to level one cache and then the misses from that go to a larger level two cache cash, the question is what will we get? And the answer is, unless we explicitly try to force inclusion or exclusion, we will get this. Most of the stuff that is in L1 will also be found in L2, but there is no guarantee for that. So there could be blocks in L1 that have been replaced from L2. Let's look at the simple example, when the L1 cache has two blocks and is fully associative, and the larger level two cache has four blocks and is fully associative. And both of them use LRU policy. Now let's suppose that the processor accesses block A. It's going to be a miss here, and a miss here. It's brought from memory. Placed here and then that is fed to the L1 cache so it places it here, so so far we have inclusion. Let's now access block B, it gets brought from memory here and here. We still have inclusion. If we now access block C, it will be here and here and we still have inclusion, but let's suppose that when we have the blocks A and B brought in, we reaccess A. That is going to be a level one hit. So know the areal count is in level one of this, but because there has been no access when A was reaccessed to level two, because it was a level one hit, so we don't go to L2. Here we still have that B is the most recently accessed, A is the next most recently accessed, and let's say these two are like this. Now let's say we access C. What's going to happen is we have a miss in the L1 cache, we have a miss in the L2 cache. We bring in C and replace the least recently used block which is this. The counters now get updated to be three, two, one, and zero. And then C is brought into this level one cache and replaces B, because that's the least recently used block. And the counters become one and zero here. So now let's say A's accessed again. That changes the counters back to this and there is no axis to the level two cache. Let's say now we access block D, it's going to be fetched here. The counters are updated to three two, and zero. And D is also brought into the level one cache and placed here. Of course that updates the counters this way. Now, let's say A's accessed again. Now, we have the counters again, one, zero. Let's say, now, E is now accessed. It's a miss in both caches. We bring E in and now in the L2 cache, it replaces the least recently used block, which is A. However, in the L1 cache, it will replace the least recently used block, which is D. So at this point, we no longer have inclusion. Because A is in the L1 cache but is no longer found in the L2 cache. So inclusion does not necessarily hold when you have a cache hierarchy, because things that are frequently accessed in the L1 cache, will not be frequently may be accessed in L2, because all of the accesses to that block are going to L1, and the L2 never sees them. So eventually that block gets replaced from L2. To maintain inclusion properly, we have to have a so called inclusion bit added to L2, which is one. If that block is also in L1 and when we replace things from L2 we prevent blocks whose inclusion bit is one from being replaced. That ensures that we never replace something that is is still in L1. So we can maintain inclusion, but again, that requires an additional bit. So let's see if we understood inclusion. Let's say that we have a level one and level two cache that maintain inclusion property. Let's say that the dirty block is replaced from the level one cache by something else, and we need to write it back. This write-back access, can it be a level two hit, and can it be a level two miss? So check here. If the right back axis from level one can be a hit in L2, check here, if the right back axis from L1 can be a level two miss. Now let's look at the level one and level two cache, but this time, they make no attempt to maintain inclusion. Again, if we have a dirty block replaced from the level one cache, the level one cache has to write it back. This write-back, can it be a level two hit and can it be a level two miss? Let's look at the answer to our inclusion quiz. If the two caches do maintain inclusion property, the block that was in L1 still is in L2 as well. So when we want to write it back, it will be a level two hit and it cannot be a level two miss because inclusion guarantees that, that block still exists in L2. After we do the write back, we can try to bring the new block into L1. And that may replace this block from both L1 and L2. But until it's written back, it's a block that exists in L1. So it also is in L2. Thus the write back is a hit. In contrast, if we make no attempt to maintain inclusion when a dirty block is replaced from the level one cache. That block may or may not be in the level two cache as well. So we may have a write back be a level two right hit or if the block is not in L2 anymore it will be a miss. So write back handling becomes slightly easier if you do maintain the inclusion property. In this lesson we'll learn how modern processors use multi-level caches and other cache tricks to make up for main memory being so slow. But why is main memory so slow? Let's see in our next lesson. We have seen in our pipelining lesson that each control hazard will waste many cycles in a pipeline processor. Branches and jumps are relatively common, so performance will not be very good unless we do something about that. But what is it that we can do? That, you will discover in this lesson. So before we discuss branch station, let's have a reminder of what a branch does in a pipeline. A branch instruction like this will compare registers R1 and R2. And if they are equal, jump to the label. This is usually implemented by having, in the immediate part of the instruction field, the difference between the next instructions PC and the PC that should be at the label. So that the branch effectively, if R1 and R2 are equal, would just add the immediate operand to its current PC. So that the branch, if R1 and R2 are equal, will add the immediate operand to the PC that it computed for the next instruction. Now the problem with branches is that if the branch condition is not met, just increment the PC. For example, if the size of the branch instruction is four, then the PC will just move by 4 bytes. However, if the branch condition is satisfied, in this case if R2 and R1 are equal, then the branch will increment the PC and also add the immediate to it, so the next instruction we fetch will be at this label. Now let's look at what happens when a branch goes to a pipeline. Let's say we have a classical pipeline with a Fetch, Read, ALU, Memory and Write stages. And let's say that towards the end of the ALU stage is, we actually figure out if a branch is taken or not. In the first cycle, we fetched this branch. In the second cycle, the branch moves here, where it reads the operand. So it's reading R1 and R2 but we have no chance of telling whether the branch will be taken or not. So it's no telling whether the branch will be taken or not. So we either don't fetch anything here or we fetch something here. Let's say we fetch some sort of a green instruction here. At the end of this cycle, we still don't know whether the branch will be taken. We have now read R1 and R2, but we haven't compared them yet. So when the branch is in the ALU stage, the green instruction we have fetched after the branch moves here, and we fetch another instruction, let's say that is a purple instruction. At the end of this cycle, we finally know whether the branch is taken. Now there are two possibilities. Either we have fetched the correct instructions, for example, the branch is not taken, and we have taken the instructions that follow the branch. The branch is not taken, that means the PC just gets incremented and we fetched the right instructions. In which case, we have no bubble in our pipeline. We can just proceed. We just fetched the right instructions. The second possibility is that the branch for example is taken and we fetched the next instructions. In which case, we have to cancel these instructions. In the next cycle, these two cancelled instructions move on through the pipeline. And we finally fetch the instruction that we know is correctly fetched. So what happens is if we correctly guessed what should be fetched after the branch, then there is no penalty. The branch finishes and then the next instruction will finish right after the branch. However, if we mispredicted what will happen during the branch and refreshed the wrong things, then the branch effectively took three cycles to execute because the branch actually finishes in one cycle. But then there are two empty cycles when we don't finish anything. So, overall, the cost of the branch was really three cycles instead of one inner pipeline. Meaning, the branch cost us one cycle, which is normal for every instruction, plus two cycles because of a misprediction. Now you also see why it never pays to not fetch something after the branch. If we don't fetch anything after the branch until we are sure what to fetch, then we are guaranteed to have two empty slots after the branch. So, somehow, in that case, regardless of whether we would have guessed correctly or not, we have a two-cycle penalty. So we'd rather have the two-cycle penalty some of the time than all of the time. Another thing that is important to note is that at the end of the fetch cycle where we fetch the branch, we don't really know anything about this instruction yet. We have just obtained the instruction word, meaning, the 4 bytes that present an instruction. But we haven't even begun decoding the branch. So what happens is next cycle we have to fetch something based only on the knowledge of the branch address. We don't even know whether its a branch or not. So when we are fetching an instruction, we don't it is a branch but we already have to make a prediction of whether it's a taken branch. This brings us to Branch Prediction Requirements. What do we need in order to successfully predict whether a branch is taken or not? And where is it going if it's taken? Branch prediction needs to work using only the knowledge of where we fetched the current instruction from. And what we needed to guess is the PC of the next instruction to fetch. So branch predictor must correctly guess, is this a branch? If not, then it's certainly not taken. If it is a branch, is it taken? And if it is a taken branch, what is the target PC? Where is this branch going? These two decisions really can be boiled down to one which is. Is this a taken branch? It doesn't matter if we have a non-branch instruction, or a non-taken branch instruction. In both cases, we just fetch the instruction that follows the branch in memory. So the only thing we really need to decide is whether this a taken branch, or something else. So now let's look at branch prediction accuracy and how it affect performance. Our CPI can be written as one, which is the CPI we get with ideal pipe lining, plus the cycles we add on average per instruction because of branch mis-predictions. And that can be written as how often we have missed predictions times the penalty we pay in terms of cycles every time we have a missed prediction. Note that this part is determined by predictor accuracy. It really says how often do we mispredict. This part is determined by our pipeline. Where in the pipeline we figure out that we have mispredicted. That's approximately how many cycles we pay. Let's look at a more specific example. Let's look at the processor that resolves branches in its third stage of the pipeline. And at the processor that resolves branches in its tenth stage of the pipeline. This is actually much closer to what modern processors do. Lets also look at different accuracies for predictors. We will look at the predictor that is 50% accurate for branches and 100% accurate for all other instructions. And we will look at the predictor that ID 90% accurate for branches and 100% accurate for all other instructions and in all of this we will be assuming that about 20% of all instructions are branches. This is pretty common in programs. Now, let's compute our CPIs. here we have a CPI of 1 plus how many mispredictions did we get per instruction. We get 50% this prediction for branches. So we get 0.5 of branches are mis-predicted, but branches are only 0.2 of all instructions. So these two really are the mis-predictions per instruction, it's really the mis-predictions per branch times branches per instruction and this is multiplied by the penalty that we get per misprediction. If we resolve branches in the third stage at that time we have two stages of the pipeline where wrong stuff is already fetched. So the penalty will be two cycles. Note that it will resolve the branch in the first stage, then there would be no penalty because next cycle we can fetch the correct instruction. So branch is being resolved in some stage means really that we pay one cycle less than that in penalties. And we get an overall CPI of 1.2 here. If we resolve the branches in the tenth stage, this part is the same, the accuracy of the predictor and the frequency of branches, but the penalty now would be nine cycles. So we have 1 plus 0.5 times 0.2 times 9 in this case for the overall CPI of 1.9. This is a significantly worse CPI than we were getting here. Now let's look at the more accurate branch prediction. Now what we have is 1 plus mispredictions per instruction are going to be how many mispredictions we have per branch. We had 90% accurate, that means we have 10% mispredictions times 0.2 because not all instructions are branches, times 2, which is our penalty. And we add that with 1.04, so this is a significant improvement from a more accurate predictor. Let's look now at this processor. Here we have 1 plus 0.1 times 0.2 times 9, and when we compute this, we get 1.18. What we can conclude from this, is that a better branch vector will help us regardless of whether we have a shallow or a deep pipeline, but the amount of help from a better predictor changes depending on how deep the pipeline is. Here we have a speed up of 1.15 when we go from worse to better predictor in a shallow pipeline, but here we have a speed up of 1.61 when we go from the same worse predictor to the same better predictor in a deeper pipeline. As you can see, the deeper the pipeline, the more dependent it is on having a good predictor for good performance. And that is really why research in branch vectors continues to this day, although our predictors and now pretty good. They're significantly better than 90% accurate for branches. However, our pipelines are deep, so we still benefit from further improvements. Now let's do a quiz on the benefits of branch prediction. Let's say we have the classical 5-stage pipeline. The fetch, decode, execute, memory and write. The branch is fully resolved in the execute, the third stage. Fully resolved means that we finally know the correct PC from which we should fetch the next instruction. In this processor, we fetch nothing until we are sure what to fetch, and we execute many many iterations of this loop. We add minus one to R1, we add R2 to itself, and then we branch if R1 is not equal zero back to the loop. The question for you what is the speed up over this achieve if we had perfect predictor? Meaning for after every instruction fetched we know which one should be fetched next. Let's discuss the solution for our branch prediction performance quiz. The first thing we want to do is figure out how many cycles are we spending per iteration of this loop in the case where we are fetching nothing until we are sure what to fetch. There is a trick to this. Which is when we fetch the ADDI here we don't know if it is a branch. So we are not sure what to fetch in the next cycle. When the ADDI moves to the decode stage at the end of that cycle we know that it's not a branch. So actually the ADDI's costs us two cycles. One to fetch it and then there is a pipeline bubble untill we can decode it. Similarly, the add costs us two cycles. One to fetch it, one to be sure that it's not a branch. For a branch, we fetch it in once cycle, we decode it in the next cycle, we're still not sure what to fetch, because now that we know it's a branch, we need to know whether it's taken or not. So the branch costs us three cycles. When the branch is in the earlier stage we have two pipeline bubbles already because we wouldn't fetch anything yet. And only after that, we know what to fetch. So there are three cycles worth of branch and doing nothing before we can fetch the next instruction. So overall, we're spending seven cycles per iteration of this loop. Now let's see what happens with a perfect predictor. With a perfect predictor, when we are fetching the ADD, we can perfectly predict that the next instruction to fetch is this second ADD. So we spend one cycle here, one cycle here, and with a perfect predictor once we have fetched this branch we magically know that we should be fetching this ADDI. So we spend one cycle here for a total of three cycles. So the speed up of having a perfect predictor, in this case, is 7 over three, which is 2.33. So with a perfect predictor, we would have more than twice the performance than we're having when fetching nothing until we are sure what to fetch. So, now that we have seen, what the performance looks like when we refuse to make any predictions. Let's see what the performance is with the so called not-taken prediction, which amounts to simply fetching the next instruction, as if none of the instructions was a taken branch. When we refuse to predict anything, a branch, every branch, would cost us 3 cycles. And even a non-branch instruction would cost us 2 cycles, even in a relatively shallow five-stage pipeline. When we predict that the instruction we are fetching is not a taken branch, we get that branches now cost us 1 or 3 cycles. One cycle for not taken branch. Three cycles for taken branch. And a non-branch instruction always costs us 1 cycle. Because, in that case, the not-taken prediction is always correct. So as we can see, the predict not-taken always wins over refusing to predict. For non-branches, we are always 1 cycle better. For branches, we are either the same or 2 cycles better. This is why every processor that has a pipeline will do some branch prediction, even if the prediction it is doing is simply increment the PC and fetch the next instruction. Note that that type of prediction is really cheap. All you have to do is increment the PC. You don't actually have to know anything about branches or other instructions. So it's very easy to do it. You have to be able to increment the PC anyway. So now let's see what happens when we try to do multiple predictions simultaneously. Suppose we have the classical five stage pipeline with a fetch, decode, ALU, memory, and write register stages, and suppose that this is where our branches are resolved in the third stage. Suppose also that we have the following program. A branch that jumps to label A if registers R1 and R2 are not equal. Another branch in the program right after it that compares R1 and R3 and jumps to label B if they are not equal. Then we have some instruction A, B, C that are not branches. And then we have label A where we have instructions X and Y and we have then label B where we have instruction Z. Suppose that we are using a not taken predictor and that this branch. And also this branch, are taken, assuming that we start execution right here. The question for you is, how many cycles are wasted on mispredictions until we finally get to execute instruction Y, because what should be happening is that we jump after this branch here and execute x and y. What really happens is that, because of our not taken prediction, we go on and actually fetch this branch instead. So again, how many cycles are overall waited on mispredictions until we correctly get to Y? Let's look at the solution to our multiple predictions quiz. Branches are result in the third stage which means, that, we will fetch this branch, and then we will fetch this. And then we will fetch this. And at that point, this branch will be here and be resolved. So we will cancel this and this instruction, and start fetching from X and then Y. So overall, we have wasted two cycles. Note that this penalty here is the same as if the second instruction here was not a branch at all. So basically here we actually mis-predicted two branches. But the second one was mis-predicted in the shadow of the first mis-prediction. This branch would never have been fetched if we predicted this correctly. However, the penalty's unaffected. So when we mis-predict a branch, we just need to worry about getting on the correct path and we pay a penalty for that, but we don't pay additional penalties. For mis-predicted branches that themselves should never have been executed because of mis-predictions. Let's see why this is so. It is because the first branch here results in flushing the pipeline when it gets to the stage where branches are resolve. The branch that was fetched after it is at that time here. And the instruction A is here. And what now happens is, we flush the pipeline by converting these two into pipeline bubbles. And next cycle, we will be fetching X. So what happens is really if a branch has been fetched in error because of a previous mis-prediction, that branch never gets a chance to cause a pipeline flush, because it itself gets flushed before that. And that's why we had a two cycle penalty. And not four cycles of penalty because technically these two were miss predicted. But really this one should have never been executed in the first place. But this one was flushed before it could trigger a flush of it's own. So let's talk more about the predict not-taken predictor, which is the simplest predictor we can have. It amounts to simply incrementing the PC. So this predictor is very simple because all it requires us is to increment the PC. We know the PC from which we fetched the branch. We know how big the instruction is. We can just increment the PC. So there is really no memory to this predictor or anything else. We have to do the increment of the PC anyway. Once we figure out it's not a branch, so this predictor pretty much uses the hardware we have to have anyway. But the question is how accurate is it. A rule of thumb is about 20% of all instructions are branches. So 80% of the time this predictor will be correct, because it's simply not a branch instruction. For branches a little more over half of the branches are taken. So, let's say, that 60% of branches are taken, so what we get is that this predictor is correct. 80% of the time, for non branches. And also, another 8% of all the instructions because of the 40% of branches that are not taken. It is incorrect, 12% of the time. It is incorrect, 60% of 20%, which amounts to 12%. Now, if we know the misprediction penalty, we can easily compute, what is the impact. On CPI of branch mis-predictions. The CPI will be one plus, 12% times the penalty. In a five stage pipeline this would be two. So the CPI would be 1.24 So we have seen that our predict not taken predictor, that is very simple to implement, is pretty accurate. For about 88% of all instructions it predicts correctly. So why do we even need any better predictors? Because anything better than this will require some investment of hardware to do the prediction better. We will answer this question in two ways. First, we will compare the CPI with the not taken predictor, that is, let's say 88% accurate. And with a better predictor that let's say 99% accurate. And we will do this for a five stage pipe line that results branches in the thirrd stage, so it has a two cycle penalty when ever there is a misprediction. So it has a two cycle penalty when there is a misprediction. We will also do it for a 14 stage pipeline that results branches in the eleventh stage, so it has a 10 cycle penalty for a misprediction, and for another pipeline that has 14 stages results branches in eleventh. Stage but this one executes 4 instructions per cycle, so this is very close to what modern processors look like. So for a not taken predictor and a 5 stage pipe, the CPA we get is 1 plus how often we mispredict, which is 12% of all instructions, times the penalty of a misprediction, which is 2 cycles, which gives us a total CPI of 1.24. What happens when we have a better predictor? Well, we now get 1 plus 0.01 only 1% of the time we mispredict and we get a 2 cycle penalty then. So the CPI now is 1.02. And now let's look also at the speed up that we get from a better predictor. If the frequency of the processor stays the same and it executes the same number of instructions, then the speedup is simply the improvement in the CPI. So the speedup would be the CPI of the not taken predictor but divided by the CPI of the better predictor. And we get a speed up of 1.22. This is actually a noticeable speed up. But things get more interesting when we have a deeper pipeline. In that case, we have a 1 plus still 12% I couldn't see for not taken. But this time the penalty because where is all the branches in the eleventh stage will be 10, and we get an overall CPI of 2.2. This is much worse than the 1 that we will be getting with no mispredictions. With a better predictor we get 1 plus 1% misprediction rate times 10 for a total CPI of 1.1. And that gives us a speed up of 2. So as we can see now, the better predictor is giving us twice the performance of the not taken predictor. And we have seen this already, that the deeper the pipeline, the more benefit we get from a better predictor. But what happens when we also do multiple instructions per cycle? So this processor can do four instructions in a cycle. So the ideal cycles per instruction is one-quarter of a cycle. We add to that how often we have missed predictions times how many cycles do we waste for a missed prediction? We still detect missed predictions in the eleventh stage. So everything done in the ten cycles after the branch was fetched, is wasted. So, we still have a ten cycle penalty here. And we get an overall CPI of 1.45. That's actually less than one instruction per cycle. It's better than a single instruction per cycle processor, but not that much better for something that should be four times as capable. Now, let's look at the better predictor, which should give us the ideal CPI plus 1% of the time ten cycle penalty and that results in CPI of 0.35, or about three instructions per cycle even with the branches. If we compute the speed-up it's 4.14. So as you can see, if we have a deeper pipeline or if we are able to execute more instructions per cycle, then the better predictor is more important than in simpler processors. So lets see if we understood the impact of branch predictors on performance. We will consider the old Pentium 4 Prescott processor which has an extremely deep pipeline. It fetches then does 29 stages worth of work and only then it resolves branches. It of course uses branch prediction and is capable of executing multiple instructions per cycle. So that in a program, where about 20% of all instructions are branches, only 1% of branches might be mispredicted, and lets say that that gives us a CP1 program of 0.5. So the question for you is, if we had a slightly worse predictor, so that 2% of branches are mispredicted in the program, how would that change the CPI? What would the CPI be? Let's look at the solution to our Predictor Impact Quiz where we considered the Intel Pentium 4 Prescott processor, that spends 30 cycles until it finally resolves branches. Uses branch prediction and multiple instructions per cycle, so that in a program where 20% of instructions are branches, and where 1% of branches are mispredicted, it gets a CPI of 0.5 overall. In this CPI, we are getting some ideal CPI plus, for branches that we mispredict 1% of the time, we pay a 30 cycle penalty, because we resolve branches really in the 31st cycle. And that means that the ideal CPI is equal to the actual CPI we are getting, minus the CPI penalty from branches. And if we multiply this out, we get 0.06. So x here, the ideal CPI with a perfect branch predictor, would be 0.44. Now when 2% of branches are mispredicted, our CPI will be equal to x plus, for branches we mispredict 2% of the time. And we pay a 30-cycle penalty, so we get 0.44 plus this, which multiplies out to 0.12, for an overall CPI now of 0.56. So the key to solving this again was that we were given the actual CPI. We needed to compute what the ideal CPI is after we remove the branch penalty. And then, we compute the new branch penalty and add it to this ideal CPI to get this answer. We said that we will explain why we need a better predictor in two ways. This was one way, the CPI gets a lot better. The other way will be to see how much waste there is from a mis-prediction. So in a five stage pipeline, what's happening really we have three stages that are relevent. The one where we re, resolve the branches. And then we have to destroy two instructions that we have fetched if this branch has been mis-predicted. So a mis-prediction costs us two instructions here. In a deeper pipeline, what happens is, we have our branch, and for ten cycles, we have been fetching things. Some a mis-prediction now costs us ten instructions worth of work, and that means that avoiding mis-predictions becomes more important. And finally, when we have a four instruction per cycle processor that resolves branches in the 11th stage. After we gave fetched this branch, we have fetched a lot of instructions. After we fetch the branch, we fetched four instructions in the next cycle, and four instructions in the next cycle. So now when we miss predict a branch we waste 40 instructions that we could have done if the prediction was correct. So the cost of a mis-prediction is higher in terms of instructions that we have just wasted. And that means that again it's much more important to predict branches correctly. So now we have seen that we really want to have a better predictor, but the question is how. We have seen that the predict, taken predictor computes the next PC as a function of the current PC, by just incrementing it. So question is, if all we know is the current PC, is there any better function that we can use to form a better prediction of what's coming next? And the answer is, if all you know is the current PC, then probably you cannot make much of a better prediction. It will help us, if we knew, is the instruction a branch. But we don't know that, we're still fetching the instruction while we are making the prediction. Is this instruction going to be taken, as a branch? Again, we have no clue about that. What is the offset field of the instruction? If this is a branch, this is what will be added to the PC to form the target address if it's taken. But we don't know that either because we haven't fetched the instruction. So really, we don't know for sure anything that might be really useful in forming the [UNKNOWN] prediction. So it seems like it's hopeless, but it's not. So what do we know if we don't know anything for certain. What we do know is what the current PC is. But we also know something about how that branch at this PC was behaving in the past. And we can use that because it turns out that branches tend to behave the same way over and over again. So really that is what we can use. We don't know for sure what this branch is going to do but we can know what it did the few previous times when it was executed. So the simplest predictor that uses history is called the Branch Target Buffer. What it does is it takes the current PC of the branch, and uses that to index into a table, which we call the BTB. And from this table, what we'll read out is our best guess for what the next PC will be. But how do we know what to put in this table? Well, we carry the PC of the branch and the predicted PC with it through the pipeline. So, this is what happens at fetch. Later on in the pipeline, we have the correct PCNew. At that point, we will compare the predicted one with the correct one, and if they're not equal, we will treat that as a misprediction. And, also, we will use the branches own PC to index again into the BTB, and write this new PC, that is correct, into the table so that next time we see this branch. We get the correct prediction assuming that it's jumping to the same location. But there is a problem with our BTB, how big does it needs to be. We want it to in once cycle, given the current PC predict the PC, so it needs to have a single cycle latency. That means we want it to be really small. However, it needs to contain an entire instruction address. Let's say we're using 64 bit addressing, that means each entry is 8 bytes. And we need one entry for each possible PC from which we can fetch. The program can be large. Note that an instruction can be four bytes, yet we need eight bytes in the BTB. So really the BTB needs to be as large as the program itself, which is really large. So we cannot have a dedicated entry in the BTB for every possible PC address. We need the BTB to be much smaller than that. How do we do that? So how do we make this BTB realistic? It needs to be small yet it needs to work this way. First of all we don't need to have an entry for every possible PC. It's enough if we have enough entries for all the instructions that are likely to execute soon. So for example if our program is executing a loop that has about a hundred instructions of code we really need only slightly more than a 100 entries. After the first iteration of the loop the BTB will be populated with instructions in the loop, at which point we will keep finding what we need in the BTB. So now we have relatively few entries in the BTB. Let's say that we do our timing experiments and we find that only 1,024 entries can be accessed in one cycle. The question is now, there are many, many possible PCs. How do we map each PC to an entry in a way that avoids conflicts among different PCs that map to the same entry? And note also that we need this mapping function to be really, really simple because any delay in computing the mapping function means that we need an even smaller BTB so that the whole thing finishes in one cycle. The way we do this is the program counter from which we are fetching, is let's say is 64 bit to byte. So it has bits starting from zero, to 63. We need 10 bits to index into our BTB. So what we will do is we'll take the least significant ten bits and use that as the index into our BTB. This mapping function is really fast because we do is we just take things ten bits and we just feed that to the BTB as an index. Why are we using the least significant bits and not the most significant bits? Think about the typical program where an instruction has some sort of PC address and then the next instruction will have an address that is very close to it. For example, just increment by four and so on. So if we were using the most significant bits of the PC, all instructions that are in the same part of the program would map to the same entry. because they have the same most significant bits. Thus, if you have a loop of some sort here, all of the instructions in the loop would map to the same entry, so that as we execute this loop, these instructions are kicking each other's stuff from the BTB, which we don't want to have. We want each of these to have an entry. If we use the least significant bits, what we get is that each instruction gets a different entry from nearby instructions, thus ensuring that things like loops and functions and so on neatly map to the BTB. So now that we have seen how to access the BTB, let's do a quiz. Let's say we have a BTB with 1,024 entries. Note that these entries will be number 0 through 1,023. Let's say that this particular architecture uses fixed-size 4-byte instructions that have to be word-aligned. Which means, an instruction needs to begin at the divisible by four address. So, an instruction can be at address 0, 4, 8, etc. But it cannot be at addresses something like 3, or 5, or 17. And let's say that the program counter is a 32-bit register, meaning that this particular processor uses 32-bit instruction addresses. The question for you, is which BTB entry, again, they're numbered from 0 to 1,023, should be used for PC, whose address is x0000AB0C? Write your answer here. Again, the answer should be between 0 and 1023. Let's look at the solution for our BTB quiz. The question was, which BTB entry is used for a particular PC. It might seem easy because we need 10 bits to index into this BTB, so we will just take the least significant 10 bits of this PC. And index into the BTB with that. However, note that instructions are fixed size four bytes, and were the line, which means that not all addresses are possible for the program counter. If you now look at what would happen if we used the least significant 10-bits to index into the BTB, we would see the the entry number zero would be used. An instruction can have the least significant 10 bits be all zero. Entry number one, however, requires the least significant 10 bits to be 000001 and that entry cannot be used because the instructions are word-aligned. As we said, addresses such as all zeros and then just one are not possible. Basically, all addresses actually need to be not only even. But also divisible by four. So if you take the lower most bits that means that the only possible instruction addresses are of the form something something something and then the least significant two bits say zero. That means that we are really using only every fourth entry in our BTB. Pretty much of the 1024 entry BTB we are really using only 256 entries which is a huge waste. So in this case what should happen is we should not use the least significant ten bits, and instead we use these ten bits so that the least-significant bits that can be different among instructions. Basically these bits are always zero so they're just going to waste entries. We need to take the least significant ten bits that are still different among different instructions. So in this case our instruction has bits C is 1100. Zero is 0000. B is 1011. We're going to ignore these two bits. And take the next one, two, three, four, five, six, seven, eight, nine, ten bits. So this is the index into our BTB. And now let's convert that to hex, this will be the first hex digit so, it will be three. This will be the next hex digit, so it will be c. And this will be two. So we are talking about the entry number 2c3. So, how are we going to do the direction prediction? We are going to have a table that we will call, a Branch History Table, or BHT. We will take the PC of the instruction, grab the least significant bits, and that's what we index into this table with. So, it's the same, as when indexing into the BTB. But the entry here is much smaller. The simplest predictor will simply have a 1-bit here, that tells us if this is a not a taken branch, meaning it's a non-branch or a not taken branch or when it's one, it says that this is a taken branch. If we get a zero from here, we just increment the PC. If we get a one, we use the BTB to tell us where to go. Just like we were updating the BTB, we can update this once we resolve the branch. If we resolve to a taken branch then we put a one here and we write the destination address into the BTB. If we get a zero, if we, decide that the branch is not taken or that the instruction is not a branch. Then we put a zero here and we don't update the BTB so that we can use the BTB only for target addresses that we really, really need the BTB for. Because an entry here can be only a single bit. This table can be quite large, so we can have lots of instructions. Have entries here, avoiding conflicts between instructions that commonly execute, while still reserving the small BTB, which had much fewer entries only for branches in that code. So now that we have seen how the BTB and the BHT work together, let's do a BTB & BHT QUIZ on that. Suppose we have the following program. This is the address of the instruction. This is the instruction itself. And these are the labels for branches. So, what this program is doing is it's moving 100 into R2; it's moving 0 into R1. And then as long as R1 and R2 are different, it's going to continue here. Once R1 and R2 are the same, it's going to jump to the Done label and exit this loop. In the loop, it's adding R1 to R3 to form R4, uses that to load a value from that address and then adds that R4 to R5, adds 1 to R1 and then unconditionally branches back to this loop. So this is the loop. This is where we exit from the loop. And this is the sort of initialization for the loop. Suppose that we have a branch history table that has only 16 entries and makes perfect predictions about branches. Suppose we also have a BTB with 4 entries. Also makes perfect predictions. And as you can see, all instructions are four bytes in size, and we know that. So the question for you is, how many times do we access the BHT for each instruction? Right here, how many times we access the BHT with this address because we are fetching this instruction? Here, how many times we access the BHT with this address because we're fetching this instruction? And so on. There are eight instructions here for each one of them. We need to say how many times we access the BHT. So, let's do the solution to our BTB & BHT quiz. We have a BHT with 16 entries which gives perfect predictions. We have a BTB with four entries, gives us perfect predictions. The question was, how many times do we access the BHT for each of these instructions? We access the BHT in order to tell us whether this instruction is a taken branch. We do that every time we fetch an instruction. And because we have a perfect prediction for both BHT and the BTB that means that we will never fetch any instruction that actually is not supposed to execute. So we now just need to figure out how many times each of these instructions execute. This instruction here initalizes R2 to 100 and that instructions executes only one time once we have done executing it we will go into the loop, loop here and then exit never coming back here. Same here. This puts zero in R1. That executes only once. Now we have a loop. This loop will execute as long as R1 and R2 are different. R1 initially has a zero value, goes through here gets incremented, and then we branch back. Now we execute this loop once more, with R1 being equal to 1, and then 2, 3, et cetera. The last iteration we do will be with R1 equal 99, and finally R1 will be incremented, we jump back here. FInd out that R1 and R2 are now the same, because RI is now 100 and exit the loop. So, there is 100 iterations of this loop, and each of these instructions does executes 100 times. This instruction here executes 100 times when we're staying in the loop, and once we, for the 100th time jump back, it executes once more in order to exit the loop. So this instruction executes for 101 times, and each time any of these instructions execute, we're gong to access the BHT to see whether those instruction is in taken branch or not. Let's do another quiz with our BTB and BHT. Same BHT 16 entries perfect. Same BTB four entries perfect. Same program with the same addresses. The question for you now is which BHT entry do we access for each instruction? There are 16 entries, numbered 0 through 15. The question is when we are fetching this instruction here, what is the BHT entry that we're going to be using to predict it, and then for this instruction, you need to write it here, for this instruction, here, and so on. Again there are eight instruction, so we need to have eight numbers here. Let's look at the solution to our BTB and BHT quiz. The question this time is, which BHT entry do we access for each instruction? There are 16 entries in the BHT, and we said that we access the BHT using the lowermost bits of the instruction address. That they are not the same for all instructions. In this case, our instruction address has the following bits. For the move instruction here we have C000, because every instruction is 4 bytes in size, the lower most 2-bits are never changing; they're always going to be 0 for our instructions. And now because we have 16 BHT entries, we take the next 4 bits and this is the number of the BHT entry. In this case we're accessing the BHT entry number 0. Now let's look at this other move instruction here whose address is C004. C00 are not changing, what is changing is 4. Four in binary 0100. Again we discard these bits because they're never changing among instructions, and this is the number of the BHT entry. So the entry that we access here is 1. Now you can tell that for instruction 8 here, this is going to be 10, so we are going to access the BHT entry number 2, 3, 4, 5, 6 and 7. If we reached 15 we will then wrap around to 0, but we never reach it. So this is really what we get Let's do another one quiz on BTB and BHT interactions. Again we have the same BHT, 16 entries and perfect. BTB's four entries and perfect. Same program as before. The question for you now is how many times do we access the BTB this time for each instruction? So, how many times, when we fetch this instruction, do we access the BTB. How many times, when you fetch this instruction, do we access the BTB, and so on. Remember that these two instructions execute only once. This loop has 100 iterations, so all these instructions execute 100 times, except for this one, which executes 101 times. The question for you now is, how many times do we access the BTB for each of those instructions? Let's look at the solution to our BTB and BHT quiz. This time the question was how many times do we access the BTB for each of these instructions. We have a perfect BHT and a perfect BTB. The BTB has four entries, the BHT has 16 entries. This doesn't really matter in this part of the quiz. What you ned to know is that when you have both the bhd and the btb, the btb is accessed only if the bhd says that this is a taken branch. If this is not a taken branch, then we can just increment the pc. Because this instruction is not a taken branch. And because the BHT has perfect prediction, the BTB will never be accessed. Same thing here. And now I can tell, that for all non-branch instructions, we will just have zero BTB accesses, because they are never predicted to be taken branches. This branch here is executed 100 times and is always taken, so here the BTB will be accessed 100 times. This branch is more interesting, in every iteration that stays in the loop, iterations where R1 is 0. 1, 2, et cetera, until 99, this branch is not taken. The BHT is perfectly predicting it, so we don't access the BTB at all. Only on the last iteration when we are having the R1 be 99 and we wrap around here and try to compare again, and this time we see that R1 is 100, and jump out. Is this branch taken? Because again, we have a perfect BHT, that means that the only time the BTB is accessed for this instruction is that one last time when we are jumping out of the loop. So the BTB will be accessed only once because there is only one time that this branch has actually taken. Let's do yet another quiz on the BTB and BHT interaction. Same BHT and BTB as in the previous quiz, the same program. The question for you now is which BTB entry do we use for each of these instructions. Leave the field for the instruction blank if none of the BTB entries is used by it. Let's look at the solution to this part of our BTB & BHT quiz. The question now is, which of the four BTB entries do we use for each of these instructions? Leave the field blank if none is used. We already know that we use the BTB once here and a hundred times here, and that's all of the BTB uses. So the only instructions that we really need to put something for is this and this. How do we determine which BTB entry do we use? Well, we look at the address and just the same way that we did it for the BHT, in this case, the address is C008. The BTB has four entries. We take the lower most two bits, in this case, because we need two bits to tell us which of the four entries. But, we don't use the bits that are always the same for all instructions. In this case, the lower most two bits. So, really, this is the number of the BTB entry for the C008 instruction. 10 is 2. So we will use the entry number 2. The other instruction that we need to figure out is at the address C01C. So it's going to be C01C. Again, ignore the lower most bits that stay the same. Take the next two bits because we need to know which of the four entries to use, and we get 11, which corresponds to the entry number 3. As you can see, only entries number 2 and 3 are used in this loop. If this instruction were a taken branch, it would have used and entry number 0. This one would have used 1, 2, 3, and then, because we have 4 entries, this one, again, would be using entry number 0, 1, 2, 3. But because these are the only ones that actually use these entries, all we have to do is access the entry number 2 here. It's accessed once, if you remember. And access the entry number 3 here. It's accessed a hundred times, if you remember. Let's now do another BTB and BHT quiz but this time we have a BHT that has 16 entries. And each entry is a one bit predictor and that one bit predictor starts predicting not taken. So all of the 16 entries initially start out predicting not taken. The BTB is a four entry perfect predictor still. The question for you this time, is how many mispredictions will we have for each instruction using this BHT and BTB? Write the number of mispredictions here for this MOV instruction. Write the number of mispredictions here for this MOV instruction, and so on. Let's discuss the solution to our BTB and BHT quiz. We have a BHT that has 16 entries and it's a 1-bit predictor. Each of the entries is a single bit that tells us what the last outcome was. And initially they start off at predicting, not-taken. The question was, how many mispredictions do we have for each instruction? Initially the BHT says not-taken for all of the entries and we have already seen that the BHT entry that we will use are all different. So these branches any other instructions are not going to interfere with each other. Each of them just uses its own BHT entry, because the initial prediction is not-taken. And this is indeed not a taken branch. We have 0 missed predictions here, same here. When we enter the loop, this branch is actually not-taken, so we have no missed prediction the first time we execute it. This instruction here is predicted not-taken the first time. It's not-taken for real, so every other time that we execute it it's going to be predicted not-taken correctly, so there is 0 mispredictions. And same thing happens here, and here, and here. For this branch, things are more interesting. The initial prediction in the BHD is going to be not-taken, but this branch is taken, so we have a misprediction the first time this branch is taken. Then, the BHT entry for that instruction is updated to say taken, so every other time that we execute this instruction it's correctly predicted as taken. So we have only the one misprediction initially. For this branch, it's never taken until the very last time, the 101st time we do it. Every time it's not-taken, we have a correct prediction, because we start out in the not-taken state of the predictor, and just keep doing that. The last time we need to execute this branch, it is taken. And the last outcome that we have seen was not-taken. So there will be again a single misprediction here. So as you can see the 1-bit predictor is very accurate. We are executing a lot of instructions. This one is done a 101 times, this one is done a 100 times, these are done a 100 times each. So, for what amounts to more than 600 instruction executions, we only have two mispredictions. This shows that the 1-bit predictor really works well for things like loops that execute many iterations. However, we will see later that it doesn't work well for some of the other things like, for example, loops don't have many iterations or if then analysis So we have seen in the latest quiz, that the one bit predictor works reasonably well for rules that have a lot of iterations. So what are the problems with one bit prediction that have related to better predictors. The one bit predictor predicts well,branches that are always taken it might make a mistake that first time it's used the branch, but after that it's going to start predicting taken and because of branches always take it always works well. Similarly always not taken is predict as well because the very first time it might mispredict, but after that it learns the behavior and it stays the same so we just have the always not taken. It also predicts well,the branches where the number of taken outcomes is vastly more than the number of not taken outcomes. Here it's going to learn after the first taken outcome that the next outcome should be predicted taken, and very, very rarely it's going to be wrong because of a not taken outcome. Similarly, the one way predictor predicts well when the number of not taken outcomes is vastly more than the number of taken outcomes for the same reason. Basically we will see not taken, not taken, not taken many, many times. The first time we see one of those we're going to learn it. And then the taken outcome will be mispredicted. So what's wrong with the one, one bit predictor. What's wrong with it, can be glimpsed from these. Let's look at the branch that is for example, more taken than not. So it's been taken many times. And then there is a not taken outcome and then it's going to be taken many more times. Let's now look at whether the one we predicted would be right on these. It's been taken many times, so it's going to predict taken, and then because it's been taken it's going to predict the next one as taken. The next one is going to be predicted taken, taken again. When we reach the not taken outcome, the last outcome was taken, so we will have a misprediction. This would not be much of a problem if not taken occurs very rarely, because we are basically mispredicting just the very rare occurrences. However, there is an interesting problem here, which is now that we are back to our taken behavior, which is the vastly dominant behavior in this case,. The last outcome was not not taken. So we're going to mispredict this branch too. Now, we're predicting not taken and the branch will be taken. After this, the previous outcome was taken so we predict taken. And we continue predicting taken. So the problem really is. That each anomaly, such as we have a branch that is almost always taken but sometimes it's not taken, results in two mispredictions. One for the anomalous behavior and then once more for the normal behavior that follows the anomaly. So the one bit predictor will not do so well, when the branch is biased towards, let's say, doing more taken branches but the number of not taken branches is still significant, or when there is more not taken branches. But the number of taken branches is significant. Because for each of the less dominant behavior is going to do mispredictions not just one. Typically also a branch predictor like this will not do well on short loops. Why? Because in a short loop we have a branch,that for example, exits the loop. Then we have some loop behavior and then a branch that loops back. The branch that exits the loop. Will be predicted correctly as long as we stay in the loop by this predictor. The exit from the loop will be mispredicted. The branch vector will now be trained to take this path here. When we come back to this short loop. It's going to mispredict the very first iteration for this branch. Because now it's going to think it should be this way. But, in fact, now we're staying in the loop. So have, for example, an eight iteration loop. We will have two mispredictions every 80 duration's. One at the end of the loop and one at the beginning of the loop because the previous ending of the loop trained the predictor that way. And then of course the one with predictor will be pretty bad. When the number of taken and not taken outcomes is similar, yet predictable. We will talk about this more later. But now, let's first devise a predictor that will fix this. So, can we devise a predictor that behaves similarly well here, but does not mispredict twice whenever there is an anomaly. The predictor that fixes the two mis-predictions per anomaly behavior of the one bit predictor is called a two bit predictor. Often we write that as 2BP,or even a 2BC which stands for a 2-bit counter. We will see that this predictor behaves like a 2-bit counter. The 2-bit predictor has possible values zero, zero, zero, one, one, zero, and one, one. The way it works, is that the more significant bit of the predictor tells us what the prediction should be. So this is what we will call the prediction bit, the more significant bit of this predictor. This prediction bit really functions similarly to. How the 1-bit predictor functions. This lower bit, here, is called the Histeresis or Conviction Bit. This tells us how sure are we that we should be predicting what the prediction bit is saying. Usually, the state zero, zero is treated as a strong not-taken state. This means that we will be predicting not taken. And we are fairly convinced that not taken is the dominant behavior of this branch. The zero one state will be the weak not taken prediction state. Were still going to predict not taken but this time we are less sure that that's the dominant behavior. Similarly here for the, predict taken, states we have, Weak Taken, and the Strong Taken state. So how does this predictor work? The 1-bit predictor only had the, not-taken and the taken state. If, our state is zero, we will predict not-taken. If we indeed see the not-taken decision later, we stay in this state. Next time we'll predict not-taken too. If however, we're predicting not-taken, but this decision turns out to be taken, we change our mind and move to the taken state. In the taken state, we stay there if the branches in it taken, but if we predict taken and see that the branch is not-taken, we go back to the not-taken state and state predicting that. So pretty much, one outcome changes our mind completely about what to do with a branch. And that has resulted in two mis-predictions per anomaly. Lets say that the dominant behavior is not-taken, the first time the branch is seen to be taken we move here and then the branch moves back to the dominant behavior we will predict taken and mis-predict again before we move here. With 2-bit predictor we have four states. Zero zero, zero one, one zero, and one one. Now this is our strong not-taken state. And of course if we see a not-taken decision we stay there. We were convinced that the branch will be not taken and the outcome really confirms that. If we see a single taken decision from the strong not taken state we move to the weak not taken. So now we're predicting that the branch is still not-taken, but we are less convinced about that. In the case when the not-taken behavior is the dominant one this would be the anomaly and we would move back to the strong not-taken state. Once we see that the branch is not-taken, so the idea is that, unlike here, where the anomaly itself will result in a mis-prediction, and then we will predict differently so that the dominant behavior again will be mis-predicted for once. Here, this taken while in the strong not-taken state will be a mis-prediction. But then, we will predict not-taken. It's just a weak prediction. So we will be right for this not-taken outcome. So if the branch really is mostly not-taken, we only have a mis-prediction. When the anomaly happens, not also, not when the dominant behavior happens after the anomaly. Similarly, if we are in the strong taken state, we believe now that the taken behavior is the dominant one. A single not taken will result in a mis-prediction, but move us to a weak taken state. So that if we see a taken outcome again, we don't mis-predict and we move back to the strong taken state. Of course we stay in the strong taken state as long as we are seeing the taken outcome. So the idea is that if not-taken is a dominant behavior, we expect the predictor to stay in these two states. If taken is the dominant behavior, we expect the predictor to stay in these two states. Either way, it's predicting so that only the anomaly gets mis-predicted, but not the dominant behavior that follows the anomaly. However, we can have a branch that, for example, tends to be not-taken for a while, and then changes its mind about it and starts being taken for a while. Thus, if we are in the weak taken state, and we see another taken outcome, we will go to one of the taken states, and similarly, if we are in a weak taken state, and we see another not-taken, we will move to one of the not taken states. So, the idea is, if we see a branch that is mostly not taken. Periodically it's maybe taken and then again stays not taken. We stay in these two states and keep predicting not taken. But if the branch starts being taken very often from now on, it will take one, two mis-predictions before we are in the taken states and start predicting taken again. So somehow, a single anomaly will cost us one mis-prediction. A switch in behavior will cost us two mis-predictions. So why we call this a 2-bit counter is because if you look at what this is doing, is, it's a counter that counts up. When the branch is taken, and saturates at three. And counts down when it's not-taken, and saturates at zero. So really it's a 2-bit counter that just counts up when the branch is taken, and counts down as the branch is not-taken. This also means that this is a relatively easy behavior to implement. Basically it's very easy to implement this type of predictor Now that we have four possible states for our two bit predictor there is an interesting question of does it matter in which state we start the predictor off. So with the one bit predictor we had to start it in either zero or one state. If we start it in the not taken state, zero, that means that if the branch is not taken it will start predicting correctly right off the bat. Is we start predicting zero but the branches actually mostly taken we will have a single mis-prediction before we start doing taken. For a two bit predictor, however, the question is really should we start in one of the strong conviction states or should we start in one of the weak prediction states. If we start in a strong state, for example zero, zero, and the actual decisions are not taken, then we will stay in that same strong state and never have any missed predictions. However, if we start in the strong state zero, zero, but the branch tends to be taken. We will mispredict once, move to the zero one, the weak not taken state. Missed predict again and then start predicting correctly. If we start in a strong state and it's the correct strong state we have no missed predictions, if it's the wrong strong state we have two missed predictions. If however we start in a weak state such as zero one. And we started in the correct weak state. We will just move to the strong state and still have all correct predictions. So, in the case when we guessed right what should be the dominant behavior in the starting state, we have perfect prediction regardless of whether we start in the strong or weak state. However, If we were wrong about where to start and we start in, let's say, zero, one, but now the branch is mostly taken, we will have one missed prediction here and now we will be moving to the weak taken state and start having correct predictions from then on. So if we start in a weak state, we don't lose anything if we were right. But we gain one less misprediction, if we were wrong about what should be the bias. So this argument means that probably, we should be starting in the weak state. Pick one, doesn't really matter. However, there is a flip side to it. Let's say that the branch is taken, not taken, taken, not taken. And we start in the strong state. The branch is taken so we move to the weak state. Then it's not taken so we move back to the strong state. And so on, we missed predict here, but we correctly predicted here and here, so half the time we have missed predictions. If we started in the one, one state, we would have a similar behavior, we would mis-predict every other time, but if we start in the wrong weak state and have the same behavior here. If the branch is taken and we have a weak not taken. We mis-predict and we move to the weak taken state. Now the branch is not taken so we mis-predict again. And move back to the weak not taken state. Branch is taken again and so we mis-predict again. And we keep mis-predicting like this. So as far as worse case behavior on alternating branch decisions is concerned. Starting in a weak state, around some more risk of having this worst case behavior of always missed predicting. Fortunately, this is much more common behavior than this. So probably it's a good idea to start in a weak taken state because taken branches are slightly more common than not taken. But in reality, as you can see, it doesn't really matter much because if the branch is mostly taken, one missed prediction initially is not really going to be very costly to us because from then on it's going to behave the same way regardless of what we started off with. So in reality, although it seems like it's a good idea to start in the weak taken state. In practice the overall accuracy of the branch predictor is effected very little by what exactly do we start of in. So we might a well just start of in zero zero state because that the easiest state to initialize into. Now that we have seen a 2 bit predictor and how it works, let's do a quiz on 2 bit predictors. Let's say, you have a 2 bit predictor, that has the four states. The 00 state is the strong not taken. Then we have the weak not taken, weak taken and the strong taken state. Let's say that we start in the strong, not taken state. And remember, that if we have a taken decision, we move towards the strong taken state and stay there as long as we have a taken decision. But for a not taken decision, we count down towards the strong not taken state. And stay there. The question for you is, is there a sequence of branch outcomes that results in never predicting correctly. So is there a sequence such that when we start in this state, and here obviously we're predicting not taken, so the outcome needs to be taken. And then after we update the predictor, we have to mis-predict again and again. If yes, there is a sequence, then write the. First five outcomes of that sequence here. If no, then put a check mark here. Now let's discuss the solution to our two bit predictor quiz. We have a two bit predictor like this. The question is, is there a sequence of branch outcomes, such that this predictor, starting in the strong note taken state always mispredicts. And the answer is of course yes. Every single predictor has the worst case scenario of always mispredicting. What you have to do to find out what that worst-case scenario is, is simply design a sequence such that whatever the state of the predictor is, the sequence goes the opposite way. So, we start in the strong not taken state, thus the first decision will be taken, and that will take us to a weak not taken state. The predictor will still predict not taken, so we make the branch be taken next time. Now the predictor goes to the weak taken state and now we know how to cause the alternation between the states. So in the weak taken state, we go not taken. It moves to weak not taken, now we say taken and so on. So you can just keep doing this. So pretty much, even if we do start in a strong taken state, there is a way to move us to a weak state, at which point we can do this alternation. Note that for this particular sequence, there is a way to change this predictor so that this behavior doesn't result in 100% misprediction. But in general, every predictor no matter how it's organized, will always have some sequence as it's worst possible outcome. It's just a matter of how likely is that sequence. A good predictor will not have this worst case behavior for some thing that is a very likely sequence to happen. So again, every predictor has a worst-case scenario where it's doing 100% mispredictions. So we have seen that moving from 1-bit predictor to 2-bit predictor improves things. Primarily because the one off occurrences of the other behavior are not going to change it, our complete decision. So now the question is, if the increase in the predictor size this way worked. Why not do the 3-bit predictor or a 4-bit predictor? The bad part of using more bits per predictor is that the cost goes up in proportion to how many bits we need. We still need to have the same number of predictor entries in order so that each branch has its own predictor entry and not share it with another branch that behaves differently. So, for the same number of entries, we will have, this doubles the cost, this is going to add another 50% to the cost, and so on, so we are increasing the cost. It might be worth it if the prediction accuracy is improving. So, the good side is that sometimes the branch prediction accuracy does improve as you go to 3-bits or 4-bits. When? Well, adding to 3-bits or 4-its is going to increase the [UNKNOWN], of four. A longer time, we will keep guessing the old way, even though decisions are going the other way. So really, the only time when this is good, is when anomalous outcomes come in streaks. If you have a streak of, for example, not taken all of the time, and then it happens that you have taken, taken very often. After that and then again not taken for awhile then actually having a 3-bit predictor would solve the problem of being convinced the other way when the taken taken happens. But the real question is, how often do we really have in real programs that anomalous outcomes come in streaks? And the answer is not very often. So what we might have is that the 3-bit predictor might be worth it, but definitely not the 4-bit predictor. So somehow we really want to have either a 2-bit predictor or a 3-bit predictor. And most of the time, we're just fine with a 2-bit predictor. We're getting most of the benefits we would get with more bits for a lower cost. So we really should stay with a 2-bit predictor and maybe move to 3-bits if we really, really want to. But if moving to more bits like this is not helping us, then the question becomes, but how do we then improve prediction beyond the 2-bit predictor? We have already seen that neither 1-bit nor 2-bit work really well when we have things like not-taken, taken, not-taken, taken, etc. And now, a human being that looks at this sequence not-taken, taken, not-taken, taken, etc., will correctly guess that the next outcome here will probably be not-taken, followed by taken, and so on. So this branch turns out to be perfectly predictable, it's just that it's not predictable with a 1-bit or 2-bit predictor. So now we will look at so called history based predictors, which are trying to do well on things like when we have an not take and take and not take and take and repeating pattern. Or something like two not takens followed by a taken and then again, two not takens followed by a taken, et cetera. Again, it's a repeating pattern. That even a two bit predictor will miss predict the taken branch, so it's one term miss predicted, and even more complicated pattern based things. It's important to note that these are really 100% predictable. These are repeating patterns, once you figure out what the pattern is, the branch becomes 100% predictable. It's just that these branches are not predictable with simple N-Bit counters. What we need is a predictor that somehow learns the pattern really. And then it becomes a very accurate predictor. How do we learn the pattern? Well, we don't just look at what's the majority outcome here. For example, here there is no majority outcome at all. Here, not taken is the majority outcome. But takens happen very frequently, so if we just look at what's the majority behavior, we'll have a lot of mispredictions. What we want to look at, is the so called, history, of the branch. What were, the latest so many outcomes? For example, if we have the not taken, taken, not taken, taken, and so on behavior. Let's ignore the first one for now. And look at this particular prediction here. What we know, is that the previous outcome, was not taken. And if we know what the pattern is, then we know that when the branch has been not taken previously, our prediction should be taken. And then here, the history says the branch previously was taken, so our prediction should be not taken. And, if you are doing it this way, then you will be predicting correctly every single time. So what we need to learn is that under the history of not taken, our decision should be taken. Under the history of taken, our decision should be not taken. Let's now look at this NNT branch. In that case we have not taken, not taken, taken, not taken, not taken, taken. The history now, needs to consider the last two decisions. So when see not taken, not taken, we should be predicting taken. When we see not taken, taken as the previous two outcomes we should predicting not taken. And when we see taken, not taken, we should be predicting not taken. And then we are back to not taken, not taken history where we should be predicting taken. So if we consider for different histories, different predictions, then we will train our predictors so that for it, for this history predicts one outcome for this history predicts possibly another outcome for this history predicts a different outcome and so on. And then this becomes a very predictable branch. So lets look at the example history based projector that has a 1-bit history with 2-bit counters, for each history. So the general approach to this branch vector stays the same, we take some bits of the PC, we index into the branch history table, but instead of having a 1-bit counter or a 2-bit counter in each entry. Now we have a single bit history in each entry, a two bit counter for when the history is 0, and the separate two bit counter for when the history is 1. So the state of the predictor is now not just the state of a single 1-bit or 2-bit counter, it's the state of a single bit history and two of these counters. So now let's look at the behavior of predictors over time, like this. We have the predictors state, the prediction, the outcome, and whether the prediction was correct. Let's start the predictor in a state where the history bit is and the 2, 2-bit predictors are both in the strong not taken state. So again all three of these together are now the state of the predictor. The prediction in this type of predictor is formed by using the history bit to tell us which of the two predictor to use. Because the history is 0, we will be using the first of the two counters. It's saying strong not taken, so the prediction will be not taken. Let's say that this is a branch where the outcome is taken, not taken, taken, not taken, et cetera, and in this case the prediction and the outcome differ. So, we have an incorrect prediction. Now we will update the state depending on the outcome. The way we update the predictor, is to use the history to index into the two counters. Meaning we will be using this counter and modify the counter according to the outcome. In this case, the outcome is taken, so the first of the two counters will be moved to the weak not-taken state from the strong. So this counter becomes weak not taken, this counter is not affected, and then we shift into the history the actual outcome, which was taken, so the history becomes 1. When the history is 1, we're going to be using the second of the two counters. So pretty much, we use the history to kind of index into this array of counters that only has two counters, so when it's 1 we use this one. The prediction will be not taken and it will be right. The state of the predictor is now updated, so that we use the history to index again into the counter, modify that counter according to the outcome. It stays strong not taken, because that's already as convinced about the not taken outcome as it can be. The other counter just stays the same. And the history now shifts in the zero because of the not taken outcome. So now we have the state of 0 weak not taken, strong not taken to predict the next outcome. Our prediction now will index into the array of counters, get the first one. Weak not taken means that we still predict not taken. We are wrong. We update the predictor by finding the counter that corresponds to the history and this case 0 means we use this one. We update it using the outcome from weak not take we go to weak taken. The other counter stays the same, the history shifts in the one for taken and this is the new state that we use to predict the next outcome. 1 means we use this counter, that counter says strong not taken, so we predict not taken. We are correct, update the predictor, 1 means we update this counter with not taken, so it stays strong not taken, this one stays weak not taken because it's not affected by this. Not taken is shifted into the history so we can 0, and now we use this to predict the next outcome. 0 means we use this counter, this counter say weak taken so we predict taken and we predict correctly. We update the predictor by using 0 to tell us which counter to update. Taken is used to update the counter, so from weak taken it becomes strong taken. The other counter stays the same. The history shifts in a 1, so we get this. 1 means we use this counter next. It says not taken. We predict not taken and we are right. Update the state of the predictor. 1 means we update this one with not taken. It stays strongly not taken, this one stays the same. History shifts in a 0 so it becomes 0. We use 0 to tell us which counter. This one it's strong taken, so we predict taken and we will be right. And as you can see, from now on we get perfect prediction in this predictor. Why? Well, because if the previous outcome was 0, we are now guessing that the next outcome will be taken. If the previous outcome was 1, then we are predicting that the next outcome will be not taken. And now we do this continuously because both of these counters have learned strong decisions, because every time we have seen a 0, it was followed by taken, every time we have seen a 1, it was followed by not taken. So this predictor learned the pattern of taken, not taken, taken et cetera. So now that we have seen how the 1-BIT History works, Let's do a quiz about that. So we have 1-aof history and a 2-bit counter per history. And the pattern we will try to predict is the not-taken, not-taken, taken and then that repeats. This is a regular expression, so BIT match is not-taken, not-taken, taken, not-taken, not-taken, taken, etc. The 1-bit history, is 0 at the beginning. The 2- BIT counters both start at the strong not taken state. And the question for you is, after 100 repetitions of this pattern, so there are 300 outcomes. NNT, and that repeats 100 times. What is the overall number of mispredictions that we will have seen? Write that number here. Let's look at the solution for our 1-Bit History Quiz. We have a 1-Bit history that starts at zero. And two bit counter for each possible history, all counters start at strong not taken. And the pattern we are trying to predict is the not taken, not taken, taken, etc. After 100 repetitions of this pattern, what is the total number of mispredictions that we will have seen? One way to approach this problem is simply to work through the table like we have seen before. So we will have our state, the prediction, the outcome, and whether we are correct. And now we will just work through the outcomes. The outcomes will be N, N, T, N, N, T, etc. And of course we will not go through a 100 repetitions of this. Pretty soon the predictor will start to be very consistent about what it's predicting in each round of the pattern. The state initially is zero, strong not taken, strong not taken. The prediction as a result of that will be use 0th counter, which is saying strong not taken. So the prediction will be not taken. This agrees with the outcome, so we are correct. Update the predictor. So the state will be zero because the outcome was not taken. Strong not taken will be updated with a not taken outcome, so it stays strong not taken, and the other counter just stays the same. Now the predictor again, will predict not taken, using zero and strong not taken. And it will again be correct. So, the history stays at zero, the strong not taken stays at strong not taken, and the other counter just stays the same. And at this point, we will again, with the history of zero use this, predict not taken strongly and be wrong this time. So after one round, we have a single misprediction. The history will be one now. And the predictor here will be updated to weak, not taken, because there has been a taken outcome. And the other predictor just stays the same. Now we will use a history of one, to index say strong not taken. So the outcome will be not taken and be correct. Next, we will keep the strong not taken here. The other counter stays the same. The history becomes zero because of this, and we will use a history of zero to index into the weak not taken state, and thus predict not taken again. Again we are correct in our decision. Next, the history stays zero, because the decision was not taken. The update of the weak not taken goes back to the strong not taken, because the decision was not taken, the other counter stays the same. And using the history of zero, we will index and use this counter. This counter says strong not taken, so we will predict not taken. This will disagree with the taken prediction, so we will have a misprediction. This concludes round two. As you can see, at the end of round two, we have the same state that we had at the end of round one. So now, pretty much what happened in round two will repeat more times. Overall, in the 100 repetitions of this, in each repetition we have a single misprediction. So, the correct answer is that we have 100 mispredictions, or one-third of our decisions are wrong. As you can see, the 1-bit history predictor is not a great predictor for this type of a pattern. So we have seen in the previous quiz that the 1-bit predictor can not really predict well the pattern N N T that repeats. We will see that a 2-bit history will be successfully predicting this pattern. How does a 2-bit history predictor work? Same as before. We take the PC and we index into the branch history table from which we get an entry. Now, however, the entry we get has five pieces. It has two bits of history. So these are the last two outcomes of this branch. And then we have 4 times a 2-bit counter. One for each possible history. So with two bits we will index into this array of counters and use the appropriate counter for the given history. So as you can see, the overall cost of this predictor now is 2 bits of history plus 4 times 2, so it ends up costing us ten bits per branch. Instead of going through a lengthy example of how this works, when we try to predict this pattern, let's try to figure it out. The sequence of outcomes is N, N, T. N, N, T etc. When we are seeing the outcome of T, and trying to predict it, that is we are trying to predict what will become T, our two bit history will be zero, zero. It's saying that the last two outcomes we have seen, have been both not taken. This history will make us use this counter here. So that counter, which we'll call counter number zero, will be counting up. When we are trying to predict this particular outcome our history will be zero, one. We will be using counter number one. And we will decrement it because we are seeing a non-taken decision. When we are trying to predict this the history will be one, zero. This is, corresponds to counter number two, which will also be decremented. For this, now we will again use the history of zero zero, because the last two outcomes have been not taken. This means that the counter number zero, because of this taken decision, will be incremented. And so on. As you can see, counter zero always counts up, so very soon it will start predicting taken. Which means every time we see a history of 00, we will start predicting strong taken. Counter one keeps counting down, so very soon after seeing this history of 01, we will predict strong not taken, and similarly counter two is always counting down. So very soon we will start predicting not taken when we see a history of one zero. As you can see what's now going to happen is pretty much this counter zero here, will start to predict strong taken. Counter one will start to predict strong not taken, and counter two is going to predict strong not taken. Counter number three, which is the last counter we have, actually never gets used. Because we never see a history of one, one. That would mean that we have seen a taken followed by a taken, which doesn't happen in this particular pattern. So, as you can see, this predictor starts being a perfect predictor after awhile, after these counters have been appropriately modified depending on their starting state. So, unlike a single bit history predictor, a 2-bit history can predict this pattern accurately. So after the initial warm-up period, we have seen, that the 2-bit history predictor will start predicting this, with 100% accuracy. We also see, that this predictor is really wasting one of its four 2-bit counters because, it's only using three of the four, to predict this pattern. And the fourth one simply never gets used. So we have it in the entry, but we really didn't need it. Now let's see what happens if this same 2-bit history predictor is trying to predict, the alternating pattern, the not taken taken, not taken taken, and so on. For that pattern, when we are trying to predict this non-taken here, we're seeing a history of 01. That means the counter 1, will be counting down. When we are trying to predict this, we have a history of 10, and when we have 10, that means counter 2. Is counting up. For this, we again have a history of 01. So, counter 1 counts down. And when we try to predict this, we have 10, in history. So counter 2, is counting up. As you can see, for this pattern, counter 1 and counter 2 will very soon start predicting correctly. And there are really, only two of these counters used. One because the history can be 01, and one because the history can be 10. We never can see the history of 00, or 11, which means, after the initial warm-up period, this predictor will also predict this with 100% accuracy. We have already seen that the 1-bit history predictor can predict this with a 100% accuracy, which is good. But, this predictor is not using half of its 2-bit predictor entries. So basically, in the entry that corresponds to this branch, we are really using only two of the four 2-bit counters, so the waste is larger. In general, an N-bit history predictor, will correctly predict all patterns of length. Less than, or equal to, N plus 1. We have seen that a single bit predictor, can predict correctly the pattern that is of line two. A 2-bit predictor can predict correctly a pattern of length three and also of length two. Both of them, of course, can predict a pattern of length one, which amounts to always taken, or always not taken. In general, an N-bit history predictor, will predict all patterns of length, its history size, plus one, or less than that. However, the N-bit history predictor, will cost us, the N-bit of history plus, 2 to the Nth, 2-bit counters. Per entry. Each branch will have an entry that we index into that will be of this size. So the entries become very large. For example, if it's a 10-bit history we're talking about several kilobits worth of stuff, for a single branch to be predicted. If we have, for example, a loop that has something like a. Loop out branch and a loop back branch. That will spend two of these entries. And, how about the waste? Most of the 2-bit counters that we have in our entry will be wasted. Why? Well because, if we have for example,. A 10-bit history, there are 1,024 entries. We can only predict patterns that are up to 11th in length, and a pattern of length 11, will have 11 possible histories at most. So of the more than a thousand histories, we will really be using. Only up to 11 of them, for accurate prediction. The rest of the histories are mainly, waste. So as as we can see, by increasing the N here which is the length of the history, we're improving, our ability to predict longer and longer patterns. But, our predictor becomes very expensive, very quickly and much of the cost that we are adding is waste. So the question is, why would we want to have N large? Why not just settle for something like, patterns of length three? Well it turns out, that a very common type of a pattern, is a loop, where we have something like, this would be. Don't exit the loop, don't exit the loop, exit the loop. So it's a tri-iteration loop, and then we repeat that loop many times. So for example, in order to do something like a eight iteration loop, we have to have a pattern, of eight or nine length. Which means we need, an 7 or 8-bit history in order to predict that successfully. So we do want to have a long history, but we'd rather have it, without this huge cost and without the huge waste that the cost comes with. So, now that we have seen how two or more bit histories work, let's do a quiz about them. Let's say we have an N-BIT History, with a 2-BIT counter per history in each predictor entry. Let's say that we need 1,024 entries in order for each branch to, and each other instruction, to have its own entry, so that they don't interfere with each other. So, for the length of the history of 1, 4, 8 and 16, what is the cost of the entire predictor in bits? I'm going to tell you that the cost with a 1-BIT of History is the 1-BIT History, and two 2-bit counters per entry. So it's going to 5 bits times 1024. Is this predictor good for each of the predictors for a pattern of N N N T that repeats. And we know already that the 1-bit history predictor is not good for this pattern because it's too long for it. And finally what is the number of two bit counters in an entry that we actually use when trying to predict the repeating NT pattern? Meaning not taken, taken, not taken, taken and so on. We already know that in the N equals 1 predictor we need to both of our counters, so the answer is 2. To further help you, I'm going to tell you that the cost of N equals 4 predictor should be. The history is 4 plus 2 bit predictors I need 2 to the 4th on that times 1,024. You can compute what this is, you should put the real number here. So you should multiply things out for these two, here, you should just say yes or no. Yes, if the predictor will predict this successfully after the initial warmup period. Now, if it want, and then how many two-bit counters are we using for the not taken, taken, not taken, taken pattern in this particular predictor. Let's now look at the solution for our N-Bit history predictor quiz. We said that we want to have an N-Bit history predictor 2-bit counters per possible history and we need 1,024 entries like that. And now we want to compute the cost for these predictors, how well they do on the N N N T pattern, and also how many of the 2-bit counters do they actually use in the NT repeating pattern. The cost can be computed as simply the size of the history, plus we need 2 to the nth 2-bit counters for each entry. And if you compute what this is, it amounts to 532,480 bits. So this is around half a megabit of predictor stuff for just 1,024 entries. With N equals 16, we're talking about 134,234,112 bits. So this is many, many megabits worth of predictor stuff. Again, for the same number of entries. As you can see, this is probably not a realistic predictor at all. And even this one probably you don't want to spend that much harder on just branch prediction. Let's now see how well they do on the N N N T pattern. This is a pattern of length four, so anything that has N equal three or more will handle it well. So from that perspective, all three of these will do well. These two will do well on even longer patterns, and finally how many of those two big counters did we have are we actually using for the, not taken taken not taken, taken. Think about it this way, for the N equals 1, when we need to predict the taken branch. We have a history of just 0, because it's a single bit history. When we need to predict the no taken branch, we have a history of just 1, so we're using both of these. When we have N equals 4 and trying to predict this not taken branch, our history is 0 1 0 1. When we're trying to predict the taken branch, the history will be 1 0 1 0. When we're trying to predict the next not taken branch, the history again, will be 0 1 0 1. So as you can see, really, we will only be using entries 0 1 0 1. And 1 0 1 0. So we're still using only two of the 16 2-bit counters. If you look at the 8th-bit history, the history will just be 0 1 0 1 0 1 0 1 and 1 0 1 0 1 0 1 0 here. So we will still be using only 2. Of now, 256 2-bit counters. So, we are now using less than 1% of the counters we have in each entry. And finally, for this history, you can see the trend here, we will be using only two 2-bit counters of more than 65,000 that we have in each entry. Let's do another quiz about history predictors. Let's say we have a program in which we have two nested loops. One i goes from zero through seven. And then within we have a nested loop of j goes from zero through seven. Let's say we do something that doesn't have much branching behavior in it. This type of nested loop structure is very common when you have things like matrices or even image processing. In particular, for image processing, it's very common. For example, compression or decompression are working in blocks of, let's say, 8 by 8 pixels in this case which will mean that there are loops like this all over the code. Let's say that the code, when it does this nested loop, just does something else and then very quickly comes back to it to do another round of this, so this is not just executed once, the whole thing is executed many, many times. If we are building a history based predictor that should do well on this code, it needs at least how many entries, and each of these entries should have, how many bits of history, and how many 2 bit counters do we need to enter as a result of this history? Let's look at the solution to our history predictor quiz. In c a loop exits at the beginning. So the first loop will do something like initialize i to zero. If i is equal to 8. Jump to done otherwise do everything that is need for intraitation of the outer loop. And then, branch back here. The inner loop, we'll just do j equals zero. And then if j is equal to eight, go to sum label D1. Do something, jump back here. And the label D1 would really be here, after which we're going like this. So how many entries do we need? We need at least one entry per branch. Although in fact we need on entry for each instruction but at least we need to have entries for branches. So we have a branch that does this here. We have a branch that does this here. We have a loop back branch here and we have a loop back branch here so we are talking about at least four entries. In reality we need many many more. In order to do well on all four of these branches, we need to see what is the longest history that any one of them will have. The loop branch is always taken so, really, we don't need the history for it. Even a single 2-bit counter will do well on these two. But this one will be testing eight times, for zero through seven. And stay in the loop and then the ninth time it tests it jumps out. So really it has a pattern of length nine which means we need an 8 bit history. With an 8-bit history, we need two to the eighth two bit counters. So that leads to 256 2-bit counters. Now, what is the pattern of this branch here. It really is eight times it's not taken, followed by a taken, and that repeats. The inner branch does exactly the same thing, except it does it many more times. So, if you look at how many possible patterns we actually have, of length eight. We have a pattern of eight no takens. We have seven no takens followed by a taken. We have a pattern of six no taken, then taken no taken, ect. So we're really using only nine of these two big counters, which again, shows that. A predictor of this type with a very long history, will waste a lot of it's many, many entries. So now let's talk about how to reduce the waste, how to have fewer than 2 to the nth counters and yet have a long history. So, we will talk about the history-based predictor with shared counters. So again, the motivation for this is that with N-bit history, the way we've just seen it, we will need 2 to the nth counters per entry, if we want to do things the way we've just done them. And we also know that for repeating patterns, only about N counters out of 2 to the N are really used. In reality we are using about N plus 1, but basically the important thing is that we are using the number of counters that is proportional to the size of the history. But we need to have 2 to the nth counters, because we don't know which histories will be important. So the idea here is to share these two-bit counters between entries instead of dedicating 2 to the nth two bit counters to each entry. We will have a pool of two bit counters that entries will then use, but it is possible that different entries with different program counters and different histories end up using the same two bit counter. So there is a possibility of conflict, but if we have enough of these two bit counters, because each entry really doesn't need too many of them usually, we will have very few conflicts So this is how our history with shared counters would work. We have the program counter. We take some of the lower most bits, use that to index into what we will call PHT or Pattern History Table. This is a table that simply keeps the history bits alone for that branch. So if we have an 11-bit history, this table will have 11-bits per entry, so it doesn't have the 2-bit counters. But we do need the 2-bit counter to eventually tell us whether the branch is supposed to be taken or not taken. So what we do is we take this history from this table, combine it in some way, usually using an XOR with a bits of the PC. And use that to index into what we still will call the Branch History Table. This Branch History Table has entries. Each of these entries is just a single 2-bit counter. So each entry's very small. And that entry will tell us whether we should be predicting taken or not taken. When the branch outcome is known, we use the same history and PC combination to index back into this 2-bit counter, increment or decrement it, based on the current decision on a branch, and then we shift in that pattern also into the PHT entry, so that this history is ready for the next prediction on this particular branch. So the idea is that the program counter here tells us which branch we're talking about. We hope that the Pattern History Table has one history per branch that we have. Different histories and different pieces will result in different 2-bit counters being used, and thus, the decisions will be made for kind of like the combination of the history and the PC. Note that it is possible for another PC to map somewhere here, but that history, when XOR with that PC gives us, maybe, the same 2-bit counter. So it is possible to have some overlap here. But the idea is that this way, for each PC, we have a single history of all the possible histories that is, that is two to the Nth of them. We are really using only a very small number of counters because very few histories are actually happening, and that means that if you have a relatively large array of 2-bit counters the possibility of conflict is very low. But this allows us to have a lot of bits of history. For example, if we take 11-bits of PC, and we index into the PHT with them, we will need 2 to the 11th histories times, let's say 11-bits of history. We excerpt those 11-bits with these 11-bits. So we have 2 to the 11th 2-bit counters. So the overall cost of this predictor will be 2 to the 11th times 11 for the histories plus 2 to the 11th times 2 for the counters, which ends up costing us 26 kilobits. This is much less than what the cost would be if we had an array of 2-bit counters for each possible history in each entry over here. Pretty much, these 2 to the 11 times 2, will need to be in each entry here, if we didn't do the XOR like this. Note also that we don't have to have this numbers here and here equal, we can, for example, take something like 10-bits of the PC, to index into the PHT, which gives us only 2 to the 10th histories and have a 16-bit history, so that we can have 2 to the 16th entries here and so on. It still will keep the overall cost in the, you know, tens of kilobits, not megabits or gigabits that we would get if we tried to put together long histories with dedicating 2-bit counters for each possible history in each entry. Now let's see how well this works when we have something like a branch that is just always taken. In that case, the entry for that branch will just be all ones in the history. That branch has a fixed PC. It always has the same history. When we XOR them, we will be using only one 2-bit counter. So pretty much, although we have a long history, if the branch is just a repetition of the same thing, we are ending up using only one of the 2-bit counters. So the additional cost, relative to just having an array of 2-bit counters, is that now we are keeping an 11-bit history for this branch. So still the cost is not just 2-bits, it's really, let's say 11 or 16-bits plus 2, but it's much less than a long history plus 2 to the Nth times a 2-bit counter. If we have a branch that is never taken, we can do the same reasoning and end up using a single counter. If we have a branch that has something like NT, NT, it really only has two possible histories, zero one, zero one, zero one, etcetera or one zero, one zero, one zero, etcetera. So it ends up using two counters. So if we have a lot of these counters as you can see, most branches are going to be like these. We will be using very, very few entries by these branches, which leaves us plenty of counters for branches where we have a very long pattern. Where we will be using something like, you know, 16 histories and thus use something like 16 counters. As you can see in this organization pretty much, the branch is that need more counters get them; branches that need relatively few counters get them. The only really additional cost relative to just having the counters is the histories. The downside is that again, some branches with particular histories might overlap in which counter they use, with other branches and other histories. Pretty much, my branch XOR with its history, might map to the same counter as some other branch, and its own history, when they're XOR. But this happens very rarely if the BHT is large, and now it can be large because each entry is just 2-bits. What we have just seen is called a pshare predictor. It has a private history for each branch. That is, each branch should have its own history in the branch history table, and it has shared counters so that different histories and different branches, might map to the same counters. This predictor is GOOD FOR things like EVEN-ODD behavior like taken, non-taken, loops with few iterations, for example, those 8-ITERATION LOOP that we have seen and so on. So, pretty much whenever the branch's own previous behavior is predicted of its future behavior, pshare tends to do well. Another type of similar predictor that we can make Is called gshare, it has a global history and shared counters. So a global history means that there is only a single global history that we use to predict all branches. So what we have is now the PC a single history that we exert together and index into the array of two bit counters. This history, every branch decision, regardless of what the PC of the branch is, gets shifted in here. So what is gshare good for? It's good for so-called correlated branches. That is, branches whose decision is related to what the other branches in the program were doing. Now I would think, when would that happen? And it turns out that it happens in programs quite often. Think about something like this. If my shape is a square then do something. Then do something that you do either way, regardless of what the shape is. Then if sh, the shape is not square, I have something else to do. This branch and this branch are entirely correlated. If this one was taken, this one will not be taken and the other way around. So if this branch isn't the history, when this one is being predicted then we can perfectly predict this branch. Note that however, if we look at this branch individually or this branch individually, the history of this branch might be completely random because, it really depends on our data, whether shapes or squares. So if we randomly have shapes that are squares or not, we cannot really predict much here. So, this branch might be mispredicted often because we simply don't know at that point. But this branch becomes perfectly predictable if it's using the history produced by this branch. Now that we have seen Pshare and Gshare, let's do a quiz about them. Let's say that we have this fragment of C code, where I goes from 1,000 down to zero. And for odd number I's, we are going to add the I to N, otherwise we just don't do it. This code might translate into a similar program like this. This branch here is the test for exiting this loop, when r1 is zero. This is testing the least significant bit of r1. And if that bit is zero, that means we have an even number, so we jump to the even label. If the number is odd then we add it to R3 which in this case is representing n. Either way we decrement the R1, this is the I minus, minus and we unconditionally branch back to the loop. And the exit label for exiting the loop is here. If we want to get good accuracy on all of the branches in this code, then the history should be how much for Pshare, write the number here, and for Gshare, write the number here. So we're just talking about how many bits of history we need, to get good prediction accuracy for all of these branches. Let's have the solution for our PSHARE versus GSHARE quiz. The question is, for all of the branches in this code, how much history do we need to make them all very well predicted? We have 1, 2, 3 branches. This branch here is easily predictable even without any history. So for this one, any history will work. The exit branch. We do 1,000 iteration, each time it's not taken except the last time. So for this branch again we don't need much history. Even with no history using just a 2 bit counter we will predict this branch accurately 1000 times followed by one misprediction so it's a very good prediction rate still. This branch however, requires some history. This is the even-odd branch, so we really need to know whether the previous outcome was even or odd, so that we can do the opposite. For PShare, the even-odd handling can be done with just a history length of 1 and the history length of 1 will work well for these two as well. So we just need a history length of 1. For GShare, when we are doing this branch, we want its previous outcome to be part of the global history. If we look at what the global history looks like, this branch is not taken, so the global history will be 0. This branch in the very first iteration, because i.e is 1000, that's divisible by 2, is taken because the number is even. This branch is is then taken. Then we have this branch 0. Now it's going to be not taken here, followed by a taken here, and this pattern will repeat. So really, we need this to be part of the history when we're predicting this. We need this to be part of the history when we're predicting this and so on. In order to do that, we need the history to contain at least this much which is 3 bits. And similarly here we need this to be our history, so it catches this bit. Know that really, there are only 2 possible histories for this branch. One is the 1, 1, 0 and the other is 0, 1, 0 because the other two branches are just having the same outcome every time, so we need a 3-bit history. As you can see, GShare can do a similar job to PShare except that it needs a longer history. Even for branches that are actually kind of self determining. Note that GShare can also do those correlative branches that PShare cannot do. So now that we have seen that GShare can do some qualitative branches that PShare cannot do, but on the hand PShare can do things with shorter history that require GShare to have more of a history. The question is if we need to choose one, which one should we choose in our processor? Many earlier processors would choose either GShare or PShare, but very quickly people figured out. That you really want to have both of them. You want to have a GShare so that you can do correlated branches. And you want to do PShare because it does better figuring out those self similar branches, even if it has a shorter history. So that brings us to a so-called tournament predictor. The idea is that you have two predictors, one is better for some branches, the other is better for other branches. Now what you want to do is to get the best behavior for all types of branches. So what you want to do is, when you have these branches, use the first predictor. When you have these branches, use the second predictor. But you don't know which branches are going to benefit from which of your two predictors ahead of time. So what you really have, is your two predictors. Let's say that they're GShare and PShare you will use your PC of the branch to index into both of the predictors. You will take the decisions that they generate, and now you need to decide which one to use. How do you do that? And the idea of a tournament predictor is to use a so-called meta-predictor, which is just an array of 2-bit counters. You also use the PC to index there and the entry of the meta-predictor does not tell you a prediction for the branch. It tells you which of the two predictors is the more likely to give an accurate prediction for that branch? So what you do is you use this to choose among the two decisions. So pretty much this meta predictor gives you a way to select among the two decisions so that the final outcome is the decision, that the meta predictor thinks is a better prediction. You train the individual predictors just like before. On every branch outcome, you train each of the predictors. You train the meta predictor a little bit differently. Instead of counting up when taken and counting down when not-taken, the meta predictor is instead trained on how well these two are doing. If your two predictors are both correct, then your meta predictor doesn't change anything. If your first predictor, like in this case your GShare is correct, but the other one is not, then you count down. Counting down will result in selecting the GShare result more often. If your first predictor is wrong, but your second predictor is right, then you count up. Counting up will result in choosing this prediction more often. And finally, if both of them are wrong, then the meta predictor doesn't change because again there is no benefit in trying to select one over the other. So the idea of the meta predictor is that the prediction bit of the 2-bit counter is telling you really which of these to select. The [UNKNOWN] is there just in case GShare is overall the more accurate but sometimes PShare beats it or the other way around. And again each branch has its own meta-predictor entry. So, this type of decision and this type of selection is done dependant on branches, so pretty much for here for example loop exit branch it might be that the PShare works better. For your correlated branches, it might be that the GShare works better and they will have different meta predictor entries, so that they will use the friend predictors for their outcomes. Now let's see another type of predictor that combines prediction decisions which is called a hierarchical predictor. It is similar to the tournament predictor, but the tournament predictor typically is trying to combined two good predictors. One is good for some branches, the other for others. The hierarchical predictor is trying to combined a good predictor with an okay predictor. Here you're basically doing two good predictions for every branch. Just to use one of them. And each of the good predictors costs you a lot, for each entry. Here the idea is you that you have a very good predictor that costs you a lot for it's entry. But you don't want to be using the good predictor for branches that are very easy to predict anyway. So the idea is that for those branches, you want to use the okay predictor. And use the good predictor entries, which are very expensive, only for branches that really need that good predictor. So in the tournament predictor, you will update both predictors on each decision so that they can both stay current with what's going on. And thus, they both try to do their best on each branch decision. But in the hierarchical predictor, you might update the OK predictor on each decision. But you update the good predictor, only the OK predictor did not work well for that branch. The idea here is to not use the entries of the good predictor if the OK one is doing fine. On this branch so really what that means is we can have very expensive entries in the good predictor. And we have relatively few entries because most of the branches will be handled just fine with the OK predictor. If we try to compare the tournament predictor with the hierarchical predictor the hierarchical one usually wins. Because it turns out that there are many many branches. That can be predicted just fine using something like just a simple two bit counter. On the other hand, there are some branches that really require a very strong predictor to predict them. In a tournament predictor, each of the predictors is a balance between cost and accuracy. In this predictor, you can build a very expensive predictor with very few entries. And an OK predictor, it has a lot of branches. So those branches can handle like this, which leaves the rest of the resources for the good predictor. And that means it can have extremely long histories or be a very, very fancy predictor. You can even have a hierarchy or for that matter an tournament with more than, just two predictors. So let's look at a real example of a hierarchical predictor. We will look at Intel's Pentium M processor which has these predictors. First it has a cheap predictor that contains just two bit counters. Then it has a local history predictor and a global history predictor. So what the processor is doing is it has a large array of two bit counters, a local history predictor that stores the local history for each branch, and an array of two bit counters for different histories. And the global history predictor with the global history that is longer than the local history is here. And also, an array of counters for that predictor. So, if we try to predict a single branch, what we would do is, we would look up the PC for the branch in the two bit counter array. We would also try to look up the local predictor and the global predictor prediction. And then, we form the actual prediction for the processor by using the result of the global predictor if the global predictor says that the branch is predicted here. And for that, what we do is we keep another array here that is just saying whether this branch, so this is kind of a like a tag array that says whether this branch is going to be predicted by the global predictor. If the global predictor does not have a matching tag, that means that this branch was not inserted in the global predictor, in which case we shouldn't use it. If that happens, we will use the prediction from the local predictor, which will also have a dagger A that says whether this predictor should be predicted by the local predictor, and if so then we use that. Otherwise, if the branch is not in the local predictor, we just use the result of the two bit counter. When we update the predictor, however, what we do is we update the two bit counter. If we have the branch in the local predictor, we update the local predictor. If we have the branch in the global predictor, we update the global predictor. But then what happens is if we are using the two bit counter and the branch is not in the local predictor, then what we do is if the two bit counter is mispredicting the branch, we insert it into the local predictor. So now the branch will be present here next time. If the local predictor is not giving us good prediction for this branch, then we insert the branch into the global predictors. So what's going to happen is pretty much branches that are almost perfectly predictable by two bit counters, and that's a lot of branches. For example every, always taken branch or branch that is dominantly taken is going to be mostly predicted by the two bit predictor. So we never even consult the local and global predictors. And that save space in this predictors for other branches that need them. Saving space means that we can have fewer of the two bit entries and the history entries than we would otherwise need if we were trying to predict all the branches with the local or even the global predictor. So how do we say whether the branch is present or not? Well, what we really do is we insert some bits of the branches address into the corresponding line of the predictor. So the history, entry here we will index with some bits of the PC to find the history. And in that same entry we will also insert some of the upper bits of the PC, so that different branches that map to this entry only one of them gets to be predicted by this entry. The rest of them say that they're not found here. So the idea is that if most of the branches that end up mapping to this entry are really predictable by the two bit predictor, then we don't really need to use this entry for those branches. So let's see if we know how to use multiple predictors and combine their decisions. Suppose that we have a program in which a 2-bit predictor works just fine for 95% of our instructions. A PSHARE predictor works just fine for the same 95% of the instructions, plus it also works fine for 2%. That this 2-bit predictor doesn't cover. So, overall, it works fine for 97% of full instructions, but the 95% here and here overlap. A GSHARE predictor, again, works just fine for the same 95% that all the other predictors are just fine for. Plus it works fine for the other 3%. So overall, it's 98% accurate. 95 of those are the same as the 2-bit predictor. But the three that it covers in addition to that are not the same. But the three that it covers in addition to that are different from the 2% that the PSHARE predictor covers. So together, these three predictors should be able to correctly predict all instructions. So the question for you is how do we combine them. So the overall predictor that we will use is a, what kind of predictor. That chooses between a what kind of predictor and what kind of predictor. And this last predictor here, itself, will choose between what kind, and what kind of predictor. Instead of writing in the names of the predictors. Just select among the following. So for each of them say if it's A, B, C, D or E. A is the 2-bit predictor. B is the PSHARE predictor. C is the GSHARE predictor. D is the Tournament predictor. E is a Hierarchical predictor. And F, is the Return Address Stack. So again, for each of these five boxes, put which letter corresponds, to the predictor, that is the right answer for that box. Let's look at the solution to our multi-predictor quiz where we have a program for which the two bit predictor works for many instructions. PSHARE covers some additional instructions and GSHARE covers the rest that PSHARE doesn't cover. But all of them can correctly predict the same branches that the two-bit predictor can. So, because the two-bit predictor covers the most branches, and it's the cheapest predictor, what we want to do is use the two-bit predictor for most of the branches. So the correct answer here is E, Hierarchical predictor, that chooses between a two bit predictor and a predictor that combines the PSHARE and GSHARE. So, what this first part does is it basically makes most of the branches just go to the two bit predictor so that the space in the more expansive predictor here. Will be reserved for only the branches that actually need to go there, because the 2 bit predictor is not predicting them correctly. Because PSHARE and GSHARE have different strengths and neither of them is clearly superior to the other, we are not going to combine them using a hierarchy co-predictor, we are going to combine them using a tournament predictor. That will choose between PSHARE and GSHARE. So, the correct answer is that the overall predictor is a hierarchical predictor that chooses between a two bit predictor and a tournament predictor which itself chooses between PSHARE and GSHARE or the other way around. Now let's talk about a Return Address Stack predictor or RAS. We have seen that there are several types of branches that we need to predict. For conditional branches, we need to predict the direction. Is it taken or not? And by now, we have seen a number of predictors that predict direction reasonable well from these types of things. If this branch is taken, we will need to also predict the target address. And for that, a simple BTB that just remembers the previous target when the branch was taken will do just fine because we always have the same target. Another type of branches are unconditional jumps, function calls, et cetera. As far as their direction is concerned, it's trivial; they're always taken. So even the simplest predictor of the direction predictor type will do well on these. As far as the target prediction is concerned, most of these either jump to a label or call a specific function. So simple BTB just remembers the previous target when this instruction was executed will do just fine. So it looks like we have handled both the direction and the target for most of the common branch types. However, there is a type of branch, specifically the function returns, which is always taken, so the direction prediction will do just fine. But the target is often difficult to predict. If this function is always called from the same place, then the return will always jump back to the same location, and in that case, the BTB will do fine. But usually, we create the function so that it can be called from multiple in the program. Something like this, we have an instruction that can call this function; somewhere else in the program, we have another instruction that can call the same function. This function has a return instruction, and now the problem is that this return instruction should either jump here or here, depending on where it was called from. The BTB, in this case, will not do too well. Why? Well, because when we call this and return here, the BTB will remember that this is the target address. Then we reach this point, call again, return. We should be returning here, but the BTB is predicting this. So, we have a misprediction. The BTB now will learn that this is the correct target. Next, we come back to this code again, and again, the BTB is predicting here what we should be returning here and then again and again. So, it never predicts correctly. So the question is how can we predict returns accurately? And the answer is we will use a return-address stack, which is a separate predictor, dedicated to predicting function returns. The way it works is that we have a small stack in hardware, with a pointer. As we execute the program, when we have a function call, we will push the return address, in this case 123. 0 is the address of the call, so 1234 would be the return address. To the return address stack, and move the pointer. In the function, when we encounter the return instruction, we will pop from this return address stack and use the 1234 to return, and in that case, get the correct target address. Now, our pointer is here, we don't really have to delete this entry because the pointer being here means that it's free. Next, when we reach the next place where the function is called from, we will see the return address, in this case, 1254, on to the return address stack. The function executes when we see the return instruction. We pop the return address and we again predict correctly. So why is this a predictor? Why not just use the actual stack of the program? Well because this predictor needs to be on a chip very close to where the rest of the branch prediction is happening and needs to be very, very small. So unlike your traditional stack, where you would push something onto this stack. Call another function from there, call a nested function. And you can do that many many times until you run out of memory. In this particular case, we can have very small hardware structure so that it can make a prediction very quickly, like in one cycle. Thus it can only have a limited number of entries. So what happens when we exceed the size of the RAS? Let's say we have only four entries. Let's say that we call a function. Call another one. We call another one. We call another one. And now, the question is what do we do? So if our RAS is full, what do we do? There are really two choices. One, don't push anything. Pretty much preserve what we already have on the RAS, so that we don't overwrite anything. The other choice is, once we have filled these four entries, just wrap around and keep filling them if we call more functions. So, let's see in a quiz, which one is better when the RAS is full? The don't push approach or the wrap around approach? Lets look at the solution to a RAS full quiz. So the question was, is it better to not push anything anymore once the RAS is full, or simply wrap around and keep on writing all the addresses with new ones. It turns out that the wrap around approach is much better than the don't push approach. To see why, keep in mind that our program would look like this. We have the main function that executes for a while, then calls some other function. This results in pushing something onto the RAS. Our high-level function here will do a lot of work, occasionally calling other functions and so on. We push to see what happens here without running out of space. Let's say that our RAS only has two entries. The first entry will be consumed by pushing the return address when main calls do it. The second one, will be consumed when we call the function func from do it. At this point, we're calling the function dolesss, and it might call, for example, the add function, many, many times. So, we're getting to the place where functions are really small. And ideally we would, you know, push each diamond and pop when we return and so on. If we only have one entry, then really our entire rest will be used just when we call do it and as long as we stay and do it, which might be a very large function because it might be doing most of the actual work in the program, all of the function calls that we are doing really end up being mispredicted because we ran out of ram space and the one time that we will have a correct prediction, is when finally return from do it. If we have two entries, a similar thing will happen. We will consume the first entry by basically pushing the first thing. So pretty much we are kind of using an entry here just to save one missed prediction at the final return. Here again, we are using the second entry just to save the return from func, whenever that happens. And func itself could be calling a lot of functions. So the don't push approach basically spending entries on kind of long-term things that need to be remembered so that we can finally save a misprediction once we reached the return point from the large function. In contrast, the wrap around approach is going to push this, push this, push this thus overwriting this entry. So now what we are doing, is we are basically going to be correctly predicting calls to small functions and returns, and we can have many of those, in return for inaccurately predicting the final returns from large functions, because we have many more small function calls than large function calls. In the end, we are more effectively using the few entries we have. Pretty much here, we are kind of using the entry to save a misprediction over a very short period of time does this entry get utilized, and then, it relieves very quickly. Whereas here we're using an entry for a long time to save a single misprediction, and in the end we want to save as many mispredictions as we can. So that's why the wrap around approach works better because it ends up correctly predicting causal small functions, many of which we can have in return for not correctly predicting the return from large functions to main, et cetera. Another thing to keep in mind with the RAS is that it is a predictor, so either way we will be having some mispredictions. We are allowed to have mispredictions. All that will happen is that there will be a branch misprediction. Basically we will fetch the wrong instructions and have to recover from that. So, in the end, the discussion about what to do with the RAS, has to do with achieving a larger number of correct predictions. But neither of these two ways can actually do things perfectly. So, in the end we need to just choose which one gives us more correct predictions, and it turns out that the wrap around approach predicts better than the don't push approach. While both of them, result in mispredictions, anyway. The last thing is but, but, how do we know it's the right instruction? Remember we need to make our prediction while we're fetching the instruction. So if this is a return instruction, we need to use the return address stack, before we even determine that it is a return instruction. We cannot just keep pushing and popping things from the return address stack, before we know it's a return. Because, for example, if there is an ad and we pop from the return address stack, then it's not going to work correctly. We actually need to figure out, or at least very accurately predict, what's our return instruction so that we can use the RATs appropriately. So the problem is that we are using the RAT while fetching the instruction. It's not been decoded yet, so we don't know it's a return instruction yet. So how do we do it? One way is to just use a very simple predictor that will be trained on whether the instruction we're fetching is the right or not. Simply, trying to predict or one way or the other depending on whether it's a rat or not and then make that predictor tell us whether to use a [UNKNOWN] or not. This would be a very likely accurate predictor. If, at the particular PC, we have seen a RET instruction previously then it is very likely that if we see the same PC we still have a return instruction. So we can use a single bit predictor very easily here. Another approach, is to use a so-called predecoding. We will soon see that the processor really contains a cache that stores instructions that have been fetched from memory. And its fetching instructions from the cache, and only if the cache doesn't already contain the instruction it goes from memory. So the predecoding works by, when I'm fetching from memory, I decode enough of the instruction to know that it's a return or not, and I store that information along with the actual instruction in the cache. So pretty much my memory will, for example, have 32-bit instructions. In my cache I will store 33 bits per instruction, 32 bits of the actual instruction. And 1 bit that tells me whether the instruction is a return or not. So the idea is that as the instruction is coming from memory, and we put it to cache. We can pre-decoding instructions, keeping some additional bits in the cache about the pre-decoding of the instruction. So that when we fetch, we already know that information. The alternative to this would have been to just fetch the instructions from memory to cache as they are, and then every time we fetch we have to figure out what this instruction is. Because it's also more power-efficient to do this once and then fetch instruction many times rather than redo it every single time, pre-decoding is a very popular approach. So pre-decoding is used for things like, Is it a return instruction? Is it a branch instruction at all? If we know it's not a branch, than we can completely omit the use of any branch vectors, thus saving a lot of power. If our instructions are of variable sizes, predicoding can also tell us, for example, how many bytes this instruction is long so that we can fetch the next instruction quickly and not. Rely on just the code in the instruction before we can fetch the next one, and so on. So there is a lot of things that modern processors do in this pre-decoding phase. In order to avoid doing it when we are on the clock, after we have fetched the instruction. We now know how the processor is predicting which instruction to fetch next. You will put this knowledge to work in a project in this class. But some branches are very difficult to predict. So next lesson is about helping the compiler completely eliminate these branches from the program so we do not have to predict them. In this lesson we will be discussing the basics of cache coherence. Coherence is needed to ensure that when one core writes to its cache, other cores get see it when they read it out of their own caches. Let us first see what the problem is with cache coherence, that is do we even need cache coherence. Our programmer is expecting to see shared memory behavior. That means that when one core writes, let's say, 15 to a variable. And then another core reads that variable. It will read what the last write put there. This is how cores communicate through shared memory. One writes something to a shared memory address. Another one reads it and expects to see the data that was written there. But in our hardware, each core has its own cache. And we need cores to have their own caches at least into level one because a single large level one cache would be too slow and would not have enough throughput to keep up with the requests from all of the cores. So because we cannot do this, we have private or per-core L1 caches. So each core has it's own. However, once you have this private caches, here is what can happen. This is the core A with the level 1 cache for core A. This is core B and it has its own level 1 cache. So we said that let's say core A writes x equals 15. Suppose this a cache miss, we use the address effects to index into our cache. Check the tag whether it corresponds to x. And then, if this is a cache miss, we go to our main memory and fetch the whole block into our cache. Let's say that in this block x is 0. Once we fetch it into our cache, we'll write our 15 here. So now we have a 15 in our cache here. Now let's say that core B wants to read x. So it wants to see what x is. It uses the address of x to index into its own cache. Sees that it doesn't have what it's looking for there. And reads the block from main memory. This block here has 0 for X still because we didn't write back from A's cache yet. So, we end up reading 0, which is not correct. And know that this doesn't occur only the first time when we have a cache miss. >From now on, A can write to its own L1 cache many, many times. B can read from its own cache many, many times. A will keep writing to its cache and not updating memory. B will keep reading from its cache and not checking back from memory because once we bring it into the cache, it's going to be a hit. B is never modifying it. It's always going to read 0 no matter how many times A writes 15 or something else here. So obviously we are not getting the communication of data we want from A's writes to B's reads. And thus our shared memory doesn't work. When we get in a shared memory system behavior like this we say that it is incoherent. The same memory location as seen from different cores can have different values. Which is not what should be happening in a shared memory. In order to not get a incoherent system, we need an active approach to cache coherence that will make this whole thing behave as a single memory. Even though in reality there are multiple copies of the data floating around. Let us see if we can understand how caches become incoherent. Suppose that we have a memory location A that initially contains a zero value. In this system we have three cores. Each core has it's own write-back L1 cache. And these write-back caches do not have any coherent support. This means that this write-back L1 cache is simply behave as if each of these caches was just alone in a uni-processor. So they behave correctly for a uni-processor cache, but they do not try to maintain coherence. And this is what the three cores do. Core 0 loads from A into a register, increments the value in the register, and then stores the register back into memory location A. Sometime later, core one does pretty much the same thing. It loads A into a register, increments, and writes the register back to A. And finally, core two does the same thing. Note that these are happening one after the other, so without cashes, core zero would be loading zero, writing a one, read one on core 1, put a two back in A, and then here we read two and put a three. But with the three L1 caches and no coherent support, the question for you is the value that core A is writing to A here can be, what? Can it be 0, can it be 1, can it be 2, can it be 3, can it be 4 and can it be something larger than 4? Put a check next to all of the answers that might be true for this right here. Let's look at the solution to our cache incoherence quiz. We had three cache and three cores. Allocation that starts at zero and then each core reads, increments, and then writes to that memory location. So the question is, what's the final value we write here? The correct behavior would be for core zdero to do this, and then it replaces this from its cache so that core one reads A1 here. Increments that writes A2, and then core two reads that, increments, and writes A3. So this three here is a possible answer if core zero happens to replace this block that contains A from it's cache before core one tries to read it and then core one replaces before core two reads it and so on. In that case we get actually A three here, which is the correct answer. So let's see now if we can get two. Well, if core zero does a replacement, so the memory location after this actually contains A one. Then core one does this but doesn't replace the block from the cache. Then core two will still see a one. It will not see what core one produced. Because core one keeps it in it's own cache. So core two here reads one increments it and then core two ends up writing a two so this is a possible answer and then we can get a one here by, basically, core zero delaying the replacement, and then core one can fetch a zero as well. Put a one there, but not replace until later and that means that when core two reads this, it gets a zero and then puts a one here so this is also possible. It is not possible for core two to write a zero because what it reads it will increment and write. So for core two to produce a zero here, it would need to read minus one. Note that each of these cores is still behaving as a correct unit processor so zero is not possible. Similarly, four is not possible because for four to be produced here we need to read a three and there is just no way that two increments from zero can produce a three here and similarly more than four is not possible. So the correct answers here are one, two, and three are possible at this point. So let's down define what does it mean for caches to be coherent. Intuitively, they're coherent when the whole thing behaves like there was no caches. But let's define more precisely what exactly does it mean. There are three requirements for coherence. The first one says that the read, let's call that read R, from some address X on some core C1 will return the value written by the most recent write, let's call that write W, to the same location, and that write was on the same core C1, if no other core has written to that location between the accesses w and r. So this first part of the coherence definition really says that if one core is operating on a location alone, then its reads should be getting the most recent writes from the same core. This basically says that a coherent behavior includes correct unit processor behavior as long as the location is accessed by only one core. The second part of the definition says that, if core C1 writes to our location X, and C2 reads X after a sufficient time, and if there are no other writes that happen in between C1's write and C2's read, then C2's read must return the value C1 wrote. So pretty much, if there is a write and the read happens sufficiently after that write, then the read has to read what the write wrote, except if there was another write in between, in which case the read is allowed to return that value instead of this one. So this part of the definition really prevents the case when, for example, C1 writes something to its own cache, C2 reads forever from its cache a stale value. To be coherent, if C1 writes, then after some sufficient time, C2 must start reading the new value, even if maybe a normal cache with no coherent support would just keep reading the stale value because it never gets replaced. So we have to do something in order to make this part of the finishing work for private caches, that the caching data in a coherent system. So we must do something to make this caches behave this way. And finally, the third part of the definition says that if there is more then one write to the same location, then these writes must be serialized. That is any two writes to the same location must be seen to occur in the same order on all the cores. What this part of the definition really says different cores should not be able to disagree about which write occurred first and which occurred second. Note that second part of the definition doesn't really require this to happen. Let's say there was one write on C1 and another write on C3. When C2 reads, it's allowed to return after a sufficient time either C1's or C3's value, because it's allowed to interpret this order as either C1 and then C3 wrote, or the other way around. What this part of the definition says is that whatever C2 sees, if C2, for example, sees that final value is that from C1 and not from C3, then all the other cores, when they're in their location after C2, need to also see that order. So, it cannot happen that C2 thinks C1 wrote last. Some other core, sometime later, reads and still sees C3's value. So, what this is really saying is that everybody, including the writing cores, will see the value that is consistent with there being some particular order to the writes. It doesn't require any particular order. For example, the writes can happen almost simultaneously, in which case, we are free to choose which happened first and which happened second. But we cannot allow some cores to think that one write was first and other cores think that another write was first. So let's see if we understood the coherence definition. Let's say that we have two cores, in a system that is supposed to be coherent. Let's say that core 1 executes this program. It sets A = 1, then waits for A to change from 1, so it's going to loop here as long as A == 1. Sets A again to 1, and then prints done one. At the same time when core one is started on this, core two starts on another program that writes zero to the same location A in shared memory, waits until a changes from zero, sets A to zero again, then prints done two. And remember that core one and core two, need not necessarily execute these two programs in lockstep. So it's not necessary that core one executes this while core two does this, core one does this while core two this and so on. It could be for example that core one executes several of these while core two is still not even beginning. So you can insert arbitrary delays in between any of the statements here and here. The question for you is which of the following print outs are possible? First, core one prints Done 1!, then core two prints Done 2!. The next possibility, first core two prints Done 2!, then core one prints Done 1! The next option, core one prints Done 1!, but core two gets stuck and never gets to print Done 2!. The next option is core one gets stuck and never gets to print Done 1!, but core two successfully prints Done 2!. And the final option, is that nothing is printed because both of the cores gets stuck, so neither of them prints anything ever. So which of these five options are actually possible when these two programs run in a coherent system? Let's look at the solution to our coherence definition quiz where we had two cores running in a coherence system executing some code, and the question was basically which of these five possibilities for what gets printed are possible? Let's see what can happen in these two programs. If core 1 gets to go first with A = 1, it's then going to spin here for as long A = 1. So, it gets stuck until core 2 does something. Eventually however, core 2 will do A =0 in which case, Core 1 gets to continue and now core 1 will set A to 1. Core 2 will wait here until A is set to 1, because as long as it's set to 0, the way it set it, it's going to get stuck here. So what then happens is Core 1 when it sets this releases core 2. Now core 2 can do this and this first, while core 1 is still being delayed here or it can be that core 1 gets here first and then core 2 gets here. So definitely these two outcomes are possible. They basically depend on after core 1 waits for core 2 to release it and then it itself releases core 2 who gets to print first. Now let's see if it's possible for core 1 to never get to this point. For that to happen core 1 needs to get stuck here in this while loop which means that A needs to stay 1 forever. That can only occur if core 2 is done with its program, and the resulting value of A is still one. Or if Core 2 also gets stuck here, and the resulting value of A is still one. So, Core 1 is stuck here, and A is one. If that happens before Core 2 gets to A equals zeron, then Core 2 is going to release Core 1. So A = 0 had to happen before this. If A equals zero happened before all of this, then what happens is A equals zero, A equals 1. Now we are stuck waiting for 1, but core 2 will continue and it has to see the one, because this is a coherent system. Eventually, because A equals one happened after A equals zero, core 2 has to see the one and proceed. Once it proceeds, it's going to set A to zero, core 1 has to see that, because of the second part of the definition, and proceed. So it's not possible to get core 1 start here by just moving the A equals zero to the beginning. Now let's see if it's possible for CORE 2 to finish the whole thing before this happens, in which case CORE 1 will get stuck here. But that's not possible because CORE 2 now needs to execute this whole program here before CORE 1 even begins. That's not possible, because core 2 sets A to zero, and then waits for it to become nonzero. So core 2 cannot finish the whole thing until core 1 does at least this, at which point we are back to the first case of pretty much core 1 will eventually see that the A 0 because core 2 set it that way after A became one. It's also important to notice it's not possible for example A equals one and A equals two to happen one after the other and then core 1 thinks that this was the last write. So it gets stuck here while Core 2 thinks this was the last right and then gets stuck here. Because the third part of the coherence definitions says that both of them have to think that the same access was the last one. So if they agree that this was the last one then this core gets released and then ends up releasing the first one. If they think this was the last access, then this core gets to be released, and releases the second one, so either way, it's not possible to get any of the three printouts. In an incoherent system, it can easily happen that you get these or any of these other three? So for example, nothing can be printed then by simply storing 1 to the cache and then never really checking anything else, just looping on the value in the cache here. Meanwhile, this one stores another value in its own cache, then just infinitely spins on it and then you can get the other two possibilities here by simply one of them not being entirely incoherent. So, as you can see, a coherent system prevents some of the behaviors that are possible in an incoherent system. But all the behaviors that are possible in the coherent system are also possible in an incoherent one. So coherence is allowed in an incoherent system. It's just that incoherent systems don't guarantee that all their behaviors are going to be coherent. Okay, so we have seen that we want coherence, and now the question is how do we get it? So how do we make caches become coherent? The first option is don't do caching, but this is really bad for performance, so we will not seriously consider this. But if there are no caches, then all of the accesses go to memory, and then writes to that memory will be seen by the reads to that memory. And also, the order in which the memory receives the writes is the order in which the writes will be seen by all the cores, so the system will be coherent. The second option is if all cores share the same level one cache, then the same argument for coherence applies. The order of writes is the order seen by this level one cache. And all of the accesses are going to go to the same cache where they see the same version of the block. So the system will be coherent, but will still have really bad performance, better than this, but not by much. Now we can try do do things like having private caches, but make them write through. In that case, the memory's actually seeing all the writes, but the problem is that these caches are still allowed to cache data that has only been read forever. So once we read the value, it doesn't matter that all the other cores are writing their values through to memory, because the cache that only reads will potentially see forever the same failed value even after many, many writes have occurred, and for a long time. So this is really not a way to get coherence. Even with write-through caches, private caches are not necessarily coherent. What we must do really, is to force reads made to one cache to see the writes made in another. So we cannot allow caches to simply provide a same value over and over once it's a cache hit. Somehow, once a write occurs in one cache, another cache needs to eventually start returning that value to its local processor. So how do we do that? One way is for every write to be sent to all the other caches that might potentially have this block, and they need to update their value. So pretty much I write to my cache. It's kind of like a write-through cache in that it continues past the cache. But now what really happens to it is not only does it get rid of backup memory, it also gets sent and seen by all the other caches, which now take that new value. If that happens, then when I write, other caches now have my new value, and then reads on those scores will actually provide the same value. This is called write-update coherence, because writes updates all of the other cache copies. The other approach for maintaining coherence is for writes to prevent other copies of the block from having hits. So after I write one cache, all of the caches that had a copy of this block need to behave like they're having a miss, or they need to behave like the block is not valid in that cache. So this is called write-invalidate. A write will invalidate all the other copies. Note that these are just the two general approaches. We will be looking at several write-invalidated coherence protocols, and also we will be considering a write-update protocol. So this ensures, really, that reads see the values produced by writes. So the coherence property two is maintained by either this or this. So we need to pick one of these two approaches to maintain coherence property two. For coherence property three, we need to decide what's the order of writes and all the cores need to see the same order. One way to enforce the same order of writes being seen by all the cores is that all writes are broadcast on the shared bus, and then the order in which they appear in this shared bus is the order that should be seen by everybody. Coherence done in this way is called snooping, because when writes are broadcast on the bus, everybody snoops those writes in order to tell what order they occurred on, and also to update or invalidate their values in the cashes. Of course, the shared bus can now become a bottleneck. The other approach is that each block is assigned an ordering point, and different ordering points can be used for different blocks. So that for each block, all accesses are ordered by the same entity. But different blocks use different entities, so they don't have contention. This ordering point is called Directory when doing coherence. So this family of approaches is called a Directory Based Coherence. And this directory both serves as an ordering point for a block, and also figures out who to update or invalidate. When multiple caches have a version of the same block, an overwrite occurs. So among these two approaches, we can also pick one. And indeed all four possibilities, this and this, this and this, this and this, and this and that, are all possible. So you can have write-update with snooping, write-update with directory, write-invalidate with snooping, write-invalidate with directory. So let's look at the write-update snooping coherence first, let's look at two caches only, each of which has two blocks, with a single valid bit per block in the tag, and then the data, and both of these caches are connected to the same bus, to which we also connect the memory. So now let's say that the caches are initially empty, so the value bits are all zero, and this processor reads something in block A. We have a cache miss here, we send the request to memory. This cache is always monitoring what's going on, on the bus, but it's only interested in writes. So this read is really going to be seen and ignored by this cache. The data for A comes back from memory and now we put this block in the cache, and the block becomes valid and the title message just indicates that's this is the block A. Now let's say that this processor writes to A. So now we have a situation, where this processor, even if it does a write through access directly to memory, a read on this processor would still read the stale value of A. Let's say that the value we read here was 0. Now let's say that this processor writes 1 to A. This write, even if we made it go all the way to memory, would not result in coherent behavior for this cache, because a subsequent read here will still hit in the cache and get the 0. So this is when write update and snooping come into the picture. We have a cache miss, but because it's a write we send a request for the block along with an indication that it's a write, and what is the value and what's the address. So now what happens is this request goes to memory, and the memory writes 1 to A. But more importantly this cache is monitoring the bus and sees that block A is being updated. It checks its internal state and sees that it has block A, and then it takes the value that is being written. So this part is the snooping part where we see what's going on on the bus. So we cannot miss any of the writes happening on another core, because we are monitoring the bus, and all writes have to go through the bus. That's the snooping part. The right update part is that when we see a write, we update our copy of the data with the value that is being written. So this cache now gets a 1 here, if we read A after this, we're going to get this one, so this ensures correct behavior. If there are multiple cores, they can not disagree about which right happened first, which one happened last because all of the rights are sent through the bus and because it's a single brotecuz bus, it happens one at a time. So all of them see rights, in exactly the same order, in the order in which writes go to the bus. Now let's see what happens if both of these processors tried to write to A at about the same time. So this is a situation where they might disagree about whose write happened first. Let's say that this one is trying to write 2 and this one is trying to write 3. Well there is a single bus, so there is a single set of wires that can carry the value and the address and so on. So processors, when they want to send something on the bus, first have to arbitrate for it, and this arbitration for the bus basically decides who gets the bus next. If this core won, then it sends its write first, now both of the caches will have two. This one because this is where the write happened. This one because this is what we pick up from snooping, and only then, after this is complete, this right here gets to occur, so now we go to 3, broadcast that here, and it gets here. So as you can see, both of them agree which write was first and which was second. Obviously, the one that wrote 3 was the second one, because that's the last value we kept. So the write update is ensuring that writes are seen by reads on all cores, and the snooping of the common bus and the serialization on that bus, basically going one at a time to the bus is guaranteeing that everybody sees the same order of writes. Let's take a moment to check if we understand how write-update coherence works. Suppose that a core has a single block cache with a valid bit, a tag and the data, and suppose that we have four such cores numbered 0, 1, 2 and 3. Suppose that core 0 reads from 700 first. So now it gets the valid bit to be 1. The tag is 700 and let's say that the data rate is 6. After this, core 1 reads the same location. Then, core 2 writes a value of 17 into this location. And finally, core 3 reads from that location. After all this, in core 0 what is the valid bit in the cache? The tag is still 700, and what is the data that we get? And the same for cores 1, 2, and 3. What is the valid bit in each of these caches, and what's the data value placed here? You can use this as an example, where the valid bit is either one or zero, and the data value in this case is 6 after this read. So, what's going on in these four caches after we do this sequence? We complete this, then this, then this, then this. Let's look at the solution to our write-update coherence quiz. Where what we had is four accesses like this happening in a write-update coherent set of caches. After the first read completes here, we have data 6, tag 700 hex, and the valid bit is one. So when this read completes, we have a valid with 1 here, the data is 6 and the tag is 700. At that time, in these other caches, the valid bit is simply zero, because they have not accessed anything yet. When core 1 reads, it sets the valid bit and fetches the same data. When core 2 writes, it broadcasts the value in the address, so these caches that already have this block will update what they have now to 17. And now, of course, on core 2, this block starts being valid and has a value of 17. And finally, when core 3 reads, it also gets the value of 17. So, this is the final state of the caches. All caches have the block and all caches have the same value of the block. Which means that really the caches are coherent for this block. Now we will look at two possible optimizations of a write-update protocol. The first optimization we'll look at is about avoiding memory writes. Remember that in a write-update protocol, every write in any of the processors,needs to be broadcast on the BUS and needs to update the memory. Because memory is big and slow, it cannot keep up with all the writes, and that means that the memory throughput becomes a bottleneck for the system. Pretty much it's not about how much traffic can the BUS take, it's about how much traffic can the memory take. So we want to avoid unnecessary memory writes. These caches previously behaved as write-through caches. Every write was going through the cache to memory. And we have seen already that write-through is bad for memory traffic even with a single core. Now we are adding more than one core, so it's going to be even a bigger problem. So the solution to reducing memory traffic here will be the same. We will add a dirty bit to each block in each of the caches, and this dirty bit will allow us to delay a memory write until we actually replace a dirty block from our cache. So now let us see how this work and how it avoids memory writes. Suppose that this cache reads A. It gets this value from memory, let's say it's 6. The tag will be A. It's valid and it's not dirty. Now this cache writes A. At this point, it will broadcast the write with the address of A and the value that it writes, let's say 17. So this cache here would update the value. Now without the dirty bit, the memory would also be updated with A = 17. We want to avoid that. Why? Well, because there could be a lot of writes here until we finally replace it in a write by cache. So what we will do is we will write 17, tag it this way, and mark this as dirty. Dirty means two things here. One is that the memory is not up to date and we need to update it. This is exactly the same as in a normal uniprocessor write-back cache. If we replace something dirty, we have to send it right back to memory. In our write-update protocol, dirty has another meaning that we will now illustrate. Suppose that this cache replaces this block and then wants to read it again. So it issues a request to read on the BUS. And now normally the memory would respond with a value, but the memory still has the old value of A = 6. This cache here has the only up-to-date copy. That's what the dirty bit also stands for. So now this cache needs to snoop reads as well. Notice that we are reading the same block that it has in a dirty state. So instead of the memory being allowed to respond, we respond first. The cache is faster than the memory, so it will respond quicker, and say that A is 17. So now here we get A becomes 17 in this cache and it's not dirty. Now after this, let's say that we write a new value here of let's say 20. We put the new value here. We broadcast the new value on the BUS. The snooping here gets that 20, but the memory again doesn't get updated. If this block, however, gets replace from this cache, then an update is sent to memory. Another interesting situation here is what happens if we have the block dirty in this cache, and now this cache writes 30 to A. In that case, this cache broadcasts the write. This cache snoops the right, puts 30 in its cache. Now this cache is no longer responsible for writing back to memory, because the new writer takes that responsibility and the memory still doesn't get updated. So as you can see, we can now do many writes without updating the memory. We can even move the responsibility for updating memory to another cache and keep writing there, without the updates to memory. Only when the last writer replaces the block that is still dirty will we have a single write to memory. So the benefits of having the dirty bit are that we can now write to the memory only when the block is replaced. So we can do many writes without updating the memory, thus saving a lot of the writes to memory. And also we avoid memory reads because now we allow reads from memory only when nobody has the block in a dirty state. If the block is dirty, for example here, then whoever asks for this block, this is the cache that will provide the value and the memory doesn't need to do that. So we are also saving reads from memory this way. And now let's look at the second possible write-update optimization, which is about reusing the number of bus writes. So after we added our dirty bit, what we got is that the memory now gets relatively few writes. Only those writes that are coming from replacements and also the memory no longer needs to respond to all the reads, but the bus still gets all of the traffic for all of the writes. Every single write will need to go on the bus and broadcast the value. So now the bus, which sees all of the writes will become the bottleneck in the system, because every single write from every single core ends up going to the bus and the bus can only take so much. The bus broadcasts for writes that are needed to update other copies of the data, need to happen, because this is a write-update protocol. It needs to update other copies of the data, but the optimization we can do is about the writes that do not need to update anybody. So we will add another bit called the shared bit to each block in each cache and that bit will tell us, whether the block is shared with others or not. Let's see how things would work. Suppose we read A here. We get this valid not dirty, but for the shared bit, we need to know whether others also have this block or not. For this purpose, we add a line to the bus. When our read goes to memory, other's snoop this read and if they have the same block in the cache, then they pull the shared line to one. If nobody pulls the shared line to one, then it will remain at zero. So for this particular block, because this is the first read, nobody pulls the shared line to one, so the block we know is not shared. This is the tag and let's say, we're at six. Now let's see what happens when this processor writes to A, and let's say puts a value of 17 there. It's a miss, we wrote us both the write and the value. The first time and we see now that somebody is writing and this cache here, snoops that write and does two things. One, it now knows that the block is shared, because somebody is accessing it now. Second, it updates the value. Third, it pulls this line to 1, so that this cache knows the block is shared and now it has the block in the dirty state with a value of 17. So far, this has behaved exactly as it should without the shared line and without the shared bit. So what's the benefit of the shared bits? Well, the benefit is that now this cache, if it wants to write again, it sees that the shared bit is one, which means that it does need to broadcast same as before. So the broadcast will happen exactly as before, if there is another sharer, because we need to update that sharer to the new value for the data. The benefit of the shared bit is in that, if this block reads, let's say, B and gets a value of two here and it's valid and it's not dirty. And it sees that so far nobody has B, so sharing is zero here. And then once you write five to B, it checks the shared bit here. And because we know that nobody else has this block, we can write the value here. Make the thing dirty, because the memory's no longer up to date, but because the shared bit stays at zero. Meaning, we are the only one who has it. We don't do a bus right here. So as long as some block is being accessed only by one core, it's no longer going to broadcast all of the writes. Meanwhile, this core could be having in a non-shared state, for example, block C and it can write anything it wants there. Can write 17, then 65, etc. So B and C, because they belong to only one core at this time will not do bus writes, while A will do bus writes. So we still get the write optic behavior when there is sharing, but now we avoid writes going to the bus when there is no sharing. This can happen a lot, for example, B could be something that belongs to the stack of core zero. C could be something that is on the stack of core one. So naturally, we will be just pushing and popping things to our own stack and never really needing to broadcast the updates somewhere else. But for data that is actually shared, we still get exactly the same write update behavior as before. So this optimization will save us a lot of writes on the bus. It will save us all the writes that are not necessary to maintain coherence among multiple copies of the same block in different caches. Let's quickly check if we understood the write-update optimizations. Suppose that we have a system with two cores that uses write-update coherence. Suppose that the two cores are executing a program where core 0 writes something to a memory address A, then core 1 reads it and this continues so that the write and then a read pattern repeats, for a total of 1,000 times. So this is the first of 1,000. After they do this for 1,000 times, these cores go to do something else, where core 0 replaces A and then core 1 also replaces A. The question for you is, how many bus uses do we see when the system uses no optimizations? So there is no dirty bit and no shared bit. When the system uses only the dirty bit optimization, and when the system uses both the dirty and the shared bit optimization. So for each of these, how many bus uses do we get during this whole time for block A? And also, how many memory writes do we get in each of these three cases? Let's look at the solution to our write-update optimization quiz where we have two cores with write-update coherence and we consider the case of no optimization, dirty-bit only optimization, and both dirty-bit and shared-bit optimization. And for this program, how many bus uses and memory writes do we get? So, let's consider the case of no optimization first. Let us first consider the no optimization case. This right here we'll use the bus and we'll write to memory. This read here, the first one we'll use the bus but we'll not update the memory because this is a read miss. >From then on, each write on core 0 will use the bus to broadcast and update memory, so we get 999 more accesses to each because of this, whereas reads are no longer going to be misses, and they find what they are looking for in the cache, so they're no longer going to either bus or memory. Eventually this cache is replace this. But nothing happens on the replacement in this case because everything is already up to date. So overall here we got 1001 bus use. And 1000 memory writes. Now let us look at the dirty-bit only optimization. In this optimization, the writes on core 0 will all be happening to core 0s cache and the only write to memory will be when the block is replaced. So we get only one memory write. How many bus uses do we need? While the first write and every subsequent write will be broadcast. So we get 1000 bus accesses because of these writes. We still have the one access because the first read is a miss after that they're all hits and don't go to the bus. And now we have an additional bus access because the replacement of A here needs to send the data to memory for which we need a bus access. So overall, we reduce dramatically the number of memory writes in exchange for one more bus use. Now let's see what happens with a dirty and shared optimization. We're right here and at this point we are the only one who has this blocked so the shared bit is not set. This would normally allow us to write to it without going to the bus anymore. But because the very next access is a read of a on core 1. The sharing starts and that means that really we have exactly the same number of bus uses and memory writes. Remember that the sharing optimization only works when we are accessing a block only on one core. This block is actually being read on one core, written on another, so all the writes on this core need to be propagated to the bus so that this core's cache can be updated. So really here the shared bit optimization doesn't result in any savings. We said that there are two types of snooping coherence approaches. One was write update, the other was write invalidate. So going back to our optimized write update setup where we had the valid and the tag bits, also the dirty bit for write back cashes and a shared bit that tells us whether we are alone in this. A write invalidate protocol will broadcast writes on the Bus so that they can be seen by everybody. But instead of updating their copies, they will invalidate them. So let's see how this works. Let's say this core reads A. We send this, get the value from memory. Now we have it as valid, not dirty. The tag is A, the value is, let's say 0, and it's not shared. Let's say now we have the write to A, and write 1 to it. We have a cache miss, we broadcast the write. We now don't need to broadcast the value with it because the other caches don't need this value. What's going to happen is as we send this to the Bus, it's going to be snooped by this cache and it sees that somebody's trying to write to the block that they have in my cache. In our write update protocol we will now need the value to put it here. In a write invalidate protocol all we do is change the valid bit to 0. So now we will miss in the cache next time we read, this particular cache now gets the valid. It's not shared, every time we do a write with broadcast we know it's not shared because all the other copies have been destroyed so our write makes., The block non shared in the writing processor. It is dirty and now it has a value of one. So now a read of A here, in a write update protocol would be a hit and just read here, but in a write invalidate protocol, the read will still get the new value. It's not going to be incoherent, because it doesn't get the value of zero. It will not be a miss, and be sent to the Bus. Now the memory has bot been updated yet, but the processor who has the dirty data and matches the address of the blog will see that there is a read request on the Bus and respond with the data, so now the blog becomes valid. Somebody gave us the data, so definitely now it's shared. We get the data, and the cache where we wrote now also know's it's shared because we gave it to somebody. So, clearly we are not the only one who has it anymore. So now note that just like in a write update protocol, if we have a lot of reads now they can have cache hits. Another write will invalidate this so there will be a cache miss also. So, the disadvantage of writing invalidate protocols is that there is a miss on all the readers when somebody writes. However, there is an advantage to it as well in that if you need to update the same block twice, let's say write two to A. We invalidate this copy by broadcasting. Now the block is no longer shared. We put two there. So we have the broadcast for this first write. If we now do some more reads and writes, they can all be done locally. We don't have to broadcast them. So basically by invalidating this copy, we made ourselves be the only one who has the block and now we don't need to broadcast all the writes. So, for anything where one core works on it for a while, then another one works on it for a while, if the work involves writing, only the first write really has to be broadcast. After that, it's a local access for both reads and writes. In contrast with write update, we would have to keep updating the other cache until it replaces the block. Just like with write update. Writing validate can benefit from the type of optimization when we are writing to different blocks and accessing it later. So in that case B is fetched here is not shared becomes dirty and C is fetched here is not shared becomes dirty and gets a value of 1. And now we can have cache hits on both reads and writes without going to the Bus for B here. And we can access C here with no going to the Bus. So for blocks that are only accessed on one core we have the same behavior for both write update and write invalidate. But for blocks that are shared we get different behavior. Write update tends to result in more hits but does more broadcasts, if we repeatedly update. Write invalidate generates a miss on all the readers after a write occurred. But it allows us to locally write after the first write. Note that with write update, we ensured for A that the second property of coherence is maintained. By updating all the copies. So when a write occurs, all copies are up to date, and thus they will return the new value of the write. With write invalidate, we ensure the same property by the write invalidating all the other copies so that now they have no choice but to get the new version of the data. So either way, reads will get the new version of the data but that is achieved in different ways. The third property of coherence is still ensured by snooping in a write invalidate protocol with snooping. So basically snooping on the Bus still ensures that any writes that need to go to the Bus because there is sharing will be ordered by the order by which they go to the Bus. So for example, a write here will invalidate everything. Now a write here has to put a request on the Bus because it's a miss. And also train the copy from here and now everybody knows what the order was. In fact, at any given time, only the last version really survives in caches. Because a write invalidates all the other copies. So it's pretty clear whose version was last, because all the readers need to get the copy that was written last anyway. So now let's see how write-update and write-invalidate differ from each other. Suppose that we have two cores and the program where core 0 writes to A and then core 1 reads from A, and this repeats a thousand times. And you can assume that the caches are empty before all of this. And that we don't care what happens after all of this. The question for you is, how many bus uses do we get with a write-update protocol that uses both the shared and the dirty bit optimization? Put your answer here. And then with a write-invalidate protocol with the same shared and dirty bits, put your answer here. Let's see the solution to our write-update versus write-invalidate quiz. We have two cores that run our program, where basically, core 0 writes, and then core 1 reads, and that happens 1,000 times. How many bus uses do we get with the write-update protocol, and then with the write-invalidate protocol? We have already seen a similar example for write-update. What's going to happen is, with the shared and dirty bits we do not really save much because there is actual sharing going on here, so we are going to have a broadcast on the bus for each of these writes, so we get 1,000 bus accesses from these writes, and the very first read is going to be a miss, so we get the bus access there. So overall we get 1,001 bus uses. In a write-invalidate protocol, let's see what happens. Core 0 writes, this is a miss. We send something to the bus, and now we get the only copy of this data. Core 1 reads, has a miss, core 0 has to supply the data. So, so far we had 1 and 1, and then what happens is ore 0 writes. We know that there is a sharer now because this core has a block, so we need to send out an invalidation. That means that when core 1 reads next, it's going to have a miss and use the bus. So this will repeat 1000 times, and we get a total of 2000 bus uses, so as you can see write-update Is a lot more efficient than write-validate, if we actually have a pattern where one core produces the data continuously and the other one keeps reading the data. If we have a pattern in which one core is the producer of all the data, and the other core is the consumer because the data flow from the producer to the consumer through the same memory location. In the write update protocol involves just the update by the write, but the read can then find the data in the cache, whereas in the invalidation protocol the write just sends out an invalidation, and then the reader has to still suffer a miss to obtain the data. Let's see another comparison of updates and invalidates. This time let's look at two cores that execute the following program, core 0 will read and then write A and this is repeated 500 times. And then core 1 will read and then write A, and this is repeated 500 times. And meanwhile nothing else happens in this core. So nothing gets replaced. The question for you is again, how many bus uses do we get with the write-update protocol and with the write-invalidate protocol in this case? Let's look at the solution to our second Write-Update versus Write-Invalidate quiz. This time we have a program where one core repeatedly reads and then writes A and then the other. So how many bus uses do we get with the write update and the write-invalidate protocol? This time, the write update protocol is going to have one bus access here to read, because this is a miss. Because there is no sharing, the write doesn't need a write access. because we know that we privately have this thing in our cache. So, this is what happens the first time. In the remaining 499 iterations of this, nothing else happens. We just hit in our cache. Once we get to core one however, the reading is going to get the data from core zero but now we have the shared bits at one and both caches. So the write here will need to do a broadcast the first time. And then the next time and so on. So overall we get 1 for the initial miss. And then 500 updates of core 0's cache. Note that this is wasteful because core 0 is no longer really interested in this. So overall we get 502 bus accesss. In a right invalidate protocol, we have the same thing here. We have a miss and the data is not shared, so the right and all of the reason right after that are just cache hits. However here what happens is the read here will make the data shared and fetch the data from the other core. So that's going to be a miss. Because the data is shared, the right will have to do an invalidation broadcast. So we will have another bus access, but that makes the data private to core one now because this version is invalidated. So for the remaining 499 iterations of this, nothing else happens. So overall we get only three bus accesses. As you can see, write-in validate is a lot more efficient if data tends to be used frequently on one core and then the frequent use moves to another core. So let's compare update and invalidate coherence approaches. We will compare them for different things that the applications often do. Here we will say, what the update protocols are going to do and here's what an invalidate protocol is going to do. If the application has a burst of writes to one address and if the address is shared, then each write will send an update in an update protocol. This is bad, because each updates results in competing for the bus, so it creates bus contention and also it consumes energy and makes writes slower. In an invalidate protocol, only the first write send out an invalidation and the remaining writes are just going to be cache hits that don't result in any bus traffic. That is good. Why would an application burst of writes to one address? Well, because when we are computing something, we may need to update it several times until we get to the final result. Another common behavior is when an application writes to different words in the same block. For example, when we are initializing the block, we will write every word of that block. If that block is shared, an update protocol will send an update for each word that we write. That is bad, because it means that one cache line worth of writes might result in something like ten updates. In an invalidation-based protocol, the first write that we have is going to invalidate the other copies and then the remaining accesses to the block or different words are going to be just cache hits that don't result in bus traffic. So this is again good for the invalidate, bad for the update. And the final scenario we will consider is the producer-consumer scenario, where the one core keeps writing data and another core is supposed to read it. In this case for the update protocol, the producer will send updates every time it modifies the data and the consumer then finds the data in its cache. For example, if there is a buffer, the first pass through the buffer will put it in the cache and then reading the buffer, keeps it in the cache. And here, writes are going to put new data in the buffer, so that the consumer just keeps getting it from its own cache. This is not great, the producer sends updates, but it's kind of the best we can do, because the data really has to get from one place to the other. So this is about as good as this scenario gets. In an invalidation-based protocol, however, the producer will do a write that invalidates the reader's copy. So when the reader, the consumer wants to read, it's going to be a cache miss and it has to get the data from the writer and then a new write will happen and invalidate the copy and so on. So this is about as bad as it gets for this scenario, because not only do we create bus traffic when we invalidate, we then have to have a cache miss and have another bus operation to get the data. So as we can see from this table update and invalidate based protocols have different advantages. So which one do we choose in practice? Well, it turns out that all modern processors use the invalidate type protocol, but not exactly for these reasons. As we can see, invalidate-based protocols have some strengths and some weaknesses. Overall, they tend to be slightly stronger than update on average, but that's not really the knock out punch that makes them win handedly. What really makes them win with no contest is when a thread moves to another core. So a thread was using one core and then simply the operating system decides to move it to another core. So what happens in the update protocol is that it now will keep updating the old core's cache. Note that our working set was in one cache. Now, it moved to another one. But in reality, the other cache still has blocks that correspond to our data. So what happens is all the writes that we make in the near future are going to be updating the other course cache until that cache replaces those blocks by running another core, for example. This is pretty horrible, because a thread movement, ideally what would happen is we would just move the data over and continue working privately on our private data. So let's see how this horrible scenario for updates works when we have invalid dates. In that case, the first write to each of the blocks will have to invalidate the old copy. But after that initial period, there will be no traffic anymore. So this is about as good as it gets without actually taking the content of the old cache and copying it explicitly to the new cache. And again, this is pretty common operating system behavior. So we really want it to perform horribly and because invalidate-based protocols are commonly used, we will now look at invalidation-based protocols for the rest of this lesson. What we will look at now is one of the simpler coherence protocols called MSI. In MSI, a block can be in one of three states in a cache. I stands for the invalid state. A block is in an invalid state either when it is present in the cache, but its valid bit will not be set, or when it's simply not present in the cache. So a block that is not present in the cache is treated like it is present, but in an invalid state. S stands for the shared state. A block that is in the shared state in a cache can be read without any further ado. But if we write it, we have to do something. A local read to a block that is in the shared state keeps the block in the shared state and doesn't do anything else. And finally, m stands for the modified state. In this state we can read the block just like the shared state without sending anything out. And we can also write to this block by the local core without sending anything out. Now, because this is an invalidation-based protocol, what happens is in the modified state, we are sure that there are no other cached copies, which is why we are allowed to locally write. We don't have to tell anybody that we are writing because we know there is nobody. So this is sort of equivalent to having valid equals 1, and dirty equals 1 in a normal cache. We just call this state modified because it's easier to understand what's going on this way, than if we have to look at individual bits. And we will see that with more complicated coherence protocols, we will add maybe a state or two, but we will need to add several bits in order to track what's going on. So now, in the invalid state, we have what's equivalent to valid bit being 0. Pretty much, we behave like we don't have the block. Either because we really don't have it, or because we have it but we are not supposed to read it or write it. So, when we have a write and the block is in the invalid state, we move the block to the modified state. So when we have a local write, meaning the local processor wants to write and the block is in the invalid state, we move the block to the modified state. But we cannot just do that. To do that, we have to put the write request on the bus. So pretty much this is a write miss, we have to tell others that it's a write miss. We get the data and now we can put it in the modified state. Why? Well, because our write request on the bus pretty much made sure that nobody else has a copy any more because the right request that we made invalidated everybody else. If we have a block in the modified state and we observe on the bus that another cache is placing a write on the bus, then we will go to the invalid state. So we do this when we snoop the write on the bus. And we cannot just do this and be done. When we snoop a write on the bus, because we are in the modified state where we have the dirty copy. Meaning really the only fresh copy of the data in the system is in our cache. It's not even in memory because it's dirty here. We have to write the block back, as we're transitioning to invalid. And we have to delay the data that would be sent to the write request until we are done with the write back. This can be done in two ways. One is to actually cancel that request, write back, move to invalid and then that request gets repeated and now gets the data from memory. Another one is, when we see a write on the bus, if it's requesting data, then what we do is we suppress the memory response and feed the data to the processor. The simpler way of doing things is just to delay that request, write back, and then let it proceed. But either way, we have to make sure that before we move from modified to invalid, we write back the data to memory. And also make sure that the write request, if it is a write miss and needs the data, gets our new data not the data from memory. Now, if we snoop a read on the bus and we are in the modified state, then we move to the shared state. So we have snooped a read on the bus, we will now not be the only one who has a copy of the data. That means we will not be able to write without telling others. So we will move to the shared state. Just like here, we have to also make sure that the block is written back. The shared state is equivalent to having V=1, and D=0. So we will not be doing a write back from a shared state, because it only allows reads anyway. So as we are moving from the dirty to a clean state, modified to shared, we are going to write back the data. The reading processor wants a copy of this block. Again, we have to either delay that request, do our write back, change to the shared state, and then let that processor proceed with a read. Or we have to suppress the memory response that would happen as a result of this read and provide our data instead as we are transitioning to a shared state. If the processor reads a block, and the block is in the invalid state, then the block will be moved to a shared state. So in response to a local read, we put a read request on the bus. And then we get the data, and then we are in the shared state for the data. Note that the data might come from memory, or it may come from another cache because it had data in the modified state, and we need it to get that data. But as far as this local cache is concerned, it puts a read request on the bus, it gets the data, and then it puts that block in the shared state in the local cache. Then we can do many reads without telling anybody. If we have a block in the shared state, and we observe a snooped write on the bus, then we move the block to the invalid state. We don't have to do anything, because the block is clean as far as we are concerned. We don't have to write it back. We simply have to do the equivalent of resetting the valid bit and move to the invalid state. So there is nothing really that we have to do except change the state so that next time we try to read it we can no longer read it here, we have to put the read request on the bus again. If we snoop a read on the bus in the shared state, we don't need to do anything. Why? Well because our shared state is okay with there being other sharers that are only going to read the data. And finally, all the local reads are okay in the shared state. If we have a local write, that is not allowed in the shared state. So a local write that finds the block in the shared state will make us move the block to a modified state, but we cannot do that silently. We cannot just move the block and write to it, what we have to do is put an invalidation on the bus. Know that things are slightly different when we move from invalid state on the local right to modify and when we moved from shared state to a modified state on a local write. Here we put a write request that asks for a block so that we can write it. Here we already have the block. This is the most up to date copy, because if there was another write after that, we would not have it in the shared state. So now we want to write to it. We already have the data, we just have to put an invalidation request on the bus. So this invalidation request does not actually request the data. It just puts a request on the bus for everybody to invalidate their copies, but it's not asking for the data. So pretty much, as soon as we put this request on the bus, we can write because by putting an invalidation on the bus, we have made sure that now everybody is invalidated. At which point, we are in the modified state. Now of course, in the invalid state, whatever we snooped for this block on the bus doesn't matter. We're going to stay in the invalid state. Also in the shared state, it doesn't matter whether we snoop somebody's request to write, or an invalidation that they put on the bus. We still move to the invalid state without doing much. And finally note that if we are in a modified state, we will observe a write if somebody wants to write, and not an invalidation. Because if we are in the modified state that means that everybody else is in the invalid state for this block. So if they want to write, they'll have to put a write request on the bus. They cannot be in the shared state and put just an invalidation on it. So let's look more closely at what is called cache-to-cache transfers, which is what happens when core one has block B in a modified state, and another core puts a read request on the bus, so that it wants to basically bring the block in its own cache. At this point, C1 has to react. Because C1 is the one that has to provide the data, if we have the block in the M state, that means that the only up to date copy in the system is our copy. Even the memory doesn't have an up to date copy. So if the data is going to be sent to C2, it had better be our data, but how, and we have briefly touched on this when we were discussing the inside protocol. There are really two solutions. The first solution is abort and retry. The idea here is that C1 somehow cancels C2's request, and for that, we need some sort of an abort bus signal, that when our request is placed on the bus, another core can assert the abort signal that tells the requesting core to kind of back off and try again later. Now that C2's request has been aborted, C1 can do a normal write back to memory. At this point, the memory has the up to date data, and when C2 retries its read request, it will get the data from memory. The problem with this approach is that really, from the time C2 makes the request If this data was normally coming from memory, we will have a memory latency, so if this is just a read miss, we will have a memory latency. But if it is a read miss where another core has the data, we really have to have two memory latency's before C2 can get the data. One for the write-back to memory to happen on C1, and then for us to actually get the data from memory. So the read miss from C1 has a latency that is twice the memory latency, plus some. So this is why this approach is not ideal. The second solution is called intervention. In this case, C1C is the read request is being made, and it tells the memory somehow that C1 will supply the data instead of the memory. So normally the memory would respond to a read miss request, now C1 kind of tells memory that I will do that, and for that we need an intervention bus signal through which C1 by asserting the interventions signal, signals to the memory not to respond. So the memory now doesn't respond with the data, but C1 does, and now the memory must pick up the data that C1 was responding with in order to put it in the appropriate memory block. The reason the memory has to pick up the data is that once C1 responds with the data, both C1 and C2 will have the block in the shared state. So both of them will think that the block is not dirty, and that means that if the memory doesn't get the data now, it will never get the fresh data. So by picking up the data, the memory ensures that it gets the data that will not be written back anymore. So the disadvantage of this approach, is that it needs more complex hardware. Now our cache needs to be able to kind of insert its data, into what would normally be the memory sending data, and also the memory has to have enough intelligence to pick up the data when a core is responding to an intervention request, instead of just picking up the data when there is a write back request. In modern processors we nowadays mostly use a variant of an intervention approach. And the reason is that, yes the hardware is slightly more complex, but fancier snooping protocols actually eliminate the problem of memory picking up the data, and the complexity to respond with the data is not too large. In contrast, here the performance suffers. So pretty much, between having a fancier protocol and having enough transistors from Moore's law to actually add this capability to caches, we pretty much can do this and not have to suffer in performance like this. So, let's see if we can do some MSI protocol transitions. Let's say we have two cores with private caches and that the caches are using MSI to maintain coherence. Initially, block X is in memory and no cache has it. So here are the actions of the caches in C1 and C2's cache. Core 1 issues a read X. Meanwhile, core 2 doesn't do anything. What is the state of block X in C1's cache after this read is completed? And what is the state of the block x in C2's cache, after this read on C1 is completed? So put your MS or I state here and another one here. Now, lets say that the next axis is that C2 read X after this is completed, what is the new state of X in C1 and C2's cache? And finally C1 does another axis write X, what is now the state in C1's cache after this write is complete? And what's the state in C2's cache after this write on C1 is complete Let's look at the solution to our MSI access quiz. We have two cores with private caches and MSI coherence, initially block access in Memory, which means that it's really invalid in both of the caches. After we read X and C1s cache, C1s cache with transition this block from I to S, the shared state. Because this was a read we move to the shared state. Even though it is not really shared, this is our way of saying basically that the block is clean, and that we only want to read it. In C2s cache it stay invalid. When C2 then does a read, it also gets the block in the shared state. And if we snoop a read request in the shared state, we stay in the shared state. So finally when C1 writes to X, because it has a block in the shared state, it needs to send an invalidation out. That results in C2 moving the block to the invalid state. It can no longer be read until we get the new copy from C1, and C1 after writing has the block in the modified state. Let's now do another quiz on MSI. Same thing, two cores, private caches, MSI coherence, Block X initially in memory. But now, C1 is reading, and then C2 is writing, and then C1 writes. So what is the state of C1's and C2's cache after each of these things occurs? Let's look at the solution to our MSI quiz. Read X results in the same behavior as in the previous quiz. Both of them have the state as I, and then when read X occurs in C1's cache, it sends out a read request, gets the block from memory, and now it has the block in the shared state. C2 still has the block in the invalid state. Now when C2 writes to X, it sends out a write miss request. It will get the data and then write to its cache, so it gets the block now in the modified state. C1 meanwhile transitions from the shared state to the invalid state because it observes the write request sent by C2. And then when C1 wants to write, it does pretty much the same thing as C2 just did. It puts a write miss request on the bus, gets the block, writes to it, puts it in the modified state. Meanwhile, C2 has to intervene and provide the data that C1 gets, and then, because we snooped a write, we need to transition to an invalid state. Note that if a block is in M state in any cache, all the other caches have to have that block in the invalid state. If a cache has a block in a shared state, all the other caches need to have the block in either the shared or the invalid caches. So, pretty much we can have one has M, all other have I. Or we have several can have S and the rest have I. We cannot have M and S in the system at the same time for the same block, and definitely we can not have M M. So now, we have seen that in MSI even when we have an intervention, we need to write to memory every time we have a cache to cache transfer. So, for example, C1 has the block in the M state because it just wrote it. C2 wants to read the data, so it puts a read request on the bus. C1 responds with the data. So now C2 has the date, but C1 can no longer just write to this block. So C1 gets the block in the shared state, C2 gets the block in the shared state, and that's why the memory had to grab the data when C1 was responding. Now what can happen is C2 writes so it sends an invalidation because it already has the data. Now C1 gets invalid state, C2 gets the modified state, and and then let's say this read write behavior moves to another core, like C1. So pretty much, if one core reads and writes, then another one reads and writes and so on, we can have a repetition of this. So pretty much the data is kind of moving around the caches, but the problem is, every time it moves between caches, the memory also gets written. And the memory doesn't have as much bandwidth as the caches do. So the problem really is that here we needed to do a memory write. Another problem occurs because, when C2 has written, for example, and has the data in the modified state, and then C2 responds with data and that causes a memory write, we again get C1 has the block in shared state. C2 has the block in shared state. Now if C3 reads, because both of these have the block in the shared state, the memory provides the data. So we have memory reads here even though one of these could have responded with the data, if we only knew which of the two should respond. Definitely both of them should not respond. So if we had a way to tell which one of them should respond we could avoid this memory read, by the one that is responsible for responding providing the data as long as there are new requestors. This can happen a lot of times. So if another core reads, same thing, we get the memory read, although now there are three cores with the data in the shared state, and so on. We want to avoid the update to memory as long as there is a cache that still holds the most recent value of the block. And we want to avoid memory reads if there is a cache that can provide this block. Because again, memory bandwidth is a lot lower than cache bandwidth. And also memory writes are a lot more expensive in terms of power and latency then cache reads and writes. But to do this, we need to make a non-modified version of the block in one of the caches responsible for giving data to other caches when they want it so that the memory doesn't need to respond and eventually writing the block to memory so that the memory doesn't need to be updated every time we do something like this. So what we really need to know is which of the shared copies is the one responsible. And we will know this by introducing another state called O, which stands for owned. So there will be one owner of the blog if others are sharing it. The O state will behave just like the shared state, except that whenever there is a request for the data in the block, the owner is one of the sharers that gets to respond. And also if the owner replaces the block from the cache, it gets right back to memory so that we ensure that memory gets updated. So now we have what is called a MOSI coherence protocol. MS and I are the same as before, but now there is a new O state. So the MOSI protocol is like MSI, except when we are in the M state and we snoop a read from the other core, we provide the data as before. But we now move into the O state, not the S state. When we are providing the data, the memory now doesn't get accessed. In MSI, the memory would now need to be written with the data. Here, we simply provide the data to the requester, move to the O state without updating the memory. While the block is in the O state in a cache, it behaves similarly to the S state, except that if we snoop a read, we continue to provide the data just like we did in the M state. And that we need to write the data back to memory, if the block in the O state is replaced from that cache. And that's because all of the other caches will have the block in either invalid or shared state, so they don't know that the block is dirty and needs to be replaced. So the M state indicated both that we have exclusive read/write access to the block, nobody else has it and that the block is dirty. S indicated that we had the shared read access to the block, others may also have it in shared state and that the block is clean. The memory has an up to date copy. Now O combines the properties of M and S states in that it is a shared read access state, but the block is dirty and we are responsible for putting the data in memory and also because there is only one cache with the block in the O state. That cache knows it can respond to all the read accesses to the data, so that the memory doesn't. So, we have used the O state to avoid inefficiency of the MSI protocol that had to do with the memory, getting unnecessary accesses that could be satisfied by caches that already have the data. There is another inefficiency that both MSI and MOSI have. And that inefficiency is related to thread private data, or data that is only ever accessed by a single thread. And thread private data is for example, all data in a single threaded program will be thread private. So if we have a four core processor around four single threaded programs, they never share any data. All of the data in these programs is like this. Even when we have a parallel program, we still have data such as for example, the stacks of the individual threads that are still going to be accessed only by a single thread. And then efficiency occurs when we read the data in a thread, and then we write it, and this is really the only thread accessing this data. With both MSI and MOSI, what's going to happen is the data was invalid before we had it. We read it. We have a miss. Now we get the data in the shared state. When we want to write it, because it's a shared state, we need to send out an invalidation, and then we have the block in the modified state. And for every single block of data, we have to do this. In contrast, if we had that uniprocessor, the block would have a valid bit of 0, we read it, we have a miss. Now, the value bit becomes 1, and then we have a hit when we write it, and then we just change the dirty bit to 1, so somehow the uniprocessor gets to do a miss here and a hit here, whereas to do MSI or MOSI, we have to do a miss followed by another bus request. Granted, it doesn't carry data, but still we have to wait for the bus, send something outside the ship and so on, so this is still a lot slower than just a cache hit. And it's okay to pay the penalty for having coherence if we're actually sharing the data, because a uniprocessor would not provide coherent behavior. But here, we actually have no sharing whatsoever. So what we want to do is avoid this invalidation, and to do that, we introduce a new state called exclusive. So, what does the E state do? The M state gives us exclusive access, so we can both read and write and makes us the owner of a dirty block. So, we have to respond with the data and also update memory. The S state gives us shared access so that we can only read and we are not responsible for giving others our data and also we are not responsible for updating memory. We have seen that the O state also give us shared access so we can do reads. But makes us responsible for updating memory and giving data to others, so that we avoid memory writes that occur when we don't have the state. What E does is it gives us exclusive, so that we can read and write, although the block is still clean. The memory still doesn't need to be updated. So when a core does read A followed by Write A. In the MSI protocol we move from invalid to shared and suffer a miss, and then we move from shared to modified and need to send out an invalidation. So we have two bus accesses. If we have the MOSI we have the same sequence because really the O state is not helping us here. It's helping us save memory accesses after we had the write, and others start reading. If we have the MESI, with no O-state, or a protocol that combines all of the states, when we do a read, we now detect that although we have read the block, we're the only one having it. So instead of going to the shared state we go to the E state and we suffer a miss. But the VNR is that when a write to A occurs we know we have exclusive access so we can just write without telling anybody although we have to transition to the N state now because now the block is also dirty. So now we get the same behavior that we would get if we did this sequence of accesses on a cache in a uniprocessor. So this is not a shared block here, and we want to get the same number of misses and accesses as we do in a uniprocessor that doesn't share data. So here we will have a miss. Followed by a cache hit. We want to have a cache hit here and the E state allows us to achieve that. So now let's try our hand at the moesi protocol. Suppose we have three cores maintain moesi coherence. And the variable x starts out by only being in memory. So it's not in any of the caches. So we have our three cores C0, C1 and C2. And the first access that happens is read x on core zero. What is the new state of this block in the cache that belongs to this core? Next C1 does read X, what is the new state of the block on this core and what is the state of this block on C0 after C1 is done with this read? Now C2 does read x. What is it's new state for this block? And what is the new state of this block in C1's cache? Or does it just stay the same? And finally C1 there's a write to x. What is the new state of the block in C1's cache? In C2's cache? And in the cache of core C0. In a MOESI protocol, the state can be M, O, E, S, or I. Initially, all of the cores have the block in the I state in their caches. When C0 reads, it detects that it's the only one who actually has a copy of the block, and because it's doing a read it doesn't go into the modified state, it goes into the exclusive E state. The others keep the block in the I state. When C1 reads, it detects that it's not the only one who has the blog, so it normally goes to S state. Furthermore, C0 now snoops another reader, so it's no longer exclusive and it moves to the shared state too. C2 still has the invalid state for its block. When C2 reads, it also detects that there are two other sharers, so it gets the block in the shared state. C1, nothing happens to its shared state. When we snoop a read, and we are in the shared state, we just stay there, because others reading is compatible with the shared state. C0 is also in the shared state, although you were not asked to do this. And now finally when C1 writes, because it was in the shared state, it will put an invalidation request on the bus, and once it does that it can move to the modified state. Others see that invalidation request and go to the invalid state. Pretty much, after our write, we are always in the modified state, and all others are always in the I state at that point. Now lets compare the MESI, MOSI, and MOESI protocols. We have three cores and we start off with block A being in the invalid state in all three caches. Suppose that we have sequence of axises. Core one reads A. Core one, then writes A. Core two reads A then writes A. Core three then reads A. Core one, then reads A. And core two also reads A. Assuming that A's the only thing we access so we don't have to worry about replacing A for these caches unless it needs to be replaced because of coherence. How many memory reads and Bus requests do we have in this sequence of events for MESI, MOSI, MOESI protocols? Let's look at the solution for our three protocol comparison quiz. Where we had 3 cores, a block A that starts in an invalid state in all three caches, and then cores access this block in this way. We were asked, how many memories and bus requests do we have in each of these protocols? When a read A occurs, we will have a memory read in each of these protocols, because this is a miss on core 1. At that point, C1 has the block in the shared or exclusive state, depending on whether the protocol has a shared state or not. So in the MOSI, we will have shared state. In MESI and MOESI, we will have the exclusive state because at this point, C1 is the only one who has this block at all. When C1 writes to A, if we had the block in the shared state, we need to put an invalidation request on the bus because we don't know whether there are any other sharers. And, of course, if we did the memory read, we'd also have a bus request in each of the three. Now when we do a write A for protocols that have the E state, we just move to the modified state. We don't have to put anything on the bus or read memory. So here, we don't do anything, but for MOSI we were in the shared state. We have to put an invalidation request on the bus. There is no memory read here, we already have the data we just need to invalidate others because we think there might be others. So now we have the block in the modified state in C1. Everybody else still has the block in the invalid state. When C2 reads A, C1 must supply this data. So this is a bus request because we have a read in all of them. C2 will get the data in the shared state because C1 will be downgraded from modified to either shared or owned. But either way C2 will be shared. C1 will have the block in either the O state, if there is an O state, because from modified we go to O once we start sharing. Or, in the shared state assuming we have intervention base supply of this value to C2, there is no memory read. Because C1 is able to provide the block to C2. Now, when C2 writes to the block, because it has the block in the shared state, there will be a bus request to send out an invalidation. Either way, C1 now has the block in the invalid state, and C2 has the block in the modified state. So from this read to this write, having the exclusive state didn't help us save an access on a write, and that's because here we had no exclusive access to the block anymore. C1 also had the block. Now let's see what happens when C3 reads. When C3 reads, C3 will get the block in the shared state, and this is a read miss, it needs the data, so that is definitely a bus access in all of them. C2 however, moves to either the owned or the shared state, depending on whether we have the owned state. In this case, again, C2 supplied the data to C3 so there was no memory read. However, when C1 reads, if we are in the shared state in C2, C1 ends up reading memory. So if we have the O state, C2 is able to intervene and supply the data to C1, and then C2, if we have the shared state, then there are memory accesses in both of these. C1 had the block in the invalid state, so definitely the read request will be put on the bus. And now what we have is basically C1 and C3 have the block in the shared state. C2 retains the block in the owned or shared state. So there was a bus request here, and there was a memory request if we didn't have the O state. So here we have an additional memory read. Now C2 reads the block. Either way, it can read the block, because at this point, it's either owned or shared. So there is no bus request, nor memory read in any of these protocols. So overall, we have 1 memory read in protocols that have the O state and 2 in the protocol that doesn't. For bus requests, we have 5 bus requests in protocols that have the exclusive state and 6 bus requests in the protocol that doesn't. We said already that is only one of the two ways to do cohe snoopingrence. The other way is directory-based coherence. So let's first see why do we even want directory-based coherence? What's wrong with snooping? In snooping, we had to broadcast all of the requests we make so that others can see them. And also to establish ordering, for example, for writes, we need to establish the ordering so that we can obey the third coherence property. But that means that there needs to be a single bus on which all of the requests from all of the cores go, both cache misses and also coherence requests that are just sending out invalidations and so on. So the bus that we were using for broadcast becomes a bottleneck, and as a result, snooping does not really work well if we have more than something like eight to 16 cores. Beyond that point, most of the time the cores are just standing waiting for the bus to put requests on it, so really we get no further benefit from additional course. So what we need to solve this bottleneck is a non-broadcast network over which we're going to send our requests. So that many requests could be going on at the same time, but then we have the problems of how do we observe those requests that we need to see. So, for example, if we are in a shared state, it had better be that when there is a write by somebody else, we observe that write so that we can move to the invalid state to avoid incoherence. And also, how do we decide what the ordering is for requests that go to the same block? So, for example, how do we decide what's the ordering between writes done at different cores, if this request can be made on different parts of the network and the structure that does this is called a directory. So the directory is a distributed structure across the cores. It's not centralized in one place, like the bus. So not all requests have to go to the same part of the directory. Each slice of the directory will serve a set of blocks, where a slice of the directory is the part of the directory that is next to a particular core. And that means that different blocks will be served by slices that are in different places. And that's how we achieve a higher bandwidth because really each slice could operate independently because it serves a disjoint set of blocks. What does a slice do? It has one entry for each block served by that slice. And the entry for a block tracks which caches in the system have this block in an non-invalid state, of course, because the block in an invalid state is like it's not in that cache at all. And the order of accesses for a particular block is determined by the home slice for that block. That's the slice of the directory that has the entry for this block. We call that the home slice of the directory. So pretty much, accesses that go to the same block are almost serialized in how they can access their entry. And the order in which that happens is the order in which the accesses are officially said to happen. Note that with a directory protocol, the caches still have the same states they did with snooping. It's just now when we send a request to read or write, that request no longer goes on a bus. It gets sent via our network to the directory. So we can have many requests traveling to their individual slices. And then the directory figures out what to do. The directory entry has one dirty bit that indicates that the block is dirty in some cache in the system, and for every cache in the system, the directory entry has one bit that indicates whether the block is present in that cache. If the presence bit is 1 for a particular cache, that means that we think that cache has a copy of the block. If the presence bit is 0, that means that we know for sure that that cache does not have the block in a state that is not I. So let's say we have one block caches at core 0 and 1, and let's say this is the directory entry for block B, where this is the dirty bit and these are the presence bits for cores zero, one, two, three, four, five, six, and seven. So, this is an eight core system, so we need eight bits to represent presence of the block in the caches. Initially, the block is not dirty and is not present in any of the caches. Let's say that core zero reads from the block B. The block is not in its cache, so it's treated like it's in the invalid state. We put the read request out. Normally, this read request will be put out on the bus. And because no other cache has this, the memory would respond. But now we don't have snooping anymore. So all requests for a block would be sent first to the home slice of the block. And we determine what is the home slice by looking at the address of the block. Different blocks are distributed among different slices so that we get an even load between the slices. So let's say we determine that for block B, this is the slice where we go, and once the request gets there, we look at the directory entry for it. The directory entry says that the block is not anywhere in the system. So now we can get the data from memory, and send the data back to the cache 0, and we need to tell the cache 0 everything that the bus would normally tell it. So we tell it that it has exclusive access, because there are no other sharers. And we give it the data. That response gets back to cache 0, and now if it has the exclusive state, it can put the block in that state and put the data for B here. When the directory sends the data to Cache 0, it changes the presence bit for that cache to 1. Also, we sent the data with exclusive access. If cache 0 modifies the data, it doesn't have to notify the directory back, so we also set the dirty bit. The dirty bit here will not cause a write-back. It will cause us to find if a cache needs to do a write-back. So now why does this work better than a bus? Well because while cache 0 was doing this with this directory slice, cache 1 could have done something like that for another block, with another directory slice, completely independently. However, if cache 1 tries to write to B at about the same time when read B was occurring, we would send our write request to the same directory entry. And now, between these two messages, the directory controller needs to select one. In this case it shows read request first, so officially, this read happened before this write. So the core here gets to read the block B, and only then, the write gets processed by the directory. So when this write request arrives here, again, this write request would be placed on the bus, seen by this cache. As a result, this cache would invalidate, and we will get the block in the modified state. With the directory, we send the request to the directory controller. It sees that the block is present and possibly dirty in this case, cache 0. So what now happens is the directory forwards the write request for B to the caches that are present. In this case it forwards it to cache 0. Cache 0 now sees this request just as if it was snooped from the bus, and because it has the data in the exclusive state it can choose to respond with the data or it just may keep quiet and confirm the invalidation. So, what happens here is once the write request has been given to this cache, we need to send the acknowledgement, at least, if not the data, back to the directory controller that says that we are done invalidating our copy. So this point we have invalidated our copy. When the acknowledgement arrives here, the directory controller can change this to 0. If the data didn't arrive, then we can erase the dirty bit. And now because we don't have the data still, we will read the memory and send the data to cache 1. And because we are acknowledging a write request, we again set the dirty bit, and now the presence factor gets a 1 for the bit that corresponds to cache 1, and now we get to set M here and write to the B here. So as you can see, this is how the caches can do their normal coherence, but now instead of everybody seeing everything, that we need to send a request to the directory, which then sends out the messages that need to be sent. Only to those caches that actually might have the block. So if a block is shared by only two cores, then we will be only sending a very few messages. In contrast, the bus would force us to basically broadcast to all of the cores. So this also saves a lot of bandwidth because we're using point to point network links to send just this. Meanwhile, other cores could be doing something like this with another directory entry, completely independently over a network. Now we are going to do a slightly more detailed directory example, to show how the directory doesn't become a bottleneck when a bus would. Here we have four cores, each with its own cache and also, next to each core is a slice of the directory. Let's assume that each slice just keeps track of one block. So this is the directory slice where we keep the block X information, and then next to cache one there is directory Y, Z and W. And now let's assume that the core 0 write X at about the same time, where core 1 is trying to read Y. This is a cache miss, and we send the request to the directory that is the home for X, which happens to be around nearby slice of the directory. When we read Y we go to our own slice. Let's say that cache 2 also reads Y at about the same time. So what happens now is the write to X, and the read to Y here are probably going to be processed at about the same time, completely independently by these directories. Meanwhile, the read Y is travelling through the network to this directory. In each of these directories, we see that the block is not present in any of the caches, so we allow exclusive access for the read, and here, of course, the block will be modified, anyway, so here, we will set the dirty bit and note that block X is now in the cache 0, and here we will set the dirty bit because exclusive can become dirty easily, and set the presence bit for cache 1. Now the read Y is traveling to this directory, let's say that cache 1 is being accessed with a write to X and it's a cache miss so it gets sent to this directory. So this will probably arrive here first. We will check the presence vector and the dirtiness information and see that cache 1 is the one that has the data, and that it might be dirty there. So we forward the request to read to cache 1. Cache 1 acknowledges that request, changes its state to shared, we get the response that it doesn't carry data, so now we know that the block was not dirty, and we change it to not dirty. We then get the data and forward it to cache 2, and now we know also that cache 2 might have the data, and that cache 2 eventually will get the data, so it will eventually get the shared state and the block Y. Meanwhile, the write X is getting here, and because we need to write we're going to send an invalidation and an external write request to cache 0, so here the cache needs to responds with the data because it's a write request, it sets the state to invalid and responds with the data. This is also a write, so the block will remain dirty, but now the directory knows that cache 0 no longer has the data, and we send the data to cache 1. So now the cache 1 will eventually get the data in the modified state, and, as you can see, the write to X and the read to Y are largely proceeding independent of each other, except for maybe possible connection in the network. So, if our network is good and has many different paths between these caches and directories, then really, there was no point where these requests competed with each other for anything. This one was processed by this cache and this directory, this one was processed by this cache and this directory, and we involved other caches if necessary. But in this case, it was this cache, so these requests were still largely independent. Now let's say we want to write to Y in this cache. This will be sent to the directory for Y. So the message travels over here. The directory for Y now sees that caches 1 and 2 might have the data and that none of them are dirty, so it just sends the invalidation request to caches 1 and 2. When they receive this invalidate the block Y and send back responses that indicate that they have done so. When the first response arrives from, let's say, cache 1, the directory eliminates this bit. When the second response comes, the directory eliminates this bit. Now that we know there are no more remaining sharers we can send the data to the requesting writer and note that it's going to have the block and that the block will be dirty. So now we send the block over here and this block becomes modified here and so on. So again this write only involved the caches and the directory entry that actually needed to be involved for this block. This cache and directory, for example, were completely operating independently of this, and these two directories could've done something else. So let's see if we can figure out what the directory will be doing in a MOESI protocol if we have 4 cores numbered 0 through 3 and if a directory slice for location A is in slice 0. If what happens is this, core 0 reads A, core 0 writes A, core 1 reads A. Core 2 reads A, core 3 reads A, and then core 0 writes A again. The question for you is in this sequence of accesses, assuming that A's not in any of the caches at the beginning of this, how many requests are sent by these caches to the directory only counting the requests? Then the directory forwards these requests through how many messages? Note that it need not be the same number here. As a result of sending these, the directory gets how many responses and replies? And finally, how many responses does the directory send itself? Again, the protocol is MOESI for this course, and we have a directory protocol with 4 cores. Let's look at the solution for what happens in our directory MOESI quiz. When Core 0 issues a read it sends a request to the directory. The directory doesn't forward or get any responses, it just sends a response. When there is a write, the Core 0 gets this block A in exclusive state because MOESI has such a state. Core 0 can do the right without telling anybody. So there is no request sent to the directory, and then there is no direct activity as a result of that. When Core 1 reads A, it sends a request to the directory. The directory will now send a request to the only core that is involved in sharing this block, which is Core 0. So it sends one message. Core 0 responds with the data and acknowledges that now it's okay to read this block. And when the directory gets this message, it sends to Core 1 a response. Now, Core 1 can read the block. So now we have Core 1 has the block in the shared state. Core 0 is now in the own state, because it downgraded itself from modified to owned. Core 2 now tries to read. Sends the request to the directory. The directory, if it has only the state that we have seen on the previous segments. Does not know who is the owner of this block, but it doesn't need to send anything, so it gets to memory. The directory here doesn't send any messages forwarded, because it no longer sees the block in the directory state. So it doesn't forward anything, it doesn't get responses, it just sends a response back to Core 2. Note that here we're unable to benefit from the own state. Simply because the directory doesn't know to ask Core 0 to provide the data. With a fancier directory, we would be able do that. But not with a directory that only tracks presence and dirtiness. So now C2 also has the block in the shared state. The same thing happens for C3's request, there is a request, the directory doesn't forward anything, it just sends a response, because it has to read the data from memory, and now, Core 0 writes. Because it's in the own state, it is not able to write without notifying the directory, so it notifies the directory. This write is an invalidation request because we have the data. But we need to send invalidations to these three processors, so that the directory knows that Cores 1, 2, and 3, their caches have the block. So it sends three messages to these three destinations. It gets three responses from each of them. Once they have all acknowledged that they have invalidated, it sends a response back to Core 0, and now it's able to write. So over all we had five requests sent to the directory and that also means that there are five responses from the directory. Because every request will have a response. The directory has forwarded four messages. Note that this is not like four out of five, got messages forwarding. In fact, one of these was for one message and the other three were for the second message. And then we got four responses to these messages. Again, every message needs to get a response because the directory cannot acknowledge and allow a core to write until all of the sharers have indicated that they have invalidated their state in the cache. So lets now revisit the topic of cache misses when we have coherence going on. We have seen that caches have 3 C's, 3 types of misses. Compulsory misses, conflict misses and capacity misses. Remember the compulsory misses, we have them because we're accessing the block for the first time. So, because it was never in the cache, we have to have this miss. Capacity misses we have because the block doesn't fit in a cache of this size. And conflict misses we have because the cache doesn't have enough associativity. With coherence, another C, we can now have a cache miss caused by coherence. For example, when we read something, somebody writes it, and we want to read it again, that would not be a miss except for coherence, so we call this type of a miss coherence miss. So remember that there is actually four Cs. The three we have seen before, and coherence miss. There are really two types of coherence misses. What we have seen so far is the true sharing type of miss which occurs when different cores access the same data, so naturally we have to perform coherence and thus have some misses. So for example if we read A and then write A and then read A again. There are some misses necessary in order to get the right behavior for the reads. But these were all happening on the same data. There is another type of coherence miss, called false sharing, which occurs when different cores access different data, so there shouldn't be any coherence messages because of that. Except that these data items are in the same block. So it's called false sharing because, really, there is no true data sharing, but as far as coherency's concerned, we do everything at the relative cache blocks. So two items being in the same block makes them behave like the same item, as far as coherency's concerned. So let's see an example of false sharing. Let's say that these are two memory blocks, Block zero and Block one. The words in Block zero will be A, B, C, and D. So, for example, this is a 16 byte block that has four, four byte words. And the words in Block one are X, Y, Z, and W. So which of these execution fragments have false sharing misses? If these axises happen, are there any false sharing misses? If these axises happen, are there any false sharing misses? And if these axises happen, are there any false sharing misses? Let's look at the solution to our False Sharing Quiz. We have two blocks, a block that has A, B, C, and D in it, and a block that has X, Y, Z, and W in it. We're looking for which of these, if any, have false sharing misses. So let's first look at this one. There is a read X and a write X. The first will be a miss. The second will also be a miss. Both of these are compulsory misses. X wasn't in C0's cache. X wasn't in C1's cache, so they have to be compulsory misses. Read A here is also a compulsory miss. This block wasn't in C2's cache. Finally, C3 is writing to B. This block wasn't in C3's cache so this also a compulsory miss. So in fact, this sequence has no coherence misses whatsoever. All of them are compulsory misses. Now let's look here. This is a compulsory miss for C0. This is a compulsory miss for C1. This is a compulsory miss for C2. And this is a compulsory miss to C3. So regardless, whether we had coherence or not, these four accesses would be misses because each of these caches needs to fetch the block for the first time. So there are no coherence misses in this sequence either. Now, let's look here. C0 reads X. C1 writes to X. This invalidates X in C0. This invalidation needs to happen in order for X to be written in C1. C2 now writes to W. That invalidates the data in C1's cache. The data in C0's cache was already invalidated at that point. When C0 tries to read X, the reason it doesn't have the data in its cache is because C1 invalidated it, and C1 was accessing the same word. So this is an example of true sharing. We got invalidated because somebody was trying to write the same word that we were accessing. So this is actually a sequence of three compulsory misses, followed by a true sharing coherence miss. So in this sequence there is also no false sharing, so none of these should have been selected. In this lesson we have seen that we don't need each core to have it's own cache. But that these caches have to be kept coherent so shared memory programs can work. We then learned several ways of keeping caches coherent. In the next lesson, we will see what we need to do so course can coordinate their work in a parallel program. In this lesson we will be reviewing caches. You should already know most of this, but it is extremely important to understand the basics of caches before we move on to virtual memory and other more advanced caching topics. So this lesson has much more detail than your typical review type lesson. To understand caches, we must first understand the concept of locality, which is also called the locality principle. It says that things that will happen soon are likely to be close to things that just happened. This means that if we know something about past behavior, we're likely to be able to guess what's going to happen soon. We have already used this locality principle, for example, when we did branch expression. Now we will use it for caches. Let's do a quiz to see if we understood locality. Which of these are not good examples of locality? It rained three times already today, so we can conclude that it is likely to rain again today. We ate dinner at six every day last week, so we can conclude that probably we will eat dinner around six this week, too. And, it was New Year's Eve yesterday, so we conclude that probably, it will be New Year's Eve today, too. Again, you need to find which of these are not good examples of locality. Let's look at the answer to our locality quiz. We were asked, which of these are not good examples of locality? The statement that it rained three times today so it will likely rain today again is a decent example of locality. Usually if it rains often, it's probably going to just continue raining often for a while. We ate dinner at 6 every day last week, so probably we will eat dinner around 6 this week too. This also tends to be true. People tend to eat meals around the same time for a while. And finally, it was New Year's Eye yesterday, so it will be probably be New Year's Eve today too. This one is not a good example of locality. Why? Well, because some things do not have locality. For example, it being New Year's Eve one day usually means that the next day it won't be New Year's Eve, because New Year's Eve is one of those things that do not have much locality. If it was yesterday, then we know it's not going to be today, which is kind of opposite of what the locality principle is about. As we can see, some things do have the locality property and some things do not. In computer architecture, we are interested in the locality property of memory references. If we know that the processor has accessed the address x recently, the locality principle says that it is likely to access the same address again in the near future. And, it is likely to access addresses close to x, too. This is what we call temporal locality. Once we access an address we are likely to access the same address again. This is what we call spatial locality. If we access an address, we are likely to access nearby addresses so on. Let's do a quiz about temporal locality to see if we understood it. Consider this code fragment, which is basically iterating through an array and adding every element of that array to the sum. Which of these memory locations has temporal locality in this code? The variable j, the variable sum, and/or the elements of the array arr. More than one might have temporal locality so check all of those that do. Let's look at the answer to our temporal locality quiz. In this code, the question was which of these memory locations have temporal locality, and the choices were J, sum, and the elements of arr. Temporal locality says that once we access a memory location, were likely to access it again. For the variable J, this is true, we are using it here, here, here in every iteration of the loop. So the first time we access it, if we conclude that it will be accessed again soon, we will be right. Actually, once we begin accessing its going to be accessed a lot. It's going to be accessed to initialize it, to check if it's less then a 1000, to access the RA, to increment it, to check, to access, to increment many, many times. So this variable definitely does have temporal locality. The variable sum, same thing. It is being initialized and then in every duration of this loop, it is going to be red, so that we can add something to it, and then written back to memory. So this definitely also has temporal locality. Once we access it, we can conclude that it will be accessed soon again. For each element of the array arr, however, and note that the array has many, many memory locations, we do not have temporal locality. Element arr of 0, once it's accessed in the loop, that's the only time we access in this loop. We will access out of one, out of two etc., but for each element it's accessed only once. So, if we conclude that when it's accessed it's going to be accessed again soon, we would not be correct. So, this choice should not be selected. This variable does not have temporal locality in this code. Let's do another quiz on locality this time about spacial locality. If we have the same code as in the previous quiz, and the question now is which of these memory locations has spacial locality in this code? Check all of those that do. Let's look at the answer to our spatial locality quiz. The question is, which memory locations have spatial locality? Remember that spacial locality says that once we access a memory location, we are likely to access nearby memory locations soon. For the variable j, we keep reaccessing j, but we don't know what's close to j so we can not really say that j has spatial locality. If we access j, there is a lot of temporal locality but not necessarily any spatial locality. Same for sum. We are accessing sum very often by itself but we're not really sure whether we're accessing anything that is close to sum. For elements of arr, we have seen that there is no temporal locality. Each element is accessed only once in this loop. However, there is a lot of spatial locality. Once we access arr of zero, in the next iteration, we'll be accessing arr of one, and then, arr of two and so on. So it is true that once we access a memory location in this array, we're likely to access nearby location soon. So this variable definitely has spatial locality. It turns out that the way this program is compiled, usually means that there will be some spatial locality between j and sum. J and sum are likely to be allocated by the compiler on the stack near each other, so when we access sum and then access j, that might be an example of spatial locality. So now that we know what locality is, let's see how do we use it to improve data accesses. First, we will look at an example that considers borrowing books from the library. In this example, the library represents a data repository that is large. So a lot of data there. But, it is very slow to access. We need to go there, find the book in the building, check it out, bring it back home, and so on. When we're borrowing books from the library, typically there is a lot of temporal locality to it. For example, if we are a student, we might need to look up the definition of locality very often, while we're studying. And usually also, there is a lot of spatial locality to using books. For example, if we look up some computer architecture definition once, we are likely to be looking up other computer architecture stuff too, in the near future. So continuing our library based locality and data access example, we have seen that the library's large but very slow to access and that accesses to information in the library have temporal and spatial locality. So when a student needs a piece of information from the library, what should he do? Should he go to the library, find the information, read it and then just go back home? Should he borrow the book, take it home so that future accesses to the information in the book are much faster? Or should he take all the books and then build a library at home? Let's consider each of these three options. If we go to the library every time we need something, we're wasting a lot of time. Finding the books, and then looking up only one piece of information, and then returning the book because we know that there is both temporal and spatial locality, which means we will be probably reading the same book, and nearby books, very often. So, the problem with this approach is that it does not benefit from locality, and this is why typically, people do not use it when they need to study a topic, and use library resources. If we go the library and borrow the book, we bring it home, and now we have the book on the topic that we're currently working on close to us. So this is an approach that does exploit temporal and spatial locality and eliminates the problem of the library being large and slow because we have very few books borrowed at home so we can quickly find what we need in them. This is why most people will use this option. The approach of taking all the books and building a library at home is very expensive and also provides very little benefit. Yes, it saves us the trip to the library and back, but it doesn't save us the time of searching among many books, finding them on the shelves, and so on. So really, we'd rather have relatively few books that we're currently interested in, than have many book and have to look them up very slowly. So just like people when faced with these choices about the library usually choose the second option because it's a good trade off between being able to find information quickly and not being overwhelmed with a quantity of information that we have brought in, when we have a processor that needs to access main memory we will use the same approach. Instead of going to memory for every single memory location, we will bring the content of memory locations that we are interested in closer to us, but we will try not to bring too much close to us because that will result in a very slow structure close to us. So, we will, again, have slow accesses. So what you want is something small that keeps only the information we are really interested in, and that small repository of information is called a cache, when we're talking about memory accesses. So now let's try to apply the concept that we have learned to an actual cache. We know that the main memory in a computer is large and very slow to access compared to processor speed. And also we know that there is lots of spatial and temporal locality present in our data accesses. Most programs exhibit a lot of spatial and temporal locality. So for an access to a memory location what do you think we should do? Should we go to the main memory for every access? Should we have a small memory inside the processor core, and then we bring stuff we access there so that next time we need it, we can find it quickly? Or should we have a huge memory next to our processor chip, because it won't fit on the processor chip? And then, as we read from the main memory, we bring everything into this memory that is next to us. Let's look at the answer to our cache quiz. These three are very similar to the options we have when we want to get books from a library. This is like going to the library every time we need to look up something, and clearly we don't want to do that. Main memory is just like the library, slow and very large. So if we go to the main memory every time, we're spending a lot of time just waiting for the main memory to respond with data. Having the small memory inside the processor core and bringing the stuff we access there is the approach of basically caching the memory locations. And this is very similar to borrowing the book from the library so that we can use it for awhile while we're interested in a topic and then return it. And finally having a huge memory next to our processor and then bringing everything in there is solving some of the problems of going to the main memory because this memory is hopefully somewhat closer to a processor. But this memory is still going to be large and thus very slow. So this is very similar to just building a library closer to home. Yes, you are saving some time on each access but most of the time you spend per access is still there simply because it takes a while to find data in such a large memory. So caches are exploiting spacial and temporal and locality. And overcoming the problem of having a slow and large main memory, just like borrowing books from the library is exploiting spacial and temporal locality, and overcoming the problem of accessing a huge library slowly. So, now that we know that the cache is a small memory inside our processor where we are trying to find the data before we go to the main memory. Let's take some time and recap what we need out of cache. We need it to be fast. Which means it has to be small. Because it's small, not everything will fit. There will be a lot of memory locations that we do not have room for in our cache. So when a processor wants to access some memory, we can have a cache hit, which occurs when we find what we are looking for in the cache. The memory location we are interested in was already brought from the memory to the cache before. So now we're accessing it, and we can just find it in the cache. And the access is then very quick, so this is what we want. We want fast accesses for most of our memory accesses, and that happens when we have a lot of cache hits. But because the cache is very small and not everything fits in it, we can also have a cache miss. This happens when what we're looking for is not in the cache, and in that case, we have to access the slow memory. When we have a cache miss, we want to copy this location to the cache so that next time we access it, and hopefully we will because of locality, we will have a cache hit. So it is necessary to have cache misses because that's how we really bring stuff into the cache in order to have hits. The more locality we have, the more cache hits we will have and the fewer cache misses. Because one miss will bring the data into the cache and then hopefully we will use it many, many times. When we a cache miss, that is slow, that access is going to take some time. But that should happen very rarely. Most of the time when we need something, we find it in the cache. Or, at least we hope to do that. And we want to design our caches so that that's how it works. So let's see what does it take to have a good cache? Or what are the properties of a good cache? We want our system to have a good average memory access time, or AMAT. And this is the access time to memory as seen by the processor. The AMAT is equal to the cache's hit time, how quickly can it return the data when we do have a cache hit, plus how often do we have a miss, times the miss penalty. We want the AMAT to be very low, the lower the better. This means that we want the hit time to be low, which means we want a small and fast cache because that will contribute to having this be low. We want a low miss rate, which means we want our cache to be large and/or smart. If it's a large cache, more stuff will fit in it, so we will miss less often. Or if it's smart, it will know better what to keep inside, so, again, we will have fewer misses. The miss penalty for simple cache is simply the main memory access time. This is going to be very large, usually tens or even hundreds of processor cycles. So really when we are designing caches, we're trying to balance the hit time and the miss rate. Small and fast versus large and intelligent, and intelligent typically means slightly slower. So some caches can be extremely small and fast, have very good hit time, but sometimes that's at the expense of having a larger miss rate. Some caches will have a relatively large hit time but extremely low miss rates so that we can reduce the overall contribution of main memory accesses that contribute a lot each time we have them. Sometimes we also talk about miss time, which is the overall time it takes to have a cache miss. A miss time is simply the hit time followed by the miss penalty, because we try to find what we have in the cache but fail, and then we still have to pay the miss penalty. We can also think about miss time is the memory access time when we are missing. So miss rate is one. So again, it will be the hit time plus the miss penalty. So another way to write the AMAT expression would be to say that it is 1 minus miss rate times the hit time. So this is how often do we have the hits times the time for the hit. Plus how often do we have the misses times the miss time. Usually we want to use this expression, simply because the miss time typically includes checking whether what we have is in the cache followed by whatever we need to do on a miss. So somehow, every time it will have the time it takes to hit but sometimes we will also pay the miss penalty. Now that we have seen the relationship between hit time, miss time and cache performance, let's do a quiz about them. Which of the following are true for a well-designed cache? Is it true that hit time is less than miss time, that hit time is larger than the miss penalty, that the hit time should be equal to the miss penalty or miss time is larger than miss penalty? Note that more than one, possibly all of them, might be true or maybe none of them are true. So check all of those that are true for a well-designed cache. Let's look at the answer for our hit time quiz. In a well-designed cache, hit time is significantly lower than miss time. Miss time includes the hit time and the miss penalty. Typically, we want the hit time to be very small and that means that the miss time should not be anywhere close to the hit time. And if there is any miss penalty, miss time will be larger than hit time anyway. So this should be true for every cache, even if you didn't design it very well. Hit time is larger than the miss penalty, we definitely don't want that. The miss penalty is the memory access time, if our hit time is similar to that or larger than that, then the cache is not going to be useful at all. So this should not be selected. Hit time equals to the miss penalty. No, we want the hit time to be significantly less than the miss penalty. If it is equal to the miss penalty, we might just as well go to memory every time, because that's what the miss penalty is about. So we should not sell like this. And finally, miss time larger than the miss penalty. Miss time is equal to the hit time plus the miss penalty, which means that the miss time should always be larger then the miss penalty anyway. So this is true for every cache not only a well designed one. As we can see, for a cache to be beneficial, it needs to have a low hit time. In fact, in a well designed cache, we can say that the hit time is much less than miss time while the miss time and miss penalty should be similar to each other, because the hit time is so small and the difference between the two is the hit time. Let's also do a quiz about the miss rate. Keep in mind that the hit rate is equal to one minus the miss rate. The miss rate is how often do we have a cash miss. The hit rate is how often do we have a cash hit, and the two adapt to one because we either have one or the other. So which of the following are true for a well designed cache? Is it true that in a well designed cache the hit rate should be larger than the miss rate, the hit rate should be less than the miss rate, that the hit and the miss rate should be balanced, as close to equal as possible, that the hit rate should be almost one, that the miss rate should be almost one? Again, more than one, possibly all, possibly none of them might be true, so check all of those that are true for a well designed cache. Let's look at the answer to our miss rate quiz. In a well designed cache, we want the hit rate to be high as possible and the miss rate to be low as possible because the hit rate determines how often do we have just the hit latency and the miss rate is how often we have to pay the full memory latency. So, we definitely want the hit rate to be larger than the miss rate. We definitely do not want the hit rate to be smaller than the miss rate. We don't want them to be equal to each other, because that means half of the time, we are going to memory, we won't hit much more than that. We want the hit rate to be almost 1 because that means that almost always we are going to have a cache hit. And we definitely do not want the miss rate to be almost 1 because that means that we are almost never hitting in our cache. So, we have seen that we want the cache to be small and fast, but not too small because then it keeps very little stuff. So, that it has very few hits. So, what are the cache sizes that we have actually observed in real processors. There is a complication there, in that modern processors. Usually have several caches not just one. So depending on which cache are we talking about it might be larger or smaller. So let's look at caches that we call L1, or level one caches which are the caches that directly service the read and write requests from the processor. So when the processor wants to read or write, it first goes to the L1 cache to check if it has a hit. If the L1 cache has a miss, then things get complicated because before we go to the main memory, we go to other caches. And in several lessons, we will actually see how that works. But for now, let's just see how big are these level one caches. In recent processors, typical sizes of the L1 cache have been 16 to 64 kilobytes. This is large enough to get about 90% hit rate, meaning only one-tenth of all the accesses from the processor end up going beyond this cache. Yet, these are still small enough to have the hit time that is equal to 1 to 3 processor cycles. So we spend very few process cycles waiting for the data to come back from this cache, if it's a hit. Recall that memories have hundreds of cycles, co this is very, very fast compared to the memory. Now that we know what the purpose of the cache is, and how big approximately it needs to be, let's look at how it is organized internally. The question we need to ask ourselves is how do we determine if we have a hit or miss, so we need to know what data do we have in the cache and then when the processor gives us an address, we need to determine whether that data is or is not in the cache and we need to do that very quickly. And the other important question is, how do we determine what to kick out from the cache, once we are out of room in it, and we need to put more stuff in it, because we are just having a cache miss. So, for the first question of, how do we know whether we have a hit or a miss. We want something that is very fast. That means it needs to be a table of some sort that we can quickly index with some bits of the address and it tells us what we have in the cache and if we do have what we are looking for then it gives us the data. So conceptually, the cache is a table which we index with some bits that we take from the address of the data. So some stuff here needs to tell us whether we have a hit and the rest of the stuff here is the data we want if we do have the hit. How much data we have in each cache entry is called the block size or the line size, because the entry in the cache is also called a line and they tell us how many bytes are in each of these entries. If we have only one byte per entry, then every address will map to different to entries. If you have a block size of one, that means the entry in the cache is only one byte. Every single byte address will have a different entry. That creates several problems. One, usually the processor can issue accesses not only to a single byte, but also for example, loadward and storeward will access a four byte location. So for a single access, now we would need to look up four different lines in the cache. That complicates and slows down the cache a lot, so we definitely don't want the block size to be this small. We want the block size to be at least as large as the largest single access that we can do in the cache. And hopefully slightly larger then that, just so that when we do a loadward or storeward we find the data in the same cache block most of the time. The next thing that we want to consider when deciding the block size, is spatial locality. If we miss in the cache on one axis, we will bring in an entire block worth of memory stuff into the cache. So if there is no spacial locality, we want to bring in just what we are currently accessing. But if there is any spacial locality, we want to bring more than what we are just accessing. So typically, block sizes of 32 to 128 bytes work well, both from the perspective of their larger than a typical access. And also they capture much of the spacial locality that exist in programs. Now let's look at block sizes of something like 1KB, in that case what happens is we fetch a lot of data from memory every time we have a miss, and if there is not enough locality a lot of the data will not be used, so we are occupying space in our cache. Remember that the cache is only 16 to 64 of these kilobytes. So if we bring in a block like this, we are consuming a significant part of the overall space in the cache. And we are doing it probably with data that we won't use much. So it turns out that for level one caches at least, we don't want the block size to be too large. So block sizes of 32 to 128 kilobytes are a balance between getting use of spacial locality when we bring stuff in, versus not being dependent too much on there being a lot of spacial locality because some programs don't have that much of it. So let's do a quiz on block sizes now. Consider a cache that is 32 kilobytes in size and has a 64 byte block size. So let's consider a program that accesses variables X1, X2, etc., up to some XN, and these are scalar variables, not part of an array, and let's assume that the program has lots of temporal locality when accessing these variables but no spatial locality. The question for you is what is the largest N, the number of these variables, that still results in a high hit rate in this cache? Put the number here. Let's look at the solution to our block size quiz. We have a 32 kilobyte cache, with 64 byte block size. And the program that accesses N variables, that have lots of temporal locality. Meaning each of these variables is accessed often in the program, but no spatial locality, meaning they're nowhere close to each other. The question is what is the largest N, the number of these variables, that still results in a high hit rate? Now X1, when we bring it into the cache, we will bring an entire block worth of stuff around it, and because there is no spatial locality, that means that none of the other variables will be in that block. When we access X2, same thing, we will bring 64 bytes worth of stuff that doesn't include any of the other variables. So every time we access one of these variables, we will need 64 bytes worth of space in the cache. If we want to have a high hit rate, because these variables have lots of temporal locality, we want all of them to remain in the cache. That is, we want all of them to fit in the cache. So the largest N that still results in a high hit rate Is how many blocks can we fit in our cache, really? It's a 32 kB cache, there are 64 byte blocks, how many blocks do we have, so the answer is 32 kB divided by 64 bytes, which gives us 512. So the answer here is 512. If we have 512, or fewer variables, then the fact that each one of them has its own cache block in the cache, although the variable is probably only 4 bytes in size, is not going to hurt us in terms of hit rate, but if we have more than 512 variables. Then even though there is plenty of space in the cache, if it's four byte variables, 512 variables is two kilobytes worth of actual stuff that we are using. The block is much larger than that, but because we are fetching an entire block when we have a miss. We will not have a good hit rate if we have more than 512 of these variables. In order to be able to have more variables than this, we really need them to be close to each other, so that we can benefit from spacial locality. Meaning that a block we fetch for one variable needs to include some of the other variables. So now that we know that when we have a cache miss, we fetch an entire block worth of stuff from memory, let's talk about where can these blocks begin in memory. One option is a block can begin anywhere. So let's say we have 64 byte blocks. That means that a block, for example, might go from address 0 through 63 bytes. Or it can be from 1 through 64. Or it can go from 2 to 65, and so on. So pretty much if the starting address of the block can be anywhere, then these are the possible blocks that we might fetch in our cache. This complicates things a lot. First, our cache is a table. We want to index into the table using some bits of the address that we are accessing. But let's say that what we are accessing is the address 27. The problem is the block can actually be found wherever the beginning address of the block maps. So block 0 to 63 might map here in the cache. Block 1 through 64 might map somewhere else in the cache and so on. So basically, there are many possible places where the address 27 might be, simply because each of these blocks might map somewhere else depending on what its beginning address is. Second, there is another complication, which is these blocks overlap. So the block 0 through 63 contains stuff that overlaps with most of what 1 through 64 contains. So this, other than this, is the same stuff. Now, the problem is when we are reading, it's easy. We just need to read one of these copies for 27. But when we are writing, we need to determine which of the copies exist and write to all of them so that reading any of them will still be okay. So pretty much, we don't want this to happen. We don't want to have multiple places where the same byte address might be. So in order to both reduce the complexity of accessing the cache, and eliminate the problem of this repetition of stuff in the cache, we will only have caches where the blocks start at block aligned addresses. So for a 64-bit block, we will have a block that contains data from 0 to 63 bytes. The next block begins at byte 64 and goes to 127 and so on. So pretty much any byte address can be found in only one of the possible blocks. That also allows us to simply use some bits of the address that tell us which of these blocks are we talking about, and index into the cache, and find it there. So there is only one place in the cache where each of these will fit, and so on. So for all reasonable caches, we will really only consider aligned blocks. Meaning, you cannot fetch any 64 bytes, or 32 bytes, or whatever block size is from memory into the cache. We need to fetch the 64 byte block that contains the data and is block-aligned. So now that we know that cache blocks need to be at aligned memory addresses, let's look at what the memory and cache look like in terms of blocks. Our memory can be seen as just a huge array of memory locations. Let's say this is location number 0. And let's say that each location is 4 bytes wide. So the next location is at address 4. And then 8, and so on. Let's say that our block size is 16 bytes. That means that a possible block in memory is this. This would be the first block. This would be the second block. This would be the third block, and so on. A cache, then, can be considered to be a number of slots where a block can fit. In this case, we have just a two-block cache. So, the cache size is just equal to two block sizes. So memory has blocks of data, and our cache has lines which are basically slots where a block can fit. We can say, for example, that block number 0 can be fetched into the cache and placed in line number 0. So when we want to make a precise distinction between the space in the cache where we can put a block and the actual memory content we put there, the space in the cache is called a line. And the stuff we put there is called a block. The line size and the block size are the same. So a line is effectively a slot where a block might fit. So now that we have seen what the cache line is and what its size does let's do a quiz about that. So which of these are not good line sizes in a 2 kB cache? 1 byte, 32 bytes, 48 bytes, 64 bytes And 1 kB. Again, we are looking for those that are not good line sizes in a cache of 2 kB size. Let's look at the answer for our cache line size quiz. We are asked to determine which of these are not good line sizes for a 2 kilobyte cache? A 1 byte line is not a good one because it doesn't exploit spatial locality and because Word size accesses will need to access multiple blocks or multiple lines in the cache. So this is definitely not a good size. Thirty-two bytes is a good size. It exploits spatial localities, it's not too large, and it's a power of 2, so we can easily locate blocks. If I give you an address, you can easily find what block does it belong to. Forty-eight bytes is a good size, as far as, it's not too large or too small but it's not a power of 2. So if we are given an address and we need to find out what is the block that contains that address, we need to divide by 48, which is much more complicated then dividing by a power of 2. For example, for a 32 byte size, the lowest 5 bits of the address tell us where we are within the block, this kind of telling us where within the 32 bytes of a block we are and the upper bits of the address basically are the block number. We divide by 32, simply by discarding the lowermost 32 bytes. In contrast, dividing by 48 bytes would actually need to be a division by 48. So this is not a good line size. Sixty-four bytes is also a reasonable block size. It's larger then 32 but still a lot of them fit in a 2 kilobyte cache. One kilobyte is good because it's a power of 2 but only two of them fit in a 2 kilobyte cache. So this is not a good line size because it is too large given that the cache is only 2 kilobytes in size. What we want to have as a block size is something that is a power of 2, exploits spatial locality but still a significant number of them fit in our cache. So let's talk about how we determine what the block offset is and what the block number is given an address. Let's say that the processor produces a 32 bit address, with bits numbered from 0 through 31. So this is the address of the location the processor wants us to find in the cache. The cache can be seen as an array of lines each block size in size. In this example, let's look at 16 as the block size. Some bits of this address tell us which block are we talking about and some bits are telling us where in the block are we looking for our data? With a block size of 16 we need to determine how many bits tell us where in the block we are. The block has 16 bytes, 2 to the 4th is 16, so we need 4 bits to tell us where within a block we are. That means that bits zero through three, are going to tell us, once we find the block in the cache, which part of the block to read. The remaining bits, bits 4 through 31, tell us which block are we trying to find. So these bits that tell us where in the block are we going to find our data, are called the block offset. And the bits that tell us which block are we interested in are called the block number. So what we do is we access the cache trying to find the block according to the block number. If we find the block, we use the block offset to get the right data from the block because there is more than one location in the block. So let's see if we can put this to work in a quiz. Consider a cache with a 32 byte block size and a processor that creates 16-bit addresses. If the address in binary is this, there are 16-bits here, so if that is the address, what is the block number that corresponds to this address? And what is the block offset that corresponds to this address? You can put binary numbers here because it's a lot easier that way. Let's look at the solution to our block number quiz. If we have a 32 B block size and a 16-BIT ADDRESS like this, the question is what is the BLOCK OFFSET and the block number. For a 32 B block the block offset will determine where within the 32 B are we. We need 5 bits to tell us that because 2 to the 5 is 32. So we need 5 bits. To tell us which of the 32 bytes are we interested in accessing. That means that the least significant 5 bits of this address, are the block offset. So the block offset is this. The remaining bits of the address are the block number. They're telling us which of the 32 byte blocks contains this address. So this is our block number. So now that we know how to find the block offset and the block number, let's talk about the cache tags, which is how we determine which block do we actually have in the cache. So our cache can contain some number of lines. In this case, we have a four-line cache. We could have something for example, 64 bytes of data in each line. Let's say that any block from memory can be placed in any of these four lines. In that case, when we are given the address by the processor, the least significant bits of the address will tell us where in the line we are after we find the line. But each of these lines in the cache could contain any block number from the memory. So the question is, how do we find whether our access is actually a cache tag or not? And the answer is, in addition to the data of the blocks, the cache also keeps a so-called tag that tells it which block does it have here? In this particular cache, the tag will contain the block number that the cache has in each of these slots. So we need four tags because there could be four blocks in this cache. So given the address, we take the block number part of the address and compare it to each of the tags we have in our cache. If one of these says one, that means that the tag matches the block number and that means that our data is in the corresponding line in the cache. Let's say that this is the situation we have, so we have a tag match here. That means that we have a hit and that our data is here in the cache. Now we can use the block offset to tell us where in that line our data is and we take the data and supply it to the processor. In this particular cache, we have the offset part of the address here, and the block number is, in this case, called the tag part of the address. When we have a cache miss, we will bring the data here and we will put the block number in the corresponding tag so that we can find this block later. We will see later that sometimes the tag part of the address is not exactly the same as the block number, but in this cache it is. Let's check our knowledge of cache tags using a quiz. Which is always true about the cache tags? It contains the entire address of the first byte of the block. It contains at least one bit from the block offset. It contains at least one bit from the block number. It contains some bits from the block offset and some from the block number. Select all of those that the tag always has. Let's look at the answer to our cache tag quiz. The cache tag always contains the entire address of the first byte. We know this is not true. The address that we supply to the cache on an access contains the offset. And the block number. We have seen that the tag only needs to contain the block number part, not the offset. The block always begins at an aligned address, so the first part of the block actually always has zero here. So there is no need to store those zeroes. In the cache as a tag. The tag only needs to give us the bits that we need to identify the block, the bits that we know are always zero are not necessary. So this is definitely not true. It contains at least one bit from the block offset. Again this is not true. We need only the bits that tell us which block we have. The block offset tells us where are we within a particular block, so these bits are never necessary. That also means that this is not true. However, the tag does contain at least one bit from the block number. We have seen in our example that the entire block number can be a tag in that particular cache. In general, it might be fewer than all of the bits, but there is a least one of them. So, this is the only correct answer. Another piece of information that the cache maintains about each of the blocks it has is the so-called valid bit. If we look at the cache with four blocks of data and the tag that tells us which block we have in each of the lines, the question is, what happens when we turn on the processor? At that time the cache contains no useful data. But the data will have some bits here. Even if all of the bits are zero, we need to know that whatever address we produce, it doesn't match any of these blocks here, so that we need to access memory and bring the actual data into the cache. We do this by matching the tag. But what does the tag initially have? If the tag initially just has all zeroes, what if we're accessing address like this where it might happen that this address actually has a tag bit that correspond to zero? In that case the initial tag would match this and we would be accessing this garbage data in the cache. Which hasn't been brought from memory, so we're really accessing something that is not equal to what we should be accessing. Note that any possible value for the tag might match the tag of an actual address. So, it is not that it is wrong to initialize to 0, it's that any particular initial value here, would produce the same problem. So what we do, is we add an additional bit of state to the cache for each line. That tells us whether the tag and the data are valid. Initially, the valid bits need to be set to 0, which means that if our tag matches the address but the valid bit is 0, we treat that as not a hit. So the hit condition is really the tag matches our block number and the valid bit is set to one. By setting the valid bits to zero initially, we make sure that none of these can hit until real data is brought in. And thus, we also remove the problem of initializing the data and the tag. We can leave anything here. So these don't need to be zeroed out as long as the valid bit is set to zero correctly. So when we boot up the processor, we set all the valid bits to zero. And then, when we bring stuff from memory, put the data here, set the tag to correspond to the block's number. And at that time, we also set the valid bit to one. There are several types of caches when it comes to which blocks can be placed where in the cache. The cache examples we have seen so far are the fully associative caches. In this type of a cache, any block from memory can be placed into any line in the cache. So if the cache has room for 16 lines, then we need to search up to 16 tags in order to find whether the block we're looking for is actually in the cache. The opposite of that is a direct-mapped cache where for a given block that is exactly one place in the cache where that block can go. That means that we only need to check one line in the cache if it contains that block because that's the only line that it can possibly be. Different blocks, of course, will still be able to map to different lines, but a particular block can only go into one place in the cache. And then, of course, there is the middle ground between them, which is called a set-associative cache, where there are N lines where a block can be. Now, N typically will be larger than one and smaller than the total number of lines in the cache, so we can really view the direct-mapped cache, as a case of a set-associative cache, where N is one and a fully associative cache is an extreme case of a set-associative cache where N is equal to the lines in the cache. But what we normally call a set-associate cache has a relatively small N compared to the number of blocks in that cache, yet that N typically is two, four or eight so it's larger than one. Now let's look at each of these types of caches. We have seen a glimpse of fully associative caches, so now let's go from the other direction, which is let's first look at direct-mapped caches. If this is the memory, and it contains blocks 0, 1, 2, 3, 4, 5, 6, etc., and these are the block numbers, not the addresses. So I'm assuming that this thing here is an entire block worth of memory which contains multiple locations. And let's say we have a cache with room for four blocks. We can number the lines 0, 1, 2, and 3, and in a direct map cache, block 0, if it's in the cache at all, needs to be in line number 0. Block 1, if it's in the cache at all, will be in slot number 1. Block number 2, if it is in the cache at all, has to be in this place here. Block number 3, if it is in the cache at all, has to be here. And now block number 4 will, again, get to map to line number 0. Block number 5 maps to line number 1. Block number 6 maps to line number 2, etc. So, pretty much for any given block address there is only one place in the cache where that address can be. If we look at the memory address, the least significant bits are still the block offset. They tell us where in the block are we, assuming that we find the block. The block number now contains some bits that tell us where in the cache this block can be. The index bits need to be sufficient to tell us which of the lines in the cache are we talking about. So if there are four lines in the cache, then we need two index bits to tell us where a block can map. And that is why in the block number it was 0, 1, 2, 3, 0, 1, 2, 3, etc. It was determined by the lowermost bits of the block number, and the remaining bits of the address are now the tag. Now, there is an interesting question of why the tag does not include the full block number. So, why doesn't the tag cover also the index bits? And the answer to this is that the tag needs to tell us which of the blocks do we have in this place. And the answer is that the tag needs to identify which block we have in the cache. Considering that the block would some in those bits has to be in one place in the cache, the tag for that place in the cache doesn't need to tell us what the index bits are. By looking at this particular line we have already determined that these index bits have to be the ones that correspond to that line. So all we have to do is tell us which of the lines that can possibly be here do we really have, and for that we only need these bits, because the index bits are pretty much predetermined once we are already looking here. In other words, if we stored the index bits in the tag, all of the tags that we could possibly put here would have the same index bits, which means we don't have to store them, because we already know them. So let's look at the upsides and downsides of Direct-Mapped Caches. When we access the Direct-Mapped Cache, we need to look in only one place. That makes this cache fast. If we need to look in more than one place, then we need to wait for all of those to be read out. We then need to figure out which one of them, if any need has it and so on. With the Direct Mapped Cache, we just check in one place. If it's there, we have it. If it's not there, then it's a miss. So we can very quickly determine whether we have a hit or not. So it's very good for hit time. It is also cheaper than more complex caches because of it. We only need to do one comparison on every access to the cache. So we only need to have one type comparator. One valid bit checker and so on. And it also makes it energy efficient. We only need to do one type of comparison, and one valid bit check per access, which spends less energy than if we had to check more than one. The down sides of this cache are related to the fact that the block must go in one place. To see why this is a problem consider what happens if the processor frequently accesses blocks A and B like this, where both A and B map to the same place in the cache. When we access A, we will bring it to the cache. When we access B, we need to put it in the same place where A is currently. So A needs to be kicked out. There is no choice about what we kick out, A has to go out. When we access A again, B needs to be kicked out and so on. So the downside of this cache is that it can have these conflicts, where particular blocks that we want to use. Fight over the one spot in the cache, although the cache has plenty of other spots that are under utilized. What conflicts do is they increase the miss rate. So we can have a cache that has a fast hit time, but suffers in the miss rate in return. Let's check our knowledge of direct-mapped caches and how they handle conflicts. Let's consider a 16 kilobyte direct-mapped cache with 256-byte blocks. The question is, which of these addresses conflict with 1234567 hex? The first choice is 12345677. The next choice is 11335577. The next choice is 11115678. And the last choice is 12341666. So which of these will conflict with this address in a 16 kilobyte direct-mapped cache with 256-byte blocks? The key to solving problems like this is to figure out how does the address breakdown into offset, index and tag this. Conflicts will occur, where different blocks have the same index bits. In this particular cash, 256 byte block size. Means that there are 8 bits to the block of set. In hex, conveniently, that means that the least significant two digits are the offset and we can safely ignore those, because they are not apart of the index. Now we need to figure out how many bits are in the index. We have 16 kilobytes and 256 byte blocks. The number of blocks in the cache is this divided by this and that gives us 64 blocks. 64 is 2 to the 6th. So we need the next 6 bits here to tell us what the index is. The 6 bits will be contained in the next 2 hex digits. So for 56 hex, we can write it out as binary, this is 5, this is 6 and we are interested in these bits here. So we're really looking for, which blocks of these have the same bits here. Those that have 56, definitely we'll have the same index bits. So we will definitely have a conflict here and here possibly. But in order to actually have a conflict, it needs to be a different block. This address here does not conflict with this one here, because they're actually the same block. They have the same block number. So this is not a conflict. They don't have the same place in the cache, but they're actually the same block, so it's not a conflict among blocks. This, however is a different block number that definitely maps to the same index, so this is a conflict. 55 looks like this, these are the 6 bits, they're not the same. So this block will not conflict with the one here. Finally, 16 looks like this. These are the bits that we're interested in and they do, indeed match the ones of this block. So this goes to the same place in the cache as this and it's a different block, because this is 123416 and this is 123456. So this will also conflict with our block. Let's do an example of direct-mapped caches and how they're accessed. Here is a direct-mapped cache with eight lines, each of which is 32 bytes in size. The processor produces the following sequence of accesses, one at a time. 3F1F hex, 3F2F hex, 3F2E hex, and 3E1F hex, in this order. The question for you is what is the content of the cache after these four accesses? Let's look at the solution for our Direct-Mapped Cache map access Quiz. We have these four axises in this cache. The cache block is 32 bytes. And there are eight blocks in the cache. So the address break down will be the offset bits will be 5. Because we need five bits to tell us which of the 32 bytes we are talking about. The index bits will be three, because we need three bits to tell us which of the eight slots in the cache to use. And the remaining bits are going to be the tag. Now let's consider which of these map where in the cache. As you can see, the least significant eight bits are going to determine where in the cache something goes. So we can conveniently look at just the least significant two hex digits. 1F 0001 1111. Five bits are the offset, and 000 is the index. So this goes here. 3F2F has 0010 here and then 11111 but it goes to index 1. 3F2E has two and then E, it also maps here. Because it is actually the same block as this one. They will both be in this slot here because really the entire block 3F2 something is there. And finally 3E1F is going to map to 001 F, so the index is zero, and it's a different block from the one we had previously. So this one will be replaced by this. The final content of the cache really is that the block 3E1 something is here and 3F2 something is here. Now let us look at set-associative caches. We say that a cache is N-way set-associative when a particular block can be in one of N possible lines where it can go. So the overall cache now is divided into regions called sets. In this case we have four sets. A given block can go to one of its sets. So pretty much we use some bits of the block's address to tell us which set does it need to go in. Within a set there could be a number of lines that can contain a block. In this case we have a 2-way set-associative cache because there are two lines in a set, so a given block could be in either of the two lines. Note that different blocks might go to different sets, but a particular block will map to one set and then it has a choice between the two lines there. In this particular cache, we have eight lines, four sets, and two ways. Note that N is 2 here. So we say that the cache is 2-way set-associative, not when it has two sets, but when there is two blocks in each set. So now, let's look at how do we form the offset index and tag in the address for set-associative caches? This is a two way set-associative cache that has four sets. When the processor produces an address, the least significant bits that tell us where in the block we are, are still determined by the block size. The indexed bits are the ones that tell us which set are we talking about. So the number of index bits is determined by how many sets we have. In this case, we have four sets, so we need two index bits. The remaining bits are the tag. Like in a direct mapped cache, once we have placed the line in for example set zero, we know that everything that maps to set zero will have bits zero zero here, so we don't have to keep that in the tag. In set zero, we only need to make sure that what we now need, matches what we have in the uppermost bits, not in the indexed bits. Because the fact that we're looking at set zero, means that we have already determined that these bits are zero zero, and everything that we could have placed here, also has zero zero bits in the index. Interestingly, a direct-mapped cache of the same size will have one more index bit, so it would have slightly smaller tag. Now let's do a quiz with the 2-way set-associative cache. Let's say we have a cache with 32 byte block size, and four sets with two blocks each. So that's why it's 2-way set-associative, every set has two ways. If the processor does these accesses in this order, what is the final content of this cache? Let's look at the solution to our 2-way set-associative cache quiz. As before, the answer to this type of question requires us to figure out which bits in the address determine what in the cache. So the address will be broken into offset, in this case 5 bits, index, in this case, two bits to tell us which set we have, and a tack, seven bits are used by the index, and the offset, with means, that the index will be determined by the least significant two digits. So we can figure out that 0003 is like this. Five bits are the off set, the next two bits are the index. So this maps to set zero, and we will put F303 there. Next one is F503. It maps to the same set but this set now has two places, so we can actually put F503 in the other line in that set. Next, we have F563. 6, 3 is like this. This maps to set number 3, and finally EF63, is going to map to the same set, and we'll put it here. So as you can see, with the 2-way set associative cache, we can put more than one block that maps to the same place in the cache and not have conflicts. This is why we want to have a 2-way cache or more because it reduces the number of conflicts. However, it complicates the tagged checks because now when we're looking for something we have two places to search before we're sure whether it's there or not. And finally, let's look at the fully associative cache. If we have an eight entry cache, a fully associative cache is one where any block can map to any of these lines. In this case the address breaks down into the offset part. The number of index bits is the number of bits we need to tell us where in the cache we can be. But because we can be anywhere in the cache, we don't need any bits to tell us that. So there are no index bits when using a fully associative cache. All of the remaining bits are simply the tag. As we already said before, direct mapped, and fully associated caches can be considered to be just special cases or set of associated caches. So for example, a direct mapped cache, is actually a 1-way set associative cache. And a fully associative = N-way SA where N = # of lines. For all this caches, the address that the processor supplies is broken into the offset, index and tag components. The number of offsets bits is the number of bits that we need to tell us where in the block size we are. So it's log2 (block size). The number of index bits is log2(# of sets) we have. In a direct mapped cache, that is the number of blocks we have. In a fully associative cache, that's 1, so log2 of that is 0, and we have no index bits. And the tag bits are the remaining bits. When you're trying to figure out which bits are the index bits, we actually need to know which bits are the offset bits, too. Because knowing that we have, for example, three index bits doesn't really tell us whether they're here, or here, or here. That depends on the offset bits. So make sure that when you're determining these bits, you do it in that order. First figure out how many offset bits then the next so many bits are indexed bits and then the rest is the tag. Now that we know how caches find data when we are looking for it. Let's talk about what happens when we need to replace something from the cache to make room for new data, because we have a miss. So the situation, when we need replacement, is the set where the data needs to go is full. We have a miss, so we need somewhere to put the new block that we are bringing in. And we need to put it in that set. And the question for cache replacement is which of the blocks in the set do we kick out to make room for the new one? Possible replacement policies are, random. Just pick a block randomly from those that are in the set. FIFO, which kicks out the block that has been there the longest. So in FIFO, let's say we have two blocks. We bring in A, then we bring B. When we want to bring in C, we're going to kick out A because it's been there the longest. Then we're going to kick out B to bring D, and so on. The next policy is LRU or Least Recently Used. We want to kick out the block that has not been used the longest. So we definitely don't want to kick out the block that is the most recently used. And then the next most recently, and so on. So pretty much we track which block has been used more recently. And when we need to replace something, we choose the block that we haven't used for the longest time. It turns out that LRU is a really good policy. So, there are several policies that are trying to approximate it because this is not easy to do. One such policy, for example, is NMRU or Not Most Recently Used, which tracks just which block has been used the most recently, and then picks randomly among the remaining blocks. So pretty much, don't pick the block that was just used. So how do we implement the least recently used policy that works really well? And it works really because it exploits well locality. The most recently used thing is likely to be used very soon afterwards. The thing that we haven't used for a while means that there is less reason for locality to kind of cause it to be accessed again. Let's say we have a four way set associative cache and let's just track what's happening in this one set. So there is four blocks in it. For each of these blocks we keep the tag and the valid bit. And in order to track which block was accessed when, we are going to also have an LRU counter for each block. These counters take values that correspond to the size of the set. In this case, 0 through 3 because there are four things in the set. So let's initialize them to 0, 1, 2, and 3. They have to all be different all the time, and we will see how that is maintained. So, we start like this. When we want to replace something, we will replace the block whose count it is currently 0 because that's going to be the least recently used block. So, let's say we access some block, A. And we need to put it somewhere. We will choose the place with the 0 counter and put it there. Now that the block is placed there, the processor accessing it and suddenly this becomes the most recently used block. So, it's counter now needs to be 3. In order to make sure that all of the counters still have different values that means that all of the other counters now need to be decremented. So, the one that was 1 becomes 0. 2 becomes 1, and 3 becomes 2. And now, let's say that we access sum block B. We put it here, because the counter is 0, now, here. When B is accessed after putting it here, that counter becomes 3. This one becomes 2. This one becomes 0 and this one becomes 1. We will now put in block C here. That becomes 3. This becomes 2. This becomes 1. This becomes 0. D, 3, 2, 1, 0. Now we're back to this situation. So I'm going to clean up here. And now, if we access E, we're going to find what was the least recently used block. And A, at this point, is because it's been accessed the least recently. So A will be replaced by E. And the LRU counter here becomes 3 which brings these one notch lower. So this is the state we have. Now let's say that B, which is the least recently used block now, is re-accessed. What happens then is this counter needs to become 3. All of the other ones now need to be changed lower. What happens if the most recently block, let's say, B is accessed again? Then this count just stays at 3 because that indicates that this is already the most recently accessed block. What happens if block D, which is neither the most nor the least recent, is accessed? Well this counter needs to become 3 and now we have an interesting situation. We don't just lower all the other counters a notch down. We take the ones that were above the original value of this counter. So we take the counter that is above 1, and those are the ones that get a notch down. But the ones that are below one stay the same. That ensures that the counters still have different values. Lets do another one of those where lets say block B, that is almost the most recently used block, is accessed. In that case this counter needs to become 3. All counters that were above its original value of 2 need to be decremented. So, this becomes 2, but these two stay the same because they're below 2. So again, we have this situation. So now you know how to maintain the LRU counters properly. After every step you can verify that the counters have different values. If they don't, then you probably made a mistake somewhere. So as you can see maintaining LRU is relatively complicated. For an N-way set-associative cache, we need N counters whose size is LOG2(N). In this case the size of the counters is 2 bit because they need to have a number between 0 and 3 to tell us where are we in the order of access in the set. So it's a four way set associative cache, that means we need two bits per counter and four counters. So for highly associative cache it's something like 32 way associated. You would need to have 5 bit counters and 32 of them per set. So this was the cost. It's pretty high. Worse, there is an energy problem. We need to modify up to N counters on each access, even cache hits that can happen very frequently. So the LRU approximations try to keep fewer counters and do fewer updates on cache hits. Now that we have seen how LRU works, let's do a quiz about it. Let's say that this is a single set in a cache that is eight way set associative, so there are eight lines in the set. Let's say that the blocks we have are A, B, C, D, E, F, G, and H. And let's say that this LRU counters that correspond to these are like this where 0 is the least recently used. What is the new content of the cache, after we perform axises to A, then B, then A again, then D, then K? You only need to say which blocks are in each line in the cache, not what their LRU counters are. But you do need to track the LRU counters in order to arrive at the correct answer. Let's look at the solution to our LRU quiz. We had this situation in the cache and these counters before these accesses. And now we're performing accesses to A, B, A, D, and K. A is the most recently used block. We reaccess it, none of the counters change. We access B, Bs counter becomes seven from three. That means that all the counters above three will be decremented. Two stays the same. One stays the same. Zero stays the same. So these are the new counters after Bs access. Next, we access A again, this counter becomes 7. All the counters above 6 are decremented, so this is really the only change. Now D is accessed, this counter becomes 7. Those above 5 are decremented. The remaining ones stay the same. All of these have been hit so far so nothing has changed in the cache we still have exactly the same stuff there. The only change will be when we access K which is in this. So now we need to choose who goes out. And the one that goes out is the one whose counter is zero. So H is replaced by K. The other blocks are remain the same as they were and this is the final answer for this quiz. The final aspect of how the caches work, concerns the write policy. They're really two parts of it. First is the so called allocate policy. Which is about do we insert into the cache? Meaning allocate and entering the cache for blocks that we write. So we have a write miss. The question is, does the block get brought to the cache or not. There are two types of caches. The first type of cache is according to this is a write-allocated cache. It brings the block that we write into the cache. The second type does not. So when we have a read miss. The block is brought into the cache, but if we have a write miss The block is not brought into the caches. Most cachestoday are write-allocate, simply because there is some locality between reads and writes. Meaning if we write to something, we are also likely to read it. So write allocate helps us have read hits, even if we have the write miss. The other aspect of the Write Policy is when we have a write, do we write just to the cache or also to the memory? We can have a write-through cache, which updates the memory immediately. So when a write happens, we write to the cache and just propagate the write also to memory. That's why it's called write-through, because the write kind of goes through the cache. Or we can have a write-back cache, where we only write to the cache, and we write to memory only when the block is replaced in the cache. We cannot just discard the block at this time, because the memory does not have the most recent version of the block, so the memory has to be updated once the block is no longer in the cache. But as long as it's in the cache, we will just keep writing to the cache, and not keep memory up to date. Write-through is a very unpopular cache for caches that send stuff to memory directly. We usually want to have a write-back cache, because that way writes which have a lot of locality will update the cache possibly many times. And only send a write to memory once, once the blocked is replaced. So we can have many writes. And only one memory write for that. That prevents the memory from being overwhelmed with the number of writes that we might have. The results of some relationship between this choice and this. If you have a write-back cache, you want to also have a write-allocate cache. Because, you are saving writes to memory, by writing only to the cache. If you have a write miss, you want to have future writes go to the cache so you do write-allocate. Now let's talk about what happens in a write back cache. Write through is fairly simple. We just send writes to memory and the cache works just as we described it before. But for write back caches, we can have a block that we did write since we last brought it from the memory. In that case, when we replace the block we need to write it to memory, but we can have a block that we didn't write since we last brought it from memory. That block has only been read since we brought it in. In that case, when we replace, there is no need to write that block back to memory. But how do we know which one of these we have? One choice that we might make is that we don't want to know. We'll just every time that we replace the block. Unfortunately, there is a lot of read only stuff that we will now end up writing to memory over and over, all that's unnecessary. So instead, we choose to add a dirty bit to every block in the cache. The dirty bit says whether we have written the block since it was placed in the cache, or not. So if the dirty bit is zero, it means that the block is clean, or it was not written since it was last brought from memory. Or, we can have the dirty bit be one, in which case we say that the block is dirty, or it was written since we last brought it from memory. And that means that we need to write this block back to memory on replacement. When this block is kicked out of the cache it needs to be written to memory. If it's claim then we don't have to write it to memory. So let's look at an example of how a write-back cache would work. Say we have a small four entry cache that is direct map. Each line has a valid bit, a tag, and a dirty bit. We don't need the counter, because in a direct map cache, we know exactly what to replace. We don't need counters to tell us that. Let's say that the processor is doing this sequence of axises. And A, B, and C map to different sets in the cache. The first axis is right A. So what we do is we have a miss. All of our valid bits are 0. So it doesn't matter what the rest of the cache has, because none of the lines are valid. When we have the first right to A, we will bring A here. We will change the value bit to one and we will change the tag to correspond to A. Because this is all right, the processor,as soon as the data is brought in will write it and the dirty bit will be set to one. Next, we have a read to A. The processor checks the tag and the valid bits, sees that the tag matches A, and the valid bit is 1, so it can just use A. The fact that the dirty bit is 1, doesn't change anything. Next let's say we have a read B, we don't find anything that matches B in the cache, or maybe if this does, the value of it is 0 so it doesn't matter. So we will have a miss and bring B into the cache, change the tag to correspond to B, change the value bit to one to indicate that we do have something here now, and because this is only a read, the dirty bit will be 0. Next let's say we read C. It's an S, bring C in, change the tag on the valid bit, it's a read, so the dirty bit starts out as 0. Next what we have is a write To C, we check if it's a hit and it is. We write only to the cache and change the dirty bit to 1. And now as you can see, the dirty bit for every line, indicates simply, was the line ever written, since it was last brought into the cache? Lets continue this by an access to E that is RD and lets say that E maps to where A is. So now what need to happen is, we're going to check here to see whether the tag matches E, it doesn't so we have a miss. Our replacement policy is simple, for direct map caches, we simply replace whatever is there. So at this point, we cannot just bring in E from memory. First, we have to take the data for A, that we have written, and not written to memory yet. Because the dirty bit is 1, this block gets sent to memory as a right. After that happens, we can bring E in, set the tag and value bit. And the dirty bit to 0 because it's a RD, and this continues on. Let's say that now we have a RD to F, which replaces B. Where we want to replace B, we check the dirty bit is 0, so B doesn't get sent to memory. It simply gets overwritten with a data from F, we fetch, and because this is a RD, we get the 0 here, and so on. Now it's your turn. Let's do a quiz on write-back caches. Suppose we have a direct map cache and this is one entering that cache. And all of the axises we will do will map to that one entry. The valid bit starts out at 0. The dirty bit starts out at 1. And the tag starts out at A. The processor does the following sequence of axises. It reads A, reads B, writes B, reads C, reads D, writes D. And all of these, A, B, C, and D, map to the same place in the cache. The question for you, is, after these accesses, what is the new state of the cache? Also you need to determine how many cache misses do we have in this sequence of accesses and how many write-backs to memory will happen during the sequence. Let's look at the solution for a write-back cache quiz. When we have read A, we're going to check the tag and the valid bit. The tag matches A, but the valid bit is 0, so this is a miss. Now, we have one miss. We will make the valid bit be 1. The tag becomes A. This is a read, so the dirty bit is 0. Note that the fact that the dirty big was one, doesn't matter here because this was not valid. So it simply doesn't matter what's here, if valid bit is zero for that line. Next, we read B. We check the tag and the valid bit. The tag is a mismatch, so this is a miss. We have a miss, we need to kick out A. For that, we check the dirty bit. It's zero, so we don't have a write back. We replace A with B here. The valid bit is 1. The dirty bit is 0 because this is a read. Next, we have a write to B. Check the tag and the valid bit. This is a hit, so it's not a miss. Because this is a write, we set the dirty bit. Next we read C. Check the tag. Mismatch. This is a miss. We need to replace B. So we check, is it dirty? Yes, it is. We need to write back B to memory. Then, we can bring in C. Set its valid bit to 1. And because this is a read, the dirty bit becomes 0. Next, we read D. It's a miss. We need to kick out C. C is clean, so we don't have a right back. We just bring in D and overwrite C in the cache. It's a read, so the dirty bit is 0 and the valid bit is 1, because the tag is valid. Next, we have a write to D. It's a hit, but the dirty bit changes to 1. So overall, we had 1 write-back and 4 misses. And the state of the cache is that the valid bit is 1, the dirty bit is 1, and the tag corresponds to D. So, now that we have seen multiple aspects of how caches work, let's summarize them for a more realistic cache. Let's say we have a 4kB, 4-way set associate cache with a 64-byte line size with a write-back and write-allocate write policies. The processor, let's say, creates 64-bit addresses to access the cache. When accessing this cache, the 64-bit address will be divided into offset bits, index bits and tag bits. First for offset bits it's a 64-byte line, so we need 6 bits, bits 0 through 5, there are 6 of them. Are going to tell us where in the cache block we are. Next we need to determine how many sets this cache has. It's a four kilobyte cache, that's two to the 12th bytes. The number of blocks is that divided by 64. Which is two to the sixth. So we have two to the sixth blocks in the cache. It's a four-way set associative cache which means that every set has four blocks in it. So we divide the number of blocks with the number of blocks in a set and we get two to the fourth, or 16 sets. Now we know that we need four bits to tell us which set we are in, so bits six, seven, eight, and nine, four of them are the index. The bits that remain, bits 10 through 63, 54 of them are the tag. Now, in our cache, we will have for each block. A valid bit. It's a write-back cache, so you also have a dirty bit. Then there will be a tag, 54 bits. It's a 4-way set-associative cache, so we need to pick a replacement policy. Let's say the replacement policy is LRU. For a 4-way set-associative cache, each block needs to have a two bit LRU counter. Here we have one, one, 54, two, and then we have 64 bytes' worth of actual data. As you can see we have 58 bytes in addition to the data. So there is some overhead. The cache size is usually expressed in terms of how much data it contains. But the actual size of the cache array is larger because it needs to contain all of these extra bits for each line of data. Now, let's see how for this same cache, we use this address to access the cache. To make things easier to represent, we will draw the lines in the same set horizontally, so these are the lines that belong to set zero then we will have additional lines. And there will be lines that belong to set 15. There are 16 sets if you remember. So the way we access this cache is we take the index bits and they tell us which set we have. Let's say we have set 0. We then read the tag invalid bits for all of the blocks in that set. The tag bits that we read are compared to the tag. The valid bit needs to be also 1 in order to have a hit. And we do this for each of these blocks in the set simultaneously. So the tag is distributed to all of them to compare. As you can see, there is four separate activities in order to determine for each of these whether it's a hit. And then there is OR circuit that checks if any of these is one. If that OR returns a one, that means that we have a hit. In that case, what we do is we find which one had the hit and go back and read out the data for that block. Once the data is read out, let's say this is our 64 bytes of data, we use the offset to tell us where our actual data to return to the processor is. If this was a write, then this offset tells us where to write. And if this is a write, we will also set the dirty bit here. Note that we don't actually check the dirty bit if it's zero and set it if it's zero. We just set it to one regardless of what it was. It's faster to just set it than to check it and then set it. So if we have a hit, this is how we find the data. What if we have a miss? What if all of these have returned zero? In that case, we will check the LRU counters In these to find the one that is 0, check the dirty bit of that block. If it is 1, we will first take this data and write it to memory and then put the new block here. If it is 0, then we simply bring in the new block. We update the tag and everything here. We set the LRU counters accordingly and we then supply the data to the processor. So as you can see all of the stuff that we have been talking about in the cache really happens kind of simultaneously on every axis all of this needs to happen. If it's a hit up to here. If it's a miss all the way to writing back to memory, fetching another block from memory and updating the tag invalid bits. Now, let's do a quiz on the overall cache behavior. Let's assume we have a cache that is only 256 bytes in size, with a 32-byte line size, two way set associative, write back, write allocate. Let's say that the processor is generating 32-bit addresses and that the bits of the address are numbered 0 through 31. So the question for you is the tag bits are bit what through what? The index bits in the address are what through what? And the offset bits are which bit through which bit? Let's discuss the solution to our cache summary quiz part one. We have a 256-byte cache with 32 byte lines, and 2-way set associative. We have a 32-bit address. We need to know which bits are tag, index, and offset. Remember to always start from the offset. Bits 0 through something are the offset. Which bit? Well, 32 B is two to the fifth bytes. So we need 5 bits to tell us which byte we have in a line. That means that we're talking about bits 0 through 4. This is five bits because we begin at 0. The index bits are the next bits in the order of significance. So they will begin at bit 5. The question is, how many do we need? To determine that, we need to determine how many sets do we have in this cache. The cache has 256 bytes divided by 32 bytes. This is the number of lines, and the result is 8. So we have 8 lines. It is a 2-way set associate cache, so we have 8 over 2 sets. This is four. We need two bits to tell us which set we have. So it's going to be bits 5 and 6, and that means that the tag bits are starting from 7 all the way to 31. So, this is the second part of our cache summary quiz, in the same cache that we have seen in the previous quiz. Assuming that this is the sequence of [INAUDIBLE] that happens and that the cache is completely empty at start, meaning all of the valid bits are zero, the question is, how many misses do we have in this sequence and how many blocks are written back to memory during the sequence? Let's look at the solution to our cache summary quiz. We already know that the least significant five bits are the offset bits, that there are eight blocks in this cache, and because it's two way set associative, that means that the next two bits are the index bits. So all of these have the same index bits, which means they map to the same cache set, so we only really need to consider what happens in the one set where they go. This is a load. It's going to be a mess, because the cache is empty. So we have a miss. Now the valid, dirty, and tag bits in the set are going to be 1 here and 0 here. Dirty is 0 and the tag is BCDE00. It's actually going to have slightly more bits, but this is the only part that will really differ between these. The next axis is CDEF Is going to be a miss. And we'll fetch into the other entry here. The dirty bit is 0 because this is a load. Next, we have a store hit because we are accessing this. It's a hit because it's a store. This becomes dirty. Next we have a store to another word in the same block. So this block becomes dirty. And then we have a store again to BCDE, so it's just a hit that sets this dirty bit again, to one. So overall, we have two misses and zero write backs in this sequence. In this lesson we have reviewed how caches work and what are the concerns and choices when designing caches. We will use this knowledge in most of the subsequent lessons in this course, starting with lessons on virtual memory and advanced caches. And we will definitely need this knowledge for our projects. We have completed our discussions of out of order processors that try to execute more than one instruction per cycle. Now, we will discuss how the compiler can help these processors. So, let's discuss, can compilers help improve IPC? Which is the number of instructions per cycle, that the processor achieves. There are really two things that the compiler can help with. One, is the ILP of the program itself may be limited. For example, due to dependence chains, which is when we have instructions that depend on each other, so really if we have a long chain like this then there will be very limited ILP in this program because we can really execute only one instruction at a time. So, the compiler, as we will see, might help us actually avoid these kinds of dependence chains. And also, the hardware has a limited window into the program. For example, there might be independent instructions, so an ideal processor might be able to achieve good ILP on that program. But because these independent instructions are far apart, a real processor simply can not see those instructions. Because, for example, it runs out of ROB space before it reaches those instructions if they're independent. So, in the same example, if we have a long chain like this followed by, this instruction doesn't depend on any of those in the chain, but if the chain is too long, we run out of ROB space before we reach this instruction. So, as far as this processor is concerned, this instruction cannot execute in parallel with those just because we never see it. As we will see, the compiler can help us put these independent instructions closer to each other, so that the IPC achieved by the processor is closer to the available ILP. One example of a technique to actually improve the ILP of the program is tree height reduction. Let's consider a program that looks like this. Let's say that we need a sum of these four registers in R8. One way to get that is to add the first two numbers into R8, and then add the second number, and then add the third number and then the fourth number. However, this forms a dependence chain because we have the dependence here and the dependence here, so really these instructions have to be done one after the other. So tree height reduction works as follows. The compiler figures out that instead of adding one number at a time to the sum, and thus creating a chain of dependencies, what we can do is we can group the computation this way. Thus, getting, we first add the first two numbers and put that in R8. Then we add the second pair of numbers, put that in R7. And then we add up the R8 and R7 into our final result, R8. The dependencies here are this and this. So, now we have two instructions that can execute in parallel with each other because they don't depend on each other. And then a third instruction that does depend on them, too. So if this needed three cycles to execute, this can actually execute in two cycles if we have a processor that can do at least two instructions per cycle. Tree height reduction can not always be done. It uses associativity of the addition operation, and you have to keep in mind that not all operations are associative. So we have to be careful. We can only do this when the final result is the same as if we did things in the proper order. So now let us try our hand at some tree height reduction. Suppose we have these six instructions. We are adding R1 and R2, putting the result in R10. Then subtracting R3 from that. Then adding R4. Then subtracting R5. Then adding R6, and then subtracting R7. If we now do a tree height reduction on this, what do we get? And I already pre-populated most of the answers for you. So, we're still going to add R1 and R2, and put the result in R10. But then, what do we add and put in R11? What do we add R11 to and put that in R10? What do we then add up and put the result in R11? And then we add R7 to what, and put the result in R11? And finally, we will subtract R11 from R10, and get the final result in R10. And this should be the same result that we got here. So, the question for you is what should be in these five places? And also, the ILP here is clearly six cycles for six instructions, which gets us a one. Why? Well, because each of the instructions depends on the previous one. With your new tree height reduced code, what is the LP? We still have six instructions. How many cycles do we need to do them now? Let's look at the solution to our tree height reduction quiz. We had this code, and we are trying to transform it through tree height reduction into something that minimizes the overall number of cycles it takes to execute this code. Here, the code takes six cycles to do because each of the instructions depends on the previous one. Now, when doing tree height reduction, what we need to do is really figure out what is the expression that this is trying to compute. And then, rewrite that expression. So, here's what we have here. We have R1 plus R2. Put the result in R10. Then subtract R3. Then add R4. Then subtract R5. Then add R6. Then subtract R7. So this is what we are trying to compute. How do we compute it with tree height reduction? Well, the idea is to compute R1 and R2. That's what we're doing here. And then, compute the rest of the things and then add them up in R3. So here, what we need to do is we need to add up four numbers and then subtract three numbers from that. So first, we will add up the four numbers. We already added up R1 and R2. Now we're going to add up R4 and R6. Next, the result of our two additions are going to be combined. So we're going to add up what we have for R1 and R2 and what we have for R4 and R6. So now in R10 we have the R1 plus R2 plus R4 plus R6. Now we're going to subtract R3, R5 and R7 from it. If we do it one at a time, that's going to be slow. So what we are going to do is we are going to add up R3, R5 and R7 and then subtract that from our R10. That's why we ave two adds here. So clearly, what we are doing here is we are simply adding up R3, R5. And then R7 is added to that. So this needs to be R11. So now R11 has R3 plus R5 plus R7. And now we subtract that from R10, and we get the final answer in R10. Now let us see how many cycles we need to do this. To do that, let's draw the dependencies. Here we have the dependence again. Like this. So each instruction needed to be done right after the previous one is done. Here there is no dependence between these two. There is a dependence here and here. So the third instruction is to wait for the first two to complete. There is now, no dependence here. So this one can be done right away, even though it's the fourth instruction in our code. And then there is a dependence here. And then finally, there is a dependence here, and also our final R10 here needs to be brought this way. So, if you look at what can be done in the first cycle, this can be done in the first cycle, and so can this. This cannot because it has to wait. This can. So this is also done in the first cycle. And then, this has to wait for that and so on. So in our first cycle, we've already done three instructions, half of the instructions that we had. In the second cycle, we can do this, because these two complete in the first cycle. We can do this. Because it only waits for this and that's been done. And, finally, in the third cycle we have to do this because it has to wait for the results of the second cycle. So we have completed our six instructions now in three cycles and we've got an ILP of 2, which is twice as good as we had before the tree height reduction. Next, we will look at techniques that make independence instructions easier for a processor to find. So ideal processor would find them because it is able to look at an infinite number of instructions, potentially, but the real processor can only see so many instructions ahead. So it actually doesn't see that there are independent instructions further down the code. We will really look at two techniques. One is instruction scheduling for simple branch-free sequences of instructions. Then we will look at techniques specific to loops, such as loop unrolling, and how they interact with the instruction scheduling, and then, we will briefly look at the more powerful technique called trace scheduling. We will look now at instruction scheduling in the compiler. No that this is different from what happens in a Tomasulo type scheduler, where the hardware is trying to reorder instructions, in a different way than they were arranged by the compiler. Now the compiler will actually try to do a similar thing. Here's a sequence of instructions that can happen. We load, a value from the address in R1 into R2. We add R0 to that. We store the result back into that memory location. We next move the memory address four places forward, so that we are looking at the next for example, element in our array in memory. And then, we compare the pointer to some sort of end of array pointer, and if they're not equal, we jump back to the loop. So these are five instructions, out of which a single iteration of a loop is composed of. So let's say that we have a very simple processor, that can only look at the very next instruction. So it's not really trying to have reservation stations and execute out of order. In that case, what might happen is. There would be this load. And let's say, it takes two cycles to actually get the value into this register from the cache. That means that the next cycle, the processor cannot do this ADD, it has to stall, because the result in R2 is not available yet. In the next cycle, it will be adding R0 to the value in R2. Let's say now, that the ADD takes a few cycles. Let's say, three. So only here, can we store. The next instruction, doesn't depend on the store. So let's say we can do it in the next cycle. But then, this ADD again, takes three cycle, let's say. So we have two Stalls, before we can do this branch. So let's say that the compiler is trying to help the processor do better than this. If we still have the same processor. But we wanted to execute these instructions faster than one, two, three, four, five, six, seven, eight, nine, ten cycles per iteration. How can we do that? Well what the compiler will try to do is, it knows that after this load we cannot do this ADD until, two cycles later, so it's going to try to find something that can be here. So, we need something that doesn't depend on the load and can be done here. Now, we can see that the ADD does depend on the load, so it cannot go here. The Stall, needs the result of the ADD so it cannot more here. But the ADD actually can go here because the ADD doesn't depend on the R1 value here. So, the compiler is going to try to move this ADDI here. So we want a schedule in which the ADDI is here, because this processor can do only one instruction at a time, in the order in which they're placed in the program. That would mean, moving this ADD here in the program. Let's see how that, would be done. So conceptually, we want this ADD here. Note however, that while the load gets the same value of R1 that it did before, the Stall now gets R1 that has already been incremented by 4. So we have moved this ADD, because this R1 is now, larger than it should be. We have to fix the offset here so that, it loads from R1 minus 4. When we remove this ADD, the branch still sees the correct value of R1 because it needs to see the incremented value and the new program looks like this. The black stuff is the stuff that has moved or changed. And the, blue stuff is what has remained exactly the same as before. Now let's see what happens to our schedule. Now we have a load. And, instead of the ADD here being the next instruction so we had to Stall, the next instruction is this ADD. This ADD, can proceed in parallel with the load. It doesn't depend on the, R2. So we can actually do this ADD here. So, instead of a Stall, we can now do our ADDI. This eliminates one of the Stalls. The Add here, doesn't depend on this ADD, so it can still stay where it is. Now, in order to get to the Stall instruction, we still have to do the two Stalls because the ADD here takes two cycles of stalling before we can use the result. So, this Stall needs to stay here. But at this point, we have already done this ADDI. It's moved here. Of course, the Stall now need to be modified this way. So this instruction is now like this. The ADD now goes away. So we have just removed one cycle here. Next, when can we do this branch? Well the ADD has happened here. We cannot use R1 until one, two, the third cycle from there. So pretty much at any point after here, the branch actually is safe to go, as far R1 is concerned. Note that here we have to stall because of this. So we have just eliminated, two more cycles here. And now we can do our branch. So pretty much, we had plenty of Stalls then. Now we have only two Stalls and the rest of the loop body is scheduled. So, pretty much, the idea of instruction scheduling is to find instructions that can be done in place of stalls. And thus, avoid having too many stalls in the processor schedule. Okay, now that we have seen an example of instruction scheduling, let's do an instruction scheduling quiz. Let's say that this is a sequence of instructions that we have to schedule. And let's say that load word takes two cycles, add takes one cycle and store word takes one cycle. The question for you is how many cycles does this take to execute as is and after instruction scheduling in the compiler this takes how many cycles? On a processor that can do only one instruction per cycle and it has to do instructions in order. Let's look at our instruction scheduling quiz solution. As is this takes one cycle to do the load. If the load latency was one cycle, then the add could happen the next cycle, but because the load takes two cycles, the add can only happen in cycle three. So, we have one cycle three. The add takes only one cycle so the store can proceed. Then we can do the load, it doesn't depend on anything so we can do the load in cycle five. The add however can now happen only in cycle seven and finally the store happens in cycle eight. So as is, this executes in eight cycles. The question is now can we schedule this better to avoid some of these stalls? And we had a stall here and here. To avoid a stall here we need to place between the load and the add an instruction that can be done here without violating dependencies. We can see that this load here doesn't depend on any of the previous three instructions, except that we will destroy R1 if we move that instruction here. So the correct solution to this is we will move the load there, but we will rename our R1 into something else. So the program becomes now, load word R1 zero of R2 as before. Now we move the other load there, put the result in some R10, let's say. Now we can do this add because in the third cycle we can do it. Now that we have done this add we can actually do this the load has already happened, we are now in the second cycle since then. So the second add can happen. The add was using R1 now we have to use R10. Next, we will do our store and if last remaining store. As you can see, now we have a processor where whenever we need two cycles for the load there is something in between the load and the use of it's result. Here R1 is loaded, we then don't use R1 and then we add to it. And here R10 is loaded and then we don't do anything with R10 and then we add to it. So this can be done in six cycles, eliminating both of the stalled cycles that we previously had. Now that we have seen how instruction scheduling works, let's see how it interacts with things like if conversion. Recall that if conversion works like this. We had some code and then there is an if then else, which means there is a branch. Sometimes we do this, sometimes we do this, and then we go back to doing something after the if then else. Let's say this is the orange code and let's say that this is the green code. Before if conversion, we can easily reschedule code within here. We can also move things around within this, we can move things around within this, and we can move things around within this. But it's hard to mix the code from these together. For example, take an instruction from here to fill a store here. Because if we end up going this way that instruction doesn't get done, so it gets a lot more complicated. However after If conversion we do the stuff that was before the if then else. We then have the orange code predicated so that it only results in execution. And then we have the orange code predicated one way. Greens are code predicated the other way and then the stuff after the loop. And, now because this is branch free code and, because all of these instructions really execute every time, it's just that they are predicated sometimes. We can easily take instructions from here and put them here to avoid stalls. And that's nice. We can even take instructions from here and put them here or instructions from here and put them here, for that matter we can even take instructions from here and put them here. So overall, we get a lot more opportunities for filling stall cycles with something useful. So if conversion is one of those things that. It helps with branch prediction because you're going branches, and it also can tremendously help the compiler in making a better schedule for a processor. We have seen that we can If-Convert If-Then-Elses, A similar technique would be very useful for a loop, but can we [UNKNOWN] If-Convert a loop? Let's go back to our loop that loads from some memory address, add something to that, Stores that back in memory, adds 4 to the pointer, to the beginning of the loop, unless the pointer has reached some final value. Let's say that in this processor, every operation takes two cycle to do. So we will have a stall cycle here, because we need to use R2 two cycles later, another stall cycle here because the add needs to operate for one more cycle before the store can see that value. There is no stall cycle here because this add can actually proceed immediately, it doesn't depend on the store, and then the branch gets delayed by one more cycle because it's using the result of the add. So we have three stall cycles here, and again this is a process where every operation just takes two cycles, we have already seen that we can schedule this loop differently. So that we load and then we do the add I and then we do the add, this way we avoid this stall cycle here, however, after this add, the only things that are left to do in this situation of the loop are the store and the branch. We can not do the branch before the store, so the store has to happen here, and thus, we haven't eliminated this store cycle here. And then the branch can check R1 for whether it reached the final value here, because without a stall, because R1 has been manipulated here and then we had to change this store, if you remember, because the add of a constant has moved here. So that it uses a minus four offset to compensate for this, so we manage to eliminate two of these stall cycles, but one still remains, now it you look at this branch here, If conversion would allow us to kind of do the next iteration of the loop here. And then eliminate this branch, because it would be predicated. And then we could maybe find something here, that can go over here but the problem is that. A loop is not really suitable for if-conversion, because every time we have a new iteration we would have to introduce a new predicate and then every new iteration would introduce yet another predicate, and we simply cannot deal with so many predicates and so many things that get done only if the predicate is true. Pretty much if we are executing a million iteration loop sometimes and sometimes only one ite,ration Then almost a million iterations would have to be just predicated out which would be very, very slow. So we want to do something about loops that is like If conversion to allow us to kind of find things in future iterations to move here, but without really doing if conversion, So we cannot truly if convert but there is something else we can do. And that something will be called loop unrolling. Now we will see how loop unrolling works. Suppose we have this loop in the C language, where i goes from 1,000 down to zero. And in each iteration of the loop, we add something to the ith element of the array a. When we translate this into instructions, we get a loop like the one we have already seen. We load from the array elements so R1 is now the pointer to the ith element of the array. We load that. We add R3, that would be Rs to that and then we store it back into the A of i. And then we move our pointer four places down. That's equivalent to decrementing i. And then we check whether the pointer has reached the beginning of the array, which is equivalent to checking whether i is zero. Now let's apply loop unrolling to this loop. We will do what we call unrolling once. We unroll the loop by making each iteration of the new loop do more than one iteration of the old loop. So, when we unroll once, what we want to do is now each iteration of the new loop will be doing the work that it should be doing, plus the work of the next iteration. However, if we just do it this way, then we are doing the same work twice. So, instead, what we will do is we will adjust the index so that this is doing the ith element of the array. And then, i would be decremented. And the next iteration would now do a of i minus 1. However, this loop still counts one at a time. So we do this, we add s to the next element in the next iteration of the loop. We would now add s again to the same element we already processed here. So we also need to adjust the loop counter update to update by 2 now. And now we have a new loop that has half as many iterations, each of which is doing the work of the two original iterations. We could unroll the loop more than once. For example, unrolling two times would mean that this loop is doing now three times the work of the original iteration. For this once unrolled loop, the instructions would look like this. We would have the actual work of the one iteration. These are the same instructions as here. We now want to do this work once more. This would be this. However, if we just copy the instructions again, we would be doing the same work twice. We would just add R3 again to our same element a of i. So, instead, we will adjust the offset here so that we are accessing the element right below the one that R1 is pointing to. This would be the a of i minus 1. And now we need our loop counter update and the actual loop back branch. But those we cannot simply copy, just like here we had to update the counter to count by two iterations. In this code, we were decrementing R1 to move to the element below. Now we're processing two elements at a time, so we need to move 8 bytes at a time. So the approach to unrolling once is take the original work, copy twice, adjust the second copy so that it does the second iteration's worth of work, and then adjust the loop count so that it's moving by two iterations, not by one. This is what it means to unroll once. So keep in mind that when we are doing two iterations at a time, we have unrolled the loop once because we have added one iteration of the original loop in addition to what the iteration normally would be doing. Unrolling twice would mean that we're doing three iterations at the time. Unrolling three times means that were going four times the work and so on. It is a very common mistake that when you're asked to unroll the loop twice, unroll it this way. This is unrolling only once. Again, unrolling twice is not, do two iterations at a time. It's do one plus two iterations at a time. Because you always have to do one iteration, so this loop is not unrolled once, it is not unrolled at all. This is unrolled once. So now, let's look at what loop unrolling gives us. The first benefit it gives us, is that it reduces the overall number of instructions we have to execute. We started out with this code. And the overall number of instructions that we execute in a thousand iterations that we had of this loop. And the number of instructions that we had to execute in this loop was five instructions in each iteration, times 1,000 iterations of the loop. For a total of 5,000 instructions to do this loop. Now we unroll this loop once and got this. Now the number of instructions is eight times. In each iteration of this loop we are doing the work of two iterations. And we are moving the pointer by two places. So we only have 500 iterations as opposed to 1,000 here. Which gives us only 4,000 instructions to do the loop, instead of 5,000. So we have dramatically reduced the number of instructions it takes to do the loop. And we did it by eliminating what's called the looping overhead. Basically this is the work of the iteration and this is the looping overhead to go to the next iteration. When we do two iterations at time we do twice the work, but the overhead only applies to one of the new iterations. So we get half the overhead per original iteration. Remember the iron law where the execution time. Is the number of instructions times the CPI times the cycle time of the processor. The cycle time will not change. We still have the same processor. The CPI may or may not change but definitely we have reduced the number of instructions. So we can see that loop unrolling just by eliminating the looping overhead can reduce the execution time, but let's look what happens to the CPI. So we will see now that, another benefit of loop unrolling it that is allows us also to reduce the CPI in addition, to reducing the number of instructions. To get some idea for how the CPI changes, we need to have a processor. So let's assume we have a, 4-issue in-order processor, meaning it's only able to look at the next four instructions to see what can be executed together. Meaning it can only look at the next four instructions, to see what it can execute together. And let's assume that the processor has perfect branch prediction. So first, let's look at what happens when we try to execute the original loop. So to see what happens with this loop in this processor, let's look cycle by cycle at which instructions can execute. In the very first cycle, we will try to do this load and then we will try to see whether we can execute the next instruction in order in the same cycle, but we cannot because it depends on the load. So, this instruction has to be done here. Now, we will look at the next instruction, and try to execute it in order in the same cycle, but we cannot because it depends on R2 again, so we move here. The next instruction, however, does not depend on the previous one, so we can do this add here. But then, for the branch, it does depend on R1 so it can be only done here. Because we have perfect branch prediction, this load has also been fetched from the next iteration and it doesn't depend on the branch that was done here so we will actually do the load for the next iteration. And then, we will, again, not be able to do the add. So the add happens in the next cycle. And then, in this cycle, these two happen and so on. So overall, after this initial load that had to be done by itself just because, there was no branch before it, it takes us three cycles to do all five of the instructions in the loop. So the CPI here, is 3 cycles for 5 instructions. But remember that, we can do scheduling of this loop to try to improve the CPI. So before we get into what's the CPI of the unrolled loop, let's see what the CPI is if we schedule this loop body. Our scheduling will start out with this loop body, and then try to maximize the CPI that we can get, by reordering the instructions. So, clearly, here, this load could, not be done in parallel with the next instruction, because there is a dependence there. So we need to find something, and it cannot be the branch, because it needs to stay at the end of the iteration. That we can put here, and that this ADDI can move there. So our scheduling algorithm will move the ADDI here. So now that we are subtracting 4 from R1 here, we need to adjust the store so that it adds back the 4 that we decremented here. Because, this store was supposed to store to the same element as the load. Now, however, we have moved R1, so to access the same element as the load, we need to add 4 here. Note that moving the ADDI it really the only scheduling freedom we have here because. We need to do the load, and the ADD, and the store in that order so they cannot be re-ordered, the branch needs to stay at the end of the loop, so pretty much the only freedom we have is where is the ADDI going to happen. So now let's look at what happens cycle-by-cycle. We can do the load in the first cycle. The ADDI can be done, together with the load. But the ADD to what we loaded still has to be done in the next cycle. The store cannot be done until this ADD is complete. The branch can still proceed in parallel with the store. And now, because of perfect branch prediction, we're going to move here, and we can try to do the load for the next iteration. Note that R1 has been updated here in the previous iteration, so all the way through here we really did not modify R1, so pretty much the store of the branch didn't modify R1. That's the only registered dependence that this load has, so the load can be done here. And then we can also do the R1 update here, because this ADDI only depends on R1, and the R1 has been updated here. So we can do this. So as you can see, now it takes two cycles, to do these five instructions. So the CPI we get is now 2 over 5. It took us two cycles to do the five instructions. This is significantly better, than without scheduling so, scheduling did help So going back here, with out four issuing order processor with perfect branch prediction, we have seen that our loop, gets a CPI of 3 over 5, which is 0.6. Without scheduling, and it gets a CPI of 2 over 5. Which is 0.4, if we do a scheduling on the original loop. Now let's see what happens when we unroll the loop once, and try to execute it on this processor. This is the code we constructed, and now what happens in this loop is we, do the load. The add has to wait for that result. The store has to wait for that. This load is going to a different memory element, so it can proceed in parallel with the store. The R however, has to wait for that load. And then, this store has to wait for that add. And then, this decrement of the pointer, can proceed in parallel with the store, But the branch has to wait for R1. However then, the load can proceed. And now, we are back to where we started. So overall, it takes us five cycles. Do do these 8 instructions, which gives us a CP of 0.62 slightly lower than the original on schedule loop but not much lower. So these are very similar in CPI, we still gain significantly from eliminating some instructions. So overall this loop is going to perform better but the CPI has not improved. But let's see what scheduling does after enrolling. In the scheduled loop, we will have our load, for the first iteration of the two that we are really doing. And now if we put the add here, it would again have a dependence, so the load and whatever we did here would be the only thing that can be done in this cycle. So instead of doing the add next, we will look for what else can we do here that doesn't depend on this load. And it turns out that this load here, the load for the second iteration, can be done in parallel with this load. However, we cannot store the value into R2, so we will have to use another register, let's say R10, for that. So these two now can execute in the same cycle. And if you remember so could the branch. Now that we have done this we can do the add for the first iteration of the loop, and also the one for the second iteration of the loop, in the same cycle and in the same cycle you can do the add i instruction for moving the pointer. Now these two will produce the results that the stores need. So next cycle, we can do the stores. Note, however, that this is the store that was supposed to store with offset 0 from R1, but here, we have now decremented R1 by 8. So now we need to store to 8 from R1. That's because his store needs to match the [UNKNOWN] of this load, but the R1 has moved in between. And then for the other store, we're storing R10 now, to the address that used to be minus 4 from R1. But R1 has already moved by minus 8. So now we need to store to 4 from R1, to match the address of this. This is basically minus 4 and then, we did minus 8, so now we need to add 4 to that, in order to get minus 4. And now, we can do our branch. So now let's see what can we get on our processor. In the first cycle we will be doing these two loads that can now proceed in parallel. The Add here, however, cannot proceed in that same cycle because it needs a result of this load. However, this Add, and the next one, and this one can proceed in the second cycle. Then the stores here have to wait the results of the adds so they cannot proceed in the same cycle. However they can be done in the next cycle and so can the branch because the R1 has been produced, and so can the load because R1 has been produced. And unfortunately in this cycle we can not do this load because we already executed four things so unfortunately that load is done in the next cycle after which we get the same schedule as here. The three adds, and so on. So overall, it took us now 3 cycles to do 8 instructions. Which gives us a CP of 0.38, slightly better than the scheduled CPI for the loop that we didn't unroll. So what really happened is unrolling gives us more stuff that we can reorder here so that we can find more things that don't have dependencies and thus for example, we were able to do these two loads that were not available prior to unrolling. If we unroll this loop three times, then we will have four loads here that can all proceed in the same cycle. We will have four adds here that can proceed in the same cycle. Four stores that can proceed in the same cycle. And then the Add and branch equal. So the more we unroll, the more parallelism we can get, simply because there is no. More instructions among which we can try to find independent ones. Keep in mind, that unrolling already reduced a number of instructions and with scheduling it allows us to also reduce the CPI so it kind of creates a double whammy. For performance improvement our execution time gets reduced both because we have fewer instructions, 8 instructions per 2 iterations as opposed to 5 instructions originally for one iteration. And also after scheduling it gives us usually a better CPI. So now that we have seen an example of loop unrolling lets do a loop and rolling quiz. Lets say we have the following loop. We load something from memory. We add it to some sort of a sum in R3, and then we move our memory pointer by four bytes and then if we have reached some end of an array, then we exit otherwise we loop back. So what this loop is really doing is it's summing up an array. Let's say we have a processor that is in order and executes one instruction per cycle. A load takes three cycles. So, for example here, because we are using the result of the load, there would be 2 stall cycles in between and ADD or ADDI, take 2 cycles each. So for example, between the ADDI and the branch, there would be 1 stall cycle. So the question for you is after scheduling this loop, but without applying any unrolling, it takes how many cycles to do 1000 iterations of the loop? And then after unrolling once, meaning we do the work of two of these iterations per new iteration, and then scheduling the loop, it takes how many cycles for 1000 iterations? So now that we have seen that unrolling can really help both reduce the overall work, and also schedule the work better. We might be tempted to do unrolling all the time. So is there any downside to unrolling? Well, yes there is. So first there is code bloat. Let's say that our original body was four instructions. After we unroll, for example it might be that instructions one through three,. Need to be replicated, and this is for example our branch, so we can just have one. So now there is fewer instructions executed per original iteration. We had four per iteration, now we have seven per two iterations. But the code size is much larger. Here we had four instructions in a our code, now we have seven instructions in our code. If we decide to unroll more than once, meaning. Do the work of let's say, four of these iterations. The code size will be proportionally larger. So the code grows quickly, with the number of un-rollings with we do. Second, what if the number of iterations is unknown at the beginning? For example, this could be a y loop. In that case, here, we do one iteration at a time. Here we're doing two iterations at a time. What if we needed to exit here? Even in a four loop where we know the number of iterations in advance. What if the number of iterations we need is not a multiple of N? The number of iterations we are doing per iteration of the loop. So we might still for example, we might need to do seven iterations of this loop. We unroll it once. Now we need to do this three times and then exit in the middle. So, how do we handle this? The solutions to this problems do exist, but they are beyond the scope of this class. So, these are questions that are better answered in advanced compiler class, not in an architecture class. An optimization similar to inlining, as far as benefits are concerned, is called function call inlining. And it takes code that calls a function. So we do some work. Then we prepare the function parameters. For example, put them in the right registers, according to the calling convention. Then we call a function, and then we do more work after that. And when we call a function. Lets say that the function itself was just adding the first two parameters, and putting that in the return value and then returning. So this sends us back here. And then we continue with the more work. So this is what a normal function call looks like. So now we can try to avoid the overheads of calling the function and returning from it, by doing our work. Then, we don't have to prepare the function parameters anymore. Now what we do is we simply take the work of the function and just plug it in here. So in this case, we add up adding the registers for example that did contain the values that we eventually ended up parsing here. So we didn't need to copy these registers to A0 and A1. And we put the result exactly where we need it, for example, R7, instead of maybe having to copy from RV to R7. And then we can continue with our work. So the first benefit of function call inlining is that it eliminates the calling and returning overheads, which include not only the function call. Instruction and the return instructions themselves. But also the code that prepares the function parameters according to the coding convention. And the code that possibly in function itself needs to. For example, pull parameters from the stack and so on. And then possibly also the work here after we return to get the return value into the register where we need it. All of that can be done by simply inlining the function and then just using the registers we want. And just like loop unrolling which helped scheduling do better, so does function call inlining. Because really what we have is. This is where we can do scheduling. And then we need to call the function. So these instructions cannot be moving around beyond this point. We can do scheduling here but again, we cannot do much of this work before the call. And then the function itself. Can be scheduled but we have to do it within this. We have three separate pieces of code, each of which can be scheduled. But we can not for example take something from here and use it to fill an empty spot here. On the other hand, after in lining this is just code with no function calls or anything so now all of this work is schedulable. For example the work from the function can easily be moved around, assuming that the dependencies allow it. So inlining also lets us do better scheduling just like loop and rolling did and for the same reason. Now we have more work that we can play with. Just like loop and rolling the elimination of the overheads for calling, returning and so on will reduce the number of instructions. The better scheduling will let us reduce the CPI. And that means that we can usually improve the execution time by quite a bit. The smaller the function, the more benefits from both of these. Because if the function is very small, the overheads of calling and returning are high, relative to the work we are actually doing. And also if the function is small, scheduling within the function is not going to do much, but if we put it. Into the existing code, suddenly this instruction here can probably be squeezed into an existing cycle without really adding another cycle. And finally there is a downside to inlining, and it's very similar to what we had for loop unrolling. So, just like loop on rolling, which had similar benefits, function call inlining has a similar downside, and that downside is the code bloat. Suppose we have the original program, and it calls a function, and then it continues and calls the same function from another place and so on. Let's say that the function is ten instructions and then a return. So, now when we do our inlining, what we will do is, we will effectively put the ten instructions here and replace the call, and also take the same instructions, put them here, and replace the call. So, what used to be ten instructions, plus one for the return, plus two for calling, altogether 13 instructions, now becomes ten instructions here and ten instructions here, and although we eliminated the overhead, so there are no longer function calls and returns, and possibly some small amount of argument copying and so on. Still we have replicated the body of the function as many times as there are places in the code that call it. Some functions are called from hundreds of places, which means that the body of the function will now be replicated many, many times. This means that we really need to be judicious about, when do we do function called inlining. We cannot inline all of the functions that we want. We want to inline functions that are small, because the fewer instructions we have here, the more we are likely to offset the additional instructions we put in here, by the removal of some of the overheads. But as the function goes big, the replication results in a lot more code than the original calling and returning code did So now that we have seen how function inlining works, let's do function inlining quiz. So let's say we have a load into A 0, which would be the first argument of a function. And then we call a function, and then we store the result of the function somewhere else. Let's say that what the function is doing is it's computing the square of this value, and then adding the second parameter to it, and that's what it returns. So it's simply returning the, you know, square of the first parameter plus the second parameter. Let's say we have a processor where load takes 2 cycles to do, CALL takes 2 cycles to do, RET takes 2 cycles to do, and Store Word and ADD are one cycle per instruction. And the multiply instruction takes 3 cycles before we can use the result. The question for you is, after scheduling this and this together will take how many cycles? So how many cycles from the point we start to load, until we are done with the store. And after inlining and then scheduling, how many cycles does this take? So we have seen our Function Inlining quiz. Now let's talk about its solution. After scheduling, how many cycles? Well, let's see first what the scheduling will do. We have a load. We have to do it before the call. And after the call we can do the store. So really scheduling can do nothing here. Here we need to do the multiplication. And then, the ADD depends on it. And then, we can only return. So really there is nothing scheduling can do here. So really we just need to figure out for this sequence of instructions. How many cycles does it take to do it? One way of doing that would be to say that the load begins in the first cycle. In the second cycle it's still going on. So, we can use the result in the third cycle. In the second cycle, however, we can do the call. The call also takes two cycles, so we're really can execute this instruction in cycle four. At that time, A0 is available. The multiplication takes three cycles. So in cycles four, five, and six, the multiplication is happening. And only cycle seven can we do this because it's using the result of this instruction. We can build a return immediately afterwards. So return now is happening in cycles eight and nine. And finally in cycle ten we are back here. And then the store of course takes only one cycle, so really after ten cycles we are done. Now let us see what the inlining will do. What it will do is it will simply take these two instructions and replace the call with them. Now let's see, can we do some scheduling around it? We need to load A0. Then we're going to use it in multiplication. Then we are going to use it in the addition. And then, we are going to use the result on it in the store. So again, scheduling is going to help us here. However, we have eliminated the call and the return. So now what happens is we have the load begins in cycle one. Next, we will try to do the multiplication but we can only do it in cycle three. Because the load needs to produce the value for two cycles in cycles one and two before we can use it. Next, we have the multiplication happening in cycles three, four and five. So we can only do this in cycle six. The add only takes one cycle. And the store can then happen in cycle seven. And that's when it finishes. So after inlining and scheduling, we have reduced this to seven cycles, mainly by eliminating the call and the return. At the end of this lesson, we will discuss some other IPC-Enhancing Compiler Techniques. That we will not discuss in detail, but you can find other techniques like this is more advanced compiler classes. The first technique is Software Pipelining. It's a technique that helps schedule loops, like these in a way that doesn't greatly increase the code size. But it allows us to get the effect of unrolling many times. Again, I'm not going to discuss this technique completely but the idea of it, is to treat the loop as a pipeline. Where this is the first stage, this is the second, this is third stage and so on. And then schedule the code so that we do the last stage of a third iteration while doing the second stage of the next iteration and the first stage of the next, next iteration. Why do this this way? Well, because here, we have dependencies, whereas here we don't. This store here is storing a value to which we have been adding in the previous iteration of the loop. And this ADD is using the value that we have loaded in the previous iteration of the loop, and so on. So it kind of forms a pipeline out of the loop. And then it allows us to do parts of different iterations concurrently with each other. And another technique that is very powerful is trace scheduling. You can think of trace scheduling as if conversion on steroids. Conceptually we take a code that has branches because of if then elses and other things. We find what the common path through this code is, and the blocks that are on the common path are. Then put together like this, without the branches in between. Now we can freely schedule instructions in between this, and we also put checks if the common path is not being executed for example here. What if this path should have been executed, if that happens we branch out of this scheduled code. And at the point when we are branching out, because we have intermixed these instructions, we first have to fix it. Basically undo the effects of instructions that we shouldn't have done. So the compiler, it needs to create some coded compensates for that. Then we can execute this block that should have executed. And then, we execute the next block that is on the common path. But we cannot execute it here, because we have interchanged these instructions. And then, we can jump to the continuation of this. So simply, we form a likely trace. We execute that with an excellent schedule. But any departure from the trace requires us to not only execute instructions in a slightly less efficient way. because we couldn't reschedule as well. But also, we need to execute some compensatory code. Basically, fix things that we messed up by reordering instructions. Again, this is not the full explanation of trace scheduling. I just wanted to give you some idea about how it works. But if you're interested in techniques, like software pipelining and trace scheduling. Then I strongly encourage you to take an advanced compiler class, where these techniques will be discussed in detail. This lesson introduced some of the more advanced compiler techniques that help produce programs that work better with the branch predicted, out of order, multiple instruction per cycle processors that we have today. In the next lesson, we will look at the type of the processor that simplifies its hardware by relying more on such compiler support. In this lesson, you will learn what reliability and availability are, how device faults can result in failures, and what can we do to make a computer work fine even when parts of it have failed. So, the first concept we will learn in full tolerance is dependability. Dependability is a quality of delivered service, meaning a characteristic of delivered service, that justifies relying on the system to provide its service. So pretty much a dependable system is one that provides the service in a way that makes us expect it to provide it correctly and in a dependable way. Now, the service actually has two definitions. One is the specified service, which is what the behavior of the system should look like. And then we have delivered service which is actual behavior. This is the behavior we actually got out of the system. So we could say that dependability is really about can we expect the delivered service to match the specified service. Can we get what we are supposed to be getting from this system. Now of course the system has some components which are called modules. So these are not components like transistors and so on. These are the kind of largest components of the system. So for example, a computer system will have a processor and a memory and so on. Each of these has components of its own and so on. And for each of these modules we can specify some behavior that it should be ideally getting. So each module has some sort of ideal behavior that we will want to expect from it, but of course real modules will maybe not always exhibit this ideal behavior. So when we talk about things that make the system not be dependable, we're really talking about modules deviating from specified behavior and that causing the system to deviate so that the delivered service no longer matches the specified service. So when we say that something deviates from specified behavior, we really are talking about three different things. One is faults, another is errors, and another is failure. So let's now define each one of these three to see where the difference is. A fault is when something in the system deviates from specified behavior. An error is when the actual behavior somewhere within the system differs from the specified behavior. The behavior that should be happening within the system. And the failure occurs when the system deviates from specified behavior for that system. To understand really what's going on with faults, errors, and failures let's look at an example. Things always start with a fault. And our fault example is a programming mistake. And let's say that programming mistake is an add function that we wrote for our program. It works just fine in all cases, except when we give it to add five and three in which case it returns seven instead of eight. This type of fault we also call a latent error. It's not really an error until we do something like this, but this type of a fault we call latent error because it's only a matter of time when it's going to be activated. So when we actually execute 5 plus 3 and get 7 in some register, we now have an error. If the error is a result of some sort of a latent error like this, basically a programming error, we say that the fault has been activated or that we now have an effective error as opposed to a latent one. In our case we get the effective error once we call the add function with 5 and 3 and get 7 instead of 8, and then we put that value in some variable. We get the failure when the system deviates from specified behavior. For example, in this case it might be that the time we were computing is the time to schedule a meeting for, and now we schedule a meeting for seven, instead of eight, as expected. And this is basically the failure of the system because it didn't effectively schedule the meeting for when it was supposed to. It is important to note here that you need a fault of some sort in order to get an error, but not every fault becomes an error. For the fault of this type, for example, to become an error, it needs to be activated. We need to actually use the function in a way that makes it produce incorrect results, even though it always had a fault. It always was faulty in this way. Similarly, we can have an error and never get a failure. For example here, if this value seven was never used, and programs often do this, then we have an error, a variable has the wrong value, but we don't get a failure as a result. Another example here would be something like, if we check for example whether this function returns something larger than zero, then when it returns seven instead of eight and we store that in a register, compare it to zero, see that it's still larger than zero, now we got an effective error, because a register held the wrong value, but the only thing we did with this value still caused the program to function normally. So in this case, we have an error, but we don't have a failure. So let's see if we are able to establish what's a fault, error, and failure, in an example when a laptop falls down. So we had a laptop and the first thing that happens, is that it falls out of my bag and then what happens is that it hits the pavement. As a result of that, the pavement develops a crack and then the crack expands during the winter. So the pavement breaks and needs to be replaced. So for this pavement failure is event, which of these six events. The fault is which of these events? And the first error is which of these events? So let's discuss the answer to our laptop falls down quiz. For the pavement, the failure is whatever causes the pavement to no longer perform its function properly. Here, you might be tempted to say that it needs to be replaced is the failure. But actually that's the repair of the failure. This is fixing the pavement, it's not something that causes it to deviate from its function. Really, it breaking, causing it to need to be replaced, so breaking is the failure. What's the fault? It's the event that resulted in the first error. In this case, it's not that the laptop fell out of the bag, it had nothing to do with the pavement. Is it that the laptop hits the pavement. And now what was the first error? Well, it was that the pavement developed a crack. The structure, the internal behavior of the pavement is no longer the same as the proper pavement. But we really notice this in terms of deviating from its specified function when it broke. So we can still use a pavement with a crack. The crack expanded, so the error became bigger, but really until it broke, the pavement was functioning properly. So this is what we get at the end. There are several other properties in fault tolerance in addition to dependability. One of them is reliability, and unlike dependability, which is the property of the system that we can trust it to perform its function, reliability is actually something we can measure. To measure reliability, we consider the system to always be in one of these two states. The normal state is the service accomplishment state, so the system is providing the service that we want it to provide. The other state is service interruption. So this is, at this time, the service is not providing, it's not accomplishing the service that we expect from it. So reliability can now be defined by measuring continuous service accomplishment. A typical measure for reliability is mean time to failure or MTTF. And that basically is how long do we have service accomplishment before we get into the next service interruption. So it's not enough, for example, that over the past two years the system has been in service accomplishing for one year. If we have a system that accomplishes service for a month, then is interrupted for a month, then accomplishes service again because it's been fixed, then it's again interrupted for a month, in that case, the main time to failure will be one month. That's how long you have of continuous service accomplishment, on average, between periods of service interruption. Another popular metric related to this is the availability. Availability measures service accomplishment as a fraction of overall time. So reliability measures how long do we get service accomplishment until the next failure. Availability measures what percentage of time were we in the service accomplishment state. So if the system on average was half in this state and half in this state, then availability will be 50%. 50% of the time, the system was accomplishing service. But reliability will be on average how long each of these accomplishment states lasted until we saw the interruption. So for example, if you have one year of service followed by one year of service interruption you get 50% here, you get one year here. If you get one month at a time for two years you get 50% here, but you get one month here instead of one year. So for availability, in addition to the mean time to failure, we need to know the mean time to repair, or MTTR, so this is once the system has service interruption how long does it last until it goes back to the service accomplishment state. And then our availability is the mean time to failure. How long did it function properly at the time, divided by the overall time, which was the mean time to failure plus the mean time to repair. After that, you're back in the accomplishment state. So this is how long we had both service accomplishment and service interruption, and this is just the service accomplishment. So let us try our hand at computing some reliability and availability. Suppose that the system we are considering is just a hard disk. And this disk, we buy it. We put it in our machine and now, it works fine for 12 months and then it breaks because it cannot spin any more. And then it takes us one month to replace the motor in the disk. And after that fix, it works fine for about four months. And then it breaks again and we find that the problem is that it can not move heads, and it took us two months to figure that out and then get the heads unstuck. And now the disc works fine for 14 months. And then it breaks. And we find that the head has broken. And we find this quickly but then realize it will take us 3 months to fix it. So we throw this disk away and buy a new one. So the questions for you are, what is the MTTF for this disk, what is the MTTR for this disk, and what is the availability of this disk expressed as a percentage? Let us look at the solution for our reliability and availability quiz. We had the hard disk that had three working periods and three broken periods until we threw it away. The mean time to failure is how long it works on average before it breaks. We had it working for 12 and then four and then 14 months. So on average, it worked 12 + 4 + 14 months, divided by 3, because there were 3 figures in between. And this gives us 30/3 which is 10, so the answer here is 10 months. For our mean time to repair, it's how long does it take to repair it after a failure? And we had a one month, a two month and the three month repair period. So our MTTR is simply 1 + 2 + 3 over 3 because we need the average of these three which ends up being two months. And finally the availability is what percentage of time was this disk in service over the entire period. We can compute this in two ways. One is what's the main time to failure divided by main time to failure plus MTTR. Which gives us ten months of service on average over ten plus two months of both service and repair. This is ten over 12 or 83.33%. Another way to compute availability is to add up all the months that we were in service and divide them by the sum of all the months together. In that case, we will get 30 months of service divided by those same 30 months plus the six months of repair, which is 30 over 36. And this again gives us our 83.33% availability. So now let's look at what kinds of faults can we have. We can classify the faults by cause, and we can have a hardware fault when the hardware of the system fails to perform as it was designed to do. We can have design faults. For example, software bugs are design faults, and also some of the hardware design mistakes, such as the famous Pantheon FDIV Bug, where this FDIV instruction was not implemented correctly, so if you gave it some combinations of numbers, you would get incorrect results, and it was designed that way. Then we have operation faults. For example, operator and user mistakes. For example, the operator accidentally tells the system to shut down, and now, of course, it no longer provides the service that it's supposed to provide. And then we can have environmental faults, such as fire that burns down the system, power failure, sabotage of the system, all things that in the environment may cause the system to have an error, ar not perform the service. Note that these are all false. Some of them don't result in a failure. For example, the FDIV bug doesn't result in a error or a failure, if you never really use the wrong combination of numbers for it. A software bug in a rarely used function may never result in an actual error. Operator faults may not cause a system to actually have an error. For example if you shut down the system, but at this time, for example, nobody's trying to use it to get service from it, it might not be an actual failure. And then, we can have a fire but if we protected our system from fire, a fire in the system room may not actually result in an error. For example we can have a switch over to another system. Power failure can be mitigated so it can be a fault, but may not result in an error. For example, if our battery power supply lasts long enough to overcome the power failure, and so on. So again these are faults according to their cause. Any of these faults might result in errors, and subsequently failures, but for all of them it is possible that we get a fault but not an error or a failure. We can also classify faults by duration. For how long do we have the fault condition? For example we can have a permanent fault. Once we have it, it doesn't get corrected. An example of a permanent fault is when we want to see what's inside the processor and now the processor is in four pieces and we don't know how to put it back together. We can have intermittent faults. They last for a limited duration of time but they are recurring. So it's not like we get this type of fault, and for a while it causes things to not work fine, but then it just disappears and never comes back. This is something like, every couple of seconds something happens. A good example of intermittent fault is when we overclock our system. It works fine then crashes, then we reboot it, it works fine, it crashes, and so on. So for a while it works fine, but then the same type of problem recurs. And then finally we have the transient faults, which are when it lasts for while, then just goes away. A transient fault is for example, when an alpha particle hits chips, so, for a while, it has all sorts of problems, but then once we reboot it, it's no longer going to have these problems and they're not going to occur again, until much later when something else happens. So let's see if we can classify some faults. If we have a phone and we get it wet and we try to operate it in that state and it heats up and finally explodes. So the phone getting wet is what kind of fault by duration? And what kind of fault by cause? Also, let's say that the phone was supposed to prevent itself from operating when wet. It has a wetness sensor, and it's supposed to not allow itself to be turned on, so that you can just dry it and then operate it normally. In that case heating up when it got wet is a result of what kind of fault by duration? And what kind of fault by cause? Let's look at the answer to our fault classification quiz. The phone got wet. By cause, it's definitely an environmental fault. What kind of fault by duration is it? Getting wet is a transient fault. Eventually it gets dry. The explosion would have been a permanent fault from which you really cannot recover, but getting wet is only a transient fault. Now because the phone was supposed to prevent itself from running when wet, the fact that it did heat up, and it wasn't suppose to, is an indication of a design fault. So really, it was suppose to not do this but the way it was designed actually made it do that, so it's a design fault. And design faults are typically permanent. And this is one more example of when you can have a fault that never becomes an error or failure. So the permanent fault of being poorly designed so that when it gets wet it can heat up and explode would never become an error or a failure until we actually get that particular phone wet. So how do we improve reliability and availability? We can try to avoid faults which is about preventing faults from occurring in the first place. For example, the no coffee in the server room is one example of a fault avoidance technique. If we spill the coffee on a server, that's probably going to be a fault. So we're trying to avoid that fault by preventing people from bringing coffee into the server room. Next, we can have a fault tolerance technique. They're about preventing faults from graduating into failures. A typical fault tolerance technique will use redundancy. For example, ECC can be used for memory and ECC stands for error-correcting code. So what happens is if there is a fault, for example a particle strike, it flips a bit in memory. ECC will allow us to read that block from memory, realize that one of the bits has been flipped, and actually correct that bit. And then we can also have techniques like speeding up the repair, which really affect only availability. And an example of such a technique is when we have a spare hard drive in a drawer so when a hard drive fails we can quickly replace it. So this is a technique where basically the failure will still occur but we will shorten the period that it takes to repair the system by basically, in this case, not having to wait for a new hard drive to arrive. We already have one handy on site. So we will now look more closely at the fault tolerance techniques which are usually techniques that we can apply in computer architecture to try to prevent faults from becoming failures and thus, makes systems both more reliable and more available. So the fault tolerance techniques that are used often are checkpointing where we can save the state of the system periodically, then we detect errors and restore the state of the system once we detect an error. This type of technique works really well for many transient and also intermittent faults. So the idea here is that you will save the state of the system when the system is functioning normally. If a fault occurs we will try to detect errors that result from that and restore the system into a state that was not affected by the fault. So if the the fault is transient it will not happen again and our system goes back to functioning normally. If the fault is intermittent, then this might happen a few times, until we reach the time when the fault is no longer going to be activated, and then again we will function normally. So the system continues to function normally, if we can quickly recover from each of the faults, so that it doesn't affect the overall availability and reliability of the system. If checkpointing takes too long, then this type of recovery has to be treated as service interruption. But if checkpointing and detection and restoring of the state are very, very fast it may be that the promised service is never disrupted. So for example, if the system is supposed to answer all the web queries within, let's say one second, if we manage to do all of this in such a way that we still respond to all of the things that we got in one second, then there has been no serious interruption. We can use two-way redundancy, where two modules do the same work. Then we compare their results, and we roll back if the results are different. So this is really an error detection technique, and it needs a recovery technique such as checkpointing. Similarly, checkpointing is a recovery technique. It needs something like two-way redundancy to actually detect errors and while these techniques recover and detect things separately, we can also have something like three-way redundancy which can both detect and recover some faults. And in this case three modules or more will do the same work and then vote for what the correct result should be. So if one module is malfunctioning then the two others will still produce the same result and that result will be elected by the vote as the overall correct result. So the fault in one module becomes an error in that module but doesn't become an error at the system level, because at the output of these three modules and their voter, we always have an error free result. This technique is expensive. You pretty much need three times the hardware plus the voter that you would have without some fault tolerance techniques. But, in return, it can tolerate any fault that can occur in any one of the modules. So as long as the fault that you have is affecting only one module, we can tolerate that fault. Even, for example, if that one module is purposely designed to be malicious. because the correct two are still going to out vote it and we never really see whatever that module is trying to produce in our overall results. But what happens if two modules fail, in that case we no longer can tolerate that. So let's look at the more general redundancy-based approach which is the N-module redundancy. For N = 2, we have already seen a brief example of that. We call this dual-module redundancy and we have seen that it can guarantee detection, but not also correction of one fault. Or one faulty module for that matter. Note that we can detect, sometimes, more than one fault, as long as these faults are in the same module, or as long as they're affecting different results of the modules. But all we can guarantee really, is that a single fault will be detected and we cannot guarantee its correction because really this technique doesn't correct anything. N = 3 gives us what's called triple-module redundancy, and it allows us to correct any one fault, or even all faults, produced by a single faulty module. Again, sometimes we can tolerate more than one faulty module if the two faults in the different modules don't affect the same result. But all that we can really guarantee as far as the number of faults we can correct is that all faults that occur within the same module will be corrected, and that's because the other two modules are functioning correctly and their correct outputs are going to output whatever the faulty one is producing. We also have examples when N = 5, and that would be called five-module redundancy. A good example for that is the space shuttle. It has five computers that are doing the same thing, and then they vote. If we get one wrong result in a vote, we just continue normal operation of the space shuttle. If we get two wrong results in a vote, the mission would be aborted, but there is still no failure because the three correct wrong results are still out voting the two bad ones. We are aborting our mission because if another module fails, and now we have three wrong results in our vote, we can no longer correct that and we can get the bye-bye space shuttle scenario. So the idea Is if we got one vote wrong, we know we can tolerate more than that without any problems, so we would just continue normal operation. These missions are expensive, so really if there is a single failure we want to be able to continue normally. Another one happening after the first computer has had a problem, is a very rare occurrence, and we would abort the mission because now we have some time to safely land the space shuttle while we still have three functioning modules. If it ever gets to another one failing after that, we can no longer promise that things will be correct and at that point, we are risking the astronauts' lives and possibly some people on Earth, too. So that's why the mission gets aborted before this happens. Let's check our understanding of N-module redundancy. Suppose we have a computer and we want to tolerate faults that can occur there. So we go to store and buy two more computers just like the first one and put all of them on the same desk. And our approach to redundancy is that we run every computation on all three computers, we compare their outputs, and we take that output on which two or all three agree. And with this approach we think we can tolerate any single event of the following kinds. Check each of those that can be tolerated. Can we tolerate any single alpha particle strike in a processor? Can we tolerate any single building collapse? Can we tolerate any single earthquake? And can we tolerate any mistake in single processor design? Let's look at the solution to our N-module redundancy quiz where we had three computers that are totally identical on the same desk, and we use them as a three module or redundant system. And the question is what type of event can we always overcome? A single alpha particle strike is going to effect only a single processor, and that means that the two computers that are not affected by this will still agree. A building collapse will either be another building and not affect anything, but if it is the building in which these systems are, then the building collapse is probably going to destroy all three computers, so we cannot tolerate this type of event. Earthquake is going to do a similar thing, and a mistake in processor design, because the three are identical, if there is a mistake in processor design it's going to affect all three processors at the same time. So, again, we cannot tolerate this. So as you can see, replicating the hardware with identical hardware and in an identical location only really ensures that we can survive something like a particle strike. If we wanted to tolerate earthquakes and building collapses, we would need to geographically distribute these three computers. If we want to tolerate a mistake in processor design, we would need to use three computers with different processors. So the N-module redundancy doesn't really guard against these types of faults unless the modules are in different places and of different designs. So we have seen some relatively general fault tolerance techniques. Now let's look at the fault tolerance techniques for memory and storage. We could use dual modular redundancy or triple modular redundancy for memory and storage, but they are considered to be overkill for these devices. We can get better techniques that protect us from similar problems. So dual modular redundancy and triple modular redundancy are typically used for the hardware that does the computation, which cannot be protected by some of the less expensive techniques that memory and storage can benefit from and these techniques for memory and storage are called error detection or error correction codes. So the idea is that we can store bits with some x-ray information that allows us to detect and, or correct one or more bits of error, depending on the code. So we don't have to store everything twice or everything three times in order to receive detection or correction of a single error. The simplest technique that is often used for memory storage is parity. With parity, we add one extra bit to the data bits and that extra bit, you can compute simply as an XOR of all the data bits. When we do have parity, if a fault flips any one of the bits, including the parity bit. The parity bit will no longer match the data. By flipping one of the bits, we are changing the result of this XOR to be the opposite of what it previously was. So when we read the data in the parity and we see that now the parity does not match data, we assume that there has been a bit flip and that's how we detect a single bit error. The next level of error detection and correction codes is the so-called ECC, which stands for error correction code. And a typical example of this would be a type of code called SECDED codes or single error correction and double error detection code. So this code can detect any one bit flip and actually fix it or if there are 2 bit flips, we will found out that that happened, but we can no longer fix it using this code. An example of where this type of a code is used is in data modules that have ECC. This is how they are protected. So pretty much if there is a single bit flip in such a module, you don't even notice. Everything functions well. If there are two errors in the data that is protected by the same code, then we will get notified that there's been an error, but we cannot continue operating normally, because actually we cannot correct the error in this state anymore. Hard drives use even fancier codes, such as the Reed-Solomon codes, that I'm not going to go into here. And these codes can detect and correct multiple bit errors and they're especially powerful when we have a streak of flipped bits, which happens, for example, when the head oscillates a little bit. And while the disk is spinning it for a while, just is too high above the platter and misses some of the bits. And then for hard drives in addition to these error detection and correction codes, there is a family of techniques called RAID that we will look at next. So RAID stands for Redundant Array of Independent Disks. And effectively with RAID we have several disks that play the role of only one disk, in different ways. It can either pretend to be a larger disk or they can pretend to be a more reliable disk. Or they can pretend to be the disks that is both larger and reliable then any one of the disks. Now it's important to note, before we go into explaining RAID, that each of the disks is still detecting errors using codes. So each of the disks, will know whether it has an error, and in the overall RAID skim, we will know which of the disks have an error. So, we don't have to have RAID figure that out. We know that because each of the disks still has a code at each sector that is capable of both fixing some errors and also detecting even more errors on that sector. So, what we want from RAID is better performance and we want normal read write accomplishment, basically service accomplishment as far as disk is concerned, even when there are things such as bad sectors on some disks or an entire disk fails, for example because it can no longer spin, and things of that nature. So things that a basic error detection correction code that is attached to each sector, cannot fix anymore. Possibly because we can no longer get to that sector on the disc. Not all of the array techniques will improve both of these. Some improve only performance, some improve only reliability, some improve both. And these array techniques our number. So there is RAID 0, RAID1, etc. And we will look at some of these techniques. So Raid 0 uses a technique called striping to improve performance. So if we have one disk, it will have a Track 0, Track 1, Track 2, et cetera, and the problem is that while the head is positioned to read Track 0, it cannot be reading Track 1, and also its very far away from all of these other tracks. So what happens is really, we have to serialize accesses to all of the tracks. If we need to read Tracks 0, 17 and 27, we will have to decide in what order we access them. But for each of these accesses, we move the we read the track. We move on to something else. So we cannot get more performance than just reading one track at a time at the current rotational speed of the disk. What rate 0 does is it takes two disks and makes them look like this one disk, by putting the original Track 0 here, Track 1 here, Track 2 here, Track 3 here, and so on. So the idea is that now each of these tracks is called a stripe now, and we're going to put stripe 0 really here, stripe 1 here, and so on. So Track 0 of the first disk gets Track 0 from the overall large disk. Track 0 here, however, gets the stripe zero. So instead of calling this tracks, we call them stripes to avoid confusion between what's a track on a physical disk and these things that really are tracks that would belong to a fake disk that we don't really have because really these two disks are pretending to be this one disk. As far as performance is concerned, we get twice the data throughput if we have two discs. Why? Well, because, while the first disk is reading some data, the second could be reading data, too. And because controllers and buses today are faster than what the disk can get to. Effectively we can transfer twice the amount of data assuming that those disks are actually transferring the data. You can be unlucky and all the data you want is on the stripes that are placed in this disk in which case you're going to get only the data throughput of a single disk. But on average we tend to get almost twice the throughput of a single disk by splitting the data across two disks like this. As far as latency is concerned, because we have more throughput we get less curing delay. Normally we would be able to handle requests one at a time here. Now we can actually in parallel do two requests. So in the queue, we get faster to the requests by basically handling previous requests faster. But what about reliability? Unfortunately, it gets worse than one disk. To see what the RAID 0 reliability looks like, let's assume that f is a failure rate for a single disk. A failure rate is really how many failures do we expect for a single working disk to have in a second. Obviously, this number will be extremely small. Usually we can assume that the failure rate is constant over time, that is, every second, if the desk is working at the beginning of the second, it has the same failure rate during that second. For a single disk then, the MTTF, the mean time to failure, will be 1 over f. So, for example, if the disk has one millionth of a failure per second, then the MTTF will be 1 million seconds. For disks, the entity f is also sometimes called mean time to data loss, or how long until we lose some data on this disk. And for a single disk, the mean time to data loss is the same as the MTTF of one disk. Now, let's look at what happens when we have N such disks in a RAID 0 configuration. It turns out that the failure rate when we have N working disks is equal to N times the failure rate of one disk. This is because, for example, if we can expect one millionth of a failure per working disk per second then we can expect one failure per million working disks per second. So if we start the second with a million working disks, chances are that one of them will fail during the second. If we start the second with half a million working disks, on average half of a single disk will fail per second, and so on. It also turns out that the mean time to the first failure among N working disks has the same distribution as the first failure for the single disk, so we get that MTTF for N disks in a RAID 0 configuration, where MTTF is really the mean time to data loss in this case, is equal to the MTTF of one disk divided by N. So if you have two disks, the MTTF of the RAID 0 array with two disks will be half of the MTTF of a single disk. A single disk typically will have the MTTF on the order of ten-ish years to maybe 100 years. So if we have too many disks in a raid zero configuration, we can reduce the MTTF, so the average disk will not fail in the five or six years that we actually end up using it, but if we have two or three or four such disks then we can expect the array to fail at least once until we actually get rid of it. Let us now test our understanding of Raid 0. Supposed that we are building a Raid 0 Array with four disks. Suppose that for one such disk, we get 200 GB capacity, 10 MB of throughput and an MTTF of 100,000 hours. So our Raid 0 Array can store how many GB, has a throughput of how many megabytes per second, and an MTTF of how many hours? Let's look at the solution to our RAID 0 quiz where we have an array of four disks, and for one disk we know the capacity, throughput, and MTTF. In RAID 0, we're striping the data across all the disks with no redundancy. So we can use a capacity of all four disks, and thus get 800 GB in capacity. If we spread the read and write load evenly across the disks, then one disk being able to support 10 megabytes per second and four disks being used in parallel like this gives us a throughput of 40 megabytes per second. And finally, if the MTTF for one disk is 100,000 hours, the expected MTTF for the whole array is going to be a quarter of this, if they are in RAID 0, so we will get 25,000 hours. This MTTF ends up being almost 12 years. This one is almost three years. If we plan to replace our array after something like five or six years, know that it can still fail within the five or six years, but on average it's going to last about 12 years. However, here our average array is going to fail during the five to six years. So obviously with such a low MTTF, we should not have used four disks in RAID 0 like this. The next RAID technique is RAID 1, which uses a technique called mirroring to improve reliability. So really, RAID 1 puts the same data on both disks. If we had only one disk, we would keep the data on it. With the other disk, it's just a copy of the first disk. So a write of the data from the system will cause a write to each disk. Now note that this results in the same write performance as when we had one disk alone because all of the two writes are happening. They're happening at the same time. Each disk is given the same write, so each disc does exactly what the single disc would be doing, as far as writes are concerned. So this write takes approximately the same time as it would take to do it on one disc, because they are really doing it simultaneously, but for reads, we can read any one disk. Both of them have the same data. We only need to read the data once. So we end up reading only one of the disks. So for reads, we get twice the throughput of one disk alone. Pretty much if we have more than one read to do, we're going to send half the reads to one disk and half to the other. They have the identical data so actually we can always split the disks. We never have the situation that all the data we want is on one disk because really, any data can be read from any of the two disks. And this scheme can tolerate any faults that affect one of the disks. So, it could be that for example, there's a bad sector on one disk. Well, we can just read the same data from the other disk. Or we can have an entire disk fail, in which case we are left with one disk that still has all the data. So remember when we said that we have two copies of something, and we can detect, but not correct the error. For disks this is not true, because the ECC on each sector of each of the disks will detect errors that occur already. So now that we know which one has an error, we can always use the other one that doesn't have an error there. So if you have two copies of something and you know which copy is wrong, then you can, of course, correct the error simply by reading the correct copy. But what about the reliability and the MTTF of Raid 1? Same as before, we'll assume that some F is the failure rate for a single disk in failures per disk per second. And for a single disk remember that the MTTF is simply one over F, where F is that failure rate. And then the mean time to data loss, which is the MTTF, really, of the whole disk array, in this case just one disk, would be simply the MTTF of that one disk. Well, let's say now that we have two disks in RAID 1. And remember that the failure rate when we have more than one disk is N times the failure rate of one disk as long as all of the disks are working. So what we have really is that both of our disks in raid one are okay until the time, entity of one disk over two. This is the expected time when one of the disks will fail. Now, unlike raid zero, where this is the point where we would lose data, now we have mirroring. So both of the disks need to fail in order for the whole thing to lose data. One failure is not data loss yet, so we don't have the failure of the array yet, but this time, we are down to one disk. That one disk is a working disk, and has this failure rate. So, that one remaining disk will live on for another MTTF of a single disk. This is very important to understand. If we have a working disk at some point in time, the MTTF for that disc is going to be, the overall MTTF for one disc. This MTTF is not affected by the fact that this disk has already been working for all this time. Why? Well because the failure rate is constant, it's not affected by time. So if we have a working disk, its failure rate is always the same. The only thing that changes the failure rate is if the disk fails. So if we had two disks and both work, this is the expected time of them surviving, and then if we're down to one disk that works, at the time when we have a single working disk this is the MTTF that we can expect. So overall, the mean time to data loss for RAID1 with two disks is going to be the MTTF of a single disk over two, plus the MTTF of a single disk. And now, we bought two disks, and used them in an array that has the capacity of only a single disk. So we basically paid for the extra disk, just to get reliability. And what we get is just one half more of the MTTF then we would normally get with only a single disk. So what's the point? The point is that this calculation assumes that no disk is replaced at the time when it fails. So we had the failure and we just let that fail disk just be there and we were down to one disk and didn't fix it and then we were just down to the entity F of one disk for awhile. But remember that when we want reliability we definitely will replace the failed disks. So when we have the first failure we will as soon as possible replace that failed disk. Copy the data from the working disk to it so now we're back to RAID1 working properly. So what's the MTTF for that? So what's RAID 1 reliability if we do replace failed disks as soon as possible? We have the situation that both disks are okay for this amount of time. Then a disk fails, and then we have only one okay disk for the mean time to repair for one disk, so this is how long does it take us to replace the disk? After that, both disks are okay again, and then we again have this long until the next failure. So what's the mean time to data loss for RAID 1 with two disks? The question really is, how many times can we have this until we encounter the period when a single disk has failed, and during the time it takes to replace it, another disk fails? Well, it's equal to the length of this interval. This is how how long we survive with both disks working, times, how many times can we get this until we encounter a repair period where the second disk failed during it. So, the question is really what's the probability of the second disk failing during the time to repair one disk. Now, normally, to calculate this probability, we would have to compute the distribution for this function and then see what's the probability for this distribution being under MTTR 1, and so on. But when the time to repair is a lot smaller then the time to failure of a single disk, then we can approximate this probability of the second disk failing during the MTTR for the first disk simply as the MTTR divided by the MTTF. So, for example, if the MTTR is, let's say, a single day, and MTTF is 100 days, then the probability of the second failure during the repair period is simply the length of the repair period divided by the period that we expected this to survive. So for example, if the time to repair is, for example, one day, and the time to failure is, let's say, a hundred days, then we can expect that there is a one in 100 chance off of this failing during the one day. So now how many times can we repair this before we encounter one of these? Well, the number of times we can try this until we encounter a failure is going to be 1 over this probability. So what we finally get is this. The mean time to data loss is MTTF of a single disk squared, divided by 2 times the mean to repair for one disk. How long does it take us to replace the disk and copy the data on it so that now we have a fully functional RAID 1 array? And as a reminder, this is how long do we get a working array with both these functional, and this is how many times do we do a repair until we can expect that the second disk that is covering us during the repair is going to fail on us. Not that because the MTTF Is much larger than the MTTR, this is measured in years, usually, this hopefully is hours or days. It's just how long does it take us to replace the disk. This factor here is very, very large, and now we get many, many times the MTTF of a single disk over 2. This factor way larger than 2, which means that the time to data loss with a RAID 1 array with only two disks is way higher than the time to data loss for a single disk, which means that RAID 1 is dramatically improving reliability. So let's see if we can figure out a RAID 1 array. Suppose we have a RAID 1 array with two disks, and for a single disk we have a 200GB capacity, 10 MB per second throughput and the MTTF of 100,000 hours. In that case, our two disk RAID 1 array has a data capacity of how many GB, a throughput of how many MB per second, assuming that we do equal numbers of reads and writes, and finally our MTTF is equal to how many hours? And for this type of RAID, we also need to know whether we replace failed disks or not, so here, assume that we do replace the failed disk and that the mean time to repair, meaning the mean time it takes us to replace the failed disk, is 24 hours. So let's look at the solution to our RAID1 quiz. We have a RAID1 array with two disks. A single disk in this array is a 200GB disk with a throughput of 10MB/s and an MTTF = 100,000h. And when a disk fails we replace it and it takes us 24h to do that. So, in a two-disk RAID1, the data capacity is the same as for one disk. That's because both disks are storing exactly the same data, which allows us to survive the failure of one disk. The throughput, when we are reading, is twice the throughput of a single disk because we can issue reads to both disks at the same time. But when we are writing, we are writing to both disks at once. So the throughput for writing is only 10MB/s. If half of the requests are reads and the other half of the requests are writes, then in any given second, we will be spending one-third of a second on the reads and we will be spending two-thirds of a second on the writes. That way, we get to do the same number of reads and writes per second. It's just that the reads are twice as fast, so they take half the time of the writes in that same second. In the one-third of a second that we have been doing reads, we were getting a 20MB/s throughput because we were using both disks. In the two-thirds of a second when we were doing writes, we were getting only 10MB/s. So overall during this one second using the raid one array, we ended up getting a throughput of 40 over 3MB, which gives us a throughput of 13.33MB/s, because in one second, we managed to access 40 over 3MB. So the answer here is 13.33. Note that the throughput is not 15MB/s, which is what we would get if we simply average the read throughput and the write throughput we would get in this array. Because if we did this, we are assuming that for a half a second we are reading and then for half a second we are writing, and thus we get 15 MB read and written in a second. But if for half a second, we read and for half a second we write. We end up reading twice the amount of data that we would for writes. So if the workload is 50-50 in terms of number of accesses, then it's not also 50-50 in terms of time spent on reads and time spent of writes. Reads are faster, thus we spent less time on them, which is why again the throughput is not 15MB/s. And finally, let us figure out the MTTF for the entire RAID1 array. The MTTF of a whole RAID1 array with two disks, we set, is equal to the MTTF of a single disc divided by two. So this is how long we have to operate with two discs until one of the discs fails. And now each time we have a one disc failure, we need to repair it and we said that the number of times we can try a repair until we finally experience a failure of the second disc during the repair of the first disc is MTTF of one disk divided by the MTTR of the disc. So this first factor will be 50,000 hours. The second factor is the ratio of the time to failure and the time to repair. And when we divide these two we get 4166.666 etc. So this is how many times can we try a repair while a single disk is working until we finally experience a repair in which the working disk also fails. And when we multiply this out we get 208,333,333 hours. So this is a huge amount of time compared to the original disks MTTF. As you can see RAID1 dramatically improves reliability. This time is about 11 years. This time is about 24,000 years. So as you can see, the reliability has improved from being able to expect data loss every about 10ish years to something that goes into tens of thousands of years. We will not talk about RAID 3 because it's very rarely used today. So we will go to RAID 4, which is actually not very often used, but it helps understand how it works, to understand how RAID 5, which is more often used, works. Raid 4 uses a technique called block interleaved parity. It uses N disks and N minus 1, all but 1 of these disks contain data, which is right across these disks just like in RAID 0 and then the last disk has the parity blocks that protects all of the blocks of the N minus 1 disks. So let's look at an example with four disks. So we said that three of the disks contain data, so this would be stripe zero. Then here we have stripe three, four, and five, and so on. So we just distribute out data across all but one disk. And then we take the date of stripe zero, xor the whole stripe bit by bit with stripe one. Xor the whole thing with stripe two and what we get will be as long as a stripe. And that gives us the parity stripe for zero, one, and two and we put that on the fourth disc. Then we go to the next stripe on each disc, xor them and that gives us the parity stripe, for these three stripes and so on. So the idea here is that if, let's say one disk fails, we have the data of the two disks here, and we have the parity. We can reconstruct what was on the first disk by exporting these three. Or if there is an error in just for example this one stripe gets too damaged to reconstruct by using the error correction code of this disk in that case again, we can read this stripe, this stripe and the parity. And then reconstruct what should have been the bits of this. So effectively, now we can tolerate the failure of any one disk. But the cost is not two times as many disk as we need to store the data. Here for example, we are paying only one quarter of the whole system is spent on parity, and three quarters on the data. Whereas with mirroring, we would've spent half of the overall capacity of the system on the mirroring. You can think of Raid 4 as kind of a more general technique than mirroring, because in reality using two discs for RAID 4, it's very similar to mirroring because here we will store the bits of the data, and here we will store the same bits. Because a bit, when you don't export it with anything, you just put it here. So mirroring is just an extreme case of RAID 4 when N is just two, but you want to go to more than two with RAID 4 because that allows you to have lower costs while still being able to recover from losing one disc. So now, what does a write do? It must write one of the data disc and it must read and write the parity disc. To update the parity. Because one of the stripes have changed. A read, however, only needs to read the data from the disk where the data is. We will only reconstruct the data if that read fails. But in the normal scenario, where we get the data we want, we don't even have to consult the parity bit for reads. So let us look now at the overall performance and reliability of a Raid 4 array. For reads, we are getting, on average the throughput of N-1 disks. We can read data from all of the data disks, but the parity disk has no useful data on it, so for reads, we don't use it. This is significantly improving RAID performance because usually we have more than just two disks in an array. So for example with four disks we are really getting for reads the throughput of three disks, all of the data disks. For writes however, the data accesses are distributed among the data disks. However, every write requires the parity disk to read and then write. We need to read the old parity, update it and then write it back. So we are getting only one half of the throughput of a single disk, because the parity disk, again, needs two accesses for every write. This is a significant disadvantage, and we will see that this is really the primary reason why we need raid five. Now let's look at the reliability. What is the entity F of a raid for this? Again, we have two options. Just like raid one, the first period we get is while all of the disks are working. The time you have during that period is on average MTTF of the single disk divided by N, because that's how long we have for the first of N disks to fail. Just like grade 1, after this first failure, we did not lose the data yet. And now we have two options. If we do no repair of the failed disk then, now we are operating with N minus 1 good disks, if we do no repair or the failed disks, what we get now is at the point of the first failure, we are left with the N minus 1 disk array, and any disk failure in that array is going to result in data loss. So, if we don't do repair, then to this, we add the expected time until the first failure among the N minus 1 disks that we still have. This plus this, if N is, let's say, 4, it's not going to be better than the MTTF of a single disk. So you don't want to even consider using RAID 4 unless you're going to do repairs. So pretty much you don't do this because it's a bad idea. You're not getting an increased reliability. What you want to do is repair the failed disk, in which case the chances of a disk failing during the repair period is the MTTF of what's left of our array. With RAID 1 we had one disk left Here we have N-1 disk left, so the MTTF for the remaining disks is lower than for one disk, because we are talking about the first failure among the N disks. So for example if we have three disks, the first failure among three is going to happen sooner than the first failure among just the one disk. And then we divide that with the mean time to repair. So this is the factor with which we now multiply our, this is for how long we get a RAID 4 to work until we lose a disk. So overall MTTF of our RAID 4 array is going to be the entity of one disk times again the entity of one disk divided by N time N minus 1 times the MTTR for the disk. So note that again this is very small compared to this. So this particular ratio is extremely large. The number of disks is, let's say, four, you don't want to have too many disks in a RAID 4 array. If that is so, then what we get is the MTTF of a single disk that is fairly large multiplied by something that is usually in the thousands or more and divided by something that is usually something like, if it's four disks, it's going to be 12. So this MTTF or RAID 4, if we repair the failed disk every time it fails, we get an MTTF that is still very very large. Now let's go back to the topic of raid 4 writes. Let's say we have raid 4 with five disks here. And let's look at just one set of stripes with their parity. And now let's say we want to write new content to D0. So of course here, we will have changed the data. And the remaining data disks don't have any changes to their data. But the question is how do we compute a new parity? One way of doing that would be to read the data from the other three disks and XOR that with the new content in the fourth disk. But that would result in three reads and a write. If N is even larger that will increase the number of reads we need to do the write. So instead, we are going to figure out how to update this parity. What we need to know is what was the old data and the new data with we XOR these we get which bits of the data have changed. If we know the old parity, and this XOR change and we XOR the two together, we get to flip all the bits of the old parity that would change as a result of the bits. So basically a flip in the data here would result in a flip in the parity here. We can compute it, what to flip, XORing with the old parity flips it and that's what you write to the new parity. So that's why we have a read and a write of the old parity. In addition we have a read of the old data if we don't already know it. And we know the new data. So basically we have a read from the data disk. A read from the parity disk. A write to the data disk and a write to the parity disk. And the parity disk will be a bottleneck because the four disks here are accessed by writes in a way that reads and writes one disk. Another write would maybe read and write this disk but all of these read and write the parity disk, so it becomes a bottleneck. Because the parity disk is a bottleneck for writes, we have RAID 5 which has exactly the same reliability property as RAID 4 and very similar read performance, but it fixes the write performance. So let's see if we can now figure out a RAID 4 array. Suppose we have a RAID 4 array with 5 disks. A single disk is just like in previous RAID quizzes. 200 gigabytes, 10 megabytes per second, throughput and an MTTF of 100,000 hours. When a disk fails in this RAID 4 array, we replace it and it takes us 24 hours to do that. So then our 5 disk RAID 4 array has what data capacity in gigabytes? What throughput in megabytes per second? Again assuming that we do 50% of our accesses are reads and 50% of our accesses are writes. And finally, what is the MTTF of the whole array? Let's look at the solution to our Raid4 quiz. We have a Raid4 array with five disks. And one disk has 200 GB in capacity. The data capacity of the array is the data capacity of all but one disk. Because we sacrifice a single disk to parity. So among the five disks one is parity the other ones contain data. So we have four times this much which is 800 gigabytes. The throughput of the array for reads is going to be the throughput of the four data disks. So we have 40 megabytes per second throughput for reads. But for writes, it's going to be half a throughput of a single disk and that's because the parity disk need to be both read and written for every write. So, we get five megabytes per second here. So now we need to figure out what happens during one second. Well, this throughput is eight times the throughput of the writes. Which means in every given second one-ninth of a second will be spent on reads. And eight-ninths of a second will be spent on writes. That way we spent eight times as much time on writes as we do for reads, which allows us to do the same number of reads and writes. So now the overall throughput will be what we can read in one-ninth of a second plus what we can write in eight-ninths of a second. And that ends up being 80 over 9 megabytes. So this is what we can do in a second. So our average throughput ends up being 8.89 megabytes per second. Again, just like in our Raid1 quiz, we cannot simply average the throughput for reads and the throughputs for writes. We would get something more than 20 megabytes a second if we do that. But that would do a lot more reads than writes, because it assumes that half a second we are reading, and we managed to read 20 megabytes, and then half a second we are writing, and managed to write 2.5 megabytes. As you can see, the read to write ratio is not 50/50 here. So that is, again, why, we get this, and not just a simple average of these two. So the final question here is what's the MTTF? We know that the MTTF of RAID4 is equal to the MTTF of 1 disk squared divided by N, times N-1 times the time it takes us to replace a disk. Our N in this case is 5 so we get 5 x 4 times the s. And in this expression again this is the time that we get to work with the full RAID4 array until we have a failed disk. And this is how many times we survived the repair without the second disk failing. The overall number is 20,833,333.33 hours, which, again, much better than this. And we already said that this is about 11 to 12 years. This is more then 2,000 years. Remember that our RAID1 had 20,000 years, so this is significantly worse than RAID1, but on the other hand we are working with 5 disks here and spending only one-fifth of our capacity on parity. With Raid 1, we were working with only two disks, and spending half of the capacity of the array on parity. So we sacrifice some MTTF to get better cost per gigabyte. But we still achieve a very, very high MTTF. But before we go to raid let's see if we understood what codes, especially parity can do for us. Suppose we are using parity to detect bit-flip errors in our dram memory. Our unprotected memory has eight one bit arrays, each of which is 1024 by 1024 bits. So we have eight small arrays. Each of these arrays is given the same row and column address and each of these arrays outputs one bit of the overall eight bit value that we're trying to read or write in this memory. We want to use parity to detect bit flips, and what we can afford is a parity bit for every group of four data bits. The question for you is whether we should add two more one-bit arrays of this kind for parity. So the idea is that we would add another array here, and the bit stored here would be the parity for the first four bits here and another one would store the parity for these four data bits. Or should we add these additional parity bits every row in each array? So that we don't need the extra modules but what we do is we extend each of these modules with the extra bits that protect the data in that row. Note that the overall number of bits that we add is the same in these two approaches. The question for you is which one is better? Let's look at the solution to our parity quiz. We're adding parity to our memory, which consists of eight arrays. So now the question is should we add parity to each row of data? And then when we read out the row into the row buffer, we check for parity and report an error? Or should we have the modules as is and just add two more that will hold a parity? Both approaches have exactly the same number of extra bits we're adding, but the approach of having extra modules as opposed to changing the modules is preferable for two reasons. One is that this way, we can design these memory arrays. Both are unprotected and for protective memories. The protected memories will just have additional arrays, but the design of each array is the same. If we decided to add parity bits here, then depending on exactly how many parity bits we are adding for how many data bits, and whether we are having parity at all, we would need to redesign our memory array every time. The second reason why using redundancy at the coarser grain rather then finer grain tends to work better, is because lets say the ten array fails by reading zeroes for the entire row, this is not such a far fetched error. In that case what we have is if we have the parity bits added to each row, the row still has correct parity. So a single failure, for example, in the row decoder will just read the wrong row, it still has the right parity. If however, we separate the modules with a parity, then each of the modules will operate on its own so the error of our row decoder here will cause this bit now to be wrong in every eight bit word we read. But if this other arrays are still working correctly, then we can still detect that because the parity is somewhere else. So this is kind of similar to doing geographical distribution of our computers, for redundancy. Pretty much the further away the protection bits are, the more likely they will protect us against large failures in any single component. And finally, let's talk about Raid 5, which uses a technique called distributed block-interleaved parity to protect the disks. It does block-interleaved parity just like Raid 4, but its parity is not all on the same disk, it is spread among all the disks. So when we have four disks, the first set of stripes, in Raid 5, will be exactly the same as in RAID 4. There will be 3 stripes of data on the first 3 disks. And the parity for these stripes. Which is computed by XORing the data bit by bit In the three stripes, and the resulting parity stripe is written to the fourth disk. But Raid 4 would do the same thing and put the next parity here as well. In contrast, Raid 5 puts the next parity strip not here, but on the next disk. So we get 3 data stripes on these disks now and their parity is here. Then for the next set of stripes we again move the parity to another disk and then for the next set of stripes the parity moves again, and then for the next set of stripes the parity would again be here. And so on. So as you can see, the block-interleaved parity, what it really does is it gets the parity here, and then here, and then here, and then here. And then again, here. So each of the disks will get its fair share of parity. But for each set of stripes, we will have three stripes in this case in the parity block. It's just that the max set of four will have a parity in a different place. So what we get is the read performance where all four disks can actually be used for reading data, and we get N times the throughput of one disk. Remember that rate four was getting N minus one times the throughput of one disk, because one of the disks was not used for data. Here the data is spread over all four disks. So chances are of all the blocks we want to read they will be equally distributed among the four disks in this case. Write performance is still affected by having to read and then write the data in the parity. So a single write results in four accesses. If you remember, we will read the data block in the parity and then we would write the data block in the parities. So there are four accesses per write but they're distributed among all the disks. So we get N over four times the throughput of one disk. In this case we have four disks so right performance is equal to the throughput of one of the disks if it was used without RAID. Remember that in RAID 4, our write performance is always limited to half the throughput of one disk. In this case it would be half of the performance of one disk versus the full performance of one disk. So we win. If we had for example eight disks, here we would get twice the throughput of one disk, with RAID four, the eight disks would still have a single parity disk and the write performance would still be half the throughput of that one disk. And the reliability of RAID 5 Be the same as RAID 4. This fails if we lose more than one disk. Just like in RAID 4, in any group of four stripes here, we could lose one of the disks. If we lose the parity disk, we can still read everything and write. If we lose a data disk, we can reconstruct the data using the parity. Here, any one of the disks play the role of a data disk in some group of stripes and play the role of a parity disk in some groups of stripes. So all groups of stripes are still usable after we lose only one disk. If we lose two disks, however, then we have a problem. So now let's try to figure out a RAID 5 array. So now we have a RAID 5 array with five disks. One disk is still the same as before, 200 GB, 10 MB/s, MTTF of 100,000 hours. And again, we replace a failed disk when it fails, and it takes us 24 hours to do that. So then our five-disk RAID 5 array has what data capacity in gigabytes? What throughput in megabytes per second, assuming that we do half-reads half-writes, assuming that half of our requests are reads and half our requests are writes, and what is the entity F for the array in hours? Let's look at the solution to our RAID5 quiz, where we had the five disk array with disks similar to what we've seen in quizzes before, and the replacement is 24 hours once the disk fails. The data capacity for RAID5 Is just like for RAID4, 800 GB. Effectively, we still sacrifice one disk worth of capacity to parity. It's just that this parity now is distributed among the five disks. The throughput for the five disk array however, is much better than for a RAID4. For RAIDs, all of the disks now have equal amounts of data. So we can use all five of them when we want to read. For writes now, we distribute the parity accesses among all of the disks. So all of them can now support writes. But we need four accesses so we could be doing what needs to be done for writes at 50 MB per second throughput. But what needs to be done for writes is four accesses. We need to read the data and the parity, and then update the data and the parity. So we do four accesses for every write, which means that our write throughput is going to be one fourth of what we can do. Just like before, that means that for one-fifth of a second we will be doing what reads need to do, and for the four-fifths of a second will we be doing what writes need to do in order to achieve the same number of reads and writes. Because reads take a quarter of the time that writes do. That means that in our one-fifth of a second, we will do ten megabytes of reads, and then in the remaining four-fifths of that second will do ten megabytes of writes. That way we achieve the same amount of reads and writes, and the overall throughput is 20 megabytes per second. This is much better than in RAID4. Finally, our MTTF here for RAID5 is exactly the same as it was for RAID4. It is the MTTF of one disk divided by five. This is how long we have until one of the disks fails. Times how many times can we attempt a repair until we finally experience a failure of one of the remaining four disks in this array. So this part here is really the expected lifetime of the four disks, left to their own devices. And this is the MTTR. And what we get is the same number here as we did for Raid4 which is 20,833,333 hours which is more than 2,000 years again. So Raid5 has the same data capacity and the MTTF as RAID 4, but has significantly better throughput. For reads, it has slightly better throughput because we can use all five disks instead of just four. For writes it has a lot better throughput. So that means that we never really want to use a RAID4 array because we can move to a RAID5 array without sacrificing data capacity, without sacrificing the MTTF, but with improving the throughput. And now we have seen RAID 0, 1, 2, there is a 3 but we didn't see it, 4, and 5. Is there such a thing as RAID 6? Yes, there is. It's similar to RAID 5 except that each group of blocks, instead of getting one parity block, gets two. Let's call them parity quoted blocks. They have different types of parity. And as a results, RAID 6 can work when there are two failed stripes per group. Note that we cannot do this with two parity blocks. One of the blocks that we keep extra, one of the known data blocks is going to be the real parity block, and that's what fixes one error. The second of the two blocks is a different type of check-blocks, and when only one disk has failed, we use the parity to recover. But when two disks have failed, we can still solve some equations, where the data and the two types of check-blocks figure, so that we can still recover the content of the two failed disks. So let's compare RAID 5 and RAID 6. RAID 6 has twice the overhead of RAID 5, so we pay twice as much for this increased reliability. There is more write overhead because really now when we update a data block we need to read and write the data block and both of the check-blocks, the parity and the second one are also read and written. So there are now six accesses per write versus four per write that we had with RAID 5. So the only reason for having RAID 6, if there is a good chance of when a disk fails, another one failing before we manage to replace the failed first drive. So note that in any of the RAID configurations, once your disk fails, you don't keep going and ignore the failed drive. You keep going, but you order a replacement or get a replacement and install it so that the RAID array can re-establish itself. After that, you can lose another disk. So really RAID 6, with its ability to lose two disks, only helps if when a disk fails, there is a good chance of another failing before the first one is replaced. The faster we replace the first drive, the less chance there is that another disk fails during that period of time. Remember that the probability of a disk failing during an hour is extremely low, so even if we spent something like three days replacing a disk, the probability of a disk failing in that specific time, while we are vulnerable, is very, very low, and then it seems that RAID 6 is an overkill. So the argument for RAID 6 being an overkill is that when in RAID 5 a disk fails, it takes up to a couple of days to replace. Sometimes you have a spare disk and usually a data center will have that, so it's going to take less than an hour. And there is a very, very low probability that another disk in the array will fail in those three days. If it fails after these three days, it doesn't matter because by then we are back to being protected by the extra disk. So the failure of the second disk needs to happen in the time when one of the disk is in the failed state. But that assumes that the disk failures are independent. But the failures can be related to each other, in which case this is not correct to assume. A typical scenario where this can happen is, let's say that we have a RAID five with five disks and one of the disk fails. Let's say that disk number two in the array has failed. The system reports that a disk has failed and continues functioning normally. Remember than it can do that with one failed disk in RAID 5. And now the operator gets a replacement disk, opens the computer case and sees five drives like this, knows that the number two has failed so takes this one out and puts the new drive in. But for example, the numbering of the disks was actually 0,1,2,3,4 so we pulled out the wrong drive. This was the failed one. When we pulled the second one, that was the second failure, and now we really have two failed disks. One failed because it failed, the other one failed in the course of trying to repair the first failure. So that's, for example, a case of correlated failures, and in that case, we cannot assume independence. So pretty much when a disk fails, the actions to repair it are creating a lot more risk for the other disks and may cause a second failure. And that, for example, where you want to have RAID 6, when one fails and you're trying to repair it. If another one fails for any reason, independent or dependent on the first failure, you're still safe. In this lesson we'll learn how faults, errors, and failures are related to each other and how redundancy can help us prevent faults from becoming failures. This lesson also concludes the second part of our computer architecture course where we can learn how the memory hierarchy works. The final part of this course will focus on what we need to do in computer architecture to get correct and efficient execution in multi threaded and multi core processors. Branch piction and if conversion help us eliminate most of the pipeline problems caused by control hazards. But data dependencies can also prevent us from finishing one instruction every single cycle. So, what can we do about data dependencies? And why stop at only one instruction per cycle? In this lesson we will learn about instruction level parallelism which tells us how many instructions could be possibly executed in any given cycle. Okay. So the most ideal situation would be, if all instructions that we need to execute just go through the pipeline in the same stage. So basically, all of, all of them will end up executing in the same cycle. Let's see an example of this. So we have a nice table here. And now we're going to have some five instructions here. So let's have instructions, so here is where we're basically just going to have our instructions. And let's say that you know, the first instruction is an ADD instruction. R1 equals R2 plus R3. I'm going to draw it like this so that we don't have to actually have the opcodes and everything else clutter the screen. And then we have R4 equals R1 minus R5. And then we have xor R6 equals R7 X or R8. And then we will have R5 equals R8 times R9. And finally we will have an add of R4 equals R8 plus R9. So remember these are instructions, not actual C-operations, it's just that I am kind of showing them like this because it is a more compact representations of them. And now let's see what happens in a pipeline where we do this for a five stage pipeline. So in the first cycle, so this is cycle 1, this will be fetched and this will be fetched and this will be fetched and this will be fetched and this will be fetched. And then in cycle 2, we will decode this and read registers, and decode this and read registers, and decode this and read registers, and so on. And now if you remember your five stage flight plan, the time comes to execute this thing. So if we just get all five of them here and execute them. We will have a very good CPI. In reality, what's going to happen is that we are going to eventually write the results for all of these things. And if we had more instructions here, it wouldn't matter. All of this would be done in basically five cycles. So if we have five cycles for a very large number of instructions, the CPI is going to be five cycles for pretty much an infinite number of instructions which is going to be very close to 0. So we get a CPI for 0, which is very good. You know, you are getting basically 0, almost 0 cycles per instruction. So that's wonderful. So what's wrong with something like this? Well what's wrong with it is that when we execute these instructions what has happened is this instruction and this instruction have red registers at the same time. Remember that the decode stage is also the read register stage. So, what happens here is, we are here adding the R2 plus R3. And here, when we are writing, we are really writing R1. Okay? But when we are adding here. I mean are subtracting R5 from R1. This R1 here is not the R1 that this instruction is writing. It's the R1 that we read before that instruction managed to write anything. What's wrong with this is that this first instruction is going to execute correctly, but the second instruction because we are executing it too early, is going to basically read the R1 from before the first instruction so it doesn't get the result of the first instruction. So the programmer that wrote this wanted this R1 to be this R1. It was supposed to be like this, but it isn't. And that means that we will have to do something about this. We cannot just let instructions execute in the same cycle. We actually have to figure out which ones can execute and which ones cannot execute in the same cycle. Okay so we have seen that there is a problem when multiple instructions execute in the same cycle. And the problem occurs because we need to read registers before the previous instructions have written them. Because those previous instructions now execute at the same time as our instruction. And then we can see that problem really occurs in the execute stage because we are just operating on wrong value. So let's see in the execute stage if forwarding can help us with this. If you remember your pipelining. What forwarding does is it feeds the value from the previous instruction to the subsequent instructions because those values have actually been written to register. So let's see if that helps. If you remember, we had our instructions and now we are just going to look at the execute stage, and we are going to have our instruction just be I1, I2, I3, I4 and I5, and remember that there was a dependence from I1 to I2. So now what would happen is, we execute I1 at some point, and in the same cycle we execute I2. So the problem with forwarding is that it would be able to feed this value to the instruction in the next cycle. But not to an instruction in the same cycle. Cause basically if you look at the timeline for the cycle. This is the kind of beginning of the execute stage. This is the end of the execute stage. In here somewhere, we are kind of taking two values, operating on them, producing the result. This result is available kind of here, at the end of the cycle. Meanwhile, the other instruction is taking its values during the subtraction, producing a result. And this is where forwarding can provide this value. If forwarding worked, it would have to provide the value here, and any arrow that goes backward here means basically time travel. So pretty much you would produce a result and, you know, one cycle send it back in time one nanosecond earlier, so that it can be used here which is of course impossible. So we cannot do that. What we will do is not execute this instruction here. We will have to stall meaning this instruction has to be delayed it just cannot execute in this cycle. It will execute in the next cycle. If I3, I4 and I5 don't have any dependencies with I1, they can still execute you know they have read the register values they're going to be able to execute. So now what we have is really. We spent two cycles executing five instructions, okay? So if we ignore the filling of the pipeline and so on, basically what we have now is the CPI is equal to two over five in this case, which is approximately 0.4. Actually, it is 0.4. So what we have is a CPI of, you know, instead of it being 0.2 for these five instructions, it would be 0.4. If we have a lot of instructions, there will be a lot of these dependencies. So we will actually have to spend, you know, we will have a CPI that is not zero, even if we had an infinite number of execution units. Okay, so what we have seen just now is that basically even the ideal process that can execute any number of instructions per cycle. Still has obey raw dependencies. Meaning, it still has to wait for results to be produced before they can be used by instructions that need those results. And that really creates some of the delays that the ideal processor is going to have. And that's why the ILP is not 0, it's going to be something larger than 0 even for an ideal processor. So let us now look at some instructions. So let's say that I1 is producing the value that's being used by I2. Then I3 is producing the value for I4. And then, I4 produces the value that is used for I5. So what is going to be the CPI for this particular program. Well it's going to be CPI is equal, how many cycles do we need for this, for these five instructions? Well here's what's going to happen. I1 can execute immediately in the first cycle. And so, can I3 because it doesn't have to wait for anything. So we spend one cycle executing I1 and I3. The next cycle I2 can now go. And I4 can also go because I3 is now done. And then, we still need a third cycle to execute I5 which has to wait for I4. So we have spent three cycles for five instructions, which is going to be a CPI of 0.6. So the CPI is still lower than one. But now it's even larger. And finally, if we did have this dependence as well. Now we have to do I1 and then I2, and then I3, and then I4, and then I5. So then now the CPI with this is going to be one. Why? Well because we need five cycles to execute five instructions so the CPA's going to be one. So basically in the program the dependencies, the, the raw dependencies are going to determine basically what is the possible CPI. Even if the processor can do everything else ideally. Meaning it can fetch a number of instructions, it can decode and read registers in any number of, instructions. But it simply cannot provide time travel, which is, which is kind of the only consideration that you really consider in this case. Is just that we cannot do time travel. Everything else we can do. We still have a CPI that is somewhere between one and 0. You will have a CPI of 0 if a very large number with no dependencies. We have seen that. We have a CPI of one. If you have any number of instructions that are all dependant on the previous instructions, you really can not let anything get by So, let's say that we had these five instructions, and for now, all that matters for write of the write dependencies is, which registers are we writing to. So let's say that the first instruction is writing to R1, it's computing some result, it doesn't really matter what it is. And then the second one is writing to R4. Then we have R7 and R8 being written. And then the fifth instruction is also supposed to write to R4, so we will see that the problem will be with the second and the fifth instruction writing to R4. So what can happen is, let's say that there is a, read after write dependence between the first and the second instruction. So, what's now going to happen is, there will be some cycle, let's call it cycle 5, in which R1 is going to execute. R4 cannot execute. It has to wait. And it will execute here in cycle 6. Meanwhile these instructions will execute, because they don't have any dependencies. Now in our five stage pipeline what's going to happen is basically this instruction is going to move to the mem stage where it doesn't do anything because none of them are really memory instructions. And then so will these. And then what's going to happen is in the next cycle this instruction will write this results. So now, R1 gets written. This instruction is also going to write this results, so R7 is going to be written. This one writes R8, and this one writes R4. However the second instruction has executed here so it is going to go to the MEM stage here and write the result here and it is going to be R4. So if you look at what is the final result in R4. The final result in R4 is whatever was the last thing written to it, which is going to be this value. But it should have been this value. Because when the program that wrote this they wrote it this way so that they could expect that our R4 would retain the value from this fifth instruction and not from the instruction. So basically the programmer wanted this to become temporarily this. But then take a new value here and that's kind of the last value after these five instructions. But that didn't happen. So that is a problem. Why is it a problem, I mean why did it occur? Well because this instruction got delayed, so we have executed these two instructions out of order. This is the second instruction and it writes in the eight cycle. This is the fifth instruction, but it writes in the seventh cycle. So basically we have basically written the result in the wrong order. Our I mean, it's, it's an order different from the order in which they were supposed to execute in the program. So in order to fix this, one solution will be. So to fix this, one solution would be to not have this instruction write the result here. Instead it would have to be stalled here and here. And finally, write this result here. So a real processor might, see in some way, needs to figure out how to do this. Because these rights need to happen in the correct order. Okay, so now that we have seen two types of dependencies and how can they affect the scheduling of instructions in a processor just trying to do more than an instruction per cycle, let's do a dependency quiz. Let's say we have a processor with a classical five-stage pipeline, meaning that in one cycle it fetches, then reads registers and decodes, then executes, then performs a memory access and finally, writes registers. Let's also say that the processor has forwarding that can, if the result has been produced, it's going to feed it to the instruction correctly even though maybe the result has not been written to a register yet. But the result has to have been produced, meaning that the instruction that produces the result had to either finish the execute stage if the result is something like a plus or had to finish the memory stage if this is a load instruction. So, we can basically then fit that result to the instruction that executes in the next cycle without having to wait for the producing instruction to get to the right register stage, just as we discussed before. And finally, let's say that this processor can execute ten instructions in each stage. So, theoretically it could in one cycle fetch ten instructions, then decode all ten, then execute all ten if there are no dependencies, and so on. So, in five cycles it's going to execute all ten instructions. If we have 20 instructions, the first ten would be fetched in the first cycle. It would immediately in the next cycle fetch the ten. So theoretically, it would be able to finish all 20 in six cycles because we still have to get to the right reg stage. The question is, when do we execute and write back each of these instructions? So we have these six instructions and the question is when do we execute them. To help you get started, let's say that the very first cycle is cycle zero, so that is when we fetch them all. It reads registers in cycle one, so it will execute definitely in cycle two. Also, because the ADD here is using R2, it has to wait for the MUL to execute and then forwarding will produce MUL's value to the ADD. So the ADD has to wait and execute in cycle three. If you look at the MUL because it has executed in cycle two, it will go through the MEM stage in cycle three and write its result in cycle four. The questions is, when are these others and what cycles are they producing and writing the results? So the ADD is peer producing a value of R1. That is not a problem because the previous instruction is just for using R2. So the ADD can't write its result when it's time. And when it, it's executed in cycle three, so it's going to write in cycle five. Now let's look at this multiplication instruction over here. Remember, we can execute any number of instructions. In the, in the same cycle. The reason we had to delay the ADD is because it actually has this dependence from R2 to R2. The multiplication is using only R3 and producing R3. Using R3 has no problem because neither the mul, nor the add are producing R3 so this multiplication can actually execute in cycle two. And then, there is no problem with R3 here because there is only R1 and R2, written by the previous instruction. So this multiplication can actually write R3 in cycle four. Because it's like two, three, four. Now, let's look at this ADD here. It uses R1 and R3. The R1 it's supposed to be using is produced by this ADD. Which executes in cycle three. The R3 is produced in cycle two by the multiplication. So this instruction actually has to wait for cycle four because of this. Then it's going to be cycle five when it's in the MEM stage. And it's able to write its result in cycle six. It's going to override the result from the ADD here but because the ADD is writing in cycle five. It's okay if we write in cycle six so everything is fine. Here we can write in cycle six. Now let's look at the multiplication here. It's using R4 none of the other instructions are producing R4. So it can also execute in the second cycle. It produces R4. None of the previous instructions are writing it so we have, we don't have to wait for anything. So we can actually write our R4 in cycle four as well. Just like this amount here. And then finally let's look at this add here. It's using R1 from here. It's using R4 from here. So our execution has to be in the cycle that follows the larger of these two. The larger is the cycle four, so we have to begin our execution in cycle five here. At the end of cycle five, then we will have the result. And the MEM, we can write the result in cycle seven. But we have to check, is somebody else going to override it later. And that's not going to happen, because if we write here in cycle seven, we are safe. This is the latest instruction, it should do the write of the, of the R1 at the latest time among all the other instructions. And if you look at the previous instructions have written the results in cycle five and cycle six. So it's okay if we here write it in cycle seven. So this is the solution to the quiz. Okay so we have seen that we have to worry about the read after write dependencies because we actually have to feed the value that the instruction needs, from a previous instruction. We also have to worry about write after write dependencies possibly because we don't want, we want the last instruction in the program order to be the last one to actually. Write the result. And also anti dependencies we need to worry about them too because, we don't want to overwrite results before the instructions that need them have had a chance to use them. So, now we're going to talk about removing false dependencies. What are false dependencies? Well, we have read after write. We have write after read. And we can also have a write after a write. These, are called true dependencies. Because, you actually have to obey them, that's how the program computes. Some instructions produce values that are then used by other instructions and so on. So the instructions that use values really depend on instructions that previously wrote the results. In contrast, these. Are called, false or name, dependencies. Why? Well, because there is nothing fundamental about them. They are dependencies just because you're using the same register for two different results. So, for example, in a write after write dependency we have seen that two different instructions would write to the same register and that creates the write after write dependence. If the second of those two instructions, were using another register to write this value, then there would not, be no dependents. We will see an example of this a little bit later. But the idea is that while true dependencies, you actually. Have to delay instructions in order to satisfy them, with the name dependencies you could actually execute instructions earlier. If only you could take care of, you know kind of multiple values for the same register as we will see just in case, just in a couple of seconds. So one of the approaches for eliminating false dependencies is to duplicate register values. Let's see how that would work. We have an instruction here that adds R2 and R3 and puts the result in R1. Then we have R1 minus R5 goes into R4. Then we have R4 plus 1 goes into R3. Then we have R8 minus R9 goes into R4. And sometime later, we have an instruction that uses R4. Here we have what happens in some cycle. Let's call it Cycle 100. And in the next two cycles. And let's assume that Cycle 100 is when this instruction can execute. Now what happens is, after we execute this instruction, it can supply the register value R1 through forwarding to the next instruction, so we have that this instruction can execute here. And then R4 can be supplied to the next instruction, so it can execute here. However, this instruction, which is doing R 8 minus R 9 could actually execute here. So the problem with false dependencies, occurs in that R 4 is written by this and also this instruction, so let's see which final value of R 4 ends up in the register, so that it is used much later here. Well, if this is in execution stage, let's say that we have the memory stage next and the write register stage next. Meanwhile, this instruction goes through the MEM stage here and then write register here. So, as you can see, we first write this R4 and only then write this R4, which means that the final value of register R4. And that is the value that will be read by this instruction is actually going to come from this instruction and that should not be happening. So in order to fix this, one way is to again duplicate the registered values. So the idea is that when this instruction writes to the register of 4, it writes to a version of register of 4. And then another value is stored to another version of R4, but we remember both versions. So that's why it's called duplication, pretty much. We don't store only one value for R4. We store all the possible values that R4 had. And then, an instruction that wants R4, will have to search among all the possible versions of R4, to find the one that is before it. But the latest one before it. So in this case, we will search for R4 and find this one, and this one, and this is the latest one that we should be using. That requires two version of R4 to exist. Meanwhile this instruction here. When it executes, we'll look for version of R4 and again find that there really are two versions that it big be using R4 here and R4 here. But because this one is coming after this instruction it should be ignored and thus we use this value. So everything will be correct but it requires us to keep multiple versions of each registers which is really complicated. So, we have seen that the duplication of all the register values that have all, ever been produced is a problem. So we will located registry naming, which is another scheme that can provide relief from false dependencies, but in a way that actually is more manageable. How does the registry naming work? So, register renaming separates the concept of architectural registers, which are the registers that the programmers and the compilers use. So, this is when you write an instruction like add r1, r2, r3, the r1, r2, and r3 that you're referring to are the architectural registers. In register renaming, there is also. A concept of physical registers, which is all the places that we can actually put a value in the processor. So, there is of course a place for, kind of, the latest value of R 1, for example, but there could be additional places where we could potentially store values that should be going to R 1, and then what we do is. As the processor reads instructions and figures out which registers they're using, and remember the program is referring to architectural registers. As it's fetching instructions, and decoding them, it also does register renaming which pretty much means that it re-writes the program to use the physical registers. So, basically if it sees Add R1, R2, R3. It is going to rename that into our ad or something else and so on, in order to basically refer to the places where it has actually put those values, and how does it know where the values are? Well, it uses a table called the register allocation table or. RAT, and you should remember this because we will be very frequently referring to the RAT because it's used very very often. What the RAT is, is a table that says which physical registers has the value for which architecture register. So, for each of the architecture registers, there will be an entry in this table and it will say. What is the current location in the physical registers, where the value for that architecture registers can be found? Okay, so that is what a RAT looks like. It's a table where, for each architectural register, for example, for R0, it will say where we keep R0 currently. Then for R1, there is another entry and so on. So this table really just stores where things are. This is just for our convenience. You know, we kind of numbered the entries, but basically doesn't need to store this zero here, it's simply that the entry number 0 is for our 0, the entry number 1 is for our 1, etc. So now we will have some instructions. Let's first look at this first instruction. So let's say the processor is fetching this instruction. What it does is for all the registers that this instruction needs to read, it looks in the RAT where those registers can currently be found. Let's say that for R2, this is just saying P2 for now. So this is physical register 2, which correspond to R2 initially. And let's say that it's P3 here. In fact, let's say all them are just like this. So what the processor does is it renames this instruction, so it fetches it, decodes it, and then rewrites it so that the actual operation that will be done is going to be on physical register speed two and P3. And now the question is where are we going to put our result? Well, we're going to put our result in a new place for R1, so not in P1. So we will change the RAT to put the result in some P17 or whatever, okay? So now we're going to say that this is, this all goes to P17, and then we're going to use P23 for the inputs. Let's say that the next instruction is subtract R1 and R5 and put the result in R4. What the processor again will do is, so it always follows the same procedure. Look at the RAT for R1 and R5 to see where those values currently are, and then rename them. So we're going to look at R1, and it says P17. So this subtract is renamed into read P17, and P5, and put the result into a new place for R4. So we're going to change the RAT from now on, to put the result in, let's say, P18. Okay so, I'll evaluate three more instructions and now we can rename them too. So this XOR will be renamed into an XOR of R7 and R8. Where are they? Well, let's say they're in P7 and P8. And then we're going to write to R6. We're not going to write to P5. That's, where the previous value of, R6 is. We're going to write it to a new register P19. Then we're going to take this multiply and remember you need to update the RAT every time you produce a new value, because that's where subsequent instructions will find the value for that register. So we're going to, again, R8 and R9, still no change. So they're going to be P8 and P9. The new value for R5 is going to go to P20. And then we have this ADD R8 and R9 are still in P8 and P9. The new value for R4 is now going to be P21. Now, this is an already renamed value, so we're going to rename it again and say P21. Later on, if there is an instruction that uses R4, so something something R4, it's going to be renamed into whatever, P21. So the trick here is that, we have seen that this instruction writes to R4, and this one also writes to R4. So there is a register name dependence between these two because they're both trying to write to R4, but when we rename them this way, the dependency's gone, because this is producing P18, this is producing P21. Instructions among these that need to write the value, so basically, if something here was reading R4, it would get the value P18 instead because it's been, it, it would be renamed to read P18. Anything that follows this instruction, that wants to read R4 would be renamed so that it reads the P21. So simply, the use of R4 as a name for the value produced by this instruction and also for the value that's produced by this instruction is what was causing, for example, the write after write dependence. We eliminated dependence because here the name will be different. This value is going to be named P18. This one will be P21. There is no more dependence. Okay, now that we have seen how register renaming works, let's try it ourselves. So let's have a register renaming quiz. Okay so we will have a RAT here that will tell us where the values for R1, R2, R3, R4, R5, and R6 are. And over here we will have a sequence of instructions as they're fetched. And then we will need to write in what's renamed. Okay. So here we have the initial state of the RAT which is simply R1 is mapped to P1, R2 to P2, etc. And here are our instructions. In order to help you get started, I'm going to rename the first instruction. So after renaming, it's still a multiplication. R2 and R2 are P2 and P2. R2 is where we should put our value and, remember, we need to rename it in the RAT, so it's going to be in P7. So this is what this rename instruction looks like. Read the registers. Physical registers P2 and P2, and put the result in P7. Now it's your turn. You need to both tell me what's renamed and also update the RAT accordingly. Okay, so let's look at a register renaming quiz solution. So we have seen the first instruction renamed, let's now rename the second one. It is going to be an add. R1 and R2 are going to be P1 and P7. R1 is where we put the result. Remember, you don't use this. You rename it to the next physical register. So we have P8. Then we have the multiplication of R4 and R4. This is P4, P4. You write the new value of R2, and that means that this P7 now becomes P9. Then we have the add of R3 and R2, which are P3 and P9. Write as a result to R3, which becomes P10 before that happens. And then this multiplication will read R6 and R6 that's a P6 and P6 here. Producer is [UNKNOWN] inputted in R2. So now R2 gets to become P11. And finally, we have this add of R5 and R2. This is P5 and P11. Producer is [UNKNOWN] inputted in R5. So R5, now instead of P5 becomes P12. And, that's the solution to the quiz. So in the previous quiz we have seen that we can fetch these instructions then we rename them to eliminate false dependencies. So lets see whether we have any false dependencies after renaming and what does that do to our IPC or instructions per cycle which is the opposite of CPI. Well here we have a true dependence. And that true dependence is still there in our renamed program. Similarly here we have a true dependence and it is still there in our renamed program. And then here we have another one, and it is still there. However here we also have an output dependence from R2 here to R2 here and then another one to here. And we have an anti-dependence here and one here. So if we look at all of the dependencies then basically this is a sequential program. So for our fetched program, we have the CPI of one which is also the IPC of one. For our renamed program, however, this anti-dependence is no longer there, this output-dependence is no longer there, there is no anti-dependence here and there is no output dependence here. So really, here we can execute this, this, and this instruction in the first cycle and this, this and this instruction in the second cycle. So, for our rename program now, we have, that we can execute, this, this, and this instruction. Three instructions in the first cycle, these three can be done in the second cycle, so overall we have six instructions over two cycles. The cycles per instruction is two cycles for six instructions, giving us a much lower CPI, it's only 0.33. Or we can write it as IPC of three, meaning we execute three instructions per cycle. As you can see, our renaming gave us a much better potential for executing this program quickly. Okay, now that we have seen how to do registry naming and how to follow the true dependencies and so on, we are ready to learn what the instruction level parallelism or ILP is. So, we will now define what it is. So, the ILP really can be defined as the IPC instructions per cycle when we have a processor that can do an entire instruction in one cycle. So it doesn't need a pipeline. It can fetch the decode, execute, write result, everything, in just a single cycle for one instruction. Also, the processor can do any number of instructions in the same cycle. So theoretically, it could just fetch and execute your entire program in the one cycle. But it still has to obey true dependencies. So if one instruction depends on another, then they cannot be done in the same cycle. You have to first do one and then the one that needs its result. So as you can see, ILP really is kind of what an ideal processor can do. Subject only to obey true dependencies. How do we compute the IL P for a program? Well, we first rename all the registers. So we do register renaming first. And then, we pretend we execute. So what we do is, we say okay, in the first cycle, these are the instructions that don't depend on anything. So they can go. In the next cycle, we go through the same thing and so on until we run out of instructions to do. At which point we know how many cycles it is. Now the one thing that you should remember about the ILP and that will help you realize what it is and how to do it is that ILP is a property of a program not of any particular processor. So basically, we define the ILP to be what would this program do as far as the IPC is concerned on an ideal processor. So it has nothing to do with what kind of processor you actually run it on. It has everything to do with just the dependencies in the program. So be mindful of that, because for example it doesn't make really sense to say something like what's the ILP on this program on the I don't know, Intel Core i7 processor. Because the ILP on the Intel Core i7 processor is beside the point. Because the ILP is really the performance of this program on an ideal processor that has nothing to do with Intel i7. Okay, so just, just remember that, when you're asked to produce the ILP, you should just consider the program. It doesn't matter what the processor looks like, because the ILP is computed for a perfect processor anyway. In contrast, you could say that the IPC of this program on a given processor is something else, but be mindful of this. Basically the LP you only need to see the program, you don't really need to know what the processor looks like. So now that we know what ILP is, let's look at an example of computing the ILP just so we can be on the same page about that. First, we need a program. So let's say, we have this program. We have already renamed it so that only the true dependencies remain. So now let's look for those true dependencies. We write to P10. This instruction doesn't use P10. Actually, the first one that uses P10 is this one. We write to P6, nothing uses P6. We write to P5. This one uses P5. So there is a dependence basically here, and another one here and then here we produce P4. So we only have really these two true dependencies. So now, let's look at like what can execute in each cycle. To do that the best way is to just kind of check off instructions in counter cycles. So in the first cycle we can do this, this, this, and this because none of these instructions has an incoming dependencies yet. In the second cycle, okay, this one cannot be done in the first cycle because it has a dependence from this and this and that has not been done in the first cycle yet. So after the first cycle, all of these are done, so now we can do this. So what we have, what we get is now the ILP is equal to how many instructions, 5, over how many cycles, 2. We got an ILP of 2.5. Again, we then need to know what the process looks like. Now, some convenient tricks for computing the ILP. First, I didn't actually need to rename the program. If this program had real registers with some anti and output dependencies, I could just fake renaming by just looking at where the true dependencies are, and completely ignoring the anpi, anti and output dependencies. Why? Well, because the renaming would eliminate them anyway. All that would be left is the true dependency. So you actually, when you're computing the ILP, don't really need to look at a rename program. You can do it directly on the program as it was written. You just need to follow only the true dependencies and completely ignore the anti and output dependencies. Second, you know, when checking this off, be mindful to kind of remember how many cycles you're computing and also what has been done and so on. And then you know, just be careful also to kind of divide the right thing with the right thing. So it's 5 instructions over 2 cycles, not the other way around. Okay, now that we have seen an example of computing the ILP let's do an ILP, quiz. Let's say we have this program. What is the ILP for this program? As we said before, we have to figure out the dependencies. Even though this problem has output [UNKNOWN] dependencies, again for ILP all we care about is the true dependencies so we could rename this program and give us the true dependencies or we could just look at where the true dependencies are here. So we have here, this is using R1,. This is using R2 and R1. But it doesn't matter because this is going to be longer. R7 and R8 are not depending on anything yet. R3 and R7, there is a dependence here. R1 and R1, were dependant on this instruction here, for both of them really. And then R7 and R7 we are not using any of them. Again, we are overwriting R1 here. We are overwriting R1 here. And so on but that doesn't matter because after renaming all of that will be gone. So, now it's time to figure out what can execute in which cycle. In the first cycle we can execute this one. We cannot execute this one because it depends on that. We cannot execute this one because it has to wait for the second instruction. We can execute this one, it doesn't depend on anything. We cannot execute this one yet, because it has a dependence, this one also, but this one can execute. So in the first cycle, we managed to do, 3 instructions. Now this instruction only depends on instruction that is done after cycle 1, so this can be done in second cycle. This one actually depends on instruction that is done in second cycle so it will have to wait until the third cycle. This one depends on an instruction that is done in the third cycle so this is going to be executed in the fourth cycle. This one can actually execute in the second cycle, depends on instruction that executes in the first cycle, and that's it. So,it can be done in the second cycle. So what we have is the last instruction that finishes in cycle 4 and we managed to execute seven instructions over that time. So we have 7 over 4, which if you want to calculate it out it's going to end up being 1.75. So can say that from when computing the ILP. We completely ignore false dependencies among data dependencies. And only consider true or flaw or read after write data dependencies. So ILP really only looks at those data dependencies but there are two other types of dependencies in addition to data dependencies. There are structural dependencies and control dependencies. How do they affect ILP? And the answer is that first of all, when looking at ILP, there are no structural dependencies. Structural dependencies occur when we don't have enough hardware to do things in the same cycle. When computing the ILP, we're assuming ideal hardware, and as part of being ideal, it has no structural dependencies. Meaning, any instruction that could possibly execute in the same cycle, will execute in that cycle, and not have to wait. On some resource, like for example, there being only one adder, so we can do only one add in a cycle. We can assume that we have an infinite number of adders, so if we want to do an add, we do it. For control dependencies, we assume that we have perfect same-cycle branch prediction. Meaning that branches. Are predicted in the same cycle in which we fetch them. So we see all the correct instructions after the branch, in the same cycle in which we see a branch. Let's see an example of this where we have a program that has a flow dependence, and also an output dependence. And then we have a branch that has a flow dependence. On R1 and branches to label, if the condition is satisfied otherwise continues on. And after the branch in the program you have, an instruction, and at the label we have another instruction. So now let's see how we compute isle B here, and these are again cycles. So what happens is, in the first cycle we can definitely do this. We cannot do the MUL in the first cycle, because it, it depends on the add. So we will have to wait until the second cycle to do it. The branch can actually not be done until the third cycle, because that's where we compare this. Now let's say that this branch is actually taken. What we can assume is that. Although the branch cannot execute until the third cycle from the beginning. Since we started fetching the program, we know what the branch is going to do. So we know that the branch is going to go to the label. And thus this instruction, because it has no data dependencies on any other instruction, will actually execute in the first cycle. Although it has a control dependencies. So as you can see, not only are control dependencies not resulting in any delays after the branch happens. Actually, we are assuming perfect branch prediction,. And that means that this branch is predicted, even before it's executed, so that we can just do depend on the instructions. Pretty much, we can completely ignore Control Dependencies. The Branch acts as an instruction that simply produces no results. So it eventually has to execute but it has really no impact as far as the line Other Instructions is concerned. So in this case, not only does the multiply instruction execute. Even before the branch on which it has a controlled dependence executes. Also the instructions at this MUL will feed values to who can execute before the branch. So pretty much a branch itself might be delayed because of true dependence. But it's control dependencies on that branch have absolutely no impact on what the IP will be. Because we are assuming perfect branch prediction where all the branches in the program are perfectly predicted. In the same cycle. As soon as we begin fetching the program. And from the beginning of the program, we knew exactly which instructions will execute. So that we can just do them according to data dependencies So we already know that ILP is not equal to IPC on any real processor, actually it can be but it's not necessarily, so, and that is because ILP is really the IPC on a perfect processor that does perfect branch prediction. And only has to follow data dependencies, so now lets look at an example of a program and a real processor and see basically how do we determine ILP, and and how do we determine the IPC. So this is our program, and now let's look at what the processor looks like. So our processor is two issue, meaning it can handle two instructions per cycle. Out of order super scalar, which means that it doesn't need to execute instructions exactly in program order. And let's say that it only has one multiplication unit, and two units that can do what add, subtract, x, or et cetera. For our ILP, we can just ignore all of this, because the ILP, again, is just the property of a program. So we have to just obey dependencies. There is a data dependence here, and that's only really the only dependence that we have. So as far as ILP is concerned, we can do this in two cycles. This, this, this, and this would be done in the first cycle, and then this would be done in the second cycle because it, it depends on the instruction that is done in the first cycle. So we have five instructions, over two cycles, gives us 2.5. In contrast, the IPC for this processor needs to consider all the limitations of this processor. So we have this instruction can be done in the first cycle. This one cannot because of a dependence, so therefor the IPC we still have to obey all the dependencies, plus we have to also worry about the other limitations of the processor. This is an out-of-order processor, so we are not doing this. But we can still look further, because we can do instructions out of order. The XOR can be done because we have two add subtracted et cetera units so we can do this in the first cycle two. This instruction can be done in the first cycle, because we can use the multiplier. For example, this instruction here cannot be done in the first cycle unlike when we computed the ILP. It can not be done, because we only have two add, subtract, x, or et cetera units, and we have already consumed both of them in the first cycle for the add and the XOR. So in the second cycle now, we can do this and this. So it turns out that I, our IBC is still five instructions over two cycles equals 2.5. But it doesn't have to be that way. For example, if this processor only had one, I would subtract extra units. We would now have the IPC off. In the first cycle we can do this. We cannot do this because of a dependence. We cannot do this because are already using our one unit that we have for an [UNKNOWN], et cetera. We can do the multiply, and we can't do this add either. In the second cycle, we can do this. We can not do any of these other two because, again, we are using our add subtract XOR. This will be our third cycle, and this will be our fourth cycle. So, now we have five instructions over four cycles. And that's equal to 1.25. So this IPC is half of this IPC, and half of this ILP. So what we can say is that the IPC has to be less than or equal to the ILP. The ILPs for a perfect PROCESSOR. A real prosser will have some limitations that will maybe allow it to achieve the same IPC as the ILP available in the application. But sometimes it will not. So what we can say is that the ILP is always greater than or equal to the IPC on any real PROCESSOR. And again, we don't really need to know what the prosser is to compute the ILP, but we do need to know what the prosser looks like when we're computing it's IPC. And we have seen that IPC can be equal to the ILP, or it can be lower than that, but it can not be larger than that. Okay, so now that we have seen how ILP and IPC differ, let's do a little IPC and ILP quiz. Let's say that we have a 3 issue in-order processor, that has 3 ALUs. Meaning that for everything that we can issue, it can always executed, and these are general purpose ALUs that can just execute any type of instruction. So, we probably won't have any issues with the limited number of units. The program we are going to execute is- So for this program what we want to know is what is the ILP and what is the IPC. So let's look at our solution. When we have a problem like this, we first figure out the dependencies. There is one from here to here, and another one from here to here. And there is also dependence here from R7 to R7. So now let's look at our ILP. So for ILP, we can do this and this in the first cycle, they don't depend on anything. This one needs to be done in the second cycle, because it depends on the first instruction. This one, however, can be done in the first cycle. This one cannot, it needs the R7 to be used by this one. So this gets done in the second cycle. And then this one actually, doesn't depend on anything, so it can also be done in the first cycle. So as far as ILP is concerned, we have a total of six instructions executed over two cycles. so the ILP is going to be three. For the IPC, we have to worry not only about these dependencies, but also that this is a three-issue processor and that it's in order. So this can still be done in the first cycle. This can be done in the first cycle. As far as three-issue is concerned, this could be done in the first cycle but because it has a dependence we still have to delay it by one cycle. And now the problem is that in an in order processor. Once we don't execute an instruction, we cannot execute instructions past it because that would be out of order. So this is all that happens in the first cycle in this processor. In the second cycle we are able to execute this instruction, because now the first two are done. We can execute this one, because now that this one is done, this is eligible for execution. And then we have to stop here because of this dependence. So this would have to be done in the third cycle. And again we cannot do this. So in the last third cycle, we will be able to finally do this, and then this. Because they don't depend on each other, so they can be done in the same cycle. So now we have six instructions over three cycles, for an IPC of two. It turns out that even if this was a two issue processor, the in order property really would still limit us to IPC of two. So sometime one of the things is going to be a limitation, sometimes. Another limitation is going to play a dominant role. Sometimes all of the limitations are going to play a role in limiting our IPC. So now that we have seen what ILP is, and also how it relates to IPC, we can have a little discussion about the ILP and the IPC. We have defined our ILP as basically the IPC on an ideal [UNKNOWN] for the processor that has perfect branch branch fiction, enough resources for everything, et cetera. And we've saw that, ILP is larger than or equal to the IPC that can be achieved on any real processor. So if we consider a narrow issue in-order processor, narrow issue meaning that it can only issue about maybe one, two, maybe three instructions per cycle and it's in-order. Usually, the limitation for the IPC will be this. Basically the in-order property now is going to hurt us too much. So this processor is going to be limited mostly by the narrow issue. However, if we have a wide issue in-order processor, meaning it can do more instructions per cycle. It's going to be limited mostly by, the in-order property. Because this is going to allow you to do a lot of instructions per cycle, and if it can do closely to what the ILP already is for the application, then basically we're already kind of close to ideal as far as the IPC is concerned for, for the wide issue problem. But because of the in-order property, we just cannot find those instructions to keep our wide issue slots busy. So if we have a wide issue processor, we also need for it to be out of order, so that it can benefit from the wider issue by finding instructions anywhere where they are and not basically stopping on the first dependence. So such a processor will need to, fetch execute et cetera, more than one instruction per cycle, preferably a lot more than 1. So it cannot be just like 2, it should be 4 maybe or more, that's why we call it wide issue. It should be able to eliminate false dependencies betw, because if we have to obey all sorts of dependencies then that's going to limit our IPC too. And bring us far away from the ideal ILP. And finally, should be able to reorder instructions so that we can execute the instructions out of order. Meaning that, our execution order should be kind of similar to what we did for the ILP, and not for what we did for the in order processors. In this lesson, we have seen that we should be able to get excellent performance even when the program has a lot of data dependencies. In the next several lessons, we will learn how a real processor can actually do this only with harder structures that have limited size and complexity. In this lesson, we will see how instruction scheduling can let us execute programs faster even when these programs have data dependencies. And even how to execute more than one instruction every cycle. SO in this lesson, we are going to be looking at how to improve the IPC without a folder execution and how to design actual hardware that does that. For improving the IPC we have seen that the ILP can be good, it can be significantly larger than one. Usually it is way over four, but to achieve something close to that ILP, we need to handle the control dependencies. And we have seen how branch prediction can help eliminate control dependencies if branch prediction is correct. So if our branch prediction is very good, which, on today's process it is, then as far as control dependencies are concerned, our IPC will be very close to the ILP. Then we have to consider the write after write, and write after read data dependencies, also known as false dependencies, and we have seen that at technique called register renaming can completely eliminate these dependencies so that you simply don't have them anymore in your program at which point they're not going to be a problem for out IPC. And then the, we have seen that there are these write after read also known as true data dependencies. And we have seen that it helps a lot when you can execute instructions out of order, meaning you don't simply follow the program order, you simply find instructions whose inputs are ready and you execute them then. That is how ILP mostly was good. And finally, there are these structural dependencies, which is when you cannot execute things simply because you don't have enough resources in the processor. But that can be solved simply by investing in a, into a wider issue processor, meaning we need a processor that can handle. A bunch of instructions in each cycle and not have to delay instructions just because it doesn't have resources for it. So among these, we really need to figure out how to do these three. And first, we will focus on this. So, how do we actually do register renaming and out of order execution in a way that can be amenable to hardware implementation? So we know how to do it kind of manually on paper, but how does a processor actually do it? So this lesson will really focus on how to do this for real So the first technique that we will consider is Tomasulo's algorithm. It's really one of the first techniques for out-of-order execution and now it's more than 40 years old. It was used in the old IBM 360 machines, which are now, again, more than 40 years old. What it does is it determines which instructions have inputs ready so that they can go in the next cycle and which instructions still have to wait for their inputs to be produced. It also includes a form of register renaming, and it is surprisingly similar to what we actually use today as far as out-of-order execution is concerned. So although this is more than 40 years old, it actually is very useful to know how it works because it's kind of like the first approach, so it's simpler than what we use today, but it really well explains what's going on. The differences between Tomasulo's algorithm and what is used today are only that in the olden days, when Tomasulo's algorithm was implemented, they did that only for floating point instructions, while today, we do it for all instructions. Also, they had relatively few instructions in the window that they're looking at, meaning they only really looked over some small number of instructions into the, you know, future instructions that need to be executed well today. We look at hundreds instructions past the one we are still executing. In the olden days, exception handling for floating point instructions was not such a big problem, because they were only considering really execution of kind of floating point extensive applications. And you know, those will be programmed and so on, and you would usually run one program at a time. If there is an exception, you could do some drastic measures. Today, process include explicit support for exception handling that we will see later. So we first again look at how the Tomasulo's algorithm did it and then we consider how today's processors do these additional things. So before we go into how exactly Tomasulo's Algorithm works, let's kind of see an overview picture of what's going on in there. Just like what the helper instruction looks like. So first, there is an instruction queue which I labeled here IQ. Instructions come from the fetch unit in the order in which they were fetched and again, these are only floating point instruction and then they're queued up here. So this is basically like this is the kind of, all this instruction, the next one, the next one, the next one and we will be kind of, you know, dropping these instructions into the Tomasulo's algorithm machine in the order in which they came. We always grab the next available instruction from the instruction queue. And we want to put it into a reservation station. There are a number of reservation stations that basically, when one becomes available, we will put an instruction into one of those. And the reservation stations are where instructions basically sit for their parameters to become ready. There is a floating point register file. So this is where your floating point registers are. And when an instruction is inserted into the reservation station, the values that are already in registers, meaning the values that are actually already present and ready for execution, are going to be simply entered into the reservation station from the register file. Once an instruction is actually ready to execute, it goes to an execution unit. And you can have different types of execution units. So for example this a adder, and this is a multiplier. So simple floating point instructions will be done in the kind of adder unit, and the more complicated like multiply and divide would be done in the multiplier unit. Each of these units really have separate reservation stations, where unit instructions that are going to that unit would wait. Once such a unit has produced a result, the result will be broadcast on a bus. So of course, the result would go to the register file. So pretty much all the results that are ready would be available in the register file, so that instructions that come in would simply grab them from there and we don't have to worry where those values are going to come from. But these results would also be broadcast to the reservation stations. Why? Well, because there are instructions here that are waiting for values that they need to be produced and as values are produced, we need to tell these instructions that, you know, now your value has been produced so that it need to also go here. So pretty much here, you have some values that you already have in an interaction. Some values that you, you're waiting for and you know what you're waiting for. And then, when the value is finally broadcast, you check for each of the value that is broadcast, whether this is the one we are waiting for or not. Now, I have two of these for reservation station, because a typical instruction will actually have two inputs. Like for example, you're adding two numbers. And when you broadcast the result, either of the two or both of them might need that result. For example, if you're doing something like add R1, R1 and the instructions is producing R1 is currently in the multiply. When it broadcasts the resultant says this is R1. We want to latch it into maybe both of the operands for this instruction. So we need to compare basically what's being broadcast. Its name needs to be compared with both of the inputs that you are waiting for, and then you select the one or both of them or, none of them if this is not the value you are waiting for. And finally, if the instruction is not an arithmetic instruction it's actually a load into the floating point register file or a store from it. Then the instruction would go to the address generation unit, that's an integer operation so it's not going, not going to through that, so basically we just compute the address. And then we insert the instruction into the load buffer or a store buffer, where the instructions are going to be queued up for going to memory. And the load buffer only provides the data that I ask, the store buffer provides both the address and the data to memory. When the load comes back from memory, its value is also broadcasted on the CDB and goes to the appropriate register. And also, all the values that are broadcast from the bus, are going to go here, so that stores can get their values when they become available. So, our store instruction needs to put a value in memory, is going to wait for its floating point value that needs to be placed in memory. Until it becomes available and only then be sent to the memory. Unlike, arithmetic instructions which can execute out of order. Basically, the only way for inputs to become available and then they can go for loads and stores. They were done in order. So basically, they didn't reorder loads and stores. And it will later, consider how to do that. Modern processors will actually reorder loads and stores too. So this is a lot simpler than in modern processors, but this here is pretty much the way it's done now. Modulo some small adjustments. So before we go into kind of the details of Tomasulo's algorithm, let's just kind of give names to some things on this picture, and then we will kind of discuss what each of these things really needs to do. So, this part here, where we send the instruction from the instruction queue into the reservation station, then the load store unit is called issue. When the instruction is finally sent to execution from a reservation station, it's called dispatch. And when the instruction is ready to broadcast its result, we call that write result, or simply broadcast. So basically, every instruction will be issued,. Meaning, get to the res station. Eventually it gets to dispatch, because all of its inputs are at the end it's, kind of gets to go to the unit. Once it's done, it's going to write the result and that is when we consider the instruction completed. So now let's see what happens when you issue, when you dispatch, and when you write result. So the first step of Tomasulo's algorithm that we will consider is issue. So what happens during issue? We take the next instruction in program order from the instruction queue. This has to be done in program order for register renaming to work correctly. We then need to determine where the inputs of the instructions are going to come from. Are they already in the register file or are they going to be produced by some of the instructions that still hasn't broadcast its result? And if we need to wait for instruction, which one? So there will be some sort of RAT table there. We're then going to get a free or available reservation station of the correct kind. Note that there are some reservation stations that are for the adder, some of them are for the multiplier. So if this is an add instruction, then we really need to find a available reservation station in front of the other. If all of the reservation stations are busy at this point, meaning they're already used by some other instructions and they're, those instructions haven't left yet, then we simply don't issue anything this cycle. And in Tomasulo's algorithm, they were issuing one instruction at a time, so, so they, they would just issue one instruction per cycle. We would put the instruction in the reservation station then. And we will tag the destination register of the instruction so that the result, when it's produced, goes there and also so that instructions that want that register in the future will know which instruction is going to produce this value if it has already not been produced. So we will have a running example of, how that the muscles [UNKNOWN] work so that whenever we do something, kind of, to explain it, we will also show how it works. So, this is an example of how the issue is working. First, this is going to be our issue Q. I'm showing four instructions here. This is the first instruction. This is the second one, third one and so on. So, we will issue in program order which means the first instruction that we're going to be issue is going to be this F2 equals F4 plus F1. This is really an add but I'm writing it this way because it's more compact. Next, for each of the faulting point registers which we're going to name F1, F2, F3 and F4. There is a register alias table, or RAT where we're going to keep which instructions supposed to produce that register. If there is nothing here, that actually points to the register file, so pretty much if there is nothing here in F1, that's going to mean that you can just find the value for F1 in the actual F1 in the register file. This is a register file we have four registers and this is their actual values. When we go through the example we will actually compute with them. So again this is the register file. Again the RAT when it's empty says that basically this means F1 can just be that from the register file. You don't have wait for it. And finally we are going to have our reservation stations. We going to name them RS1, 2, 3 for the other reservation stations and 4 and 5 for the multiplication reservation station. So we really have 5 reservation stations, but note that you can use only these three for adds and only three for multiply and divides. Now we are ready to actually issue an instruction so again this is going to be the first one we issue. The first step is to take it from the instruction buffer. The next step is to look at the RAT and find whether our inputs are ready or not. And in this case, both of the inputs are ready. F1 and F4 are both available. So we're going to say that this is. And add and we are going to take the value for, in this case, F4 and F1, and that is going to be 0.71 and it is going to be 3.14, and place the instruction in the reservation station, of course that assumes that there is a reservation station available, we will see eventually, what happens when there isn't any. And the final step for register renaming is to say that the result of the instruction F2 is going to come from now on, from reservation station 1. So here, we will write, RS1. So future instructions that want F2 are not going to read minus 1. There going to wait for this result to be produced. So this is the reining in of the very first instruction. One final step that we will do of course is remove this instruction from the issue queue and what's going to happen now is this is going to shift down but for my example that's going to be very difficult to do so I'm just going to kind of cross out this instruction. So this is, this just means that you know, this instruction has been removed from the issue queue. To make things more interesting for the remaining instructions and also to see what happens when an instruction is renamed, let's issue one or two more instructions. So let's issue the next instruction, which is this one. This is a divide. We need to find the free reservation station here, say it's a divide. F2 and F3 are going to be the parameters for this instruction. F2 comes from RS1 so we cannot simply read this value here. We're going to say here that RS1 is what we are waiting for but F3 is available so we're going to get 2.72 from the registry file, and we now mark this instruction as, we're going to basically take it out of the issue queue and this is so very easy to forget but when we do that. We need to mark that the result, the F1 is from now on not going to be coming from here. It's going to be coming from reservation station 4. And we're going to issue this instruction too. It's a subtraction so it's going to go here. F1 and F2 are the parameters, we see that neither of those two can be read from the register files, so what we'll going to do is were going to mark here that that, that the first parameter is going to be produced by RS4, the second one is going to be produced by RS1, we'll going to then mark it. F4 is coming from RS2,. And we're going to remove this instruction from the issue queue. Of course, more instructions are probably going to come this way, but we're not going to look at that. Because again, that, that too many instructions for this example. Now let's say that there wasn't an issue slot here. We then. Couldn't issue the next instruction in the next cycle, until we are actually done executing one, one of these two. I'm not going to do that but just, just keep in mind that when you want to issue an instruction you first have to check whether reservation station is available. If not then we simply cannot issue that instruction. Let's issue it now because we do have room for it. It's an addition. Of F2 and F3. F2 comes from reservation station 1. F3 comes from the register file. So we're just going to take the 2.72. We're going to then rename F1 to point to this result. And now we see that F1 is already renamed to reservation station 4. That's basically, if you want to use F1, since this instruction, you would be waiting for it's result, but now F1 is being basically renamed to another reservation station, so this is how registering and naming here works in Tomasulo's algorithm, so what we are going to do is we are going to basically override this. With the name of this reservation station, which is RS3. And now we have issued all of our four instructions. In a real processor you would issue one instruction each cycle. And probably some of these instructions would have executed by the time we issue all of them but I just wanted to show you how issue works if, for example, nothing could execute for some reason. Now that we have gone through an example of issuing, let's see what the next step is Before we move on to the next stage, which would be dispatch, let's do a issue quiz. So in this quiz, we have our instruction queue, we have our RAT, we have our register file, we have a reservation stations. These two instructions are already reservation stations. There are two left to issue. And these are the values of the register file, of the RAT, etc. So your task is to issue these two instructions, or at least issue those that can be issued at, at this time. You don't need to consider when these instructions are going to finish, just issue whatever can be issued in the next several cycles, assuming that these are just not going to leave from here anytime soon Okay, so let's look at our issue quiz solution. We have these two instructions to issue. This is the older one to issue. So we're going to get a divide, or multiply reservation station. Mark that this is a divide. F1 and F2. Or maybe this parameters. Look here, you find it RS4 and RS1 are going to produce that value, so we don't get any values from the registered file. And then we're going to update for F4 that the result is going to come out of RS5. And now we're done with this instruction. This instruction cannot issue. Because it's a multiplication instruction, all of the divide multiplier reservation stations ar busy. So until one of these finishes, this instruction isn't going to issue, and so now our issue part is going to be stalled until that is resolved So now let's look at what happens when we need to dispatch instructions. So this is the situation in a cycle. Dispatch basically means to consider latching operations results that are produced, and also deciding which instructions are ready to execute. So in the same cycle, usually we'll have both of them. So we will consider the situation where we can actually, kind of at the beginning of the cycle, do this and then towards the end of the cycle, actually choose among the instructions that are ready to go. None of these instructions are ready to go. So if we did it the other way around in the cycle nothing would really dispatch. So now let's do it. What's going to happen is, when we broadcast the result, we say which Reservation Station is producing the result. And what the value is. And now what happens is this. The reservation station who's result is being broadcast know that this reservation station has already dispatched this, it was executed, it was broadcast et cetera so night comes back here. We're now going to free this reservation station base its finished its job. So the first thing we're going to do is free this reservation station. Now it's empty and is ready to receive another instruction that issues. The next thing that we're going to do is match this tag against each of the two operands in each of the reservation stations. So what's going to happen is RS4 doesn't match, so we're not going to do anything for this. RS1 here does match, so instead of RS1 here we are going to use a value of minus 0.29. For this instruction, it already has a value here, so we don't have to match anything. But RS1 here does match so we're going to take the value of 0.29. And here we have another match. Oh, zero oh, this is minus zero, okay, minus 0.29, and this value's already here. So this completes, basically, what happens when a result is kind of being broadcast as far as reservation stations are concerned. Now we need to consider which reservation stations have all the operands ready to go. This one doesn't. This one does. And this one does. Because these are going to different execution units, assuming that in each execution unit can take one operation per cycle. This instruction RS4 is going to execute in the multiplier. So the multiplier for a while it will be busy with the, you know RS4. This is a divide. And it's going to be basically doing this over this. And then here. This one is not ready to go so the only one that can really go here is this one so what we will be doing for awhile is RS3 minus 0.29 plus 2.72. Once these are done, they're going to broadcast their results and this cycle will repeat During dispatch, we haven't consider a situation when more than one instruction is ready to execute. So let's do that. So here's a situation where we have a similar broadcast to the previous case where we have reorganization station 1 is broadcasting these out of minus 0.29. So we're still going to free a reservation station 1 as before. We will still going to capture all of the results that hasn't changed. So here we're going to match minus 0.29. This doesn't need to match anything. Here because we already have a value. Here we're going to also latch zero point twenty nine, and here we're going to do the same thing. But now we have a situation where for the multiplication and divide unit it's pretty clear that this the instruction that gets to execute. So this is easy. But here, we have to choose between one of these two. Because assuming that the adder can only handle one instruction per cycle. We need to pick one of these. And, now we have to figure out which one. So there needs to be some logic that's going to pick one of these instructions, because both of them cannot go. Now we need to figure out, you know, are there any good rules about which one should go first? So our question is basically, should we dispatch RS1 or RS2? Well, ideally we will dispatch that instruction that allows us to kind of get to future instructions as early as possible. So basically we would choose the instruction that leads to the highest performance. Unfortunately that requires knowledge of the future. And this is something that hardware is pretty bad at doing. Because we only can really look here at these instructions here. And maybe the next couple of instructions in the instruction queue. While in reality the situation might change, for example this instruction. Might have a lot of dependent instructions later on, and this one might have a few. But we don't know that until we see those a lot of instructions later on. So, it's very difficult to decide that based on kind of perfect knowledge of the future. So, how do we choose these instructions? Well, there are some heuristics. We can not really do a perfect job of it, but we can make some reasonably good guesses. One choice that usually makes a lot of sense is oldest first. So pretty much whichever, whichever of these instructions has been sitting here longer will be allowed to go first. Why? Well because for an older instruction all other things being equal, it's more likely that more instructions are by now waiting for it. Simply you know, if an instruction is older. By now you could have seen more instructions that will use it's values. And also this allows us to kind of, you know, mark of some instructions as done. So basically oldest first is a typical heuristic that gets done this way. Another one that could be used is something like most dependencies first where we will check. How many other instructions need the value from each one of these. And then use the one, I mean, you know, dispatch the one that will, that will can free up more, the most of the other instructions. But this one would be very, this, how would I say, would be difficult to implement. Because we have to actually like search a lot of stuff, which is going to be very power hungry and so on. So typically, if we do anything, we will just do the oldest first. Another choice would be, to just randomly select the instruction. Like for example, whichever is comes first in order of preservation stations or something like that. So something that has nothing to do with their age in the preservation station, or whoever has most dependencies. This strategy also works because, note that. If we don't send the oldest first, we will eventually run out of things to do. Because all of the things will depend on the oldest instruction, at which point it will rot. So basically, it's not a problem regarding correctness if we choose something other than oldest first or dependence first. It's just an issue of kind of what works reasonably well. So typically, all this first does help somewhat with like releasing instructions. >From here, while, you know, random is okay as far as just, you know, correctness is concerned. So pretty much anyone of these three would work. It's just a matter of, you know, which one would lead to the best performance. And we kind of choose an oldest first as a kind of compromise between, what we have to look at. And you know what gives us good performance. So would random give us kind of the worse performance, and this would give us the best performance. But this is kind of easy to do so we kind of compromise between the two. So this is the situation we have for dispatch. This is what we're broadcasting this cycle. This is what we have in our reservation stations. And the question is why hasn't this instruction here in, RS3 dispatched before this? Because, obviously, it's not waiting for anything at this point. So the question is, if this is a beginning of a cycle, then why hasn't this already executed before we got to this cycle? The first choice is, it was issued in the previous cycle. So basically, this says that this instruction was inserted here in the previous cycle, so that's why it didn't dispatch yet. The next answer is, another instruction was dispatched to the add unit, so this one couldn't. And the third answer is that if the instruction in RS2 is older than the instruction in RS3, it comes before it in program order, than RS3 cannot dispatch until RS2 does, so simple, this instruction cannot dispatch because it's waiting for something. In this quiz, you need to, select all answers that would possibly be correct. We have our dispatch quiz. Let's look at the solution. If this instruction was indeed issued in the previous cycle, meaning it came into the reservation station. Then, depending on, when do we select [INAUDIBLE] instructions for execution. Let's say we do it, kind of, you know, towards the end of the cycle. There might not be enough time. Simply, this reservation station, when we're considering, what can be dispatch for execution. This one simply wasn't there yet, basically. In the same cycle where we select something, this got inserted here and it wasn't inserted early enough in the cycle, so this is possibly correct. It can happen that's, that's basically you cannot do both of the things consecutively in the same cycle. Another instruction was dispatched to the add unit. This is entirely possible if, for example, RS1 will sent. In the previous cycle to, to the execution unit, then this instruction couldn't have because then it can, can't take only one instruction. So this is also possibly correct. RS2 is older than RS3, so RS3 cannot dispatch until RS2 does. This cannot be correct. Because, in out of order execution in [UNKNOWN] algorithm is one of the out of order algorithms, we will dispatch an instruction as soon as its operands are ready. If we were concerned about this, when the operands are ready, we still don't dispatch the instruction, then we will have an in-ordered processor, which we don't want. Actually, the reason why this is an out-of-order processor is because, is that instructions like this can execute even if they are not the oldest instruction in the reservation stations. So this is definitely not true. So the correct answers are only one and two. So the final step in the [UNKNOWN] algorithm is this write result step of our broadcast, so let's see what happens during that step. So let's say that we have this situation where we have already issued some instructions. This one has been dispatched and is executing. And now is the cycle when this instruction is done and wants to broadcast its results. So what's going to happen? So the first thing that will happen is we put the tag, in this case RS1, and the result, in this case minus 0.29, on the bus. So it gets broadcast to all of these structures. So as a result of the broadcast, this will reach to all of these units. Now what we want to do is write to the register file, basically. This is a new value for whichever register this supposed to be writing to, and we don't have to carry that this is going to register F2, because reservation station one matches F2, so we know that the value is going to F2. So we're going to override the value in F2 with that. So, pretty much, the first step is to kind of over-ride the value in the register file. So we have changed the F2 now because of this match between the RAT and the RS1, we know that F2 should be written with this value. The next thing that happens is we update the register allocation table. Basically the entry that matches our tag needs to be changed so that it now points to the register file. We said that an empty RAT entry will be indicative of just going to the registry instead of waiting for something, so now we need to basically, you know, tag this in a way that makes it empty. Usually there is a valid bit here that just says it's empty. And the last thing we need to do, is free the reservation station. That is the reservation station whose name matches is now marked as empty so that it can accept our new instruction. In this case, we kind of just delete stuff from it, but in real harbor, there will be a valid bit here that says that this is an empty reservation station. So, this is what needs to happen. The situation we want to consider next is what if we need to do more than one broadcast? So let's say we have this situation. You know, we have some instructions renamed, there is the register file here. We have some reservation stations. But, the situation is that somehow this divide here has finished in the multiply divide unit, at the same cycle when the add here has finished in the add unit and now we, if we have only one of these broadcast buses that we can use to broadcast the result, the question is, which one of these goes first? There are first of all, several possibilities for how to support this impasse. For example, you could have a separate broadcast bus for each of the units. So, normally you have one of these buses. If you have a separate bus, note that there will be twice as many comparators basically for each of the operands for each of the instructions in the reservation stations. We need to try to match each of the tags that are being broadcast and be able to select the result or not depending on whether it matches. So there will be twice as much matching hardware if we had two buses. Similarly here and here you will have to be able to write the results and so on. So if you have only one bus then the question is, which one do we broadcast. And usually, you either just make one unit be the highest priority unit and the other ones are kind of less priority than that so that you always know. I mean, you, we, we need to choose very quickly which one is going to be broadcast. It doesn't matter so much which one it is as much as it matters that it's only one. So basically, one of them needs to be picked. A very common heuristic is that if one of the units is known to be slower than the other, let's say that the multiplication and divide are slower than adds and subtracts and so on, which is usually the case. Then, what you want to do is give the priority to the slower unit. Why? Well, because those instructions have been executing for a while longer. So what's going to happen is those instructions are going to have more dependencies probably waiting for them. Pretty much, there are more instructions piled up to use these results. So it's, so it's kind of like an age based thing, but instead of considering the age of each instruction, in which case we would have to carry additional information here. What we do is we simply give the priority to the smaller unit. So this is a very common heuristic and that's usually what ends up being used. If you really want to be kind of better than that, then you would, you know, carry the age of the instruction in which case you can do age based thing, which amounts to almost the same thing. Because usually these instruction's going to be older here just because they're kind of slower to execute to begin with. Or you do something slightly more advanced than that. But again, it's a very common heuristic that we just used. Basically, the multiplication divide unit is going to have priority in this case over the add and subtract unit. And finally, the issue we want to consider is, what if we broadcast a stale result. Let's see what a stale result is. Okay, so what we have here is a situation when this divide here, is going to broadcast this result. It's going to broadcast that it's a reservation station 4, and broadcasting this result which is the result of the division between these two numbers. But if you look closely at the RAT. None of the registers is currently renamed to this instruction. So how can this happen? Well, one way of it happening is that, when this instruction was renamed let's say that F4 was renamed to be this result. So for a while this was saying RS4, but then another instruction came let's say RS2 [UNKNOWN]. And let's say that it was taking R4 and this, and writing again to the register 4. In that case, this would not say RS2, so simply this instruction no longer has any of the rad pointing to it. So what's going to happen when we broadcast this result. What's going to happen is, for the filling the reservation stations, it's going to be done normally. Pretty much every entry, for every instruction is going to be matched against the tag here, and wherever we match the tag, we're going to replace it with a value. So in this case we're going to write instead of this RS4 here, we're going to get minus 0.11. Okay? That's the only match, so we are done. The question is what happens with the register file and the RAT. In that case, in the RAT we do nothing. Why? Well because, the value from this instruction. Will never be used by any of the instructions that come later. The only reason why we needed the values in the register file is that, after this broadcast happens, all the instructions that need that value are going to get it from the register file, so the value had better be in there. Also, all the instructions that come in after this instruction are going to look here to see. What is the newest renamed value of this result? And they're going to find RS2, for example, for the, for this value. However, if this is RS4 and this is RS2, that means that there is a new instruction that is producing the value for this register. So basically, after this instruction here has been renamed. All the instructions that come and read F4 should actually get the new value from you know, from this. So this is basically the situation. We had a divide come first, for awhile everything that reads F4 should be getting the value from the divide. After the ad here, as renamed F4 to point to it. All the instructions that come after that, are supposed to use this value when they read R4. So, so this is entirely correct behavior. So pretty much what we're doing is, when we broadcast this and capture it here, we're giving it to any of the instructions that sit between this divide and this side that are actually using this result. But because the add has already been renamed. The instructions that we are renaming now and that we are getting results from the register file, those instructions actually need the value from the add, so they really don't need anything from, from this divide. So pretty much, if the tag that has been broadcast doesn't match any of the entries in the RAT. Then we don't update the RAT, and we dont update the register file. And it, it is perfectly fine because all of the instructions that possibly need this value, are already reservation stations. When you do quizzes and so on, be careful about this because it is so easy to you know, kind of just. Make the entry here point to the registar file. For example, here, you know, it would be incorrect to delete this because it doesn't match this. Now that we have seen all the steps in Tomasulo's algorithm, let's take a moment and review the whole thing. This is our instruction queue, our RAT register aliasing table, the registers themselves, and let's say we only have the two reservation stations for the add, subtract unit to make this more compact. If you look at the steps that happen for one instruction, here is what we have. We take the instruction from the instruction queue and issue it to a reservation station and we look up the RAT to tell us where the operands for the instruction are going to come from. Now that the instruction has been issued, it sits in its reservation station and waits for the operands to actually become ready. During that time, the instruction is trying to capture the results broadcast by other instructions that are executing. Once we have captured the last operant that we were missing, the instruction will dispatch, which means it is sent for execution. And when it's done executing, it writes its result. Which means, now this instruction will broadcast its result. This is used by other instructions to capture their missing operands. This instruction also writes to the register that it is supposed to update, and it updates the RAT for that register so that future instructions start reading the register file and not waiting for this instruction anymore. What is important to note is that all all of these happen one at a time for one instruction every cycle, some instruction may be in one of these steps. So during a particular cycle, some instruction might be issued. Some instructions are trying to capture their operand. Some instructions are trying to be dispatched. And finally, some instruction is writing it's result, and that is the result that is being captured by the instructions that they're trying to capture. So all of this activity is happening during every cycle, it's just that it's happening for different instructions. So you can look at each of these steps needs to be taken by a particular instruction, but the processor doesn't do just an issue of an instruction in a cycle. It's doing the issue for that instruction while trying to capture for others while trying to dispatch another while doing the write result for yet another instruction. Because all of these things can happen every cycle, there are some interesting things that we need to consider. And those are, can we do same cycle issue of an instruction and then immediately dispatch that instruction in the same cycle if it doesn't need to capture any results? Typically the answer here is no. Why? Well because while issuing the instruction, we are putting stuff in it's reservation station, and usually to dispatch when into test what's in the reservation station. So during the issue cycle we are writing to the reservation station, and that reservation station is not ready yet to be recognized as a dispatchable instruction. So, pretty much, the reservation station is treated as empty during this cycle while we are dispatching the instruction. And only starting in the next cycle, that instruction becomes eligible for dispatch. But it is possible to design the processor in a way that allows same cycle issue in dispatch. The next thing is whether it's possible to do same cycle capture and then dispatch. So if an instruction is sitting in a reservation station after being issued, and it captures its last missing operand in this cycle can it also dispatch in this cycle. Typically the answer to this question is also no. And that is because, again, the reservation station here updates it's status from operands missing to operands available during the cycle when it is doing the capture and next cycle, this reservation station looks like it is something that might dispatch, but it is, again, possible to capture operands. And consider this instruction for dispatch. It just needs more hardware for that. And finally can we update the RAT entry for both issue and write result in the same cycle? What's happening here is that an instruction that is being issued might need to update the RAT. To change the entry that belongs to it's destination operand. Meanwhile the instruction that is broadcasting, also needs to update the RAT that corresponds to it's destination operand. So if the instruction is being issued and the one that is writing it's result. Have the same register that is their destination, then this RAT entry needs to be sort of updated twice. So can we do that? And the answer to this is typically yes, because this is not something where the entry needs to be written once and then written again. What we need to ensure is that the instruction that is issuing, ends up being the one whose value is kept in the RAT. So pretty much these two write to the same entry in the RAT. The end result of that should be, the thing that the issuing instruction wrote. Why? Well, because the right result instruction is trying to point others who need to read this register, to the registers that it's going to write. Meanwhile, the issuing instruction, which is later in the program order, is telling the instructions that are going to issue after it, that they need to look at it's reservation station for the result. And because the instructions who will read the RAT are the ones that issue even later, they need to see the latest value of the register, which means they need to see the value that the issue instruction is producing. And because this is a simple decision, here, you just write this one and not this one, if both need to be written. We can do that in the same cycle. So now let's see if we understood what are all the things that happen in one cycle in [UNKNOWN] algorithm. Suppose that this is the situation we have at the beginning of a cycle. We have an instruction that is waiting to be issued in the instruction queue. This is the contents of the RAT. These are the values for the Registers. These are the Reservation Stations. Two of them are occupied. One is still available. And the instruction in the reservation station 0 is executing in the ALU and during this cycle it will broadcast it's result. Which is 4.4. This is what is allowed to happen in the same cycle, issuing and then dispatching the instruction cannot happen in the same cycle. Capturing a result and then dispatching can happen in this particular processor. And then, issuing and broadcasting of a result for the same RAT entry, is of course, also allowed. The first part of this quiz is at the end of the cycle. Whose beginning we are looking at here. What will be the content of the two RAT entries and the two registers? So after the end of this cycle, what's going to be here, here, here, and here? Let's look at the solution to the first part of our quiz. The question was, what is in these registers and in these RAT entries? And to answer that, we need to consider all of the things that can happen in this cycle. One of the things that happens is we will try to issue this instruction. That might update the RAT. Another thing that we need to consider is that this instruction will broadcast its result, and that might also update the RAT and write to a register. We can do this in either order, but we have to figure out if they update the same thing, what happens. So let's see. Issuing this instruction results in looking at the RAT entries for F0 and F1. And then, writing to the entry for F1 the number of the reservation station where we put this instruction. Because there is an available reservation station, this instruction will issue there. So the entry for F1 will be changed to point to a reservation station 2. The registers are not updated when we issue an instruction. So they stay the same for now. The broadcast of the result might update both the RAT and the registers. Let's first see what happens to the RAT. We're broadcasting that RS0 is now 4.4. And that will be written to registers. So from now on, we will not be using RS0 as the name for this value. That means that this RAT entry will from now on be pointing to the actual register, where we write this result, and that register is F0. So this RAT entry from now on points to the registry file. Because the RAT entry was pointing to RS0 and RS0 is broadcasting, that means that the register file will get value 4.4 for that register. So the register here will change and finally this register here, nobody wrote to it in this cycle, so it stays the same as it was. This is part two of our one cycle quiz. We are back to what is the state of the processor at the beginning of this cycle. The question for you is, at the end of this cycle, what is in reservation stations? In particular, is this reservation station still busy? Is this one still busy? Is this one going to be used? What is in the three fields and what is in these two fields? Let's look at the solution for the part two of our one cycle quiz. Where we are looking at reservation stations and trying to figure out what's happening to them. To figure out what happens to reservation stations, we need to figure out whether there is something that issues. So does this instruction issue? What happens to the instruction that is broadcasting its result and is there any result capturing in the remaining reservation stations? We can do this in any order but we have to be careful if multiple updates are made into the same field. So let's try to do it just, you know, let's firs see what issuing is going to do. Well, when we are issuing we are going to look for a free reservation station. There is one. So, this reservation station becomes occupied. This is an add instruction so we put add here. And it adds F0 and F1. We consult the Rat here to check what are we going to do. Both of these are pointing at reservation stations. So our first operand is whatever RS0 produces. And our second operand is whatever RS1 produces. But that's not the end. We also need to consider what happens when a result is broadcast. What happens then, is that this instruction frees its reservation station, so this one is not going to be occupied anymore at the end of this cycle. I put an x here, but what you should have done is not select this one as still busy. This one of course, remains busy. The result that is broadcast here is captured by anybody waiting for R 0. So this here becomes 4.4, thus this is a change that you needed to make. This one here stays the same, actually this reservation station is free. So it doesn't matter what is here, but it stays the same. And then this one, that we just put in our reservation station, needs to capture the 4.4. Otherwise things are not going to go well. So this needs to happen in the same cycle. The processor won't work well if we left it at RS0, because RS0 is never going to broadcast anything. This instruction is still waiting for RS1, and that's fine, RS1 hasn't executed yet. So this is all that happens to reservation stations as a result of things that happen in this cycle. Final, third part of this one cycle quiz, is what will this patch, if anything in this cycle? So we have added boxes here for you to check, which instructions are going to this patch in this cycle. Note that it is possible for no instruction to dispatch. But if one does, which one? Again, the state of the presser here is at the beginning of the cycle. So we need to kind of reconsider, what are all the things that have happened during this cycle. Let's look at the solution for the part three of our one cycle quiz, which is about, if this is the state at the beginning of the cycle then during this cycle, which if in instruction will this batch into the ALU here. And, this instruction we already saw in part two. Is going to free its reservation station and it's already executing, so it will definitely not dispatch again. This instruction here, will capture its last remaining operand during this cycle, so it becomes eligible for dispatch after that. We said that in the same cycle capture and then dispatch is allowed, so this instruction here is a candidate for dispatching. We're also going to issue this instruction here, into our reservation station. But because the issue and the dispatch is not allowed in the same cycle, that means that this instruction, even if its operands were ready, would not be eligible for dispatch now. So the only instruction eligible for dispatch is this one, which means that, that's the one that gets to dispatch. If more than one instruction were eligible for dispatch, then we would have to pick, which one dispatch is because we only have one execution unit. And in order to do that, we would actually need to be told what are the rules for picking among instructions. Fortunately, we didn't need that because only one is eligible for dispatch. So let's do another Tomasulo's Algorithm quiz. Which of the following is not true about Tomasulo's Algorithm. It issues instructions in program order. Select this if it's not true. It dispatches instructions in program order. Select this if this is not true. It writes results in program order. Select this, if this is not true. So let's look at our Tomasulo's Algorithm quiz solution. Does it issue instructions in program order? Yes it does. Instructions come from the instruction buffer in program order, and that's the order in which we are going to issue them. You cannot issue instructions out of order. So this is true. We don't select it because the question is, which is not true. It dispatches instructions in program order, no it doesn't! Instructions can [INAUDIBLE] be dispatched outside of program order, that's why this is an out of order processor. So, this is definitely not true. And then, it writes results in program order. Not necessarily. Again, results can be written in the order in which they are produced, and it works for that. So this is not true. So basically the one that is true is one. The other two are not true so we selected two and three as not true. Now let's briefly consider just what happens to load and store instructions. Just like we had data dependencies that go through registers, this is for example, why we did register remaining to eliminate the false dependencies through registers and why we are doing these reservation stations and so on in order to kind of obey properly the true dependencies. There can be dependencies through memory. Because loads and stores, in this case, are the only instructions that can have dependencies through memory, but there can be dependencies. A read after write dependence occurs, if there is for example a stored word, to some address in memory. And then we do a load word from that address. Then there is definitely a read after write dependence between the store and the load. Basically the load uses the value put there by the store. We can also have a write after read false dependence. For example, we can have the program that first needs to load something then store it. If we reorder this, then the load gets a value from the store, not the value from before. And finally we can have a write after I dependency. There are two stores. The store to the same address. The last one should, can, leave the value at the end of that sequence. But if we reorder them then the value in the memory ends up being the, kind of, stale value from the first of the stores. So, obviously, dependencies from memory need to be obeyed or eliminate it just like register ones do. So what do we do about the dependencies? In Tomasulo's algorithm, we do loads and stores in order; basically we don't try to reorder them. When as we should these instructions, we put them load and store queue and then we just follow the order of this instruction. So for example, a load doesn't execute if there is a previous store pending even though maybe the load is ready to go but the store still is waiting for something. The second option is to actually identify dependency between load stores, et cetera, and then reorder them just like we did normal instructions, et cetera. But this turns out to be more complicated than doing it for register-based dependencies, so we are not going to do that. And that's why for Tomasulo's algorithm they chose not to do this for memory instructions, but they chose to do, do it for other instructions. Modern processors actually do identify dependencies, reorder, et cetera, even for loads and stores. But we will consider later how that is done. So, so for Tomasulo's algorithm, this was really kind of the, you know, option that they chose. So now that we have seen how Tomasulo's algorithm works and what the structures are and so on, let's go through a long example that kind of helps you do things on exams when we ask you kind of, do Tomasulo's algorithm for a few cycles, let's see how to do that. I conveniently came up with tables like this so that I don't have to draw everything by hand. Here we have the characteristics of our processor basically this is that the load unit takes two cycles to execute the load. Add takes two cycles to execute adds and subtracts. Multiplication will take 10 cycles and divide will take 40 cycles and they're done in the multiplication unit and the initial content of R2, R3, and R4 are like this. This the table that gives us our instructions in order like this so this is the very first instruction. Here we will note in which cycle which instruction issues begin execution and writes the result. This is basically a graph. This is where we will say what cycle we are currently in and this is our reservation stations. These are the two load reservation stations, these are the three reservation stations for the adders and this is the multiplication units reservation stations. The load reservation stations are all really going to be our load queue so they're going to be slightly different from the others. Here we will just check when the instruction is in a reservation station. This is going to be what the operation is. This is where we restored the value that we have for this operand. This is the value for the second operand. This is what we are waiting for in the first operand, this is what we are waiting for for the second operand. And this is whether the instruction has been dispatched, so that we can keep track of it. So let's begin with the very first cycle, cycle one. In this cycle there is nothing to dispatch and nothing to write of course. So the only thing that we really need to be concerned with is issuing an instruction. The load will look at the reservation stations and look at basically the load's RQ in this case. And see that there is an empty space there, so we can issue. So, we will mark that this instruction gets done in cycle one, as far as issue is concerned. Now, we have a busy load store Q entry. The operation is a load. Now, the load uses an address. It doesn't really use a value. So, in this case we will use our R2 which is 100. And add 34 to it. And that address will be computed before we put it in the load queue. So lets say that this value is noted here as 134. So this is really the address. We are not waiting for anything, but this load will produce a floating point value and put it in F6. So we need to rename F6 to point to Load 1. And the load doesn't dispatch in the same cycle because we said that we cannot do that. So this kind of concludes the issuing of this load. Now let's look at the second cycle. In the second cycle, we will try to issue this load and there is a free unit, so we will do that. It's 45 from R3, so it's going to be 245. Again, this is done by the address unit. It's not done by the ALUs or anything. So this addition gets done immediately. And we are not waiting for anything because a load doesn't really need to wait for an, any floating point registers. And then, F2 is where we're going to put the result. So we're going to mark this as LD2. And this is it as far as the issuing is concerned in this cycle. We will, dispatch, this load now. So its execution begins in cycle two, because everything is ready for it. And now, we need to consider, okay, so it takes two cycles to do a load. Does the write back happen at the end of the second cycle or does it happen in the cycle after that? In this case we will assume that the write back happens after the execution is done. So the execution of the load will be done in cycles two and three. And the write back will happen in cycle four. So we will kind of know, that basically we will be able to write in cycle four here, just to kind of remember that So, now let's look at cycle three. In cycle three, we will try to issue the next instruction which is this multiplication. We will look at whether we have a unit, yes we do. Is going to be a multiply instruction. F2 and F4 are LD2 and the actual value of F4. So here, we will mark the actual value of F4, this is our second operand as 2.5, because that's what this says. And then for the first operand, we will mark that we are waiting for LD2. This is going to make it very easy to determine which things are, ready to execute, because the instruction should not really have anything here if it's ready to execute. And this instruction is, not going to be ready yet, because it's waiting for the second load. And, then we're going to rename F0 so that it points to, the reservation station, in this case, ML1, that contains this, instruction. In this third cycle, the first load is still executing, now the question is, can the second load begin execution? If we have a pipeline load store unit, then, yes, it would be. Because in the previous cycle, we have begun this instruction. It's in the second stage now. So now we can begin the first stage for this instruction. But lets say we don't have a pipeline load story on it. That only our add unit is pipelined. In which case, this load will not be able to dispatch into the execution unit until cycle four because basically the execution unit will be occupied in cycles two and three by, the first load. So, we will, [SOUND] kind of mark that in cycle four, we will be able to, begin execution of this if, everything else is ready. And nothing is still broadcast in the result. The first load is still, executing in cycle three. So, now we go to cycle four. In cycle four, we will try to issue the next instruction, which is this subtract. It's going to use. And add R unit, so this is, just to mark it as a subtract, we are going to issue it in this cycle. F2 and F6, or LD2 and LD1, so this one is actually waiting like this and we also have to, mark the F8 as renamed. To 81 so this completes the issuing in this cycle. Now let's see, if we can dispatch something. Well, this is cycle four. In cycles two and three the first load has been executing now it's ready to write so basically we can begin execution of this instruction so it's been dispatched in cycle four now we no longer need this. Are any of the other instructions ready to execute? Well, not in this cycle so now we're going to write the result, in LD1. LD1 here, we kind of note it here that it's going to write the result in cycle four which is now, so now we're going to write to, the result of this instruction. So we can mark here that we are writing that result. And, what's going to happen now is basically is going to broke us with a type of LD1 and the value will be loaded from memory from this address and that value is going to be put to F6. So, what we do now is we're going to broke that value and, capture that value everywhere, where we have LD1 in this table. So, each of these is going to compare its tag to LD1 and whatever we see in LD1 we're going to get that value. So. In this case, only this particular instruction is going to latch its value. So let's say that the value is 7.1. Okay. That completes the result capture. We still have to put 7.1 in the actual register F6. So somewhere in F6. We have 7.1 now, because this was pointing to LD1. LD1 is done now, so, now we can find that value in the register file. And now we can free this reservation station. So, when you free a reservation station, what you really do is, you just, turn off the valid bit for the entry, you don't really have to erase the whole entry. So I'm going to, erase it just to prepare for the next instruction, but keep in mind that, in real hardware there is just a single bit that just gets marked zero, and that is considered to free the reservation station. Now in this cycle, in cycle four, we have issued an instruction, we have tried to dispatch an instruction, and we've found that only the load really can be dispatched. And, we have written a, our result back. Only one result can be written per cycle anyway. So, that's it for this cycle. So, let's move on to cycle five. In Cycle 5 we will try to issue this next instruction which is a divide. This divide is going to look for this type of a unit and find it, so we're going to mark that ML2 is not four. We are going to say that this is issued in Cycle 5. We're going to look at F0 and F6 as our inputs. F0 comes from ML1. F6 is actually in the register file, because this doesn't say so, for example, if this load has been a little bit later, we would have to wait for it, but because it's done, we can read 7.1 and put it here. And then we're going to rename F10 to point to this ML2 which is where we put the divide. And that concludes the issue for this cycle. Now as far as the dispatch is concerned, we have already dispatched this load and there is nothing else that is ready to dispatch, so we don't dispatch anything this cycle. In this cycle, this load is still executing, it is going to broadcast as a result in the very next cycle, in Cycle 6, and there is no result that gets broadcast in this cycle either, so we are basically done with Cycle 5. Now lets look at Cycle 6. In Cycle 6, we are going to try to dispatch this ADD. There is a unit available, so we will succeed in dispatching it. F8 and F2 are going to be AD1 and LD2, rename F6 to point to the AD2 now which is where we put this instruction. And that concludes the issuing of this instruction. Now let's look at whether we can dispatch something. Well, in Cycle 5, this instruction is still executing, so in Cycle 6 is going to write the result, but until that writes the result, nothing is ready really to execute, so basically we don't really dispatch anything, but there will be a right result by the load. As a result of this, several things will happen. First of all, we will deposit the result of the load, which is whatever is at address 245, to the register F2. So, let's say that our F2 just became, I don't know, minus 2.5. We can then provide this value also to all of the things that are waiting for LD2. In this case there are several instructions, so the operant here matches LD2. So, minus 2.5 will be captured here. And, we are no longer waiting for it. The second operant of this instruction also matches, but we are going to capture it here and mark this as no longer waiting for it. And then the first operant of this matches it, minus 2.5, so we are going to un-mark it here, too. And finally, because this entry matches LD2, we are going to change the RAT to once more point to the register file. So this concludes the writing of the result. Remember that, because now these instructions are actually ready to go, but because that happens kind of towards the end of the cycle, we consider that they cannot really dispatch in the same cycle. Because we are considering that we cannot first broadcast the result and then dispatch instructions that use that result. It's easier if we then first try to dispatch instructions and then broadcast the result, because if we now consider what can dispatch, this looks like it can dispatch, so we need to remember that the broadcast has happened in the same cycle, and this really is not eligible for dispatch yet. So that's why we did, kind of, dispatch before broadcast. Similarly, it would help if we just did the, you know, dispatch and then issue, and then broadcast, just so that, you know, kind of, we don't consider things that have just issued for dispatch. But that's easy, because only one thing gets to dispatch and it's very, very quick. So in Cycle 6, we have issued an instruction, we have broadcast the result, and nothing really has been dispatched. So now it's time to consider what happens in Cycle 7. In cycle seven, we no longer have any instructions to issue. So from now we will not be issuing things. Let's see what can we dispatch. Well, we have an instruction here that is ready and, we have an instruction here. They are going to different units, so they can actually both dispatch in the same cycle. [SOUND] So now, both of these instructions have been dispatched. And we will, broadcast the result, but there is nothing executing currently, so, so we don't broadcast anything. So we will mark that the, subtract here, which is this instruction, started executing in cycle seven, is going to become eligible for, because, adds and subtracts take two cycles. So in seven and eight, this is ex, going to execute, is going to try to broadcast in cycle nine. The multiplication here, which is this instruction, also starts executing in cycle seven, is going to take ten cycles, so in cycle 17, it's going to try to broadcast the result. The other instructions are going to wait for these two so basically. What that means is that, nothing is really going to happen now, until the cycle in which one of these two tries to broadcast anything because dispatches are not going to happen because there is nothing that can go. Issue cannot happen because we are done issuing everything. So, we can now kind of fast forward in this, I mean cycle pretty much. Nothing happened except these two are executing. In cycle nine, what's going to happen is, this instruction's going to try to broadcast. And because there is nothing else that wants to broadcast in this cycle, this broadcast will happen. So what's now going to happen is, this subtract is going to, find a register, that matches it's tag, which is F8, write the value, to F8 and the value's going to be minus 2.5 minus 7.1. So it's going to be minus 9.6. We're going to un-rename F8, so that it now forces future instructions to read from F8. We're going to find instructions that are waiting for AD1, like here. And put minus 9.6, instead of the, tag. And then, we are going to, free this reservation station. Okay and the, the subtract, has been writing the result in nine. In the next Cycle, in Cycle 10, this instruction will be able to dispatch. This is our ADD. And it's going to execute in Cycles 10 and 11. So in Cycle 12 is going to try to write it's result. So in Cycle 11 nothing really happens, other than the execution of these two instructions. But in Cycle 12 what's going to happen is this instruction is going to broadcast it's result. So, we will mark that it grows as the result. We will find the register that matches AD2 which is F6. And override the value of F6 with a new value. Which is the minus 9.6 plus minus 2.5. So it's going to be minus 12.1. So this is the value of AD2. And now we're going to un-rename, this to, make future accesses read the new F6. And no instructions was waiting for, know that this was the last instruction in the statement so nobody was really waiting for this result. But, this instruction is done now. So we can free this reservation station. Now nothing will really happen until cycle 17. In cycle 17, this multiply is finally going to broadcast its result. We're multiplying 2.5 times 2.5, and then, you know, minus. And that value goes into F0. So F0 is going to become whatever that result is, we're not even going to even try to compute that. It's going to be un renamed here. It's going to put that value over here. And now the, divide is finally ready to go. And we're going to free this reservation station. So finally, in cycle 18. To divide is ready to go, it's been dispatched, at 40 cycles so its going to be in cycle 58 that we finally produced that result. So nothing really happens until cycle 58 at which point, this value is broadcast, register F10 gets to grab that value, we unrenamed this, we free this reservation station. We'll look at who is waiting for it, but nobody is. And that concludes our example. Now it often happens that when doing something like an exam on Tomasulo's Algorithm. We just care, for example, like in what cycle something issues or in what cycle something finishes, and so on, or how many cycles it took overall. So let's go to an example that is just trying to track the timing and not trying to keep track of everything in every structure, and so on, just to kind of show you how to do this on exams. So we will have the same processor, the same instructions, the same issue, execute write. But we're going to try to do that without having all of those tables in. So let's try to go through this example. Whats going to happen is we kind of need to know how many reservation stations we have for each type of instructions. And that's the only thing that we're kind of going to be missing a little bit. So, basically, in the first cycle we know we can issue this and nothing else will happen. In the next cycle, we know we can execute this and then because it takes three cycles. We know that we're going to try to write it in cycle 4. So kind of mark this tentatively now for cycle 4. In the next cycle, we will be issuing this load here. If we consider the same condition that the load cannot overlap with another load, then we know that in 2 and 3, this is going to execute, so that this can begin execution in 4, and then execute in 5, and try to broadcast in 6. In cycle 3, we're going to be issuing this. It uses a different unit. So, again, you have to check basically. In cycle 3, is there an available multiply unit but kind of looking at what is currently using the reservation stations? And then, you have to check for dependencies. So, for example, this is waiting for F2 and F4. F2 is produced by the second load, so actually this will not be able to execute until cycle 7. 6 here means that basically we will have 7. And we can kind of mark here that our F2 comes from instruction 2, which is why he had to execute only in the cycle that follows its write. Because this is a multiplication, add 10 cycles to that. We're going to try to write the result in cycle 17. The subtract is going to issue in the very next cycle if there are available reservation stations because, so far, we have used no add reservation stations that's going to be able to issue. The subtract is going to wait for 2 and 6, so the later of the two loads is really when we can start. So we can start in cycle 7. And we will mark here again that, you know, we are waiting. Kind of just give an indication like what was the delay somewhere. And then, it's a subtract, so it's going to finish in cycle 8, 7, 8. So it's going to try to write the result in 9. Next, we're going to consider this divide. Can it issue in in cycle 5? Well, yes it can because, so far, only one unit is used, then we have two such, only one reservation station is using. We have two of them. So the question is when can it execute? Well, look for F0 and F6. F0 comes from the multiply, it's 17. F6 comes at 4, so we will be able to execute in cycle 18 here because we were waiting for F0 like this. And it's a divide, so it's going to try to broadcast at cycle 58. Next, we're going to consider, what's the next issue cycle. Well, 6. This add is going to issue in 6 because, again, we have units. It's going to look for F8 and F2, whichever is the latest. 2 is at 6, F8 is at 9, so it's going to start executing in cycle 10, execute in cycle 11, try to broadcast in cycle 12. And we're going to mark that F8 comes from instruction 4. And now, because we've been kind of doing this out of order, we have to check just are there any two instructions that are actually writing in the same cycle. If yes then we'll have to redo this. Basically, we have to postpone some of the writes in order for that to happen. But because these are all different cycles, everything is fine And this concludes our timing example. So, basically, now we know in which cycle each instruction executes issues and writes. And when does this all end without really having to go through all of those things? So if we're only worried about timing, then we can do something like this. Now let's try to figure out Tomasulo timing on our own. Suppose we have a processor with these latencies. A load takes one cycle to execute, an add takes one cycle, and a multiply takes five cycle. Suppose that the latencies of instruction execution are load one cycle, add one cycle, and multiply five cycles. Suppose we also have this number of reservation stations. We have one load reservation station, two add, and two multiply reservation stations. And finally, we need to know what's allowed in the same cycle. And for that, issuing and then dispatching instruction in the same cycle is not allowed. Capturing the upper end and then dispatching the instruction in the same cycle is not allowed in this processor. And finally, if a reservation station is freed, it cannot be used in the same cycle again for another instruction. So once we free a reservation station, it only becomes eligible for issuing another instruction in the next cycle. These are the instructions we have. And what we need to now do is figure out the timing for them, when do they share dispatch and write. To get you started, these four instructions will issue in cycles one, two, three, and four. This quiz has two parts. Part one is for you to figure out for this four instructions that issue in cycles one, two, three, and four. When do they dispatch for each one of them and when does each of them write it's result? Let's now look at the solution for Part 1 of this quiz, which is asking when do these four instructions dispatch and write their results. We will do that one instruction at a time. The load is issued in cycle one. It doesn't have any dependencies, but we know that it cannot dispatch in the same cycle when it's issued. So it will dispatch in cycle two. The load takes one cycle to do. So it will write its result in the very next cycle. The multiplication instruction is issued in cycle two. It will dispatch in cycle three or later because it cannot issue and then dispatch in cycle two. It will dispatch either in cycle three or whenever its operands become available and because this multiply instruction doesn't depend on the load here, it uses F0 and F1, and the load produces F6. That means that this instruction has all the operands ready as soon as it issues. So it can dispatch in cycle three. It couldn't issue and then dispatch in cycle two because of the same thing. Issue and then dispatch in this processor is not allowed. The multiplication takes five cycles to do. So if it dispatches in cycle three to execute, it's going to be broadcasting its result in cycle eight. The add here is issued in cycle three, so again, it cannot dispatch in the same cycle, but it could dispatch in cycle four if all of its operands are available in that cycle. However, it needs F2 and F6. These are produced by load and multiply here. So actually, it will not be able to dispatch in cycle four because the multiply result is only available in cycle eight. So what is going to happen to this instruction is it's going to wait and capture its result in cycle eight. And then, it cannot capture and then dispatch in the same cycle, so it will dispatch in the very next cycle, which is cycle nine. Because this is an add and has a latency of one cycle, it will write its result in cycle ten. For this add here, it issues in cycle four. It becomes eligible for dispatch in cycle five if its operands are ready, but they're not. It needs the F2 produced by the multiplication and the F6 produced by this add. That means that it will capture its last missing operand at the later of these two cycles, which is cycle ten. So in cycle ten, this instruction captures its last missing operand, which is F6. And then, it cannot dispatch in that same cycle because of this. So it dispatches in cycle 11 and finishes in cycle 12. And this completes the first part of the quiz. The second part of this quiz continues where the first part left off. Now that we have figured out what happens to these four instructions, the question for you is, for these two instructions, when do they issue dispatch and write results? Let's look at the solution to the second part of this quiz, which is about figuring out, when do these two instructions issue dispatch and write results. We first have to figure out when the add here will issue. Normally, it would issue in cycle five, assuming that the reservation station is available. Let's check whether a reservation station is available in cycle five. At this time we have already issued two ADDs, so we can occupy both of the reservation stations. So one is available here, only if one of these two ADDs frees our reservation station before a cycle five. So reservation station is available only if one of these two frees its reservation station before cycle five. Note that our reservation station needs to be freed a cycle earlier than when we allocate it, so if we are going to use it in cycle five, it needs to be freed in cycle four. Unfortunately, these two adds do not release their reservation stations until cycles 10 and 12. The first one that becomes available will be freed in cycle ten. It cannot be allocated to this add immediately. So this add can only issue in cycle 11 because it needs this reservation station. Once this add issues, it might dispatch in cycle 12, assuming that it has no dependencies. And indeed, it doesn't. So it issues in cycle 12, and then executes for one cycle, and broadcasts its result in cycle 13. The next add here will issue in cycle 12, because issue needs to be done in order. So if this one issues in cycle 11, the next add in the program order will not be considered for issuing until cycle 12. So we will try cycle 12, but we have to check whether a reservation station is available. This add by then has freed the reservation station but these two are still holding onto theirs. Note that this add here is freeing its reservation station in cycle 12, but it cannot be allocated to a new instruction in the same cycle. So this add here actually issues in cycle 13, and then it doesn't wait for anything because F3 and F4 are not produced by previous instructions. So it will dispatch in cycle 14 and finally write it's result in cycle 15. In this lesson, we have learned how the processor can rename and re-order instructions to work around data dependencies. In the next lesson, we will see what happens when we need to do something unexpected, like handle a divide by 0 or a memory protection exception Hi, and welcome to my online course on computer architecture, UD-233, affectionately known as CS6290 here at Georgia Tech. I've been teaching computer architecture for more than ten years now, and I'm now very happy to be bringing this course to a much broader audience like this. In this course, we will learn how modern processor cores work. How these cores access memory, and how we put them together into a multicore chip. But let's start at the beginning. You probably first want to know, what is computer architecture? So, what is computer architecture. Architecture, in the sense that we all know it, is about designing a building that is well suited for its purpose. So, it could be designing, for example, an apartment building or a house and what we want is for this building to be designed so that it work wells for whatever you want to use it. Like, for a family to live in or for a lot of people to live in. Or if it's an office building, for a lot of offices to be there and the employees to be happy and so on. So, computer architecture is similar in that it is about designing a computer this time that is well suited for its purpose. So, we can have a large desktop computer, or a much smaller laptop computer. Or even a smartphone. And we want to design each of these so that they work well. Maybe the desktop, because it's larger, can be much more powerful. The laptop provides some computational power in a much more constrained space. And the phone provides us with a good battery life and it's very light weight and still provides us with some computational power that is enough to, for example, do web browsing and so on. So why do we need computer architecture? We need it if we want to improve performance for some definition of performance, which can be speed at which the computation is going on, battery life, the overall size of the computer, its weight, energy efficiency, and so on. So we want one or more of these to become better than before. And another reason why we want computer architecture is to improve some abilities of the computer, such as adding 3D graphics support, adding debugging support so developers can develop more bug free programs, improving security, et cetera. So this is about making computers faster, smaller, and cheaper. And this is about making new things possible. We do these by taking improvements in fabrication technology and in circuit designs and figuring out how to translate that into faster, lighter, cheaper, more secure, et cetera. So this is where we need computer architecture. So let's do a computer architecture quiz. So, computer architecture is about how to build faster transistors out of which we will then build computers, how to build faster computers, how to build larger buildings, how to design energy-efficient computers, how to build buildings where we can fit more computers, and how to build computers that fit better into buildings. Select all answers that are true. More than one can be correct. Okay. Let's look at the solution for our computer architecture quiz. The question was, what computer architecture is about. Is it about building faster transistors? It is not. Computer architecture is about, maybe finding how to use these faster transistors to build better computers in some way. Is it about how to build faster computers? It is. One of the possibilities for how computers can be better if they're faster, so if we need a faster computer, computer architecture is definitely about how to do that. How to build larger larger buildings. This is not correct. How to design energy efficient computers, again energy-efficiency, that is what we're after is a way of improving computers, so we definitely are interested in that in computer architecture. How to build buildings that fit more computers. This again is about buildings, not about building computers, so this is not true. And how to build computers that fit better into buildings, is about building computers that are more suited for some purpose, in this case, fitting more of them into buildings, so computer architects might be worried about this. So computer architecture is about building future computers, and let's see how it interacts with technology trends. So, progress in computer manufacturing technology is very fast. So if we design with current technology and using the parts we already have for a computer, by the time we design the new computer, we have an obsolete new computer. Why? Well because, by the time we have designed it with current technology and parts, the technology and the pieces for computers have improved, so what we have is something that is using obsolete technology and obsolete components. So we must anticipate future technology and what will be available, and then when we design computer that way, we get future computers that actually use the most advanced technology available at that time. But how do we do that? We must be aware of technology trends. That is, how do we expect technology to change in the future, and what do we expect to be available in the future? One of the most famous trends is probably one you have heard of, it's Moore's Law. Moore's Law roughly states that every one and a half to two years, we will fit twice the number of transistors on the same chip area. So this is what technology improvement promises to do. As computer architects, we try to translate that into doubling the processor speed every 18 to 24 months. So what we try to do is, if the technology can give us twice as many transistors, we try to translate that into doubling the processor speed. We also try to reduce the energy per operation to about half it was every 18 to 24 months. Pretty much. The transistors are now half the sizes they used to be, so we try to translate that into using half as much energy for the operation of the same work. And we try to double the memory capacity every 18 to 24 months. So we can fit twice as many transistors on the same chip area. We tried to translate that into having twice as much memory, if this is a memory chip. Again note that, this is what the technology can be expected to do and this is what architects are expected to do with that technology. That way we can plan for what the technology will be able to provide us with, and we know what the computer market will expect from our future computers. Let's take a moment to process the speed doubling implications of computer technology. Let's do that by comparing the speed of trains and of processors between 1971, when the train speed record was 380 kilometers per hour. This was the French TGV train or as they say, [FOREIGN]. I will also tell you since 1971 until 2007, processor speed has doubled every two years on average over this time period. So if train speed was doubling every two years between 71 and 2007, the train speed would have been in 2007. Put your answer here in kilometers per hour to make it easy. Let's look at the solution for our speed doubling quiz. This is a period of 36 years. If speed doubles every two years, that means that the speed has doubled 18 times. So, we have 2 to the 18th times 380. And if you multiply this out, it amounts to about 99 million kilometers per hour and change. In reality, the speed record for trains has not even doubled between 1971 and 2007. The fastest thing that humankind ever built is the Voyager one probe. And its top speed was 62,000 kilimeters per hour. If trains, since 197, followed speed doubling of processors, trains should have moved this fast by 1996. So, as you can see, processors are one of those rare things that have observed speed doubling continuously for many years and are still doing that. One of the consequences of Moore's Law and the performance improvements that we expect to follow is the so-called memory wall. Here's why we have it. The processor speed in terms of instructions per second we have already said is, can be expected to roughly double every, let's say two years. Memory capacity, which is how many gigabytes of memory can we have in the same size module, is also doubling every two years. But memory latency, that is how long does it take to do a memory operation, has only been improving about 1.1 times every two years. So over time, as years go by, if we plot the speed of processors and memory, we get something like this, an exponential curve for the processors. And we get another exponential curve, but that grows a lot slower, for memories. This here is the gap between processor performance and memory performance at some time and then several years later, we have a much wider gap. This problem is often called the memory wall. Basically, our processors are getting a lot faster than memories are, yet they have to access memory every so many instructions. And because this difference in speed has been growing very quickly over many years, we have been using caches as a sort of stairs for the memory wall. So our processors now are accessing caches, which are fast, and only those rare accesses that's missing the cache will end up going to the slow main memory. We will spend several lessons talking about how to do caches and so on, but this is just one of the trends that we have to worry about when we design processors, is that they do get faster. But if the memory doesn't get faster as quickly as they do, we have to continuously keep improving our caches just to allow the processors to not suffer from the much, much slower memory. When we talk about processor performance, we typically talk about its speed, how many instructions per second can it finish, but we also have to consider its fabrication cost and the power consumption it will have. Lower fabrication costs allow us to put processors that are more powerful into devices that cannot be expensive, like refrigerators and watches and all sorts of other things. Low power consumption is important not only because of the cost of electricity, but also because it translates into longer battery life and smaller form factors for things like cell phones. So what we really want is that the speed of a processor doubles over something like two years, that the cost of this twice as fast processor stays about the same, and that the power consumption of this twice as fast processor stays about the same as before. But we don't have to use technology improvement to get this. We can get, instead, a smaller improvement in speed, let's say 1.1x, for about half the cost as the original processor, and let's say about half the power consumption of the original processor. Which one of these do you want? Well, it depends what kind of computer you're building. So, one of the jobs of computer architects is really to figure out, given that the technology can provide either this or this, or something in between them. Actually, there are many more options. Which one do I really want for a specific type of computer? So, let's do a quiz on the relative merits of processor performance, power, weight, and cost. We will have several processor models with the relative performance, expected battery life, weight of the overall computer, and the cost of the processor. The question for you is, which one of these will be the worst for laptops. The first model is the CRAWLIUM. There's very little performance, very long battery life, extremely light, and dirt cheap. The next model is the SLOWIUM. It has much more performance, half the battery life, however. It's still very light, but it's four times as heavy as the CRAWLIUM. And about three times more expensive. Then we have our LAPTIUM processor. It has twice the performance of the SLOWIUM, five hours of battery life. It's twice as heavy as the SLOWIUMN, costS about $100. Then we have the FASTIUM, which has more performance, significantly less battery life, however. It's about one pound in weight, cost $200 for the processor. Then we have the HOTIUM at more performance, 15 minute battery life, 3 pounds and $500 and the BURNIUM processor, double the performance of the HOTIUM, its battery lasts 2 minutes, it's 20 pounds in weight and costs $5,000. So, which one of these would you least want in your laptop. So, let's look at the speed versus power versus weight versus cost quiz answer. We were asked, which one of these is the worst laptop processor? The crawlium, slowium, laptium, fastium, hotium, and burnium, where performance is growing, the but the battery life is decreasing, the weight is growing, and the cost is growing. Well, most of these are actually not very good processors. Laptium seems reasonable. It gives you good performance. Five hour battery life is not too bad, four ounces for the processor you can build a laptop with that, and about $100 for the processor. The laptop will probably be about $1,000, so that's fine. Going with the slowium, it's not too bad. It's going to be half as fast. But you can still do a lot of things with it. It gives you excellent battery life. Possibly even a lighter laptop, and it's going to be cheap. So, these two are probably reasonable laptop processors. Fastium is a lot faster. Very little battery life. You'll probably have to keep it plugged in. It's relatively heavy, so this is going to be a very heavy laptop. And it's going to be more expensive, but it's still not an unreasonable laptop. So clearly, the designs in the middle are actually better laptop processors than the ones on the extremes. So, we really have to do, is decide whether the Crawlium or the Burnium is the worst laptop processor. Here we have something that is about one tenth of a performance of a reasonable laptop processor. Extremely good battery life. Probably you could make this as big as a cell phone, and very, very cheap. So, this is not such a bad processor. There are still things you can do with it, and some people might want a lot of this, of this kind. In contrast, here we have something that has four times the performance of the Laptium. The battery life of all of two minutes. 20 pounds for the processor including the heat sink and so on. So, probably this is not really going to be portable. And it's going to be extremely, extremely expensive. So, as you can see, the highest performance processor here is actually the worst laptop processor. So as you can see, between the crawlium and the burnium, here we have something that is very slow but also very light and has a long battery life. Which is probably better than having something that is significantly faster with almost no battery life, spinal injuries from carrying it around and extremely expensive, so when you drop it, you drop $15,000 worth of laptop. And you will drop it. As we can see, one of the more important things about the processor is power consumption. There are really two kinds of power that a processor consumes. There is dynamic power which is the power consumed by actual activity in an electronic circuit. But there is also static power which is consumed when the circuit is powered on but idle, so it's not really doing much. We will discuss both the dynamic and static power and also how they are related to each other and how do they affect processor speed Frist we will discuss active power. The active power can be computed as one half of the total capacitance of the circuit, where the capacitance can be seen as roughly proportional to chip area. So, larger chips will have more of this. The next factor is voltage square. This is the power supply voltage. The higher the power supply voltage, the higher the power is. And this relationship is quadratic. So, doubling of the power supply voltage will actually quadruple the power. The next factor is clock frequency. This is the gigahertz number that you normally see on a processor specification. And finally, the alpha is the activity factor. Without this alpha, we are assuming that the processor is constantly switching every clock cycle all of it's transistors. So, alpha really says that, for example, only 10% of the process transistors are actually active in any give clock cycle. Now, let's see what happens to power, if, after two years, we reduce the size of the processor to one half of what it was, because of technology improvement. The new capacitance is half of what it was for one processor. However, because we want to build a more powerful processor, we put two of them on the chip. So, the C new really is equal to C old. Let's say that the voltage and the frequency stay the same, and let's say that it's still the same processor design, so the same percentage of transistors are active. That means really that the active power is relatively unchanged if we do this. What we have is effectively a processor that consumes the same amount of power, but it does have twice the number of cores that it had before, so it's, it's actually twice its capable computationally, because each of those cores is still operating at the same frequency before. And it's really a very similar to the old core. In reality, smaller transistors are going to be faster, so we can increase the clock frequency of the processor too. So, let's say that the new clock frequency is 25% higher than the old one. That increases the power consumed. So, it's now twice as fast, because of the number of cores. Another 25% improvement because of this. But the overall power consumption is now higher. Smaller transistors, however, may let us lower the power supply voltage while maintaining the same speed. So, let's say that the new power supply voltage is equal to 0.8 of the old power supply voltage. That allows us to have a significantly lower power consumption because of this square relationship. So 0.8 squared is 0.64. If we put these two together, we can get something like the voltage. We don't lower it as much, because if we did, then we couldn't also increase the frequency, so we lower it only a little bit. The frequency, we can not increase it as much, because if we did, we would have to keep the same power supply voltage as before, so let's say we only increase the frequency by a factor of 1.1. If you now apply this equation, we get that the new power is about 90% of the old power. So, what we got is now, we have a chip that is having two cores instead of one. It has a lower power supply voltage, but a higher frequency. So, each of these cores is 10% faster than the one used to be. And yet, we have lowered power consumption. This is why it's so important for active power to have the technology improvement to reduce the size of transistors. And we also have to realize that in order to get power improvements, we really, really, really need to lower the voltage. Let's now consider static power, which is what prevents us from lowering the voltage too much. Our transistors are really like electronic faucets. Water comes here, and if the valve is open, water will flow out. If the valve is closed, no water. But in a transistor it's like this valve is controlled by water pressure from another faucet. So if the power supply voltage is dropped, there is less pressure when the faucet is closed and as a result, because there is water pressure here and the valve is not closed very well. There will be some leakage of water, because the faucet is not totally closed. So, one component of static power is this leakage. So the problem is that, as we lower the voltage, this leakage goes up. So we reduce dynamic power by lowering the voltage. But that increases the static power. If we have already lowered this voltage by a lot, then the pressure on this valve here is relatively wimpy already. If we still lower the voltage further, what's going to happen is we're going to get a much larger stream of leakage here. So if we draw the voltage on the horizontal axis and the power on the vertical axis, what we get is when the voltage is very high our dynamic power is also very high, but it drops quickly as we lower the voltage. So this is our dynamic power. However, our leakage power is very low when the voltage is high. And it grows up very, very quickly because of the increase in leakage as the voltage is dropped. So our total power does something like this. There is a voltage at which the overall power is lowest. If you increase the voltage further, you're spending too much dynamic power, so the overall power goes up and you're not reducing the static power by much anymore. Whereas, if you go much further down from this voltage, then the static power becomes very large. And you are no longer reducing the dynamic power to compensate enough for it. So this is why modern processors cannot just arbitrarily lower the voltage to decrease the dynamic power. And also why they are starting to have problems with static power because we have already lowered voltage to the point where the static power becomes an issue. Also note that the leakage through the transistor like this is not the only source of leakage that results in static power consumption. Because the transistors are very small they're acting as faucets who's valves are now leaky too simply. Some water is coming through here. Some of the water that is building up the pressure is also leaking out and so on. So there are multiple other sources for why the static power exists. But to understand the relationship between static and dynamic power, it's efficient to understand that, for example, for voltage, dynamic power and static power are trying to pull the voltage in opposite direction, and the optimum is somewhere in between. Okay, so lets do now an active power quiz. Suppose we have a processor whose voltage can be changed between 0.9 and 1.5 volts in 0.1 volt steps. Its frequency can be changed between 1.8 gigahertz and 3 gigahertz in 0.2 gigahertz steps. But there is a relationship between the voltage and the frequency you have to use. If we want to use the lowest frequency setting, our voltage can be 0.9 volts. If we want to use the next frequency setting of 2.0 gigahertz, then the voltage needs to be 1.0 volts. For 2.2 gigahertz, we need to use 1.1 volts and so on. And we can only get the highest 3 gigahertz with the highest voltage of 0.1 volts. So, we can not just lower the voltage and increase the frequency. If you increase the frequency, you also have to increase the voltage. Of course, you are allowed to use a 2 gigahertz frequency setting with any voltage that is 1.0 volts or above. So we could, for example, do 2 gigahertz and 1.5 volts, although that's, of course, going to be less power efficient than using the lowest voltage available for that frequency. Let's say also that we have measured the power consumption at 1 volt and 2 gigahertz, this setting here. And we have found that the processor consumes 30 watts of power. The questions for you are, what is the power for the most power-efficient setting among these? Put your answer here. And what is the power for the highest-performance setting among these? Put your answer here. Now, let's discuss the solution for the active power quiz. What we had was that to achieve some frequency we needed to increase voltage beyond some point. And we knew that at 1 volt and 2 gigahertz, the power was 30 watts. So, the question is, what is the power for the most power-efficient setting? And, what is the power for the highest-performance setting? The most power-efficient setting will be the one with lowest frequency and the lowest voltage. Why? Because the power, if you remember, is equal to something times voltage squared times frequency. So, we need to have, to reduce power, we need to both lower the frequency and reduce voltage. So, most power-efficient setting is this one. Relative to this setting, the power for the most efficient setting is going to be equal to, so this stand for our efficient power, is going to be 30 watts times the voltage is now 0.9 of what it was. So, it's 0.9 squared times the frequency now is 1.8 over 2. So this is, again, 0.9 of what it was and when we multiply this out we get 21.9 watts or 22 watts. So, the answer here is that the power is 21.9 watts. Now let's do the same, but this time for the highest performance setting. The highest performance setting is the one that gives us the highest frequency. What we have now is the power for the performance is going to be equal to 30 watts times the voltage is now 1.5 times what it was, so it's 1.5 squared times, the frequency is 3 instead of 2 gigahertz, so that has also increased by a factor of 1.5 and if you multiply this out we get 101.3 watts. So, as you can see, there is a huge difference between the most power efficient and the highest performance setting. Although, in fact, the difference between these two settings is not even a factor of two in performance. Why? Well, because power is not only proportional to frequency, it's also proportional to voltage squared and reusing the frequency allows us to reduce the voltage. Another relationship that we have to understand is the one between fabrication cost and our chip area. Chips are manufactured by taking a silicone disk, called a wafer, and subjecting it to a number of steps. Each of these steps sort of prints some of the aspects of the circuitry that should be appearing on each processor. And there are a number of these steps. At the end of these steps, we take the wafer and we cut it up, and each one of these small, squarish things is going to be a chip. So this is a single chip. We take each chip and we put it in a package. Has pins on it. So this is what you normally see as a chip. And then we test these chips. The ones that check out fine, we can sell. The ones that don't check out fine, we throw away because they don't work. It is important to understand that the manufacturing process up to cutting the chips up is all the same. The wafer is this. Is about 12 inches in diameter. It cost thousands of dollars to put it through the manufacturing process, at which point we hope we get a number of working chips and very few non-working chips, but the size of the chip significantly affects the cost. Again this entire process pretty much has a fixed cost. If we have more chips here because they are smaller, we will divide the wafer cost by the number of many good chips that we get and our chips will be cheaper. If we have very large chips, we will get fewer of them when we test them especially and we will divide this cost with fewer chips to get the cost. So it seems like the cost of a single chip is going to be linearly proportional to its size, but in fact, things are worse than that. So, let's talk about the fabrication yield, which is, what is the percentage of chips at the end that we get to sell. So, the yield is the number of working chips we get at the end divided by the total number of chips that we actually had on our wafer. We will see that the size of the chip affects the yield. So, it is not only that we get fewer chips when they're big but also that a smaller percentage of them work. So, let's say this is our wafer. And the wafer, typically, has a few spots on it that we call defects. It is either because the silicon was impure to begin with or because there was something wrong with the process at that place so, in the end, once we get the manufacture wafer pretty much through a given manufacture process, there will be some number of expected defects per wafer. So, lets say this wafer has two defects. Lets say that we divide this wafer into a lot of small chips. So, note that some of the chips are not really complete, because of the wafer roundness. We can only use the complete ones. So, this is a good chip. So, we got 62 working chips, and 2 bad chips. So, our yield, is 62 out of 64, which is about 97%. Now, let's start with the same wafer and the same defects, but now let's have much larger chips. Now we have six working and two bad chips. So, our yield is only 75%. So, not only did we get fewer chips out of our expensive wafer, we also get a fewer percentage of them working. So, let's look at an example of fabrication costs. Suppose that the cost of manufacturing a wafer is $5,000. So, we pay $5,000 to manufacture a whole wafer and cut it up into chips. Let's consider what happens when you have some really small chips so that we can fit 400 of them on a wafer. And then we want to manufacture large chips that are four times the size of the small chips. So, now you would think. That we would have 100 of those on our wafer. But because the wafer is round and the chips are square, we lose some chips along the edge. So let's say that we can fit only 96 of these large chips on our wafer. And finally we will consider huge chips that are four times again. So they're four times. The size of the large chip and then 24 of them would be a quarter of this. But because, again, the wafer is round and the chip is square we lose some more along the edges. So let's say only 20 of them can fit on a wafer. And let us also suppose that there are 10 defects on each wafer. What we want to compute is what's the manufacturing cost of a small large and huge chip. The chip cost will be the $5,000 for a wafer divided by how many good chips we get per wafer. So now for our small chips, what we get. Is that the cost is equal to $5,000 divided by, we have 400 chips on a wafer. But we lose 10 to defects, so we really have $5,000 for 390 chips. And that gives us a cost of $12.80 to manufacture a working chip. For a large chip, we get the cost to be $5,000 divided by 96 of them fit on a chip. But again, these defects are going to be on the wafer, and they will affect 10 of our chips, so we lose 10 to the defects. So we end up we paying $5,000 for manufacturing and get only 86 working chips. So we get $58.20 as the cost of our chips, so note that our chip is only four times the size of a small chip. But the cost is about five times the cost of a smaller chip. And finally for our huge chip. The cost is going to be $5,000, we get only 20 chips fabricated. And now there are ten defects on a wafer. But with chips this big, it seems likely that maybe two of these defects are going to end up on the same chip. So instead of losing ten chips to defects, because each defect effects a whole chip. Let's say that we only lose nine chips because of this defense. So we put in $5,000 and manufacture only 11 chips that work. This is much more than four times the cost of a large chip. This means really that if you're fabricating chips that are really large,. Any slight increase or decrease in chip size is going to significantly change the cost. We have seen that here the difference was five times instead of four. Here the difference is almost ten times instead of four. So if this is what manufacturing cost look like and if we combine this with what Moore's law gives us. Which is we can fit more stuff on the same chip. We really have two choices when we get a generation of Moore's Law to play for us. One, we can have a smaller chip that does exactly the same things that a larger chip could do previously because now we can make things smaller. And, as you can see, getting things smaller gives us much lower cost. So one way of benefiting from Moore's Law, is to getting smaller, and thus a lot cheaper than before while keeping the same functionality. The other way of benefiting from Moore's Law, is to build something new that has the same area as old thing. And that means that we get something faster, and more capable for the same cost that the old thing had. So, if we are interested in high performance processors, typically we will do this. But if we are interested in doing something that could not be done before, then this is what we usually benefit from. Take a cell phone for example. Early on, what we wanted to put in a cellphone, could not be very smart, so we had, non-smartphones. Eventually, the processor that would be required to make a smartphone, became small enough. To put in a cell phone and also cheap enough so that we can afford it, and that's when we started having cell phones. Similar things occurred with digital watches, and all sort of things where eventually what we want will fit in where we want it. It's just a matter of how long do we need to wait for it, because of this, because. Whatever functionality we need, will eventually be cheap enough and small enough to fit. So now that we have seen how to compute manufacturing costs, let's do a manufacturing cost quiz. Let's say that we can build a one millimeter squared processor for watches, which we will call Watch Processor, or WP. Let's also say that we can build a 100 millimeter squared processor for laptops, or we will call it laptop processor or LP. Which of the following is true about their cost? The cost of a laptop processor is 100 times the cost of a watch processor. The cost of a laptop processor is more than a hundred times the cost of a watch processor. Or the cost of a laptop processor is less than 100 times the cost of a watch processor. So let's discuss the answer for our manufacturing cost quiz. We had a watch processor that was one millimeter square in size,and a laptop processor that is 100 millimeter squares in size. The question was, which of the following is true? Is the cost of the laptop processor about 100 times the cost of a watch processor? Is it more than that or less than that? We have seen already that if we disregard the yield, the cost of a laptop processor would be about 100 times the cost of a watch processor. Just because the same cost wafer gets divided into 100 more of these than of these. However, logic processors also result in lower yield. So, this is why the cost of laptop processor will be more than proportionately the cost of the watch processor and this is the correct answer. We said that the goal of computer architecture is to design a computer that is better suited for its intended use. In the rest of this course, we will learn about techniques that can be used to improve the computer's performance, energy efficiency, and cost. However, before we get into these techniques, we need to learn how to measure and compare performance and other characteristics of computer designs. In other words, we need to come up with some numbers that express our intuitive notion of better In this final lesson, we will discuss what happens when we put a lot of cores on the same chip. And we will apply and put together a lot of things we have already learned. So in this lesson, we will go through several challenges that a many-core processor, that is a processor with a very large number of cores, will face. The first challenge is that as we grow the number of cores, the coherence traffic between these cores goes up. And that is because we have already seen that writes to shared memory locations. Resulting in validations and subsequent misses on those locations. And both invalidations and the resulting misses go onto the shared bus, and thus, add to this coherent traffic. As we increase the number of cores, we get more writes per second. And, as a result, the bus throughput that we need goes up, until it eventually exceeds what we can do on a bus. Unfortunately, the bus allows us to only do one request at a time. In part, that is needed because we rely on the bus to produce the easy ordering that we get between rides to maintain coherence. But that means that really the bus eventually becomes the bottleneck. So, what we need is a scalable on-chip network that allows the traffic to grow as the number, of course, grows. And then we also need to use directory coherence so that we don't rely on the bus to serialize everything for us. So let's look at what a network on chip might look like. Let's say we have some sort of a squarish style, that has a core, and maybe a level one cache. And such styles normally will be connected to a shared bus. Now let's say that these four cores are using half of the available tuples on the bus. Because their cache misses and coherence need to be taken care of through the bus. Now let's say we had another set of those cores pretty much the same thing just replicated. Now we have an even longer thus possibly a slower bus, that all traffic from all these eight cores goes to. So we got a slower bus probably with less throughput than the original small one had, yet it has twice the demand for traffic. So very quickly, it will get saturated. Now, let's consider a type of on chip network that is called a mesh. So here, what we have is this quartiles are individually connected like this, for example. Now there is no more broadcast, but this core can still send the message to any other core. If we send the message to this, this the link that gets used. Meanwhile, this core could be sending a message here. If we need to send a message from here to here, then these two links get used. Meanwhile, we can get these two links used independently. So as you can see, the total available throughput in the entire network is several times the throughput of this individual link. And because it's a short link we can have a very large throughput. Now I'm going to take this two by two mesh and copy it so that I get four of those and create a four by four mesh that now supports 16 cores, twice as many as here. Note how many new links are there. Now if you have neighbor-to-neighbor communication, we can have these two talking, these two talking, these two talking all over independent links. So as I increase the number of cores, I also increase the number of these links, naturally. So that the total throughput available in this network grows with the number of cores. So the number of cores grows, the number of links grows, and that allows the available throughput to grow, and that means that I can have many more cores than I could on the bus. There many of types of these so-called point to point networks. A mesh is one of those that is very good for chip building because none of these links intersect each other. So because chips are basically made by printing things on silicon, it's naturally good to have something that is kind of flat like this, and doesn't have links going across different nodes. But we can build chips with some amount of links crossing each other and thus we can have a mesh which is kind of like what we have seen. We can have a torus network. You build the torus by taking the mesh and connecting the end points to each other so the torus really takes the link and kind of wraps it around and then wraps it around and so on. And then you do that in the other dimension too. You can think of these processors as basically things on a donut. You would take this wrap it around, you get a cylinder. Then you take the end points of the cylinder, bend it and create a donut so that's why it's called a taurus. But in reality what you really do. Is you do this, have a long link that crosses here, and a long link that crosses here, and so on. And then you also take this and create a long link that goes here, so that's how you create a taurus. And then there are even fancier networks that are still reasonably good for on chip implementation, such as flattened butterfly. If you're interested in how these work, a very good place to start would be an advanced class on on chip interconnects. Let's see if we can compare the mesh and the bus throughput, for a small four core system, where the traffic is evenly distributed so that each core sends a quarter of all the messages in the system. And the messages that each core sends are sent round-robin, to all the other three cores. So if we are core 0, we send messages to 1, then 2, then 3, then 1, then 2, then 3, etc. If we're core 2, then we send to 0, 1, 3, 0, 1, 3, etc. And you can assume that this is totally randomly distributed, so that, we don't necessarily get any synchronicity among these accesses, pretty much just like a third of the traffic sent by each core goes to each of the other core, but you can assume that, it happens willy nilly. Each core, sends 10 million messages per second. The bus, can support 20 million messages messages per second, the mesh can support 20 million messages per second per link, and obviously, the four core are going to saturate the bandwidth, in which case they will have to slow down proportionally so that. They match the bandwidth of the bus. The question for you is, the speed up achieved by switching to the mesh, from the bus is what? So lets at the solution to our mesh versus bus throughput quiz where we had four cores with a totally random and even distribution of traffic among the cores. We are given what each core is sending, we are getting what the bus can do and what the link in a mesh can do, and now we are asked how much faster is it on the bus versus on the mesh. Our bus cannot sustain the 40 million messages per second, that the cores are trying to give it. So the cores are really going to slow down to half of their speed, so that the bus bandwidth demand matches what it can provide. So on the bus, we get 2x the execution time that each core is capable of providing. On the mesh, we can support 20 million messages per second per link. So now the question is, do our links get saturated? So let's see what we get on a mesh like this in terms of traffic. Each core is generating 10 million messages, 3.3 million are going to each other core. So of the 10 million messages sent here, one third are going to this core, one third are going through this link, and then one third are going here. Let's say that one sixth go this way, one sixth go this way. So this link will be used for one sixth of the messages sent by this core. And this one for another one sixth. And then we add one sixth to each of these, which brings us to one half of 10 million messages. So now we are done with this core. What happens from this core? Same thing. This link gets one half of the messages. One third are going here. One sixth are going here. So this link suffers one half of this traffic again. Now this link gets one half of the traffic from this core and these two links get one sixth each. And now we're done with this core. Now what happens when this core is sending, one half of the message go this way, one half go this way. One sixth of them go through this link, and one sixth go through this link, done with this core. And finally, this core sends messages, half here, half here, and then one sixtth here and one sixth here. Now we are done with this core. So as you can see, our links are actually evenly used. Each of them gets two times one half plus two sixths. So we get one and one third times the traffic generated by one core. That was what the one half was off. The traffic from one core is 10 million messages a second. So we're really getting 10 plus 3.3. So this is 13.3 million of messages per second. Which is available on, on our mesh. So the mesh doesn't slow down the processors. So really the speed up of using the mesh versus the bus is two because the bus requires the cores to slow down so that it can support the traffic that they are generating. While the mesh allows the cores to proceed at full speed at 10 million max messages per second. And still have some traffic left over. If we went to a larger net and then four cores the bus will have to proportionately slow down while the mesh would get more links, and it will be a while until any of the mesh links become saturated. So clearly, the mesh is far superior to the bus in terms of the traffic it can support when we go to more cores. So going back to our mani-core challenges, we have seen that when the number of cores grows, so does the coherence traffic on the chip. And for that, we needed a scalable on-chip network such as a mesh, and the directory coherence, which we have already seen in our cache coherence lesson. Another thing that having more cores will make more difficult is that as the number of cores goes up the off-chip traffic that we need goes up. To have decent performance we will, as we increase the number of cores, increase the number of on-chip caches. So if we have four cores, we have four level one and possibly level two caches. When we have 64 cores, we'll have 64 level one and level two caches. So each core doesn't really generate more misses, but it generates the same number of misses regardless of how many cores we have. So we have the same number of misses per core. So as we have more cores, the number of memory requests which is what we do when we have a miss, would go up in proportion to the number of cores. Now note that the number of pins, the number of those little wires sticking out of a chip, does go up slowly but not in proportion, or anywhere close to in proportion to the number of cores. So if we double the number of cores possibly we get maybe 10% more pins but that's it, because these pins have to physically be large enough to not break when we are moving the chip around and plugging it into a motherboard. So what happens is we get some little improvement in off-chip throughput while our demand for it basically grows in proportion to the number of cores. So our off-chip available throughput becomes a bottleneck. So we need to reduce the number of memory requests that we generate per core and the way we do that is to make the last level cache, which in most two day's processors is the level three cache, shared so that all cores go to that cache and its size is shared equally by all the cores. And the size of that cache needs to go up in proportion to the number of cores as it goes up. So as we double the number of cores we should at least double the size of the last level cache. But there is a problem to having one huge LLC like this. Such a large cache would be slow, and if it's really one single cache, it will have one entry point where we enter the address and expect the data out. And that entry point needs to be somewhere on our chip that now possibly has a mesh or another advanced network. And that means that that one point will be where we send all of the level two misses, so it would become a bottleneck. Pretty much now we don't get to use all of the links equally. This set of links that go to this entry point for the cache will get much more traffic than the other links. If we double the number of cores, then those links around the entry point get double the traffic. So what we do is we don't have just one big last level cache, we have what is called a distributed last level cache. Now let's see what that looks like. So a distributed loss level cache is logically a single cache in that a data block will not be replicated like it would be in private caches, where if two or three or four cores, for example, read a block, each of their caches would have that block. And thus, really the total capacity of the caches is not how much memory we are covering, because some memory appears many times, possibly in these caches. A last level cache that is distributed is logically still one cache, meaning if it's capacity is let's say four megabytes, then we really cover four megabytes of memory without any replication. But this distributed cache is sliced up so that each tile, which contains a core and possibly local caches, now also contains a part of this cache. So now what we have is in our mash, we have a tile that contains a core, a level one cache, a level two cache, and a slice of the L3 cache. So if this slice here that we use per core is let's say 1MB, is really N x 1MB where N is the number of cores, so now as you can see, if we use four cores, we get 4MB, with 16 cores we have 16MB, so we get what we want. The big cache that is there to prevent memory accesses is growing with the number of cores, but it doesn't have a single entry point. In fact, each slice has its own entry point. So if we want the data that is in this slice, we have to route our message to that slice. If we want the data that resides in another slice, we send the message there. If the data is spread among the slices so that all of them get equal demands for the data, then the natural traffic will be nicely distributed across our chip, so that none of the links get over utilized. But if we have a level two miss in a particular core, how do we know which slice of the level three cache to ask for the data? One simple solution is to spread the cache, a round robin among slices, by cache index. So if we have our level three cache with let's say, 1,024 sets slice zero gets set zero. Slice one gets set one, etc., and then let's say that we have eight cores. Then slice seven gets set seven. Slice zero gets set eight. Set nine is in slice one again and so on. So now we can relatively easily, after we break down the address to access the cache, we know the set number, and the least significant tree bits in this case tell us which slice to ask. If we have even more cores, then the number of sets in the cache will grow, and as a result the number of bits that we need to tell us which slice we have is growing and that means that the number of bits in the index are growing, and we will need additional bits to tell us in which set to look. Know that if we sequentially access data, then we would be really be looking at one set and then the next and then the next set, and this nicely spreads the load among the slices of the level three cache. Because sequential axises are basically going to be spread across the entire shape. However, the spreading of blocks round robin across the entire chip may not be good for locality, because a core is just as likely to access something that is in the opposite corner of the chip, as it is to access its own slice in which case there is no on chip network traffic whatsoever. So another way that can help with that is that we spread the data around, but this time we spread it by page number. So all of the blocks that belong to the same page would end up in the same slice of the level three, that helps because now the operating system can map pages, so that it makes the accesses to L3 more local. For example here, this core is running a thread that has a stack. The pages that belong to that stack should be those that end up in this slice. So that if you have a level two miss, we are very likely to access our own local slice of the L3 cache. Of course, some of the shared data will be elsewhere, but this dramatically improves the locality in terms of how far do we need to go to get to the correct slice of L3. Any data that tends to be accessed by one core can be now mapped by the operating system so that it's usually found in the local slice of the last level cache. So, let's see if we can figure out how the distributed loss level cache works. Let's say we have a processor chip with 16 cores, organized as a four by four mesh like this. The cores, along with their level one and level two caches and a slice of L3, are connected like this. So a single tile here in this mesh has a core, a level one, level two cache private to that core, and a slice of the shared last level, level three cache. And that level three cache is eight megabytes in size, has a 256-byte block size, is 16-way set associative, and is distributed among the slices round robin by set number. If core zero issues a load from address one, two, three, four, five, six, seven, eight hex, and if this load is a missing level one and in level two we send the level three request to what tile? Let's look at the solution to our distributed last level cache. We have 16 cores in a four by four mesh. It doesn't really matter what the organization of the network is. We need to know that there are really 16 tiles, numbered 0 through 15, and that we have an 8-megabyte cache with 256-byte block, 16-way set associative distributed round robin by set. So set zero will be in slice zero, set one will be in slice one, etc., and then it just wraps arround. So let's look at what this address maps to. The least significant eight bits are telling us what the block offset is. The next so-many bits are going to be the index, and the least significant bits of the index are going to tell us which tile to look at. There are 16 tiles, so the next digit is really the tile number. And that means that we go to tile number six. Note that we don't really need to even compute what exactly is the index because the least significant bits in this case are telling us what we need to know. Now going back to our mani-core challenges we have seen that the number of cores as it grows makes the coherence traffic grow and we looked at solutions to deal with that. We then looked at the problem of the off-chip traffic also growing with the number of cores. And we just saw that a large shared but distributed last level cache can help with this problem. The next problem we face is that the coherence directory that we need is just way too large to fit on-chip. Because we have a scalable on-chip network and no longer a bus, we need directory coherence. But a traditional directory has one entry for each possible memory block. The memory can be many gigabytes in size. That might require even billions of directory entries and that won't fit on chip. So how do we deal with that problem? So, we need a directory that will fit on chip. Otherwise, to maintain coherence between our caches that are very, very fast, we will need to look up a directory in very, very slow memory which kind of defeats the purpose of having these caches. Some of the questions about this directory are easy to answer. For example, where is the home node where we go to find the directory information about the block? And the answer to that is the same tile that has the last level cache slice where that block would be. Because, if the directory entry for that block indicates that none of the private caches have it, then we want to look up the last level cache to find that block, so we might as well do that in the same node that has the directory, otherwise there would be a lot of traffic between where the directory for a block is and where the last level cache for the block is. So naturally they should be on the same node. But that doesn't really answer the question of having a directory entry for every memory block. We have seen that we only keep the data in the last level cache slice for those blocks that we actually have in our level 3 cache. But a directory without any changes would need to be for every possible memory block that might ever come into that slice, which is a big problem because there are a lot of memory blocks. So instead of keeping a directory entry for every memory block, we need to realize that the entries that are not in our last level cache, or in any caches on chip, really are not shared by anybody, so we don't really have to maintain that information. So we will maintain what is called a partial directory. We don't maintain a directory entry for every memory block. Instead, our directory will have a limited number of entries, and then we allocate an entry in this limited directory only for such blocks that have at least 1 presence bit at 1. Basically, only for blocks that might be in at least one of the private caches. So for a block for which we know that it is not in any L1 or L2 cache, it's present only in the LLC, or only memory for that matter, we don't need the directory entry. Implicitly we know that such a directory entry would just be all zeros, but let's say that we have decided on some limited number of entries. We start allocating those entries when we put flaws in L1 and L2 caches, and eventually we will run out of directory entries. What do we do now? So now let's see if we can figure out how to do an on-chip directory, where we said that we're going to have a partial directory. The directory has a limited number of entries in each slice. We allocate an entry only for a block that might have some presence bit set at one. But as we request blocks from level one caches, we will allocate these entries. Eventually we will run out of them. So the question for you is what do we do now? We have run out of directory entries. How do we proceed? Let's look at the answer to our on-chip directory quiz. We said that we might run out of these partial directory entries, and the question is what do we do now? And the answer is, the same thing we did when we ran out of room in a cache. And that is replace an existing entry, possibly using a replacement policy such as LRU. The really interesting question is what do we need to do when we replace a directory entry? So we said that in a partial directory we can run out of directory entries. So if we need to allocate a new entry because a block is being sent to a level one and level two cache and we need to mark where it's present the question is, what do we do in our directory. And the answer is we pick an entry to replace, for example, using some sort of LRU algorithm in our directory. Let's call that entry E. So, that E is the entry we are replacing, not the new entry we are putting in. The new entry we put in, we know what to do with it. We are going to set that one presence bit to one. The question is what do we do about the entry E because that entry might have some presence bit set. For every presence bit that is set in that entry that we are replacing we need to send an invalidation to the corresponding cache. Pretty much this entry says that that block is in some caches. We're going to tell these caches to either invalidate or send us the data and invalidate, so that at the end of this process all of the presence bit can be set to zero at which point, entry E can be removed from the cache because all presence bits are zero and such an entry does not need to be kept in the cache. And now we can put a new entry there. So note that now we are not invalidating blocks because of coherence. We are invalidating them because of limited capacity in our directory. So this technically would be yet another type of miss that we can have. It's a miss that had nothing to do with either coherence or capacity or conflict or anything like that. It's a type of miss that usually we don't need to name, but just be aware that it exists. So we can have a cache miss in a level one or level two cache that was caused by an invalidation, and that invalidation was not due to coherence, was not due to a write. It was due to a replacement of that block's directory entry on the chip. So going back to our mani-core challenges, we have seen that there are solutions to the increasing the on-chip coherence traffic as the number of cores grows. We have seen solutions to the increasing off-chip traffic as the number of cores grows. And we have also seen a way to have a directory that will support our on-chip network and that solution was, as we have recently seen, a distributed partial directory, which doesn't keep information for every possible block in memory. It only keeps information for a limited number of blocks, and has a replacement policy, and invalidates on-chip caches as a result of such replacements. The next mani-core challenge we will consider is that the power budget available for the chip, for example 100 watts that we can reasonably call from a chip of the area that we can manufacture is split among all the cores. So as the number of cores goes up, the power we can spend in each core goes down. And that means that the frequency and the voltage that we can use in each core goes down. And that means that if we have a single-threaded program it gets slower and slower, the more cores we have. because that single-threaded program, if we only had one core, would get the power budget of the entire chip. With, let's say, 64 cores, it gets only a 64th part of the power budget. So what do we do about this? So let's look at the problem of multi-core and many-core power and performance. The problem is this. We have one core. Let's say it can spend 100 watts, which is the power budget for the entire chip. And that power of the one core is really some sort of a constant times the voltage squared, times the frequency of the core. Let's say that we get 100 watts consumption when we set the voltage and the frequency, such that we can operate at 3.8 GHz. Now, let's say that we get two cores. In that case, each gets half the power budget of the chip. The per core power in the two core case is one half of the single cores power, so we need to reduce the voltage and frequency such that we get one half of the power. Remember that voltage can be reduced in proportion to frequency. So the frequency at which we can run a core in a two core system is going to be cubed root of one half times the frequency at which we can operate a single core in a single core system, which is this frequency. And that ends up being 0.8, approximately times f1 or three gigahertz. So as you can see, if we now use only one of these cores, we get to run it at three gigahertz as opposed to 3.8 gigahertz that we could get here. So this is noticeably slower. So let's see how performance is affected by the number of cores. Suppose we take a single-core execution and we break down the time in it, according to how much available parallelism there would be. So for 20% of the time, we really can only run on one core. For 30% of the time, we could use two cores if we had them. For 40% of the time, we could use three cores if we had them. And for 10% of the time, we can use four cores if we had them. Now suppose that we have a chip whose total power is 100 watts. If that power is enough to get five gigahertz if the chip only has one core, then how many gigahertz can we get when we have two cores, and how many gigahertz when we have four cores on the chip? Also if our execution time is 100 seconds for a one-core chip, it will be how many seconds for a two-core chip, and how many seconds for a four-core chip? Keep in mind that you need to take into account the change in frequency for this as well. Let's look at the solution to our performance versus number of cores quiz. If chip power is 100 watts and we can operate 1 core if that's the only on on the chip at 5 GHz, how many GHz do we get for 2 cores? And the answer is that each of the 2 cores now gets 50 watts. That's half the power. The frequency in that case is going to be the cubic root of that one-half, or 0.8 times the original frequency, so we get 4 GHz here. The frequency with another doubling of the number of cores is that each gets half the power of these. So it's going to be 0.8 times this, which is going to be 3.2. Or, we could do cubic root of one-quarter of the original 5 GHz and get the same answer here. For execution time, we need to account for the change in frequency as well as the change in parallelism. So, this is what we get. We get 100 seconds when we run all of these pieces at just 1 core at 5 GHz. If we ignore the frequency and have a 2 core chip, then what we get is that for 20% of the time, we get a parallelism of only 1. With 2 cores, for the remaining 80% of the execution time, we now have a speed up of 2. So that time is cut in half, so we get 0.6 times the original time if we ignore the frequency. So it will be 60 seconds without the change in frequency. Unfortunately, there is a change in frequency. 60 seconds at 5 GHz, would translate so we get 60 seconds x 5 billion cycles per second is 75 seconds x 4 billion cycles per second. So the answer here is 75 seconds. We would get 60 if we operated at 5 gigahertz but because we operate at 4, we need 75. And then for a 4 core execution, we get that out of the 100 seconds, for 20 seconds we can't improve by running on more cores. For 30 seconds, we get the speed up of 2 because we can use 2 cores. For 40 seconds the speed up is 3, because we can use 3 of the 4 cores. And finally, for 10 seconds we get a speed up of 4. Keep in mind that this is without the change in frequency. So what we get is 50.8 seconds. Now we have to account for the change in frequency from 5 GHz to 3.2 GHz. Once we get the 50.8 seconds, that's without the change in frequency, and we get from 5 to 3.2. When we compute the speed up relative to this because of a lower frequency, which is actually a slow down, and we multiply that with this, we get 79.4, which is actually slower than a 2 core execution. So as you can see, although the 4 core execution gets some more parallelism exploited, it's not enough to make up for the loss in performance due to frequency change. So the quiz we just did shows that adding more cores might result in a slow down if we have to stay within the same power budget. But, this is only true if we cannot adjust the voltage and frequency, so that when there is no parallelism available we boost the frequency of the one or two active cores so that they get the full power budget of the chip. Modern processors do this. Let's look at an example of Intel's core I7-4702MQ. Which was released in the second quarter of 2013. It's a mobile processor, so it's design power is only 37 watts. That's what we can pull in a laptop, apparently. It has four cores. And, it's normal clock when using all four cores is 2.2 GHz maximum. It can actually save power because it might operate on a battery by reducing this frequency even when running all four cores. But the processor has a turbo frequency of 3.2 GHz when running only one core. That frequency is 1.45 times the normal frequency. If we assume that the power increases cubically with this, then we will get three times the power. So we are not using four times the power of a single core, which we should be able to get because it's four cores. So when operating one-quarter of the cores, that one-quarter of the cores can get four times the power. We're only getting three times, so the question is why not four times. Well, because it's not only about the overall chip power. What's really happening is that if we have a relatively large chip and we have four cores on it, let's say. And we keep heating up this part of the chip, the heat will spread into other parts of the chip. But this one will still be hotter than the others. So if we're spending power evenly across the chip, we can spend more power than if we spent it all in one corner of the chip. Because if we spread the power around, then the chip overall gets hotter. If we spend all power in the same place, then that place gets a lot hotter than before. So that's why three times and not four times. Because at three times the power this part of the chip achieves similar temperatures to those achieved by spreading the heat around at 2.2 GHz. Let's look at another example which is Intel's core I7-4771. It's a similar processor released in third quarter of 2013. So they're relatively similar in terms of technology and so on. But this is not a laptop processor, its design power is much higher. 84 watts. It's also has 4 cores. But the normal clock of a core is now 3.5 GHz. Much larger than in the mobile processor. The turbo clock, when running only one core, is 3.9 GHz. Or only 11% above normal. And if we assume that the power is cubically growing with frequency, that means it's only spending 1.38 times the power of a core when working with three other cores. Why is it like that? Well, because this being a normal bell processor has much better cooling, so it can cool the chip when operating at this high power. But that also means that the chip is already pretty hot and getting it even slightly hotter in any place very quickly reaches the temperatures at which it's dangerous for the chip. So we could only really spend this much more power in one core even though the others are not working. So pretty much what's happening is, when running at four cores the chip is very close to not being able to cool. If we turbo boost a particular part of the chip, it gets even hotter and we can only afford a little bit more of heat before we run into cooling issues. Even though the heat is kind of leaking to the cooler parts of the chip it's just not doing it enough. And this is pretty typical that cores designed for lower power have more flexibility in shifting that power. Whereas, if we are near the limits of what we can do, then really shifting the power so that it can all be consumed in one corner very quickly gets that one corner of the chip to overheat. So going back to our mani-core challenges. Now we have seen that the issue of power budget splitting among cores is solved by using a turbo frequency when using only one core, so that a single-threaded program doesn't suffer just because it's running in a multi-core system. Pretty much then one core gets to use all the power of the chip, with some limitations because of cooling. And the final issue that we will look at is the confusion that all of this brings to the operating system. Keep in mind that it has to deal with multi-threading, where each core can really execute several threads in hardware. Then you have cores that can share caches. Then if you have a larger system, you can have multiple chips, each with its own large caches, and you can have all of these at the same time. So let's look at the system that is not even unusual, that combines all of these three forms of parallelism. So we can have a dual socket motherboard that has two chips with four cores on each chip, and each core can simultaneously run two threads, using simultaneous multi-threading. So as far as the programmer is concerned and as far as the operating system is concerned, what we really have is two times four times two. Which means that 16 threads, or for that matter, 16 different single-threaded programs can run simultaneously. And a relatively simple operating system would just treat these as 16 largely equivalent cores. This can be a big problem. Because let's say that we actually have three threads, and the operating system not knowing any better, just maps them to the first three threads that it has. But it turns out that these two are really threads on the same core that compete with each other for issue slots and other things in an out of order processor. And then, really the first half of these threads, eight of them, are really on the same chip. So we're using only half of the overall cache capacity that the system has. Pretty much, we have a very large cache in the other chip that we are not using. So, a much smarter policy for using three threads would be to put them on different chips if we can and definitely on different cores if we can, so that they don't compete. So the idea would be that you would first run threads such that they're distributed among the chips, so that you maximize the cache capacity that you're benefiting from. Then we also want to make sure that the first map a thread to each core. And only then do we want to actually map threads to the second thread in each core. So that if there are fewer threads, they don't need to share the resources of the same core. Each gets its own core. And that requires the operating system to actually know what's going on. So most of the well known operating systems like Windows, and Linux, and so on, will actually be able to figure this out. But that means that as we have this fancy hardware that has pluralism at different levels of granularity, we also have to make our operating systems aware of that. So it's not just expose more thread contexts that the operating system can use. It actually matters how you use this thread context. So going back to our mani-core challenges, we have looked at how to handle the increase in coherence traffic on-chip. We looked at how to deal with the increase in the off-chip traffic. We looked at how to have a directory on-chip that is not too large. We looked at how to solve the problem of the splitting of the power budget among cores even when they're running very few threads. And finally we looked at the problem of, what to do in an operating system when there are multi-threaded cores and maybe multiple chips like that in the same system. In this lesson we have learned what future many core processors will probably look like. This concludes the third and final part of my high performance computer architecture course. I hope you have enjoyed this journey through the guts of a modern processor core, through its memory hierarchy, and through the intricacies of multi-core and multi-threaded execution. It was certainly a pleasure for me to teach this course. In this lesson we will learn how different types of memory work. And why we can not have memory that is large, fast, and cheap at the same time. We will also learn how memory chips can pack so many bits in such a small space. In this lesson, we will see how memory works. We will look at memory technology and look how SRAM and how DRAM works, and what do these mean? We will see why is the main memory so slow? We will see why don't we just use the cache-like type of memory for the main memory to make it faster? We will see what happens on a cache miss when we once access main memory. And we will see some techniques for making things faster, so that once you have a cache miss, the memory doesn't take that long to respond. It's still going to be ridiculously long, but not as bad as before. There are two memory technologies, SRAM and DRAM. SRAM stands for Static Random Access Memory. DRAM stands for Dynamic Random Access Memory. The random access part in both names simply refers to the fact that we can access any memory location by address without needing to go through all the memory locations. So random access is as opposed to sequential access like a tape, where you have to actually scan through the whole tape to get somewhere. With memory, we can arbitrarily access any part of the memory without really needing to get to it. So the only difference between SRAM and DRAM is this static or dynamic behavior. Static, in this realm, refers to the fact that SRAM retains its data while the power is supplied, so as long as you connect it to a power source, it's going to retain the data that you wrote to it. It's not going to lose it. Dynamic, in DRAM, means that it will lose data, even while connected to a power source, unless we refresh the data. So, pretty much, we need to read out the data and write it back in, and keep doing that on a regular basis, otherwise, DRAM will simply lose our data. Note that both of these types of memory will lose data when the power is not supplied, such as when we turn them off. So it seems like SRAM is better. We like it, because we don't have to do anything in order for it to keep the data, whereas with DRAM, we actually have to access the data and write it back in in order for it to not lose it. So why do we want to have DRAM? The answer is that for SRAM, several transistors are needed for each bit of SRAM, whereas for DRAM, we only need one transistor per bit. So obviously we can get a lot more DRAM per unit area, than we can SRAM. So SRAM is good because it retains data, but bad because it's more expensive. Here we have to refresh the data to keep it alive, but we can cram a lot of these bits on a chip. It turns out there is another difference, which is SRAM is typically faster and DRAM is typically slower. So we get speed, and simplicity, at more cost. Or, we can go slower and need to work at it, but have more bits. So let's look at what one memory bit looks like in SRAM and DRAM technology. The bit sits at the intersection of a wordline that passes by many bits, and the bitline that also passes by many bits. So memory is a matrix of cells like this. And DRAM will also have a wordline and a bitline. What the wordline does, is it controls a transistor here that can open or close itself to connect the cell to the bitline. So the idea is that, if we want to access this cell, we activate this wordline which opens this transistor, and now the memory cell is connected to the bitline. So if we want to write data, we put the bitline at the value we want, and the cell gets that value. If we want to read the data, we'll let go of the bitlines, and then the cell here will get a new value on the bitline, so we can sense what it is. In SRAM, the actual memory cell consists of two inverters. Each inverter has two transistors, that's how you build an inverter. And the idea is that, if this is 1 here, then this inverter flips it and outputs a 0 here. This other inverter flips that and outputs 1 here, so once we disconnect this transistor, this cell will keep its value. It's going to maintain a 0 here because that 0 is inverted and output is a 1 here, which amplifies the 0 here, so we have a feedback loop here that keeps the data that we want. So how to we write to this cell? So if we try to put a 1 here, this inverter here is working against us. Because it sees a one, it wants to output a zero. And the answer to that, is that this transistor here connects this cell to the bit line. If we put a stronger 1 here, because these are small inverters, we can defeat them, so that even though this inverter is trying to put a 0 here, we defeat it with a 1. And then what happens is, that now that this is a 1, this inverter outputs a 0 here, this inverter now starts outputting a one, and the cell becomes a 1. To make that easier, typically we have two of these transistors and two bit lines for the same cell that have the opposite values. So for this cell to become a 1, we put 1 here and 0 here. That puts a 1 here at the same time when we are putting a 0 here. So these are more easily defeated. When we want to read, we can connect both of them to the bit lines. And if we wrote a 1 here, we will have this one be 1, this one be 0. So this line really outputs the bit we want. And this one just outputs the opposite value of the bit we want. By looking at the difference between these bitlines, we can more quickly detect what the cell actually has. As we said, these are weak transistors here. So once we connect this cell to the bitlines, the bitline is long. And in order to, for example, raise the voltage on the bitline to a value that corresponds to 1, this transistor here are weak and cannot pull it to 1 very fast. They also will try to lower this one down to 0, and they may not be able to do that very fast. So at the other end here where we're trying to detect what's in this cell, instead of looking at when will this cross a threshold between a 0 and a 1, what we look at is in what direction is the difference of these two going. We make them, initially, about halfway between 0 and 1. We then activate the word line, the cell now starts, lets say, pulling this one up and lowering this one down. If we look at what's the difference very quickly, we'll detect that this one here, because it's drawn towards 1, is getting to be larger voltage than the voltage on this one. So we can now say that the cell had a 1 before it actually manages to pull this fully to 1 and lower this fully to 0. In DRAM, we also have a transistor that is activated by the wordline and that connects the cell to the bit line. But we said that's the only transistor we have. The cell is made out of a simple capacitor here. So, the idea is that to write to this cell we will activate the wordline, we put let's say A1 into the bitline. This transistor connects the capacitor to the bitline. Now what happens is R1 effectively charges this capacitor. Once we deactivate the wordline, the capacitor retains the value that corresponds to a 1. If we want to write a 0, we activate the wordline. The capacitor is now connected to the bitline that is connected to a zero voltage, and we empty this capacitor into the bitline, thus now our capacitor becomes a zero value. Now when we disconnect the wordline, the capacitor has nothing in it, so it retains the value of 0. So as you can see, the bit is really stored in this capacitor. When it's charged, we are storing a 1. When it's not charged, we are storing a 0. So what's the problem here? Well the problem is that this transistor is not a perfect switch. It's a little bit leaky. Here it's not a problem because if it's a little bit leaky, these two are strong enough to retain the voltage at let's say 1 and 0 or 0 and 1, while these are not trying to defeat them. But here our capacitor is slowly leaking into the bit line and eventually gets empty. So one problem with DRAM is that it will lose this bit over time, and that means that periodically we have to actually read out this bit, figure out what it was, and write it back in at full voltage so that it can again leak for a while before it gets lost. Another problem with DRAM that you don't have with SRAM is that once we open the wordline, the capacitor drains into the bit line. We can sense what the value was, but the capacitor is no longer fully charged. This is called destructive read. When you read a DRAM cell, you have to write it back in. It loses the value as it gets read. So this is what is usually called a 6T cell or 6 transistor cell. That are two transistors that are there for connecting to the cell and there are four transistors in the cell itself because each of the inverters has two transistors. This is, of course, a 1T cell. Now, the area occupied by an SRAM cell obviously is the area occupied by about six transistors. The area of the DRAM cell it looks like it's the area of one transistor and the area of the capacitor. The bigger this capacitor the longer it can retain something that corresponds to a 1 or a 0 before it gets lost, so we want this capacitor to be big. A capacitor can be viewed as two metal plates placed very close to each other. The bigger the area of the plates, the bigger the capacitance of the capacitor, it can keep more charge. So it looks like, to make a good DRAM cell, we will have to make a big cell because this capacitor will need a lot of metal area to build large plates, but this is not how we build this capacitor. The transistor and the capacitor itself are actually built as a single transistor in a technology that is called trench cell. So this end of this transistor is simply buried deeply into the silicon substrate when we manufacture transistors. So really the capacitor is kind of buried under the transistor. So we really get the area of the transistor itself and not much around it. You can see also that the DRAM cell can be smaller because it doesn't need this second bit line. We just need one, so that also makes it slightly smaller. We don't need as much wiring there. So let's see if we understood how DRAM is built. So what is the reason for not using a normal transistor and a real capacitor when making DRAM? Is it because trench cell is easier to make than a normal capacitor and transistor? Is it because the trench cell is more reliable? Or is it because the trench cell lets us make cheaper DRAM chips? Let's look at the answer to our DRAM technology quiz which was asking us, why not use a normal transistor and a capacitor when making DRAM? Is it because trench cell is easier to make? This is not true. The trench cell is not easier to make. It requires us to bury something into the silicon. So this one is actually incorrect. Trench cells are more difficult to make than capacitors, technologically. Trench cell is more reliable. This is also not true. Trench cell lets us make cheaper DRAM chips. It looks like trench cells are more difficult to make, so the chips will be more expensive. But remember that the cost of a chip grows quickly when the area is increased and decreases quickly when the area is decreased. The trench cell occupies a lot less area on the chip than the transistor and the capacitor normally would. So, if we want to build a chip that has the same number of bit with the trench cell, or this way, the trench cell results in much smaller and thus way, way less expensive chips. So this really is the correct answer. So we have seen what a single bit in memory looks like. Now let's see how the whole chip is organized. We said already that we have these word lines that activate cells. We have a number of word lines, and the thing that decides which word line gets activated is called a row decoder. What we give to the row decoder is some bits of the address that tallied which of the word lines to activate. It can only activate one word line at a time. So this is a real decoder. You give it a number. It activates a line that corresponds to that number. In memory, we call this row address. There is also a bit line here, and if you remember, a memory cell exists at every intersection between this bit line and a word line. So what a word line does is it connects this cell, this cell, this cell, or this cell to the bit line. So by supplying, let's say, two bits of the address, which was which of the four bits would be outputting on this one bit line. Of course, there are more bit lines than just one. And this, for example, is a 16 bit memory. It's a four-by-four bit memory. Four bits can output to the same bit line, and there are four bits activated by each of the word lines, so when we select the row, four bits get output out of here. Now bit lines are very long. And as we said, the cell is either a relatively weak SRAM cell, so it will slowly pull the bit line one way or the other, or it is a DRAM cell that discharges a relatively small capacitor into this relatively long bit line. If we discharge a small capacitor into a long bit line, the voltage on the bit line will change, but it will change relatively little. It will not change all the way to the level that corresponds to a one or to a zero. If we have a weak cell, we don't want to wait for that cell to raise the whole bit line one way or the other. This is why the bit lines are connected to a device called sense amplifier. What it does is it senses the small changes on the bit line and amplifies them. So it's really helping the cell raise or lower the voltage on the bit line. And it has relatively powerful circuitry for each bit line, so it's significantly bigger than a single row of cells, but you only need one of these at the end of the bit line. You don't need one of these at every cell. So you have relatively small and weak cells, and you have this beefy thing here that is helping them raise or lower the bit line. The signals that are produced by the sense amplifier, which are now correctly one or zero bits, go to a storage element called row buffer. The row buffer stores the correct values that we read from the whole row of cells. So in this case, the row buffer will contain four bits because that's how many bits are there in the row. The row buffer feeds the data it latched to another decoder that is called a column decoder. This decoder selects the correct bit among these four, let's say, using the column address, which is another part of the data address, and it outputs a single bit. If we want to build something that has more than just one bit of data for each location, we will replicate this. So we will have, let's say, two of these, give them the same row and column address, and now they output a two bit value. So we just described how to read a row into the sense amplifier and the row buffer, and then get 1 bit out of that row. So let's suppose that the bits we had here were 1 0 1 1, and let's say this was DRAM, so these bits drained into the bitline. The sense amplifier amplifies them, outputs here 1 0 1 1, 1. The row buffer now stores this value and starts outputting it here. Let's say that the column address finally selects this bit to the output. So we output a 0. Remember, that DRAM reads are destructive. Once we do that, we no longer have our bits here. So what needs to be done is, after the sense amplifier determines what the correct values of the bits are, and the cells by now have been exhausted, we reverse the direction, and raise each of the bit lines to its proper value. So now the sense amplifier is driving this correct values back into the cells, and the cells get their values again. So destructive reads from DRAM means that, really, we have to read and then write each cell. So we cannot just wait long enough to get the value, we also have to wait long enough to put the value back in the cell. This is one of the reasons why DRAM is slower than SRAM. Another reason, of course, is that the cell does not as strongly pull the bitline, so the sense amplifier needs more time to figure out what it has. After this read and then write approach, the cells have the correct values for 1011, and have been refreshed. Even if they have reduced to, let's say 90% of what they should have, once we read them out and put it back in, we put back in the full value. So now they can again leak for awhile before we need to read and then write them again. So refresh, is really, to make sure that each of the rows is red every once in a while. If the time for the cell to lose the value enough so that we cannot recover it is some sort of time, t, then every row needs to be read and then written within the time t. A destructive read, and then a write, will refresh the row. But we cannot rely on the processor to access every row in the memory so that we don't have to do anything else. In fact, when you have caches, it often happens that some rows that are very popular, and accessed by the processor all the time, are actually the ones that don't get refreshed this way. And the reason is that those become cache hits, and the cache keeps keeping that data because the processor wants it so often. So we end up actually not accessing the memory for things that the processor is accessing the most often. So we can not even rely on the program to go through the whole memory. So what needs to happen for refresh is that we have this refresh row counter. It starts at 0, we use it to refresh this row, then it moves to 1. We then use it to refresh this row, and so on. If a row needs to be refreshed within some refresh period, let's say t, and we have n rows, then every t over n seconds we have to refresh one row, so that by the time we are done, we are ready to refresh again, a row, which would only last for a period of t. Modern DRMs have a lot of rows, and the refresh period is well under a second. So we're actually seeing a lot of refreshes happen every second. This is not something that happens occasionally. This actually significantly interferes with when we can read and write our memory. because, while a refresh is going on, we cannot do a useful read, because the row decoder, sense amps, and the row buffer are used by the refresh operation, which is reading a row that we didn't need, and writing it back. Meanwhile, we have to sit and wait with our real read until that gets done so that we can move on. So, let's see if we understood the relationship between refreshes and reads. Suppose we have a memory array that has 4,096 rows and 2,048 columns and suppose that the refresh period for this array is 500 micro-second. Every cell needs to be refreshed within this time, otherwise it will lose data. Now suppose that the timing for reads for this memory is as follows. It takes us four nanoseconds to select a row. Once we activate the row, ten nanoseconds are needed for the sense amplifier to get the bit values from that row. Two nanoseconds are needed to put that data in the row buffer and four nanoseconds are needed for the column decoder to actually figure out which bit from the row buffer we need. While this is happening, we are writing the data back from the sense amplifier to the memory row, and it takes us 11 nanoseconds to do that. So this part overlaps with these two. So the question for you is how many data, or nonrefresh reads per second can this memory support? Put your answer here. Let's look at the solution to our memory refresh quiz. We have a memory with 4096 rows and this many columns. It needs to be refreshed every 500 microseconds. And this is our read timing, where these two steps and this one overlap. So basically, it takes us 4 plus 10 plus either 11 or 2 plus 4 nanoseconds before we complete one read and can proceed with the next one. Because 2 plus 4 is 6. And 11 nanoseconds is longer. That means that, really, the timing for our read will be to select a row. So we need to select a row, get the values from the row, and then write the data back. During this time, the row needs to remain selected. So the new read really cannot begin until 4 plus 10 plus 11 nanoseconds into it. So the read takes 25 nanoseconds. And that means we could be doing 40 million reads per second if all we had to do is do reads. And this is where it becomes tricky, because during a second we need to do a number of refreshes. The refresh period is 500 microseconds. So the number of refreshes we have to do it 2,000 times per second. We need to do a refresh of each of the rows. To do that, the row needs to be selected. We need to get the bit value and write it back, so it takes the same time to do a refresh of the row as it takes to do a read. Meanwhile, our memory cannot be doing the read. So, really, we're expanding 2,000 x 4,096 x 25 nanoseconds on refreshes. But because they have the same timing as reads, we can just compute how many refreshes we have per second. And we end up with 8.192 million refreshes per second. And that means that we cannot do all 40 million reads. Pretty much out of these really, this was assuming that we don't do refreshes. So because we have to do eight and change million refreshes, we have that many less reads that we can do. So the overall result here is the difference between the two. That's how many reads do we have left after we have spent time on eight and change million refreshes? So the final answer here is the difference between this which is 31,808,000 reads per second. So again, without refreshes we would be able to do 40 million. But because refreshes need to happen. We only have this many reads left in a second. How do we do writes to this memory? Obviously, we need to change one of the cells. How do we do that? We use the row address to select the row of the cell, but we can only select a whole row of cells. We cannot just select an individual cell, so if this is bit number 7 in our 32 bit world, we select the address of the word, and then we want to supply a bit here, Let's say A1, to be written here. So how do we do that? Because we can only select a whole row. Once we select a row, all these cells get connected to the bit lines. We can maybe derive the bit length for this one to write a 1. But that would lose us these bits. So the way the write needs to happen, is we use the row address, select the row, read out these bits, the sense amp amplifies them and we latch them into the row buffer. Now we write this bit to the row buffer at this position, and now we write the whole thing back into the cells. The cells lost their values when we did the read, anyway. We will have to write the values back, anyway. Before we do that, we make all the writes that we want to that row, and then send them back, so that when we are writing, we end up writing 1 1 1 and this bit is changed to 1. So a write in the run is also a read and then write operation. We have seen that DRAM accesses are slow, because really we have to use the row decoder to select the row, then get those bits onto the bit lines, then sense what's on the bit lines, and so on, and only then we get the values in the row buffer from which we can read them and send them back to the processor. But not all of the DRAM accesses have to be that slow. So we will discuss a technique called fast page mode, that makes some of the DRAM accesses a lot faster than others. Remember our organization of the DRAM with some part of the address to select a row. That row of bits gets output to the sense amplifier. We then latch those into the row buffer, and then from the row buffer, we use the column address to select a bit to output. And then, from the row buffer, the column decoder, using the column address, selects the actual bit to output. Note that once we have done all of this, the row buffer can retain the entire row, so if we want to read a bit from the same row, we don't need to go through the whole row address, sense amplifier, row buffer mechanism again. What we can do is simply keep what we have in the row buffer and just change the column address. Then the bit simply gets read out of row buffer without a new DRAM access. So this approach of finding what we need in the row buffer is call is fast-page mode. It's not called fast-row mode just because some people prefer to call a memory row a page. It usually has thousands of bits, so people call it a page. It has nothing to do with the pages that we use for virtual memory. It's just a name for something that has a lot of bits in this case. So fast-page mode works this way. We do what is called opening up a page. That amounts to providing a row address, selecting a row, sense amplification of what the cells output, and latching what we get into the row buffer. Now we can do a series of reads and writes to the page, which really happens to the row buffer. We can read or we can modify bits in the row buffer. Finally, when we want to do something on another row in memory, we do what is called closing the page. We now use the sense amplifiers to write back to the row what we have in our row buffer. It could be different data than what we read because we have been modifying it also in between. And this way if we want to do a single read we would have to open up page, do one read, then close up page to write back things to our memory cells because of the destructive read. And then finally we have to do what is called closing the page, which amounts to writing the data from the row buffer back into the row from which we read it. So if we want to do a single memory read, we would open a page. The data is now in the row buffer, and no longer in the row. Do the read from the row buffer and then close the page, which causes the data to be sent back and written back to the row. We can do a single write, which amounts to opening a page, the data now ends up in the row buffer. We do a single write, so we modify a bit in the row buffer, and then we close the page which causes the row to get the new data, and some of the bits may not have changed, which is why we had to read in the first place. But we can do a lot more than just one operation in between opening and closing a page. So usually, it's advantageous to open a page, do all the reads and writes that we can to that page before we close the page and open another page. So lets see if we can do some DRam access scheduling to take advantage of the fast page access that we can have. Lets assume we have a DRam memory that has 32, one bit arrays, just like the one we have seen before. Each array is one megabyte. And it's organized as two to the tenth rows, times two to the tenth bits per row. Each array is a 16 megabit array, organized as two to the 12th rows, which is 4,096 rows, times two to the 12th bits per row. And the 24 bit address, that we feed to this memory where bits are numbered zero to 23 is thus divided by the into a 12 bit row address which is the upper bits of the address and the address which is the lower to bits of the address. Of course the processor, when it issues this 24 bit physical address wants to access a 32 bit memory location. Each ship is fed this address, and the 32 chips together represent a 32 bit memory. So one chip, for example, might hold bit number 7, from all the words in the 16 megaword memory. Let's say we have Cache Misses currently pending for the following addresses. Let's assume that it takes ten nanoseconds to open a page, that it takes two nanoseconds to read something from the row buffer once the page is open, and five nanoseconds to close a page. These operations, how long do they take, if we do them in this particular order, and how long do they take if we are allowed to reorder them so that the time gets reduced? Let's look at the solution to our DRAM access scheduling quiz. So we have 32 arrays, each is 16 megabits and takes a 12 bit row address, and a 12 bit column address. Twelve bits is three hex digits, so the row address will be the upper three hex digits, and the column address will be the lower hex digits. Each of these corresponds to a 32-bit memory location. So what happens is each of these arrays sees the same address, but it provides a different bit for that address. So each address will have the same sequence of row and column addresses, it's just that they will implement different bits of that address. So what we have here is the first one, of course, needs to be a page open. So we have 10 nanoseconds to open this first page. We will then read in two nanoseconds, then we will close the page in another five nanoseconds, because the next one is on a different page. The row address is different. So we spent 17 nanoseconds here, we will spend another 17 here, we will then spend another 17 here because it's again in a different row. 17 here and here and here and here. Overall, we have 7 accesses, each to a different page, so each of them requires 17 nanoseconds, for a total of 119 nanoseconds. Now let's try to reorganize these. What we want to do when we reorganize these, is once we open a page, access everything on that page before we move on, which means we will open this row, and then we will access everything in that row, which means this and this too. Now, what happens is we spend 10 nanoseconds opening that row, 3 times 2 nanoseconds reading these three things, and then we need to close the row in another 5 nanoseconds. Then what happens is we open the next row, and we read everything from that row, which includes this too, so we end up opening one row, reading two things from it, and then closing it. And now we spend 17 nanoseconds for each one of these. In total, we spent 74 nanoseconds doing these accesses. So as you can see, when caches have a number of pending misses and we need to send these misses to memory, it pays to rearrange the order of the addresses we are accessing to benefit from the organization of the data. Now that we have seen what DRAM looks like, let's briefly look at how to connect the DRAM to the processor. So we have our processor. It sends its requests to level one cache, which sends its misses and write back requests to the larger level two cache, which might send its misses and write back requests to in even larger level three cache and let's say that these are all on our processor chip. Now what happens is the misses and the write back requests from the L3 cache would be made over and external connection, so we need processor pins here. And traditionally, this data would go over what is called a front-side bus. Now you want to design the processor chip, so that you can connect it to many possible memories. So you don't design the front-side bus, such that it supplies the row address and a column address and so on to the memory chips. What you have instead is another chip that contains the memory controller and that memory controller will have what's called a memory channel connecting it to a DRAM module. And over this channel, it will issue things like open a row, read something, get the data. Write something, supply the data. Close or open another one and so on and it will usually have more than one such memory channel. So the memory latency is seen by the level three cache is not only the access time of the memory are right here, it includes sending the request over a front-side bus. Having the memory controller figure it out, sending the page open to the appropriate DRAM. Sending a request to read, supplying the column addresses and everything. Getting the data over the memory channel, know that you have a level three cache miss. That means you want a whole cache line worth of data to read, so it takes a while to transfer the data here. Then the memory controller reads it from the memory channel, which usually has one bus frequency and sends it at a different data rate over to front-side bus to the processor chip, which then puts the line together and puts it into L3 cache. So the latency includes all of this here in addition to just the memory access and this can be a significant part of the overall memory latency. So recent processor chips integrate the memory controller, which means that the memory controller is put on the same chip as the processor and the caches. Now we no longer need the front-side bus, we can use lots of unshaped wiring to communicate with the on chip memory controller. So this can be plenty of bandwidth and very, very close. This whole thing is something like two by two centimeters and then we just send requests directly through the memory channels to DRAM. So now the processor chip directly knows how to talk to DRAM's and open pages and so on, which dramatically can reduce this part of the latency. And because it's not a negligible part of the overall latency, we get our data from dram a little bit faster. Actually, 10, 20 maybe 30% faster than before, but the cost of that is that now, we design a processor chip to talk to a specific kind of DRAM. And because of that, we need a relatively high degree of standardization of DRAM modules. So that, for example, when we go from 2 gigabytes to 4 gigabyte memory modules, we don't have to redesign our whole chip. So the protocols here, got a lot more standardized and uniform and flexible than before. But in exchange, we get to transfer data very quickly between the processor and the memory controller. We can make the memory controller now very smart and then access memory in a more efficient way. In this lesson we have learned the difference between DRAM and SRAM, how they work, and where we want to use each of them. Next we will look at disk drives which have even lower cost per bit but are also much slower. In this lesson we will discuss memory consistency, which determines how strictly should we enforce ordering among accesses to different memory locations. This is needed to get what we expect from our synchronization and data accesses in a shared-memory program. So, let's see what is memory consistency and how it differs from coherence. What coherence does is define the order of accesses observable by different threads, if these accesses go to the same memory location or to the same address. Coherence is needed in order to share data. Without coherence, a thread is allowed to modify memory location while another thread is allowed to read forever the old value of that location, in which case we can not really write shared memory programs that work. An important thing about coherence, however, is that it defines the order of accesses to the same address. It does not say anything about accesses that go to different location. And that is what memory consistency is about. It defines the order of accesses to different addresses. So, the big question now, before we get into consistencies, does this even matter? If we guarantee that when I write to something, all the other cores get to see it, and that happens for every address. As in, I write to A, somebody sees what I wrote. I write to B, somebody sees what I wrote. What can change about the order of accesses to different addresses and how does that even matter if I need to maintain this? It turns out, consistency does matter, and here's how. Let's say that in program order, two cores do this. The first core is going to store a 1 into location D, and then store a 1 into location F. Let's say that D and F are both 0 initially. The second core loads F into R1 and then loads D into R2. And let's say that now we print the registers R1 and R2. Note that we can have an out of order processor that reorders loads and stores. So in execution order, core 1 might still be doing the stores in the same order, but core 2 decides to reorder these two loads. Note that in a uniprocessor, reordering of loads, if this core is not writing to these locations, if perfectly okay, and in fact, even in a multi-processor, most of the time it's okay. Now, let's look at what's possible to get in R1 and R2 after this. In program order, these two loads on core 2 might execute before these two stores in core 1, so we can get R1 and R2 are 0. Here, these two loads in Core 2, although they've been reordered, can still happen before Core 1 gets to store. So we can get the same thing. These two loads can happen after the two stores. So we can get 1 in both registers here. In execution order, even if we re-order them, they can execute after the stores. So we can get 1 in both of them. In the program order, if these two happen like this, in between the two stores, we can get R1, when we load, it's still loading the 0 here, but R2, when we load, is already loading the 1. The same thing can happen here. If these two loads are in between the stores, then R1 is going to be 0 because it comes before this, and R2 is going to be 1 because it happens here. And now it gets interesting. We have seen that there are these three possibilities, so the question is, but can we get R1 to be 1 while R2 is 0? And in the program order, if we consider that, it's not possible. R1 being 1 means that when we read F we read 1. That means that this load has to come after this store. If that happens, then this load has to be after this load too. So pretty much, if we execute this here, then this has to be here, and that means that if we read one 1 from F, we will read 1 from D as well. So the R1 equals 1, R2 equals 0 cannot happen in program order. But here R1 equals 1 means that this load needs to be here. And we can get a 0 in R2 if this load comes here. So if we put the stores in between the loads in the execution order like this, we will get a situation that was not possible in the original program order. So the programmer might have expected that this code can never result in this, but in an out of order processor execution on each core, it can. And note that this doesn't violate coherence at all. In all cases, a load and the store obeyed coherence. But do we really care about that? Sure. For some obscure orderings this can happen, do we really care? When does it really matter that this doesn't happen and it does? So let's see an example of when does it really matter. Let's look at the multiprocessor that has coherent memory but out of order cores. And specifically the core can predict the branch and later verify that it was taken or not taken correctly. The store instructions in each core are done strictly in program order, so it doesn't reorder any stores. But the loads between store instructions can be reordered on a core. Now let's say that we have CORE 1 and CORE 2 where CORE 1 executes the following code while a flag is zero, it waits, and then it prints the data. While CORE 2 writes ten to data, then increments data by five then sets the flag and let's say that both flag and data are zero initially before CORE 2 gets to change them. So now CORE 1 when it prints the data, what are the values that it can possibly print? Can it print zero? Can it print five? Can it print ten? Can it print 15, and can it print anything else? Let's look at the solution to our consistency quiz where we have coherent memory but an out-of-order pair of cores, where branches are predicted, stores are still done strictly in program order, but loads can be reordered and these two variables start at zero. What the programmer is kind of expecting to happen is that I put put some values in some data, then I set a flag, and the other core waits for that flag to be set so that it can print the data that I produced. In program order, the only thing that can really happen, is that we print 15. In the execution order, that can also happen. If this core gets to this part before we even start checking, we might compute 15 in the data, set flag to one, core one, loops here, sees that the flag is one, exits, brings 15. So, clearly 15 is possible. But the question is can we get anything else? These are done strictly in order so if anything else does get printed, it's not going to be because this core did the data right after the flag was set. The flag actually gets set after the rights are complete but here's what can happen. This loop here amounts to reading the flag, testing if it's zero, and then branching based on that. This processor can easily predict this branch in a way that says we exit before it even gets to check what the flag was. So at that point we will be here and we will fetch the data to be printed. The data at that point might be zero. So what we do is we predict it goes out and we fetch the data at this point. So it's kind of step one predict, and leave loop. Step two, read the data, and it's zero at this point. Then let's say core 2 finishes this, and at that point out-of-order processor does this load, remember, these are load, they can be re-ordered, if we do this, we see that the flag at this point is one so prediction was correct. And at this point, we can print the data that we found. In this case, we will print zero. A similar outcome is possible for data = 10, simply by moving this here. So, we can also print ten if access number two, which reads the data after the prediction of the branch happens here. And finally we could get 15 and the data if we get it here, but we already knew we could get 15. Note that it's not possible to get five or anything else. And the reason is that these writes on the same core will happen in program order. It's incorrect for a core to somehow read zero here. because the only way we read data here, is if it was already written here. On the same core, we have to obey the program order. That was never in question. So the data when it's read here has to be 10. We can only increment it by 5 to 15, we cannot read 0 here. So that's why it's possible only to get 0, 10, or 15. So basically, the real problem is that somehow, although we thought we are waiting for the flag to be set before we access the data, in reality we managed to access the data at kind of any point before the flag was set. And coherence, again, didn't prevent this. What we need is consistency. So let's go back to our question of why we need consistency. We have already seen an example in our quiz, that something beyond coherence is needed for our data-ready flag synchronization to work. Another similar example is thread termination, where a thread creates another thread. Now that created thread, let's call it B, does a lot of work and updates some data. While thread A goes and waits until B exits. And it really happens by doing a system call in thread A and then the system really will wait for B to mark that it's done. And then when thread B really is done, the operating system will mark the B is done. And here we have a very similar situation inside the operating system that we have here. Which is the check for B exiting might be branch rated and then confirmed while in fact thread B is not done when we branch [UNKNOWN] like this. So what we need is some sort of additional set ordering restrictions that are going prevent the weird situations that we get into without any consideration for consistency. And again, this goes beyond coherence alone, we already saw that coherence doesn't really help us here. So for programmers the most natural type of consistency is called sequential consistency, and it says that the result of any execution should be as if accesses executed by each processor were executed in order and accesses among different processors were arbitrarily interleaved. So we are allowed to let processors take turns in terms of accessing, or let one access several times before another one does, and so on. So we can do anything we want to interleave the accesses from different processors. But what comes from each of the processors needs to be exactly In program order. So, the simplest implementation of sequential consistency is one where we remove the should be as if. So, pretty much, any execution proceeds by executing accesses from each processor in order. So, a core performs the next memory access only when all previous accesses are complete. So in the code it waits for a flag to be set and then prints the data. We cannot read the data until the read of the flag has completed. Remember that the problem in previous examples was that we branch predict this and leave and read the data then read the flag and confirm our prediction. Now, we can still predict, but when we get to reading the data we have to delay that until the flag has been read. At which point, if we read 1 here, by that time the writes to the flag and data complete and we will read the correct data here. Or if we read 0 here then we would correct the miss prediction and go back to waiting here. So either way, you cannot print wrong data anymore under sequential consistency here. So this works pretty well except that it's really, really bad for performance. So let's look at the drawbacks of the simple implementation of sequential consistency. Remember that that simple implementation basically issues the next access only when all prior accesses are complete. So if you have out of order cores, what is the memory level parallelism we get on each core by doing this? Put that number here. So let's look at the answer to our simple implementation quiz. If we issue the next access only when all prior accesses are complete, what is the memory level parallelism we get? And the answer is, the memory level parallelism is exactly 1. Remember that memory level parallelism is about having a load that misses and gets sent to memory. And then our processor continues finding independent instructions here in the hope of finding another load that is a miss and sending it to memory while the first miss is still going. What that does, is that if we look at all the misses in our program, we will do some execution, and then wait for memory to respond. And then do some execution, and then wait for memory to respond. Then sometimes a very little execution, and wait for memory to respond. So basically, our execution with memory level parallelism of 1 ends up being the execution without the misses plus the time to access memory, which is very large, times the number of misses we have. With memory level parallelism, we execute, we send the request to memory, now we'll wait for it. But meanwhile, we execute a little bit more and hopefully find that next miss. In which case, we can overlap misses, and hopefully we will find more than just two. So, in this case, we have that the cost of each miss in terms of memory access was not paid every time. We paid really the cost of maybe something like one and a half misses total, but we satisfied three misses here. So let's look at the better implementation of sequential consistency in which a core can reorder loads. But this can lead to violations of sequential consistency. So it has to also detect when sequential consistency may be violated, and fix things. This is, of course, easy to say but how do we really do that? So let's at the program where there is a load to A, and then sometime later, but not too much later, a load from B. So we need to make this execute in order, if doing other wise will violate sequential consistency. But we want to really execute them out of order, if there is no potential for sequential consistency violation. So let's look at what happens in the reorder buffer of a processor that tries to execute this. The reorder buffer keeps instructions in program order. So the first load will be known to come ahead of the second load. Now, if we actually do this access first and this second, then there is no potential for violating sequential consistency, because we are actually doing whatever the simple implementation will do anyway. So the sequential consistency violation can occur when we first do load B, in our real execution order. And later, for example, because the computation for address of A has been delayed, we do load of A. Now the important thing to note is that this is not necessarily a violation of sequential consistency. If nobody writes either B or A during this time, then the B here and the A here will be the same as if we re-ordered them back to their original program order. So we are getting the same value of B no matter where we load it. If nobody writes anything. In fact for this to actually be a violation of sequential consistency, like we can actually get a different value for B then we could in sequential consistency. What has to happen is that there is at least one store to B. Between the time we actually read B and the time when we should have read B according to sequential consistency. Why? Well because if we don't have the store here, then the value we read here is the same as the value that we would have read here, where it would be okay to read B. So now you can see how we can detect violations of sequential consistency. When we read something ahead of program order, we need to start monitoring the coherence traffic produced by other processors. So, if we se such a store, then this load and anything that it fed the data to, must be replayed. Note that this load, is repayable, because in the rub is here, we're assuming that this low still hasn't happened. Which means that the commit point for the ROB is somewhere here or maybe at the low here but definitely the load from B has not been committed so we are capable of replaying it. In the worst case we can just cancel this load and anything that follows and just refresh those instructions. So we know how to undue the damage here. But in order to detect the violation now we have to basically anything that we load out of order, we need to monitor coherence traffic until that load becomes in order. An alternative approach to trying to create better solutions for sequential consistency, which programmers naturally might expect, is to actually relax the consistency. That is, we tell the programmers that they cannot really expect sequential consistency. But we can still tell them that they can expect something. And this relaxed consistency models that are not really sequential consistency, but they're not just like anything goes, usually differ in terms of what types of orderings due they enforce. There are really four types of ordering. There is ordering between writes to different locations. There are orderings between writes to some locations and reads of other locations. There is ordering between reads and subsequent writes in program order, and between reads and other reads in program order. Sequential consistency says that basically program execution must obey all four types of ordering at all times. If we actually reorder something, we have to fix any situation where that would result in a different outcome than program ordered execution. So we're allowed to reorder things, but we have to always hide that from anybody being able to see it. In relaxed consistency models some of these types of ordering need not be obeyed at all times. Usually the first type of ordering that will not be obeyed is the read-read. So if we maintain the ordering between writes and between writes and reads, but just allow reads to proceed out of order, then we say that we have a less relaxed model than if we allow reordering of other types. So if we told the programmers that you have to write programs without assuming that reads are done in correct order, then the real question is, but how do we then write correct programs to begin with? So for example, how do we do flag synchronization if we cannot expect that after waiting on a flag, we will be able to read the new value of the data? So, really, when we do relaxed consistency, we allow a reordering of normal accesses according to the four types of ordering. So, for example, our reads can now be reordered, but only if these are normal accesses. But then we add special non-reorderable accesses. And the programmer must use these non-reorderable accesses when ordering in the program matters. An example of such non-reorderable accesses is the x86 MSYNC instruction. Here in normal execution all accesses are reorderable. We cannot rely on any specific in order behavior as far as consistency concerned, but no reordering can happen across a MSYNC instruction. so we can have any axis that we want reordered anywhere in here. But in problem order then comes the MSYNC instruction. The process guarantees that all of these are going to be done before we do MSYNC. And that the MSYNC will be done before we begin the next cloud of instructions that can again be ordered with each other. So how does that help? It helps because now we can write flag synchronization correctly. The correct wait for a flag now includes an MSYNC instruction. It basically says that, Wait for the flag to become zero, make that happen and then use the data. Now the processor will first make sure that the flag is red, and only then that the data is red. Because we cannot reorder the rates to the flag and data cross them Msync. But it is still allows all sorts of performance optimizations where we reorder instructions. In situations where we are not trying to use memory for synchronization like this. So let's see if we can correctly use MSYNC. Let's say that we have a processor with a very relaxed consistency model where all four reorderings are allowed. Including reodering writes to different locations on a core. Let's say that this is the code we are executing. We do a load link of lock into R1. We then do a branch if we didn't get the lock, back to checking, we do a store conditional trying to now get the lock. We look back, if we didn't get the lock then we load the variable incremented and store it. Then we free the lock by writing zero to it. So the question for you is where do we need MSYNC if anywhere here? Do we need one here? Do we need on here? Do we need on here? Here? Here? How bout here? How bout here? How bout here? And how bout at the end of everything? So let's look at the solution of our MSYNC quiz. This processor allows all possible reorderings. However, it only allows reorderings of accesses to different locations because cache coherence and correct uniprocessor behavior ensures that accesses to the same variable are still done in correct order. So we don't have to worry about the LL and SC and this store being reordered with respect to each other. They're still going to behave exactly as if they were in program order. However, the variable and the lock are different memory locations so they can be arbitrarily reordered here. So what can happen is that this load of a variable could be moved all the way up, before we even try to acquire the lock. So now we are loading potentially while somebody is still in their critical section, so what we need is an MSYNC here to ensure that we first get the lock and only then actually try to actually do our work in the critical section so that our work doesn't escape the critical section. Now, another problem is that stores in this processor can be reordered. So we can easily release the lock and then update the variable. Which again we need to prevent. So we need to prevent that type of reordering. We don't have to put an MSYNC here because the load of the variable and the store will happen in order. because they're to the same memory location. And we don't have to put anything here or anywhere in between because all of these accesses to the lock are going to be in program order anyway. In general, a lock function is considered to be an acquire type of synchronization. And then we put an MSYNC after it. An unlock is considered to be a release type of synchronization. And we put the MSYNC before it. So the idea is that you first acquire access to something, then you MSYNC, then you actually access it to make sure that you access it after you acquire it. And then when you want to release, you first do what you want, then MSYNC, then release to make sure that the release actually happens after the work is done. With flag synchronization we have seen that waiting for a flag is an acquire and that releasing the flag is really a release. And then a barrier is a weird case where we have both an acquire and a release in the same operation. So we need an MSYNC both before and after the barrier to ensure that the work gets done before we enter the barrier and that nobody leaves until we are done with the barrier. There is a concept of data race that plays an important role in considering how we can relax consistency. A data race occurs when there is a data dependence between accesses that are on different cores, and the accesses are not ordered by synchronization. So, for a data race to occur, we need to really have one process rating and another writing to the same variable. Or the other way around, or both of them writing. And, the access on this different course are not ordered by synchronization. So, here's an example of a data race. A data race here exists because really we can have this happen or in another execution this store might occur before the load and thus, this can happen too. So this is a data race. In contrast, something like this, where we do load A, then a barrier, and this thread participates in that barrier, and then thus store A, there is a data dependence between accesses but, they are clearly order by synchronization. This one will happen before this one because of the barrier synchronization, so there is no data race here. Conceptually, our data race allows any order of these operations and that might change the outcome of the program, whereas here we know what's going to happen. Then there is a concept of a data race free program which is a program that cannot create any data races in its execution. Pretty much that program is considered to be well synchronized so that any accesses to data are correctly ordered by synchronization, barriers, locks, flags, etcetera. A key property of data-race-free programs is that they behave exactly the same as they would in sequential consistency, even if we run them in any relaxed consistency model. Why is that so? Well, because if your synchronization is implemented correctly, then your synchronization is going to create the right orderings for things, and then, there is no opportunity for the re-orderings that might violate sequential consistencies. So data-race-free program is actually sequentially consistent, even if we run it on a machine that doesn't support sequential consistency as long as synchronization is correctly implemented for that machine. But keep in mind that while your debugging your programs, even if you intend them to be data race free, they may not actually be data race free. In which case, theoretically, anything can happen. So pretty much, while we are debugging a program. To remove data races, the behavior of that program on a weekly consistent system might be very weird and thus is makes the bugging really, really difficult. So for most programs that are correct, and that means data-race-free, we can relax the consistency model. But for the debugging it really helps if we can support sequential consistency. So some processors actually have support to flip between sequential consistency, or some more relaxed consistency that gives us better performance. So that when you're debugging, you can use sequential consistency until you're reasonably sure that your program is working correctly and is data-race-free. At which point, we can run it on a weaker consistency model to benefit from the additional re-ordering that the processor is now allowed to do. So finally, let's take a look at what kinds of consistency are there. We know that there is sequential consistency, and then, there is a family of relaxed consistency models which are allowed to violate one or more of those ordering rules. And the models that have actually been proposed and sometimes used, are called weak consistency, processor consistency, release consistency, lazy release consistency, scope consistency, etcetera. There are actually many, many relax consistency models, too many to actually explain here. But the key to all of them, is that although they allow arbitrary reordering's or maybe some reordering's among data operations. All of them support synchronization operations that allow you to ensure that the ordering that you really want in your program, gets to happen. Some of them do it by doing something like the N-sync corporation, some of them do it by using more refined memory synchronization operations. Some of them do it by actually calling library functions that are ensuring such ordering, and so on. In this lesson we learned about memory consistency which defines what is allowed to happen when processor cores are reordering memory accesses. We have seen that sequential consistency gives us the behavior we expect but that it can limit performance. And we have seen that more relaxed memory consistency models can improve performance but make program behavior more difficult to reason about. We have seen that an out of order processor will track when one instruction uses a register produced by another. But, load and store instructions can access the same memory location without using the same register for it. So let's see how to do them correctly in an out of order processor. So we have seen that using the ROB and something like the Tomasulo algorithm, we can enforce the order of dependencies on registers between instructions. The question still remains what about memory access ordering? We have a lot of store instructions. Do they really need to be done strictly in program order, or can we do something about reordering them too? So far, we have eliminated control dependencies, using branch prediction, and eliminated false dependencies on registers, that is dependencies that go through registers, and we have eliminated the false dependencies through register renaming. Next, we learn how to obey read after write register dependencies or through dependencies. And we do that through Tamasulo-like scheduling. So we have instructions wait in reservation stations until their dependencies have been satisfied and they can proceed to execution out of program order. Note that the data dependence handling that we have done so for, however, is only for register dependencies. That is when one instruction is producing a value in a register that another is going to be using. But what about memory dependencies? If we have a store and then a load, there could also be a dependence between the memory value written by the store and then the value read by the load. If these two addresses are the same address then actually the load needs to get the value from the store and in which case we should be doing them in order. However, if these two addresses are not the same, just like with registers, we actually don't need to maintain the program order between this accesses. However, we have not seen anything that handle out of order execution of memory instruction. The first point we need to consider is for store instructions, when does the memory write actually happen? And it happens at commit. We have to do writes to memory at commit because it is unsafe to update memory at any point before the instruction commits. Any instruction that has not yet committed is subject to being canceled. For example, because of a branch misprediction or exception. So doing a memory write prematurely might mean that later we have to undo access to memory, meaning we have to kind of put back the old value of a memory location which is extremely difficult to do. So what we will do is we will delay memory writes and restores untill commit. But does it mean that the load has to also wait until commit before it can get data, because only then the data is in memory? So the question is, if we write values on stores at commit, where does the load get data? And we want our loads to get data as early as possible, so that we can finish those loads and supply that data to the subsequent instructions that depend on the load. For this purpose, we introduce a structure called load-store queue. And that is where we will keep all loads and stores, and we will next see how loads get data from the stores. So because the Loads in our processor need to be done as soon as possible while the Stores do not write to memory until commit, we said we need the Load-Store queue so that we can supply the values from Stores to Loads. This Load-Store queue, or LSQ, is a structure just like the ROB, meaning we put things there in order and remove them at commit. But we only put Load and Store instructions there, so there is a bit that says which instruction do we have, Load or Store? There is a field for what is the address this instruction is accessing. And there is a field for the value that the instructions should be storing or loading. There is also a field for, this instruction has been completed. Now let's suppose we have this program, and suppose that the Load-Store queue is initially empty. So as we said, things are going to be placed here in program order. So the first instruction here is going to be a Load. Let's say that we quickly compute the address, and it's 104. This load, because there are no previous stores, will go to memory. The next instruction here is a Store. Let's say that it computes the address of 204. Let's say that the value is 15, and we mark it down. And let's say that this instruction is kind of delayed so it doesn't return yet, so we still have all of these in the Load-Store queue. The next Load, let's say it eventually computes the address of 204. For every Load, when it computes the address, we check in the Load-Store queue if any store matches that address. If there is no matching Store, like equals case for this Load because there were no Stores, we go to memory. If there is a matching Store, we do not go to memory. Instead we do what is called the Store-to-Load forwarding, where we simply take the value from the Store and that is the value that the Load produces, and we never go to memory. So again, this is called Store-to-Load forwarding. Now of course, this assumes that at the time we want to do this Load, the Stores have already been completed, and we now know what the addresses are and if we need to get the value, we know what the value is. It is entirely possible that, for example, for this store and this load, the store doesn't have the address at the time when the load produces the address. And now, what do we do? The options for this situation are. Make loads and stores execute in order. So this load is not allowed to execute until all previous instructions have been completed. That guarantees that at the time when we want to figure out what to do with this load, we have all the store addresses and values so we can have a situation just like this one here. But of course, the orders, these instructions, so for example, loads cannot really proceed. So for example, if this load has been a cache mission takes a long time, all of these instructions will have to wait for that. The waiting can be reduced by not waiting for all of these instructions to complete, instead we just wait for all previous stored addresses to be known. Once we now all the stored addresses, our load address can be compared with them and then we know what to do. We either go to memory if nothing matches. We get the value from the store if it is available or we wait for the store to produce the value if the address matches but the store is not done yet with its value. The third option is the most aggressive, and it is to just let the load go anyway. So pretty much when we produce the address of the load, we check the addresses of the previous stores. If one of them does match, we wait for it, because clearly going to memory will be wrong. But if none of them match, we ignore the fact that some of them might match once we resolve their addresses. So in this situation, this load will end up going to memory. Now two things can happen. One is, this store resolves to something like 74, in which case we confirm that this was done, okay. Or the store might actually get the address of 174 in which case this load loaded the wrong value from memory. In that case we need to recover this load loaded the wrong value and possibly supplied it to other instructions. So in this case, the stores, when they produce the address will check whether any of the loads that follow them match the address and were done. And if so, then request a re-doing of the load and all of the subsequent instructions to fix the problem of the load loading the wrong value. Most modern processors today actually go with the go anyway option because it produces the best performance. If we let the loads go without waiting for store addresses, it turns out that most of the time we are right. So this most of the time successfully loads the value earlier than if we waited. But occasionally we have a cost of the recovery because we were wrong about this. And then there are entire schemes that try to predict when is this going to happen, let the loads go if it's unlikely and for loads that often conflict with some store, wait for that store to produce the address before we proceed. So now, lets see what happens when we do out of order load store execution. That is, we try the aggressive approach of loads going to memory as soon as they know the address, if there is no preceding store that is also the same address. And it later turns out that a store has been resolved to the same address. We have something like this: This is going to be our sequence of instructions in the load store queue. Let's say that we are executing these five instructions and this is their program order. First we have a load that loads from the address 0 plus R6, and puts the value into R3. Next we have an add that takes R3, adds R9 to it and puts the result to us into R7. Next we have a store that takes the value from R4 and puts it in a memory location determined by R7. Next we have a subtraction R1 minus R2 goes into R1. And finally, we use R1 as an address to access memory and put the value into R8. In an out-of-order processor, when we try to execute these instructions, this is what might happen. Let's say, that all of these have already been fetched, decoded, etc. So, let's say that the Load, because it only depends on R6 switch. No other instruction is supposed to produce, it can dispatch. So it goes to memory and, eventually, it will come back with R3. But let's say it's a cache miss, so it's going to take a while until this Load comes back. This instruction cannot be dispatched until that happens. And this instruction cannot be dispatched until that happens. In fact, the store doesn't even know the address to which it's going to be storing, until the add completes, which is going to happen at some future point. Meanwhile the subtract can dispatch and it produces R1 very quickly. At that point the load can dispatch. It has everything it needs to know. It has the address, now it can go to memory and load from it. Let's say that the load is cache hit, so it gets the R8 very quickly and maybe supplies it to subsequent instructions. So at this point in time. What we have is that the subtract and load have completed, we think we're just waiting to commit them. The load and the add, and the store are not done yet; because they're all waiting for the load to finish and then they can do something. Eventually let's say that this load is done, now we produce r 3 feed it to this add. The add is going to be done very soon afterwards and now we feed the R7 to the store. And now let's say that this was some address X that we have accessed here. If this results do not X, meaning the address addressed by R7 is not the same address as the one addressed by R1. Then everything is going to be fine. The store is storing to some other address, we have loaded something else entirely and everything is fine. However, the problem occurs when the address that the store resolves to si also x. At that point, the value from R4 that the store is storing is really the one that the load should have been loading, however the load already loaded some stale value from memory. Our first solution to this problem is that we don't do out of order load/store execution, instead we do in order load/store execution. This is what happens in that case. The load is a miss, the add is stalled, the store is stalled because of that, because they're both, both waiting for the load to eventually finish. The subtraction gets done. We now know the address for this load. At that point in the load/store queue, we will check whether there are any preceding instructions that might eventually resolve to the same address. Or even worse, we check whether there is just all preceding instructions are done. Because this load is not done, we are not going to consider this store for execution, that is for producing the value for this address, and as a result, we are also not going to do this load. So pretty much what now happens is this load here doesn't go to memory although it knows where the address is. Eventually this mess is resolved. We compute R7 and then very quickly afterwards we are going to compute the address for the store. So now what we have is, this load gets done. This instruction was already done. Eventually, the add gets done, and eventually the store gets done. The store is considered done, not when it commits, but when it actually knows the address, and knows the value that is going to go to that address. At that point only, this load is allowed to proceed because at that point, it can just go up and check whether any stores produced the values for the address that we are interested in, and if so, just grab the value. So the load can only finish after the store has finished. And the store will not do anything until this load is done, et cetera. So really, we are executing most of the instructions out of order here. But loads and stores are still proceeding in order. Of course you can see that this is suboptimal if this store doesn't store to the same address. We have been waiting here for a very time and thus we have delayed all of the instructions that depend on the load. Worse, if the load would have been a cache miss. We were sitting there not trying to order things from memory, until many many cycles later potentially, and only then we realize we have a cache miss and have to wait many many cycles more. So this is of course not a very high performance solution, but it is a correct solution. So, now that we have seen an example of memory ordering, let's do a memory ordering quiz. Let's say we have a load, a store, another pair of load and store. Let's say that all of the loads are cache misses, so what happens is in cycle one, we begin executing this load. And, send it to memory. And, it's going to take 40 cycles for it to return a value. So, in cycle 1, we send the request to memory. In cycle 41, we finally get the value, and now we have our R1. Let's say, that we are doing stores and loads out of order. So, although this store really cannot get the value until cycle 41, at least. We proceed with this load. So, in cycle 2, we send it to memory, in cycle 42, we get the value. This store waits for that value but this load is sent to memory in cycle 3 and, in cycle 43, it gets the value. [BLANK_AUDIO]. Now what happens in cycle 42, is the store is finally done and then in cycle 43, this store is done and in cycle 44, this cycle is done. So, overall, we have taken 44 cycles to do this. But we have been doing loads prematurely as far as the stores are concerned because if this R2 plus 4 is equal to R3 plus 0, then actually, this load should have waited. So we decided not to be totally safe and go and then we, we are assuming that there will be some recovery mechanism. Your task is to figure out when is each of these instructions sent to memory and when do we get the result from memory and what's the final cycle count if we do things in order? So, if loads and stores have to happen in order Okay let's see out memory ordering quiz solution, this load is still sent to memory in cycle 1, and will come back in cycle 41. The store, however, cannot proceed fully because we don't know what the value is until cycle 42 ,So in cycle 42, this store finally figures out what the address and the value are, so now this load can be considered done. Because we need to do things in order, that means that only cycle 43, this load can start going to memory, It's going to take 40 cycles to come back, so in cycle 83 the value comes back from memory; at that point. This store can finish at cycle 84, inside cycle 85 that means that only then this load can go to memory, It's going to be cycle 125 by the time, it come back and finally the final store can be done in cycle 126. As you can see, it took almost three times as long in this case to do this as it did when we were trying to do things out of order. So there is a huge advantage in trying to reorder load store instructions, but it caries a risk of having to recover from basically loading the wrong value from memory because we should have loaded the value from the store. Before we discuss how to recovery from loads done too early is happening, let's first talk about the store to load forwarding problem. For a load, we have to ask ourselves, which earlier store do we get our value from? Basically, there could be multiple stores to the same address that our load is going to. And we have to figure our which store is the one to supply our value and also, if none of them are matching, then we need to figure that out, because then we need to go to memory. When a store is finally resolved, we need to ask ourselves, which later load do I give my value to? So there might be a load whose addresses already known, once this store figures out the address and the value, we should be waking up the load that wants that value. Note that there could be multiple loads that the store needs to give the value to. So where do we figure this out? In the load store queue. Okay, to illustrate the operation of the LSQ we will do an example. So here I have a loadster Q, where this is the oldest instruction and this is the youngest instruction in the Q. Note that the Q is storing instructions in the correct program order, so basically in program order this was the first load, this was the second instruction and so on. This says whether it's a load or a store. This tells us what address the load or store was fetched from. This is the sequence number. Basically, just, going up. And, this is the address to which the load or a store resolves to, and this is the value that we will get for that load of store. And let's say that this is the content of the data cache. Let's say that the initial content of the data cache is 42 here, 1 here, 38 here, and 1, 2, 3, 4 here. This load goes first, axises address 3290, which is this one, and loads the value of 42. Let's say now that this store here, computes its result,. It basically gets the value it's going to store. Let's say that it gets 25. Note that this store doesn't really put that value yet in the cash. That will happen when it commits. So this is only that we have gotten the value from the instruction that produces the register that we will use as our source value to storing memory. Let's say that this next load also gets a value of -16 this time. Now let's say that this load executes next, accesses the address 3418, and it tries to see whether any of the previous stores matches that address. They don't, so it loads the value from memory. This is the address in the cache that matches this, so we're going to get 1, 2, 3, 4. Next, this load tries to load a value, it has the address of 3290, it's going to search upward for the latest store that still precedes it that matches the address. And it finds one. This store is having the address of 3290. So instead of going to the cache, we are directly going to copy just the value of minus 17 here. So now we actually had a load that never really accesses the cache, it just gets the value from the store. The next load accesses address 3300. It searches the previous stores and doesn't find any that match this address. So it's going to go to the cache and load from address 3300. And it gets 1. Next, we have another store to 3290. Let's say that the value stored there is now 0. Next we have a load from 3410, it searches the previous accesses for a store that matches this address and it finds one over here, so it just copies the value of 25. So again, we have a load that never accesses the cache because it founded what it needed in the Load-Store Queue. Next we have a load to 3290. It searches the load store queue to find a store that matches this address and it finds one over here. So it just copies the value of 0 here. Noted by now, there have been several stores to 3290. We always want to get the value from the most recent one that still proceeds us. Because if things were happening in order one at a time, this is exactly what the value should be in the memory location. Note, however, that the memory location, as far as the cache and the memory are concerned, still contains the value of 42 at this time, although there have been several modifications. Why? Well, because the stores haven't sent the values to the data cache yet. And finally, we have another load to, 3300. It searches backward to find any stores to the same address. Note that this is a load. We don't do that. So there're no stores to this same address so we actually load the value from the cache, and the value is 1. Now let's say that at some point, the loads and stores start committing. So we will have this load commit. A load commits by simply copying the value to the register file. And now, the oldest pointer is here. This store commits by putting the value in the cache. So, now we take the value of 25 and put it in address 3410, so we overwrite this 38 with 25, and now the store can commit. .And our pointer moves here, we commit another store here, so the value of minus 17 goes to location 3290, so we override this 42 with 17. Now our store is committed, we move the pointer here. If you remember, we said that we need to send the values to the memory or the data cache at the time the store commits not at the time it executes. Here's why, if at this point we decide that we are taking an exception, we can just flush this from the lowest queue, and the value in the data cache in memory is exactly the value that should be there as of this point in the program execution. So just like we had our architecture register file, and we were copying values and commit into it for register; so that at any given time we know exactly what the committed register value are. Similarly, at any given time, we have in our data cache in memory exactly what is there as of the commit point. This process will now go on. The load commits and deposits its value in it's register. Another load can commit and another one. When the store commits, it overrides this 17 with a 0 and then the loads commit. And the final content of the cache is this. Let's discuss the relationship between LSQ entries, ROB entries, and reservation stations. When we issue a load or a store instruction, we need a ROB entry. Every instruction needs a ROB entry. So, when we issue a non load/store instruction, we also get a ROB entry, and we get for a non load/store instruction a reservation station of the right type. For loads and stores that we get a LSQ entry. These act as a reservation stations for loads and stores. So note that we can not issue a load or a store unless we have both a ROB entry available and an LSQ entry available. Just like we couldn't issue a non-load/store instruction if we didn't have both a ROB entry available and a reservation station of the right type available. When we execute a load/store instruction, there are actually two parts to that execution. First, we need to compute the address. Second, we need to produce the value for a load. That means we first compute the address, then we get the value from memory. For a store, actually this can happen in any order. We are computing the address, while we are also trying to obtain the value of the register that we need to store to memory. We write the result only for loads. For a store, there is no write of the result. The store simply keeps the value and the address in the load/store queue for other loads to get it, and for the store to finally send it to memory on commit. But the load, as soon as it gets that result, will broadcast that result to dependent instructions. So that pretty much all the reservation stations that are waiting for that register value can now proceed. So note that our load/store unit, although it doesn't have reservation stations, it has the load/store queue. So when it finally figures out the value for the load, it needs to broadcast it to subsequent instructions so that it can be woken up and use that value. They will get that value from the load/store queue. To commit a load, we move the ROB head, basically, we free the ROB entry. And we also move the LSQ head; that is, we free the LSQ entry. And we need to do the same thing for stores. For stores, in addition to this, we have to send the write to memory. Pretty much, we've been holding onto the address and the value until we commit the stores. So at the time we, when we commit it, we need to finally send that value to memory. So, let's do a memory ordering quiz. If you have this instructions as consecutive instructions in the program, where do you think the load gets its value from? So, does the load access cache or memory? Yes or no, write it here. [BLANK_AUDIO] And the answer is no, the load doesn't act as the cache or the memory. We know that this is the same address, so the load will get it's value from the store. So the correct answer here is no. So, in the first part of the quiz we have seen that the answer is wrong. The load doesn't get the value from cache or memory. It gets it from the store. So the second part of the quiz is, the load gets its value from. A result broadcast. Put a check mark here if this is correct. A reservation station. Put a check mark here if that is correct. A ROB entry. Put a check mark here if that is correct. And an LSQ entry. Put a check mark if this is correct. Note that in different situations it might be possible that multiple answers are correct, so put check marks for everywhere where we might get the value from. So let's discuss the memory ordering quiz solution for this. In this situation there is a store and then a load to the same address, so the load obviously doesn't go to cache or memory to get its value. It's going to get it from the store. But where does the store keep its value? The store doesn't broadcast the result. The result is broadcast for instructions that produce register result. So the store really never broadcasts its results. So this is definitely not true. A reservation station, reservation stations never provide any results to subsequent instructions. They only keep values captured for the current instruction. The store doesn't even have a reservation station. So this is definitely not correct. A ROB entry would keep a result for a register producing instruction between the time it's been broadcast and the time it commits. But because a store is not a register value producing instruction, it doesn't put its result in a ROB entry, in fact, technically, the store doesn't have a result. So this is not correct. The store does keep the values going to store in the low store Q, and if you remember, that is where the load searches for a value when it's trying to match its address to addresses of previous stores. So this is the only correct answer. In this lesson we'll learn that we need a load store queue to track dependencies through memory. So we can assure correct execution of memory instructions in an out of order processor. Modern processors have really fancy load store queues which helps them do a lot of reordering even for memory instructions. Before we learn how to make better computers we need to learn what better means for computers and how do we measure that. In this lesson you will learn about latency and throughput, which we use as a measure of computer performance. Then we will look at how to measure performance of real computers using benchmarks and a few general ways to get good performance. So, first, let's talk about performance. Usually, when we say performance within processor speed. But even then, there are really two aspects of performance that are not necessarily identical. One is latency. It's about, how long does it take from when we start something until it's done. The other is throughput, which is, how many things can we do per second. And one would think through put is 1 over latency. So for example, if it takes a half a second to do something, then the number of those things we can do per second is going to be 1 over one half and we get 2 for throughput but this is not always so. Think about a car assembly line. We start with just a chassis. We then move the car to where its engine is going to be installed. We then move the car to where some wheels are going to be added to it. We then have some doors installed and some hood put together and so on. And now we have a car. So the latency is from here to here. Here's where we began the car, here's where we have a final car. But we have an assembly line so when the chassis moves here for the engine to be installed. The next chassis gets put together. When we have installed the engine, we move the car to the install wheels station. Meanwhile the next car is getting its engine installed and so on. So let's say that the latency is 4 hours, but that it takes something like 20 steps for a car. If each of these steps takes the same amount of time and the cars are just moving through these steps, then our throughput is actually 5 cars per hour, not 0.25 cars per hour. So as you can see the throughput and the latency may not be like this. Let's take a minute to check if we understood the relationship between latency and throughput. Suppose we are running a website for ordering penguin-shaped USB drives. Our website has two servers and it works as follows. An order request will be assigned to one of the servers very quickly. A server takes one millisecond to process a single order and a server cannot do anything else while processing an order. So the question for you is, what does this website have for throughput in orders per second, so how many orders per second, and what's the latency for orders in this website in milliseconds? So let's look at the solution for our latency and throughput quiz. We have a website that has 2 servers. A server takes a millisecond to process an order, so the latency of an order is clearly 1 millisecond. That's how long it takes from the time the order arrives until the time it is actually served. The throughput of a single server, because it cannot do anything else while processing the order, is going to be one order per millisecond or a 1000 orders per second. But we have two such servers, so we can process 2000 orders per second. So this illustrates again the situation where the throughput is not just equal to 1 over the latency. If it were like that, we would have only 1000 orders per second. But we can buy more throughput, simply through replication of service in this case and thus achieve a higher throughput than the latency of a single server would indicate. So we can measure performance as either latency or throughput. Let's see now, how do we compare performance of two different systems. So we want to be able to say something like system X is N times faster than system Y. And the way we usually refer to this statement is that the speedup of X over Y is equal to N. So we compute N as the speed of X divided by speed of Y, and now it starts to matter whether we have latency as our measure of speed or throughput. For throughput N can be computed simply as the throughput of X divided by throughput of Y, but for latency, speed of X is not proportional to the latency effects. The longer the latency, the lower the speed. So what we do instead, is we divide the latency of Y by the latency of X to get the speedup of X over Y. So let's do a Performance Comparison Quiz. Let's say we have, a laptop, that takes 4 hours to compress this video. Let's say that we are buying a new laptop that can do this, in 10 minutes instead of 4 hours. So the speedup, of having the new laptop instead of the old one is? Put your answer here. Let's see the solution for the performance comparison quiz. We had a laptop that takes four hours to compress this video. This is a latency, so the speed up of new versus old will be the old latency divided by the new latency. Four hours is 240 minutes. The new latency is 10 minutes, so the speedup is 24. So the correct answer here is 24. Intuitively, a speed of larger than one means that the new thing is actually faster than the old thing. Speedups less than one mean that the new thing is actually slower. So if you divide the wrong things here, if you took, for example, 10 over 240, you would have gotten something like 0.04. And at that point, you can check the new thing is actually significantly faster. So if you're getting a number that is much lower than one, that probably means you're dividing the numbers wrongly. Let's do another performance comparison quiz. Let's say now we have the new laptop that can compress this video in ten minutes. But we drop this laptop, and it falls down the storm drain. So we have to use the old laptop that took four hours to do this. So what is the speedup of using the old laptop instead of the new one that fell down the drain? Let's discuss the solution for the second performance comparison quiz. Now the laptop that we had took 10 minutes, and the new one takes 4 hours because we are actually using the old laptop to do this. The speedup is equal to the latency of the previous thing, which is 10 minutes, divided by the latency of the new thing we're using which is 240, so this time the speedup is 0.04. So the correct answer here is 0.04. Again, if there is an actual slowdown from the previous thing to the new thing we are using, then we can expect the speedup to be lower than 1. And if got instead of 24 as our answer, that should mean that whatever we are using now is faster than before. So that should tell us that something is wrong. So pretty much, by kind of checking if it makes sense, you know, is the new thing actually having a speedup as opposed to slow down? If it is a speedup it should be above 1, a speedup lower than 1 is an actual slow down. So let's discuss the speedup numbers we can get. We have already seen in the previous quizzes that a speedup larger than one means improved performance. Either through shorter execution time it takes less to compute something than before, or higher throughput, we can do more of something per second than before. A speedup of less than one means that we have worse performance than before. Either it takes longer to execute something, or we get lower throughput that before. And finally, when computing speed up, remember that performance is proportional to throughput. So when computing speed up, we divide the throughput of the new thing with the throughput of the old thing. However, performance is proportional to 1 over latency. So, if we have something like, it takes two seconds to do something, that's a latency. Performance in this case is 1 over that. So, when computing speedups, we actually divide the old latency with the new latency. Now let's talk about measuring performance. We have seen that we can express performance as 1 over execution time. But the execution time is on what workload? So if we want to measure performance on two machines and compare them, the question is, what do we run on them to tell which on is faster than the other, and by how much. The best assessment of how do they compare, is if we run actual workloads from a particular user. But, this requires us to run many different programs, what we get is not representative of other users who might be using this machine completely differently, and there is the question of how do we get the workload data? Even if we found the user who's usage patterns are representative of many others, how do we persuade that person to give us exactly what they were doing with the machine over a course of, let's say, a day? So when we compare performance of different machines, we usually don't do this, instead, we use so-called benchmark workloads. So what are these benchmarks? They are programs and input data that users or companies have agreed upon will be used for performance measurement. So pretty much either a consortium of companies meets and decides that these are the programs and input data that we will use for performance measurements. Or a group of people, who is concerned about some performance issue, will design programs and data and allow others to measure performance on that type of a problem using those programs and input data. Usually we don't have just one benchmark program, we have a benchmark suite, it consists of multiple programs along with their input data. Each of these programs is usually chosen to be representative of some type of applications. So for example, we will not have ten different compression programs in a benchmark. We will usually have one or two because we think that those one or two are probably accurately representing the behavior of the other compression programs that we didn't put in the benchmark suite. So, let's discuss what types of benchmarks do we have out there. We can have real applications. They are the most representative of real application but they are also the most difficult to set up on a new machine. So, for example, if we are building a new computer system we may not be able to run a full fledged operating system there. And have all the, you know, graphics cards and hard drives etc., etc., connected, so it may not be possible to run, for example, a web browser, or a database on this machine yet. Next, we can have application kernels, we find the most time consuming part of an application, usually a loop of some sort, and we isolate just that loop, and say that this is what are we going to use for testing performance of machines. That way, we don't have to run a full blown application. All we have to do is find a way to run just the kernels on the new machine that we're testing. But sometimes running a reasonable sized kernel is still too difficult. For example, in the early stages of prototyping a machine, when we may not even have a compiler, for example. So, the next class of benchmarks are the so called synthetic benchmarks. They are usually designed to behave similarly to some type of a kernel. But to be much simpler to compile. Pretty much they are kind of the abstract of a kernel. And finally, we can compare machines not based on how well they perform on some actual code that they're running, but purely based on peak performance. In theory, how many instructions per second should this machine be able to execute. Usually, peak performance is only good for marketing. Doesn't tell us very much about the actual performance of the machine. Synthetic benchmarks are usually very good for design studies of potential new machines, but they're not very good for actually reporting performance to others. So we can use these just to see how well do we expect a design to behave so that we can choose among multiple possible designs. Once we actually build the machine, we can use, for example, kernels to test the performance of a prototype, and finally we should be using real applications if we are comparing actual machines that we are trying to, for example, sell to somebody. So let's do a quick check of our knowledge of benchmarks and performance reporting. Which of the following uses code from real applications, but does not use all of the code from any of, of the applications? The choices are, the actual workload mix from the user? An application suite? An application kernel? Synthetic benchmarks? And peak performance reporting? Select all of those that satisfy this. Let's discuss the solution for our performance reporting quiz. We need to check, among these, all of those that use codes from real applications, but not all of the code. Peak performance doesn't use any code. Synthetic benchmarks do not use code from real applications. This is code specifically designed for testing. Application kernels do use code from real applications but that application code has been reduced to form a kernel, so this is definitely true. An application suite uses the code from real applications and uses all of the code for these applications. So, this is not true. And then, an actual workload mix is similar to an application suite except that it comes from usage patterns of a specific user. So, the answer to this quiz is that application kernels use code from reapplication, but not all of their code. Benchmarks are very important to computer manufacturers. So usually there are standard organizations that put together benchmarks suites for use in comparing performance between machines. So usually there is a benchmarking organization that takes input from manufacturers assistants, from user groups, from experts in academia, and so on, and produces a standard benchmark suite, along with instructions on how to produce representative measurements. So this is how we got TPC benchmarks, which are used for databases, web servers, data mining, and other transaction processing, so called EEMBC benchmarks, which are used in embedded processing, such as cars, video players, printers, phones, etc. So if we're testing a processor for, for example, potential use in a car, we would use the EEMBC benchmarks, not the TPC. And the SPEC benchmarks, which are typically used to evaluate engineering workstations and also raw processors because these benchmarks, unlike the other two, are not very IO-intensive. They're mostly processor oriented. So the SPEC benchmark suite includes a number of applications, such as GCC, which represents software development workloads like the compiler, several applications like BWAVES and LBM that are representative fluid dynamics workloads, PERL which is representative of string processing applications, CACTUS ADM, this is a physics simulation related to relativity. This is a parser for XML. Several solvers of differential equations. BZIP, a compression application. To game ai applications, and so on. As you can see the spec bench are really trying to cover a variety of potential users for processors in high performance systems So, let's say we have something like 26 applications in a benchmark suite. How do we summarize the overall performance of a machine on the entire suite? So we want a single number that represents the performance of the machine. Usually, we want to use something like the average execution time. So let's say we have a computer X and a computer Y, a benchmark application A, B and C and let's say that computer X takes 9 seconds here, 10 seconds here and 5 seconds here, whereas the computer Y takes 18 seconds here, 7 seconds here, and 11 seconds here. The speed up of machine X over machine Y on each of these applications is two for application A because X is doing it in half the time as Y, 0.7 on application B because X is actually slower than Y there. And here is 2.2 because X is more than twice as fast as Y. Now let's compute the average execution time. Here we have 9 plus 10 plus 5 over 3, gives us 8 seconds. So on average, computer X finishes one of these three applications in 8 seconds. On average, computer Y finishes one of these three in 18 plus 7 plus 11 over 3, 12 seconds. If we now compute the speedup of X over Y, on average it will be 1.5 when computed this way. But if we average the speedups here, we get 1.63. So if we want to use average execution times, we can not simply average the speed ups on individual applications to get the average speed up. Pretty much the speed up using average execution times is not the same as the average of speed ups on these individual applications. If we want to be able to average speed ups, then we need to use geometric means for both of our application execution times and for our speed ups. In that case we get a geometric mean of 7.66 here, 11.15 here, and 1.456 here. So we have used the geometric mean for the execution times for each of the computers. If we use the geometric mean for these speed ups, we will get the same value here. The geometric mean is computed by multiplying out, for example, 9 times 10 times 5, and then taking the nth root, in this case 3rd root of the value, to get the geometric mean. And because things are being multiplied out and then the nth root taken you can see how, if we divide these things, we will get that the speed of computed geometric means of execution times will be the same as the mean of the speed ups, because we have third root of 18 times 7 times 11. These two divided is the same as third root of 9 over 18, times 10 over 7 times 5 over 11. So as you can see this is the geometric mean of execution times, then we can compute the speedup using them, and that's the same as the geometric mean of individual speed ups. However this only works for geometric means. We do prefer average execution time when we're computing the average times. However, just you must be careful not to compute individual speed ups and then average them. Instead, you should be computing the average execution times and then computing the speed up using those averages. You should also remember that simple average should not be used on ratios of times. So basically a speed up here is a ratio of two times. We should not be using a simple arithmetic mean on these values because they're ratios. Geometric means, on the other hand, can be used on ratios, but, for this to work, we also have to do the geometric mean on times, too. Let's do a speedup averaging quiz. Let's say we have an old laptop and a new laptop. Let's say that we have two applications that are representative for our workloads: a homework formatting application, and a virus scan application. For homework formatting, going from the old laptop to the new laptop on homework formatting gives us a speed up of 2. For virus scanning, going from the old laptop to the new laptop gives us a speed up of 8. So overall, going from the old laptop to the new laptop gives us a speedup of how much? So let's discuss the solution for our speedup averaging quiz. We only have the speedups for the two applications, and we have said previously that an arithmetic average of speedups does not make sense. You should not use arithmetic averaging for speedups, because they are ratios of execution times. So the only thing that does make sense to use in speedups, in this case, is a geometric mean. Thus, the speedup we need is a 2 times 8, and then there are two values, so we take the second root of that. And we get a number 4. So the correct answer here is the speedup is 4. Again, doing something like 2 plus 8 over 2 is not correct here, because these are ratios. You should not use the normal arithmetic mean for that. The next thing we will discuss is the so called, Iron Law of Performance. For a large part of this class we will be looking only at the processor itself. So the processor time can be expressed as the number of instructions per program times the number of cycles we spent per instruction times the clock cycle time. Or the number of seconds per clock cycle. To understand why this is CPU time, we can do the following. The number of instructions per program. Let's say this the number of instructions per program, times the cycles per instruction, times the number of seconds we spend per cycle. And then the cycle here, we can cross them out. The instruction here, we can cross them out. Then we get how many seconds per program, which is the CPU time. So why do we want to think of processor time in terms of instructions per program, cycles per instruction and clock cycle time, instead of just measuring the overall time? Well, because these three components allow us to think about different aspects of the computer architecture and surrounding areas that we can change. So the number of instructions per program is generally affected by the algorithm we use, and by the compiler we use. If we have a better compiler it might result in a more efficient program that needs fewer instructions to do the same thing. And also our instruction set might affect how many instructions we need to execute in a program. For example, if we have very, very simple instructions, it might take more of them to execute the same thing. The number of cycles per instruction is also affected by the instruction set. This time the simpler instructions might take fewer cycles to do, but also our processor design can affect how many cycles we need per instruction. We will see several techniques that allow a processor design to spend fewer cycles per instruction without changing the instruction set. And finally, clock cycle time is also effected by processor design. We can design a processor so that it does very little per clock cycle, thus spending less time per clock cycle. And also our clock cycle time can be affected by circuit design. If we have faster circuits, they will need less time to do the same thing that needs to be done in the clock cycle, and also by transistor physics, because they allow us to build faster circuits. So really, computer architecture will affect the instruction set. And here we have a choice of having complex instructions, where we spend fewer of them per program but takes more cycles to do each, or we can have simpler instructions, where we have more of them in a program, but we spend fewer cycles per instruction. And also computer architecture is affected by processor design. So in processor design we have a choice of making a processor that has very short clock cycle at the expense possibly of spending more cycles per instruction. Or we can have a processor that spends a lot of work in a cycle, thus reducing the number of cycles it needs per instruction. A good design, in terms of processor design, will balance the two. A good design in an instruction set will balance the two. Now that we have seen the Iron Law of Performance, let's do a quiz on the Iron Law of Performance. Let's say we have a program that has 3 billion instructions executing from start to completion. Let's say that our processor spends two cycles on each instruction, and the processor is clocked at three gigahertz. So we can say that the execution time is: put your answer here. Let's discuss this solution for our Iron Law quiz. The execution time is number of instructions in the program 3 times 10 to the 9th which is 3 billion, times cycles per instruction, in this case times 2 and times the clock cycle time. Now note that we have here three gigahertz which is how many clock cycles we have per second. What we need is how many seconds per clock cycle. So the clock cycle time is 1 over 3 billion. Make sure you remember that this should here be clock cycle time. Usually we express processor clocks in terms of clock frequency, which is 1 over time. And conveniently, these two can be crossed out and we get 2 seconds. So the execution time is 2 seconds. So the iron law of performance can easily be applied when we spend a constant number of cycles for each instruction. But how about unequal instruction execution times? Meaning we spend different numbers of cycles for different instructions. So this is our original item law of performance. The number of instructions per program, times the number of cycles per instruction, times the clock cycle time. If we have different clock cycles per instruction, this part becomes a sum for all types of instructions. The instruction count of that type that we have to execute times the cycles per instruction for that type of instruction. Note that, in the original iron log performance, this was really how many cycles we need to execute the program. This is how many cycles we need for each type of instructions. All of them in a program, and the sum here is simply summing up over all types of instructions. So this is still the number of cycles we execute in a program. And we still have the clock cycle time here. So the new CPU time here is like this. So if we have different CPI, meaning different cycles per instruction for different types of instructions, then we need to do this, and just multiply that with the clock cycle time. So let's do another Iron Law performance quiz, this time with unequal cycles per instruction. The program has 50 billion instructions, out of which ten billion are branch instructions for which we spent four cycles per instruction. 15 billion are loads for which we spent two cycles per instruction. 5 billion are store instructions for which the CPI is three and the remaining instruction in the program are integer adds for which the CPI is one. We do one of those per cycle. We clock this processor at four gigahertz. So the execution time is how many seconds? Let's discuss the solution for our newest Iron Law quiz. We have 50 billion instructions with different CPIs, so clearly we need to use our newest version of the Iron Law of performance. Execution time is a sum of number of instructions of a particular type times the CPI for the type. In this case, it's 10 times 10 to the 9th, which is 10 billion, times 4 for branches, plus 15 billion times 2 for loads plus 5 billion times 3 for stores. And we said the remaining instructions are integer add instructions. We have 30 billion instructions here, so 20 billion are integer adds at CPF1. And all of that times the clock cycle time, which is one over 4 times 10 to the 9th. Again, 4 gigahertz is the clock frequency. The clock cycle time is 1 over that. Now, let's do the math here. This is 40 plus 30 plus 15 plus 20 times 10 to the 9th divided by 4 times 10 to the 9th, so 10 to the 9th can be crossed out and we get 105 over 4, which amounts to 26.25. So this is how long it takes to run this 50 billion instructions at 4 gigahertz, assuming these CPIs and this distribution of instructions. Another equation that we will very often use, is Amdahl's law, which we need to use when we're about speed up on a part of the program, only some instructions, and so on. So, basically when we have a speed up, but it doesn't apply to the entire program, and we want to know what is the overall speedup on the entire program. So the overall speed up could be computed, as 1 over 1 minus the fraction of the program we didn't enhance. So for example, if we speed up 40% of the program, then this would be 60%, plus the fraction of the program we did enhance. In this case the 40%, divided by the speed up we achieved on that enhancement, how did we get there? So the speedup in this case is the improvement in performance, which is 1 over the new execution time relative to the old. So this is the part that we didn't improve, just stays there. The part that we did improve, is shorter by the amount of improvement on that part. It's very, very, very important to understand that the fraction enhanced here, is a percentage of the original execution time, that is affected by our enhancemen. Okay? So it needs to be that 40%, of the time spent on the program. Is affected by our speed up, not for example, 40% of the instructions in the program, or something else other than the time. So, now that we seen Amdahl's law, let's do a quiz. Let's say we have a program with 50 billion instructions it executes. Let's say that we have a processor clocked at two gigahertz. Let's say that we improved the branch instruction so that it no longer takes four cycles to do one. Now it takes only two cycles to do one. This is a table where we show for each of the four instruction types, integer, branches, loads, and stores, the percentage of all instructions in the program that are of this type. So, for example 40% of all instructions that we execute are integer instructions. 20% of all instructions that we execute are branch instructions. 30% are loads, and 10% are stores. And finally we have the CPI for each of these types of instructions. For integer it's one, for branches it was four. The improvement reduces that to two. Loads spend two cycles per instruction and stores spend three cycles per instruction. The question for you is, what is the overall speedup, which you will answer here. So let's discuss the solution to our Amdahl's Law quiz. The program has 50 billion instructions. This is how many instructions according to the instruction count: our integer 40%, branches are 1 5th of our instructions, stores are 10% and so on, and this is the original CPI, and the improvement we have is that the branches have improved from 4 to 2 CPI. If we try to use Amdahl's law, we might do something like this, this overall speed up is 1 over 1 minus fraction enhanced. The fraction has its 20% so it will be 0.80 is 1 minus that plus 0.2 is the fraction enhanced divided by this speed up is 2, so this will be 1 over 0,9 which will be 1.111. And if you did this, you will be horribly, horribly wrong. The reason this is wrong is that the fraction enhanced is not as a percentage of the overall execution time before the improvement. We said this is how many instructions are enhanced. So branches are 20% of the instruction count, but we don't know what percentage of execution time was spent on branches and execution time is what we are interested here. So that one of the possible correct solutions here is simply to do the following. Using the iron law, we can compute that for 40% of our instructions we were getting a CPI of 1, for 20% of the instructions we're getting a CPI of 4. For 30% of the instructions we're getting a CPI of 1 and for 10% of our instructions we are getting a CPI of 3 multiply that by a 50 billion and multiply that by 1 over 2 billion which is the 2GHz. So this would be the execution time before the enhancement. After the enhancement, we will be having the same thing, except there will be a 2 here, and if you do that, the answer will be 1.24. In order to be able to use Amdahl's law, what we have to figure out is, of this execution time, what fraction were the branches? And if we did that, we would be able to plug that number here and here as the fraction enhanced and get the correct answer here. So again, do not use something other than the fraction of execution time as your fraction enhanced, because you will not get the correct answer. We can see in Amdahl's Law. Let's talk about what it implies for our improvements. So our Amdahl's Law says that Speedup is equal to 1 over 1 minus fraction enhanced, this is the part of the program that we didn't enhance and it just figures here, plus the fraction that we did enhance. And that fraction became so much faster because of the speed up on the path. So now, let's compare two possible enhancements. This is the first enhancement, this will be the second enhancement. The first enhancement is giving us a speedup of 20 which is an extremely large speedup on 10% of execution time. Enhancement 2 is giving us a speedup of 1.6 on 80% of execution time. Using Amdahl's Law, we can now compute, what is the overall speedup resulting from these enhancements? Here, we have 1 over unaffected is 90% of execution time. Affected is 0.1. And it's shorter by 20 times. You can compute this, and get 1.105. Here, we get 1 over, the unenhanced fraction is 0.2, the enhanced fraction is 0.8 and we speed that up by the 1.6 factor and we get 1.43. As you can see, it is usually better to have a smaller speedup on most of your execution time, than a huge speedup on a very small part of it. In fact, even if we have an infinite speedup on this 10% of execution time, we would get 1 over unenhanced 0.9, enhanced 0.1 divided by infinity, so it becomes 0. And we still get 1.111. Now, it is very, very difficult to get huge speedups like this on anything. And it's, of course, impossible to get infinite speedups. So, what this means, is that if you put significant effort to speedup something that is a small part of the execution time you will still not get a very large overall improvement. In contrast, if you're affecting a large part of execution time then even a reasonably small speedup will still result in a similar speed up overall. So the overall conclusion from this is that Amdahl's Law tells us to make the common case fast. Whatever spends the most of our execution time is what should we be trying to speedup. And we should not focus too much on stuff that consumes very little of our execution time. Let's do another quiz related to Amdahl's Law. We still have our instruction types of integer, branch, load, and store. Let's say now that the percentage of execution time we spent on each is 40% for integer, 20% on branches, 30% on loads, 10% on stores. And the CPI, we get for then is one for integer, four branches, two for loads, and three for stores. We have a processor that is clocked at two GHz. Let's say we are considering the possible improvements that are reducing the branch CPI from four to three. Increase the clock frequency of the processor from two GHz to two point three GHz. And reducing the store CPI from three to two. Assuming we can only do one of these, which would you choose according to the overall speed? Lets discuss the solution to our latest Amdahl's law quiz. We have three possible improvements and now we can use Amdahl's law to tell us which one of them is the best. For improving the branch CPI this is our option 1: the overall speedup is branches are happening 20% of the time, so, so this is actually the percentage of time so we can apply Amdahl's law properly. So the unaffected execution time is 80% and the 20% we do affect, we speedup by how much? By factor of 4 over 3, so this is 1.33. And we get an overall speedup of 1.05. The clock frequency increase from 2 to 2.3 gigahertz, is affecting all the execution time, so we can just compute the speedup of this, but for the sake of exercise, let's apply Amdahl's law. One minus fraction enhanced is 0, because everything is enhanced. The fraction enhanced is 1, and the speedup we get here, by going from 2 to 2.3 gigahertz, is 2.3 over 2; which is 1.15 and the overall speedup we get is the 1.15. The third option is for stores, the unaffected fraction is at 0.9, the affected fraction is 0.1. The speedup of going from 3 to 2 is 1.5, and when we compute that we get 1.034. So it's a very small speedup. Even smaller than we got for branches. So overall improving the clock frequency gives us a significantly higher improvement overall, than the other two options. Primarily because this is a smaller improvement in terms of speedup on the fraction affected. But the clock frequency increase affects all of the instructions, versus here, we affect only 20% of the execution time and here we're affecting only 10% of the execution time. Let's discuss something that it often jokingly called Lhadma's Law. This is really Amdahl spelled in reverse. And, it says, that although Amdahl's Law pretty much tells us to make the common case fast. We should not mess up the uncommon case too badly while doing that. Let's consider an example, in which we can achieve an improvement of a factor of 2, or 90% of the execution time. So, we can achieve a speed up of 2, or 90% of the execution time. But at the cost of slowing down the rest by 10x. Meaning the remaining 10% now gets 10 times as much execution time as before. The overall speed up can be now expressed as, both of these are affected by different so called improvements. We have one of them, the 10%, is effected by a speed up of 0.1 because this is a 10 times slow down, this is a slow down of 0.1. And the other the 90% are sped up by a factor of 2 and we get 1 over 1 plus 0.45 equal 0.7. So we actually got an overall slowdown. In fact even with an infinite improvement on 90%, this will become 0. We will still have a speed of 1, meaning no improvement. So basically, if you have a huge worsening on even a small part of the execution time, the overall speedup will not be good. A final consequence of Handel's law that we will discuss, is the law of diminishing returns. Let's say that we had an original processor, which we will call GEN1 in which our execution time can roughly be divided into a purple phase and a blue phase each of them takes about half the execution time. Let's say that in the generation 2 processor we decided to speed up the blue phase by factor of two. What we got now is that the purple part is not affected and the blue part is now half as long. The orbital speed up of the GEN2 processor over GEN1 is equal to one over half is not enhanced. Half is speed up by a factor of two and we get the the speed-up of 133, which is a sizeable speed-up. Now let's say that we have a generation three processor, where we again get the blue part and speed it up by a factor of two, relative to the GEN2 processors. Now we have that the purple part is still the same, and the blue part is half of what it was. The speed up of the GEN3 processor over a GEN2 processor is 1 over and now we have to be careful. The unaffected part is now 2 3rds of the execution time. The affected part is only one third and that part is still affected by a factor of 2, so the overall speed up is 1.2 over GEN2. If we keep improving the blue part over the generations, eventually we will get to almost no speed up. Why? Because we are improving what amounts to less and less of the execution time. Furthermore, usually it is the case that improving something initially is not very hard, but after you exhaust the easy methods for improvement, it becomes very, very difficult to get new speed ups on a thing that has been already enhanced hugely. So eventually, the solutions that will improve the purple part will be cheaper than the solutions that further improve the blue part just because all of the easy stuff on improving the blue part we have already done. So not only do we get lower speed ups by improving the blue part, it's actually becoming more difficult to do so. This is sort of a lesson to computer architects to not go overboard improving the same thing, because the things they have already improved are now a smaller percentage of the new execution time, and we need to reassess what is now the dominant part in the execution time. In other words, once you have applied one improvement, you need to reconsider what is now the new dominant part in the execution time. And definitely, when applying Handel's law, we need to consider what is now the fraction enhanced, and the fraction unenhanced. Now, we know how to measure and compare performance. And we also know that overall performance improvement requires a careful balance between improving one aspect of the design and not messing up on the other aspects of that design. We're ready now to look at more advanced techniques for improving performance and efficiency. And we will look first at pipelining, one of the most important and generally applicable techniques in a computer architects bag of tricks. This lesson will explain what happens when we execute more than one thread or process at the same time. This used to be what happens only in super computers and high-end servers, but now and in the future, almost every computer will have more than one core and possibly more than one thread per core. So it can be said that all processing today really is multiprocessing. So there is a taxonomy called Flynn's Taxonomy of Parallel Machines. It distinguishes parallel machines according to how many instruction streams they have and how many data streams these instructions operate on. The first type of machine is single instruction, single data, or SISD, and logically this type of machine executes one instruction stream, and each instruction operates on one data stream. This is really your normal uniprocessor. So if we have for example, a single core machine, that's what it is. The next type of machine is single instruction, multiple data. So it executes one instruction stream, meaning there is one program with a single program counter we go through, but there is more than one data stream they operate on. The most common example of this used to be so-called vector processors, which execute a program. But each instruction, instead of operating on normal scalar values, operates on vectors. So, for example, you can execute an ad instruction, but it operates on two source and one destination vector of things. So it operates on many many data items at the time. Modern processors also have this multimedia extension instruction, such as MMX or SSE 1, 2, 3, and they're also an example of limited SIMD. Where a single multimedia instruction will operate on maybe four or eight data items. Then you can have a multiple instruction single data stream. This is weird, because we have multiple instruction streams. So there are several different programs executing at the same time, all operating on the same stream of data. So it's not used much and the closest we get to this is something like a stream processor. Where you have the first processor operating on, for example, the raw data from the camera, and then that data moves on to the next processor which is going to process it some further. Like find the edges, or sharpen it, or something and so on. But even there, it's not truly that we have a single data stream, because the output of the first processor is not the same as the raw camera feed. So this is just kind of the closest we get to it, but really, MISD is not really used much. And then we have MIMD, where there is more than one instruction stream and more than one data stream. And this would be your normal multiprocessor where each processor has its own program that it's running with its own program counter and zone, and each of them operates on its own data. So they don't have to do things in lock step to operate on the data, and they don't do something like a stream processing. So this type of processor really includes most of the processors you can buy today because most processors today are multi-core, which means really they have several processor cores. Each of which executes it's own instruction stream and can access data independently, so it has multiple data streams. And this lesson is mostly about this type of machines. So why do we want a multiprocessor? Why not just use uniprocessors? Well first, our uniprocessors would already be 4-wide. When we got to about 4 to six wide is when we switch to using multiprocessors and that's because once you get to this point, you get diminishing returns from making the uniprocessor even wider. You can make it execute let's say, six or eight instructions per cycle. But you don't get as much from that extension as you did from for example, making it from two to 4 wide. And that's because we get diminishing returns. If you remember your Amdahl's law, there will be things that benefit from this, such as programs that have a lot of parallelism between instructions. And then there are things that don't benefit from this, such as programs that have a lot of dependencies so they have to run things one at a time. Things that are 4-wide already are already making the parallel parts fast, whereas the one at a time parts are not accelerated. So by the time you get here, you already have a lot of your program suffer from not enough parallelism to benefit from either four or even six wide. Another problem with uniprocessors, if we make them even fancier, is, to make them faster, we have to jack up the frequency. To do that, we have to raise the voltage from what it would be if we'd use the lower frequency. And when we do both of these, the power consumption grows up cubically, so even if we increase the frequency by not too much, we would end up burning our processor. So we don't really want to make our uniprocessors be clocked twice as fast as before and we don't want to make them much wider than they already are. But Moore's Law continues. We get twice the transistors every 18 months for the same cot as before and in the same area as before. So, we want to benefit from this and improve the over all performance, but we no longer can do it this way. So what do we do? We just double the number of cores every 18 months. That allows us to have the cores that are not getting wider that are not getting up in frequency too much and yet, the overall computational performance of the shape is improving with Moore's Law. So we can get approximately a doubling of overall performance every 18 months. That is only if we assume that we can use all the cores. If we only have one program with one thread, then we're really using only one of the cores. It doesn't matter if the number of cores doubles because we are still using one of those cores. So we need programs that are parallel in order to exploit our many core processors that we are getting. But if we do we continue the trend of scaling performance up exponentially. To put this in another way we have multiprocessors today not because we think that we would rather have twice the cores than twice the uniprocessor performance. It's always preferable to have a uniprocessor, a singe core that is twice as fast as before than having two cores that are just as fast as before. But we ran out of things, that we can do with uniprocessors. So after a long string of years where we were doing this, just getting better unit processors, we had to switch to this if we want to get performance improvement. So let's compare what we can get from a multicore versus single core processor in a new technology. Suppose that in the previous technology generation, we got a processor that has an area of two centimeters squared. It was achieving 2.5 instructions per cycle. And it consumed 100 W, at 2 GHz frequency. And this is the highest power that we can allow this processor to have. And this 100 W is what we can reasonably call. Now let's say we get the new technology where this same exact processor would be half the area because our transistors have shrunk. Because it's the same design, it's going to have the same IPC. And because transistors have shrunk, we also reduce the power of this processor when operating at the same 2 GHz frequency as before. Now of course because this processor is smaller in the new technology than in the old one. If we want to do the best we can with 2 centimeter square, we have two options. Our first option is a better single core processor. This processor will be designed to occupy the entire area that we have, which is 2 cm square. It's a more complicated processor that allows us to achieve an IPC of 3.5. So it executes 3.5 instructions per cycle, not 2.5 as the old processor. But because it's a fancier processor and has more transistors, it spends 75 W when operating at 2 GHz. So, the question for you is first, if we allow this processor to spend all 100 W that we can call, what is the frequency it can get in GHz? And if we do that, what is the speedup this better single core achieves versus what we can get in the old technology? The second option we had in this new technology Is to simply put two cores on this chip. So it's going to be two cores like this. This is going to get us 2 cm squared. IPC is going to be 2.5 on each core. And each core is going to consume 50 W at 2 GHz. So we get our entire budget spent by having these cores operate at 2 GHz. So the question for you here, is what's the speedup of this two-core approach versus the old technology, assuming, of course, that the old work can simply be divided among the cores. So pretty much having two cores allows us just to split the same work that we had before onto two cores without adding any new instructions. And without sacrificing the IPC. So let's look at the solution to our multicore versus single core quiz. This is what we had in the old technology. In the new technology, one of these cores is going to be half the size and half the power. So we have the option of either replacing that with a better core that occupies the whole area, get us more IPC, but consumes slightly more power. And then the question is, if we do consume the whole power budget of the chip, which is a hundred watts, how much can we increase this frequency. And with that frequency, what is the overall speed up versus what we had in the old technology? The other options was just, put the two cores that we can now fit on the chip. Each now gets us the same IPC as the single core did. And together, they consume the whole power budget of the chip at two gigahertz. So the question is, what's the speed up versus the old technology with the two core approach? The first thing we need to figure out here is how much can we raise this frequency here so that we get to 100 watts. Well, we're allowed to increase the power from 75 watts to 100 watts. That's a factor of 1.33. And remember that the dynamic power is proportional to voltage squared times the frequency itself. And that we have to increase the voltage in proportion to frequency. So if we are spending power to increase the frequency, we have to proportionally get both the voltage and the frequency up. And that means that we will be spending power in proportion to the cube of the frequency. That means that the frequency that we can operate on is cubic root of 1.33 times the old frequency, and this is equal to 1.1. So our new frequency is 1.1 times the old frequency. The old frequency was 2 gigahertz. That means we can achieve 2.2 gigahertz when we increase the power by a factor of 1.33. The speed up we achieve versus the old technology will be the speed up we achieve because of a shorter clock cycle time, and that speed up is 1.1 times. So we are 1.1 times faster now, in terms of clock frequence, times the speed up we achieve because our IPC's better. And if we divide 3.5 and 2.5, we get a factor of 1.4 times the speed up which is because we have fewer instructions. But because we are executing exactly the same code, that speed up is one. We don't speed up, really. So our overall speed up is just this. And it ends up being 1.54. Now, let us look at the two core option. We have the same IPC as before. But we now have two cores that are doing that. So we are executing twice as many instructions as before, at the same frequency and at the same power. So the speed up versus the old system here will simply be two. We are operating at the same frequency, splitting the same work into two pieces, and each of the pieces is now done at the speed where we were doing only one of the pieces. So overall we are getting a speed up of two. As you can see, two cores are achieving a better speed up relative to the old system, than a fancier core. Which means that if we indeed do have programs that can use the two cored effectively, then this is the better option than improving the core and consuming the cheap area that way. So we have already seen that a multiprocessor can easily win in performance nowadays, assuming that you have a program that can use multiple cores. So there are some disadvantages to going to multiprocessors from single core processors. One is that the so-called sequential or single-threaded code is easier to develop for programmers, and actually it's a lot easier to develop. So there is significant cost involved in making an application multithreaded where originally it was single-threaded. Even when you do write a parallel application you will have some bugs. The debugging of parallel code is much more difficult than sequential code, and even if you have a program that works correctly and uses a lot of threads, chances are that it will still be very hard to get what is called performance scaling. And performance scaling is a property of the program that, as we increase the number of cores, if its overall performance keeps improving, this will be ideal performance scaling. You'll get more performance in proportion to the number of cores you are given. Unfortunately, most programs, when you write them, do something like this, they scale for while and then stop scaling and eventually you stop getting more performance from the number of cores. You can spend a lot of time improving this program and get something like this, where it scales up to some larger number of cores reasonably well, but each of these increments in scaling is going to result in a lot of work for you to achieve it. And very, very few programs achieve good scaling to a huge number of cores, and those that do usually took years to develop. So now we will go through several types of multiprocessors. The first type of multiprocessor uses what is called a centralized shared memory. So all the cores, let's say we have 4 of them, might have their own caches, but then they're all connected to the same bus, so they can all access the same main memory and input output devices. These cores can share data by simply reading and writing the main memory, assuming of course that these reads and writes go through the caches properly, and we will see that we can achieve that, so pretty much, this core can send data to this core by writing to some memory location, and then this core reads it from there. Similarly, any of the cores can access any of the other devices here. This type of a system is really what today's multi-core processors tend to look like, and it's also called UMA, which stands for uniform memory access time. But it's not UMAT, it's just UMA. But what is uniform is really the time to access the memory. Why? Well, because this main memory is at the same long distance from all of the cores. Yes, it can be slightly closer to some of them. But because they are all connected to it in the same way, the memory really is about as bad to access as any one of them. This time of multiprocessing is also called SMP, which stands for Symmetric Multiprocessor. It's called symmetric because any core and its cache look just like any other core and its cache in the system. So if we have two cores, we can just take one of them with its caches and replicate to make a four-core system. So let's see if we can figure what the problems are with UMA aka SMP aka Multicore. Let's suppose that we want to have a lot of cores and have them use this centralized shared memory in a UMA SMP fashion. Which of these are problems we'll have to face when we do this? First, memory needs to be very large and thus slow for this type of a machine. Our main memory will get too many accesses per second. Basically, we'll need a huge memory throughput that a normal memory module will not be able to provide. The next option is, we'll need a lot of small memory modules in proportion to the number of cores. Next, our caches will have very small block sizes. And finally, the pages for our virtual memory will become too small to be usable. So, select all of those that are problems for this type of architecture. So let's look at our answer to the UMA/SMP quiz. We want to have a lot of cores with this type of architecture, so what are the problems that we'll have to face? The memory will need to be large and slow. And that's because having more cores usually means that we will be running programs that access more data. And we need to keep the data somewhere, so we will need to have a large memory. And that makes the memory slow. Next, that memory will get too many accesses per second because the cache misses from all of the cores will be going to memory. If one of the cores generates cache misses that he memory can survive doing. Two of them, it might still survive. But if we have a lot of cores our memory will probably not be able to keep up with all of those accesses. We'll need a lot of small memory modules, not necessarily. The number of memory modules in a centralized memory doesn't matter. What is really needed is to have enough memory for all the cores to get enough data in there and the memory that will keep up with the bandwidth demands of the cores. How many modules we have doesn't really matter. Caches will have very small block size. This is not true either. All of the caches and all the cores can have any block size they want. The block size is not divided among the cores. Pages for virtual memory become too small. Same thing. We can have any page size we want. Having more or fewer cores doesn't affect the page size in virtual memory. So, the only options you should have selected are the first two. So let's talk about some problems of centralized main memory and why we may want to do something else. As we have seen in the previous quiz, memory size is a problem. We need a large memory, and because it's all one big memory, that one memory will be slow and, because of its large size, relatively far away from all of the cores. We will also have the problem of memory bandwidth. Because misses from all cores go to the one memory. So the requests from all of the cores create contention for memory bandwidth because our big, slow memory will have a limited bandwidth. And what that does is it prevents, really, additional cores from benefiting our performance. Because all of the accesses are really serializing in the queue waiting for memory. So if we add more cores, that just makes every core's accesses even slower because they have to wait for memory even longer. So we don't really get a performance advantage. So centralized main memory because of this problems works well only for relatively small machines, with something like 2, 4, 8, maybe 16 cores. Beyond this, we saturate the memory bandwidth and also our memory needs to know the too big and too slow if we keep it centralized. The next organization that we will be looking at is similar to distributed shared memory, except that we are no longer sharing memory, so it's called distributed memory. No sharing of memory means that only one core can access a memory slice, and the others can not. So what we now have is cores that have their own caches, and each of them has its own memory that can only be locally accessed here. So really, each of the cores pretty much has what looks like a complete single core computer system. And a network interface card that connects it to a network. But now when a cork has a cache miss, that cash miss goes directly to this memory. And if the core wants to access something that is in another core's memory. It can not simply issue an access that message in the cache and goes there. Now what it needs to do is actually create a network message using some sort of a send primiative in the operating system to actually send a request. The program here needs to receive that,see what is being requested,respond to it and so on. So now communication is explicit it's no longer sufficient to simply put data in memory and then another core just reads it from there. You have to actually send the data explicitly to another core if we want to communicate. So. That means that you write programs differently from this. Symmetric shared memory and distributed share memory pass data around using shared memory. Meaning reads and writes to memory are used to exchange data. Now we are using what is called message passing for communication. So now you are pretty much writing a program as if these were independent machines that communicate over a network. If you have a distributed memory supercomputer, for example, it's just that this network and these network cards are a lot faster than your normal internet connection. This type of a system is also called a multicomputer because really each of these is like a complete computer. It has the processor, the cache, the memory, thumb I/O, and the processor only directly accesses the local memory. So it looks like complete uniprocessor system. These are also called cluster computers, because really, you put a bunch of normal computers together into a tightly networked cluster, and you get something like a distributed memory system. These types of computers tend to scale to a large number of processors. The reason is not that they're fundamentally better at communicating than shared memory systems. The reason is mainly that the programmer is forced to explicitly think about communication because you use a different type of primitive to communicate than you use to access your local memory. So the programmer is aware of communication going on and then naturally they will tend to minimize communication. And if at all possible, access the local memory. So this approach works better, primarily because it forces the programmer. To figure out things that otherwise it might not notice. Because it forces the programmer to figure out how to communicate efficiently. Whereas in shared memory, the programmer may not even realize that some of the accesses are not local. And thus are slow and causing a lot of communication to happen. So let's see if we can figure out how we should allocate memory in a NUMA System so that it works well. Our operating system should put the stack pages for core N in the memory slice N. Put all data pages ever touched by core N in the memory slice N, and put all data pages mostly accessed by Core N in the memory slice N. More than one of these could be correct or potentially, none of them are correct. You need to select all that are correct. What should the operating system do? Let's answer our NUMA memory allocation quiz. In order to improve performance of a NUMA machine, the operating system should really put all data pages that are mostly accessed by a single core in the memory slice local to that core. That will ensure that most of the accesses by each of the cores are going to the local memory. Which both makes sure that accesses will be quicker than they otherwise would be and also the network will not be used for the majority of accesses. So this should be selected. If we, however, put all data pages ever touched by core N in the memory slice N, then we have a problem because, for example, one core might initialize all the data, and then we put all of the data in its memory slice. And at that point, that memory slice becomes the bottleneck because everybody will be accessing that slice, and the data might not even fit there. So this is not a correct answer. And then put the stack pages for core N in the memory slice N is a very extreme example of this where really it will just be the core N that is ever accessing this data, so of course it should be in its memory slice. So the stack is one of those things that the operating system can figure out very easily where to put. Some of the other pages are a little bit more difficult because more than one core are accessing it. And then the operating system needs to choose a good location either because one core is accessing it more than others, or to put it in a memory module that is kind of close to all the cores that are accessing it. So to see what the programmer has to go through, let's look at the message passing program. This is a relatively simple program. What it's trying to do is compute a sum of a relatively large array in parallel on four cores. So this is the overall array size. It has 1024 elements. This is the number of processors we have. In a message passing program, each of the processors will get one quarter of the array. They can not access each other's memory directly, so each one of them only allocates its part of the array in its memory. Each of them will have its own sum variable, and then what happens is each of them iterates over its part of the array, adding those array elements to the sum. This of course assumes that the elements of the large array have somehow been distributed to all of the processors. If they haven't been distributed already, then we need to also write code that will actually send the elements of the array so that each of them gets a quarter of the array. Once we have computed the sum, we need to compute the final sum, and to do that we have to communicate. So what we will do is we will have one of the processors act as the summing up agent, and it will, for every other processor, receive the sum sent by that processor, and add it to its sum, and then print the final overall sum. What do the rest of the processors do? Well what they do is they send to processor 0 what they have collected so far, and that's all they do. So as you can see, we need to explicitly send and then receive things in order for communication between processes to happen. And also we have to explicitly control what gets into whose part of memory. We need to explicitly also, in the program, figure out how to match the sends and receives, because, if we send something, and it's not received, it's going to be a problem. And also, if we wait to receive something, and there is no send for it, it's going to be a problem, we get stuck here. So we need to know how many processes to wait for. We need to send exactly what's expected at the time, and so on. But once we do all of this correctly, it's very easy for the programmer to see which parts of this program are going to be expensive as far as communication is concerned, because these are all local accesses, so they are relatively fast, and these are the only places where we actually communicate. Now let's look at the shared memory version of the same program. We still have the same size of the overall array and the same number of processor cores. But this time they can all access the same memory, so we can just put the whole array in memory. We will have the total sum, and each of them will be computing its own sum. So, because this array is in the shared memory, each processor will sum up its own part of the array. The big difference occurs not only here, where we are actually able to just access the part of the array that we want. But also in how we can compute the sum. Instead of having to explicitly send. The elements of the array to one of the cores. Here, each core can access the allSum variable to compute the final sum. So what they do is they synchronize, so they have a critical section around this, and then each core simply takes the overall sum and adds its own sum into it, and then frees the lock so that the next core can do it. So the course can do this in any order they want. We don't have to control that order or anything. All that needs to happen is that all of them have added the sum together. And now we still have to make sure that one of the cores prints out the sum. But because this can be done in any order, Core Zero could be the one that does this first and then prints out only its part of the sum because the others has not updated this yet. So we need to add code here, to have what's called a barrier, where basically we put a synchronization here that makes all of the cores wait for everybody to arrive before they proceed. To ensure that Core Zero when it does this, does it only when everybody else has arrived to this point and has incremented the sum already. One of the nice properties of this is not only that it makes it more explicit who does what, so that we don't have to think about this as being basically four different versions of code. There is no need to distribute the array. The whole array is all of the time in the shared memory. We just choose who is going to access what. So let's summarize the differences between message passing and shared memory. Regarding who does communication in message passing, the programmer is responsible for figuring out who sends and who receives the data. In shared memory, though communication is happening through shared variables in memory, so all of the actual sending of the data back and forth is fully automatic. The programmer doesn't need to do that, they just store to memory and load from memory, and it is the responsibility of the system to figure out when the data gets sent and to whom. As far as data distribution is concerned, that is, which cores get to have which data in their memory, the programmer needs to manually do this data distribution, by explicitly sending the data to the cores that need it. In shared memory, the system is responsible for sending the data wherever it's needed, so the programmer simply loads or stores to the data, and it's the job of this hardware and the software of the system to figure out which data should be local where at what time. As far as how much hardware support we need for message passing, this support is relatively simple. We just need to basically have a network card and a network. It can be a very fancy network, but it's just a network. In contrast for shared memory, we need extensive hardware support that automatically figures out when to send data, and to whom and where to cache in, etc., etc. So we will be spending a large portion of the rest of this course on just figuring out what this hardware support needs to look like. So as far as the programmer is concerned, the programmer worries about two things, correctness and the performance of their program. They want their program to be correct and do as well as possible on the available cores. In message passing, getting correctness is difficult, because you have to orchestrate everything perfectly and you can get all sorts of data log issues, and wrong distribution, etc., etc. In shared memory, correctness is also not easy to get, but it's less difficult than with message passing. Performance with message passing is difficult to get, but once you get correctness, performance is not far away. In contrast, shared memory, performance is very difficult to get and the difference between correctness and performance can be large. So it might be relatively easy to get a program to be correct, but it's performance might not be anywhere near good, and then it might take a lot more effort to get it to also perform well. So with message passing somehow, by solving the correctness problem, you are also solving a large part of the performance problem. In shared memory, it's easier to get things right, but the effort to get it to perform well might be even more than with message passing. So let's see if we understand the differences between message passing and shared memory. If we have a program in which one core know how to initialize an array and then all others need to read the entire array, then the number of lines of code for the data distribution of the array once it has been initialized. Does message passing have fewer lines of code than shared memory for this? Does it have about equal number of lines of code? Or do you think that it gets more lines of code than shared memory, in which case write a larger than sign? And then once we have achieved data distribution. Synchronization that is needed to make sure that the initialization is done before we can read the array adds how many lines of code in a message passing version of the code, and in the shared memory version of the code? You only need to say whether it's 0 or larger than 0. Let's look at the solution to our message passing versus shared memory quiz.If we have one core initializing an array, and then all others reading the entire array. Data distribution, the number of lines of code you need to do it in message passing, is significant. The number of lines of code we need for that in shared memory is really zero lines of code because once the array is initialized in memory others can simply read it. They don't need to really distribute the data in any way. In contrast, in message passing the core that initializes the array now needs to send it explicitly to all the other cores and the other cores need to receive it explicitly. So there is definitely more lines of code in message passing than in shared memory. Once you have the data distribution for shared memory you don't need do anything for it. For message passing you need to send and receive. Synchronization that makes sure the array is initialized before we get to read it, adds how many lines of code? In message passing, it adds 0 lines of code because the sends and receives that we needed to to do to the receive the data already insure that until we receive the array, we cannot read it. So when we get to read it after the receive, we know that it's been already initialized, because the send happens after the initialization. In shared memory, however, while the initialization is happening, the other threads could just begin reading it, so we need to add some explicit synchronization out to make sure that the reader's wait while the initialization is happening. So we here have more than 0 lines of code. In this case the message passing code is simpler as far as synchronization is concerned than the shared memory code. This is typically going to be what happens between shared memory and message passing. Message passing needs to distribute data, but in doing so it also synchronizes. Shared memory doesn't need to distribute data, but if synchronization is needed after the distribution it needs to add explicit code to do that. So what types of shared memory hardware do we have? We can have multiple single cores that share the same physical address space, so all of them can each address to any memory address. And the examples we have seen of these are the UMA and NUMA. Both of them actually are individual cores, each of them being able to access the entire address space. The other extreme is to only have one core and then do multi-threading, basically, run two threads by just time-sharing the same core. So, because they're now running really on the same core, they're naturally accessing the same physical memory. So, we get the shared memory behavior simply by virtue of actually not having more than one core. And now the question is, is there something in-between them? Because this is not really capable of benefiting from having multi-threading. We can have hardware multi-threading support in one core that actually tries to benefit from there being multiple threads. This hardware multi-threading can be at the coarse-grain where we change the thread we are working on every few cycles. And we now need to have hardware support that switches between threads this often, because without hardware support, we would be spending all of the time just saving and restoring registers of these threads. So we actually need to now have every thread has its own set of hardware registers and so on. So there is hardware support needed for this. We can have fine-grain hardware multi-threading where every cycle we are working on another thread. This, of course, needs even more hardware support because now we need to be able to very quickly change between these threads. And, we can also have simultaneous multi-threading or SMT, where in any given cycles, we could be doing instructions that belong to these different threads. This is called Simultaneous Multi-Threading or SMT. And it's also called Hyper Threading by some manufacturers of processors. So now, we need hardware support for any one of these three. And, of course, the hardware support becomes more extensive as we move from coarse-grain, to fine-grain, to simultaneous. So the question is, but what do we benefit by kind of sharing the same core by multiple threads. Don't we get the same thing as here? So let's discuss the performance of multi-threading and how can it be better than simply running things one at a time. So if we have a processor with no multi-threading support, then this is how to threads might run. These are cycles going in this direction, so each notch here is a cycle. And we have four instructions that can execute in this cycle. So this is what might happen. A green thread might be using this, and this slot in this cycle. And the other two issue slots are not used because the processor has no ready instructions to dispatch to execution in this cycle. Let's say there is a data dependence so next cycle one instruction can wake up. And then let's say that in the next cycle we can have three instructions go, but then let's say that all of the instructions are now waiting for something to happen. So let's say for a few cycles we don't get much happening. Then, when it happens, we get to execute, let's say, all four. And then maybe another cycle with two, et cetera. So things can be like this, basically. Much of the time we are running with a less than fully utilized processor pipeline. Let's say that at this point we get an interrupt, and because there is another thread that is read to run, we now get many cycles of the operating system doing its thing. So it's also going to utilize the pipeline in some way. But it spends many cycles basically figuring out which thread to run next. Eventually decides that it's going to be the blue thread and begins executing instructions from that thread on the processor. So this is where we return from the interrupt and start executing a new thread. Because let's say we have these two threads ready to go. So the blue thread in a similar way will be using the processor. Partially, let's say it has a level one miss, takes a while to get the thing from level two. Meanwhile, maybe it's able to do something for a while, but eventually runs out of things to do. And then for example starts doing something again, and let's say this whole cycle is used. So this is what might happen in a processor with no multi-threading support. In that case really, the overhead that you created by having to switch between threads, is probably larger than the gains we are getting from running them. We we're better off just running a single thread that does the work of both of these threads. So really, a single processor with no multi-threading Is capable of doing this with a good operating system, not because it's better for performance to break work into multiple threads, but because we want to be able to simultaneously run several applications and make progress in them. So that for example if you're playing games and playing music at the same time, we alternate between the game and the music player. Fast enough for you to not be able to tell that they are really executing one at a time on your one core. If you have a chip multiprocessor, we get to do something like this. Now we have two cores, each of which pretty much gets to execute. One thread, so it's faster, of course, than this, and we can benefit truly from being able to write multi-threaded programs. But, you have to have two cores for it, so the cost is about twice. The cost of a single core here. So with fine-grain multi-threading, what we have is really one core, but with separate sets of registers for these threads and some scheduling logic. With fine-grain multi-threading every cycle we can switch between threads. So what happens is, the green thread gets exactly same behavior in the first cycle. In the second cycle we do the blue thread. So we get the behavior of it's first cycle here. Then we do some of the work from the green thread again. Then from the blue thread, third cycle of the green thread, third cycle of the blue thread. And now, we would be running the green thread except that there is nothing to run there. But what happens is the blue thread has something to do. So we spend another cycle on the blue thread and then the green thread is still somewhere here. There is nothing ready to run but these operations are going on. So the blue thread gets one more cycle here. At this point, the blue thread has nothing to do, but the green one now has something to do. So, we end up doing the work of the green thread and then for another cycle. Then let's say neither of the threads can do something, but then the blue thread is able to do this because it's been three cycles since here. And then again, and as you can see, we get done faster than here because we are able to eliminate some of the cycles where we were not able to do anything. So, fine-grain multi-threading benefits from the fact that when there is a long period of idleness in one thread, for example, due a cache miss, you might be able to run still instructions from the other thread. And finally, let's see what happens in SMT. In SMT, we are able to mix instructions from different threads in the same cycle. So what we will get is the. Green one get's this. The blue one also wants to use the bottom slot, and let's say there is a unit there, for example, the load unit that you have to use here. But we can still run this in the same cycle. Next cycle the green thread does this. The blue one wants to now do this, this, and this. And we can let's say do this too. But let's say there is a dependence from this to this so we can only do this one. Next cycle, the green thread can do this, and from the blue thread we can maybe do this instruction, or maybe not. So let's say we can't because maybe this one depends also on the one that we haven't done, or the next one. Now, however, there is some period of idleness in the green thread, and now the blue thread gets to make up for missing out on some things, so it's able to maybe do this, and this dependent instruction. And, at this point, the green thread is ready to run something, so we can choose whether to run the green or the blue one. Let's say that the green thread has a higher priority, so we always try to do instructions from the green thread. And only fill the gaps with the blue ones. Now we've done this, next cycle we still do whatever we can from the green thread. We now can do this. And now we only have the blue thread left to do. If the green thread has more stuff to do, we can do this. So as you can see, with SMT, we get to populate unused slots in what would otherwise be a full speed execution of the green threads. So that the blue thread gets to make progress while the green thread really is not suffering for it. And we can do that for more than just two threads. So for example if we had four threads we might be able to actually do another thread. At a slightly slower pace than blue while we are also doing this, so with SMT what we are doing, we are really using underutilized issue slots in our out of order super scaling processor. The hardware cost to do this is very small. Our fetch stage needs to be slightly more complicated to allow us to fetch from one or the other PC. And a register file needs to be slightly more complicated because really there are now two sets of architectural registers. But other than that the whole complicated out of order engine stays the same. So let's say this is going to cost us something like 5% of the cost of a no multi-threaded processor. This here, again after we do registry naming and everything else, the process scheduler stays just the same. We just need to put instructions from multiple threads in the instruction window, so that they could be scheduled from there. And now also when we commit, we have to figure out which instruction belongs to which commit and so on, so the cost is slightly larger than fine grain, but still not very large. Because again most of the complicated preservation stations and execution units, and everything else, stays pretty much unmodified. So this is going to have performance that is maybe very, very close to a CMP. Two threads here, two threads here maybe making almost as much progress, but at a very small fraction of this cost. So let's compare the Simultaneous Multi Treading and Dual-Core processors. Let's say that we have two threads to execute, one is floating-point intensive, most of the stuff it does, it does on floating-point execution units. And another thread that is integer-only, it has no floating-point operations whatsoever. Let's say we have 4-issue core that can issue two floating points and two integer instructions per cycle. Do we get more performance per hardware dollar we put in with SMT added to a core like this, or with two such cores? So let's answer our SMT versus dual-core quiz. We have a floating-point intensive thread. It's going to use pretty much these two issue slots and very rarely use these. And then we have an integer-only thread that only gets to use these two. So, with a dual-core processor, one of the cores really gets to be 2-issue plus maybe a little bit. Another one gets to be just 2-issue because it never sees floating-point instructions that it needs to have more than 2-issue. In SMT however, we would be able to do two instructions from this thread and two instructions from this thread per cycle. So we will be getting very similar performance, and as we have seen, then this is the more effective approach because the cost that we are getting here, versus here, is significantly lower. So we get similar performance for a almost half the cost here. Now what will happen, however, if we didn't have a floating-point intensive thread and instead got two of these? In that case, the two core is slightly more cost effective because it gets the 2-issue performance in each core for twice the cost of one core. Whereas this gets us the performance of a single core. because both of these are sharing the 2-issue slots for a cost that is more than one core. So in that case, this will be the more cost effective option, so as you can see it's not that SMT always wins over multiple cores, it's that it depends on exactly what types of programs we have and exactly what's the distribution of issues among units and how different applications are using it So the floating point intensive, an integer only. And this type of a split between issues slots is an extreme example that makes the SMT look really good. So we said that the SMT processor is not that much more expensive than a single threaded processor that has the same issue within everything. Let's see what the hardware changes for SMT look like so that we can convince ourselves that it's indeed not that much more expensive. In a normal processor, we would have a program counter that we use to access the instruction cache to fetch our instructions. Because this is a super scalar processor, we are fetching more than just one instruction with this program counter. Then we get into a decoder that is capable of decoding multiple instructions per cycle. Then a renamer that gets to read and update the RAT for this thread. And once the instructions is renamed it gets to the reservation stations, and then from here we send the instructions for execution, and in our ROB, we need to track what's the next committed instruction from this thread. So if we want to add the ability to simultaneously multi-thread another thread here. We definitely need a separate PC here. And we need a logic that will fetch from these two PCs. We could do this every cycle we fetch, from both of the PCs, half of the instructions. Or we can actually do one cycle fetch everything you can from this PC, one cycle fetch everything you can from this PC. So SMT usually refers to what's happening here, but here we could actually be doing fine grain multithreading as far as fetching is concerned. Most processors that do SMT actually do it that way. Decoding is completely oblivious to where the instructions came from. So we don't really need to change anything about our decoding logic. The renamer, does the same thing regardless of where the instruction came from, except that the RATs accesses needs to be different. So now we have two RATs, one for each thread. That we need to access depending on what we're renaming. After renaming, instructions got separate physical registers for each thread. So the reservation stations are simply shared by the two threads. The execution units get to execute the operations with the values they are given so they have no clue where the instruction really came from. And then when we get to the ROB, we need to either have a separate ROB with its own commit point in which case each of the threads gets to commit each instruction in order. Or we actually put all the instructions in interleaved order in the ROB, in which case we will not be able to commit an instruction from one thread until all the instructions from it and the other thread that preceded have been committed. Because the ROB is a large In complex piece of hardware, typically we don't do this and we actually suffer, here, by just committing the threads together. But if we have sent the instructions, in order, to the reservation stations and their ROB, then the order in the ROB is still the correct order. It's just at sometimes we could have committed a instruction from one thread, but we have to wait for the other thread to also commit some instructions before that, so it's slightly less optimal, but still works fine. The thing that happens at commit though, is that we deposit the results into the architectural register file. And also when we send instructions for execution, we send it along with the data, that comes from the ARF, our threads do have different registers. Our zero here is not the same as our zero here. So we need a separate ARF and a separate path to each of the ARFs here and also the separate read ports here. So as you can see, the complexity largely lies in having multiple pieces and being able to fetch from either of them and in the logic that immediately precedes the reservation stations. But the really complicated part of the processor which is the reservation stations, the broadcast of results and so on, stays the same. So this is why the cost is not twice the cost of the processor. Most of the cost of the processor is in the cache right here, the complex hardware here, the large ROB where you can insert things and wait for them to finish and so on and commit, and also the data cache. And what you're really adding is another PC here, same size cache as before, another RAT, another ARF, but keep the complex part alone. So that's why the cost is not very, very high. Now let's look at the relationship between SMT, data cache, and TLB. If we have a virtual index virtually tagged cache, then we simply send the virtual address into the cache and use the data that comes out of the cache. So with virtual index virtually tagged cache is the problem is that our two transmitters might have different other spaces. And now we are sending them simultaneously to the cache. The cache has no clue which one of the two it's getting. So, unless we make something really complicated here to figure out which is which, we will have some alias in between the two threads. Note that, normally with the VIVT cache, when we switch between threads in a coarse grain we would flush the cache between that, but now that the two threads can pretty much every cycle you're getting one or the other, we cannot flush the cache in between those. So with the virtual index virtually tagged cache, we are running a real risk of getting the wrong data because the same address in the two threads that are accessing the cache, pretty much alternatingly in a very haphazard way, might map to different physical addresses. But the data cache doesn't know that, so one thread saves data to its stack, another one pops data from its stack. And if they're at the same virtual address, we get the data forwarded from one thread to the other which we didn't want. If we use a virtual index physically tagged cache that avoids aliasing, then we are fine. We'll find the set in the cache and the physical tags that come out will be compared with those from the TLB. So assuming that the TLB does what it is supposed to do, we will have the same virtual address from the two threads. Look at the same set, but find different data based on the physical address in that thread. However, that assumes that when we use a virtual address to access the TLB, it doesn't look at just the page number in the virtual address. Because the two threads could have the same page number translate to different things. So for virtual index physically tagged, or physical index physically tagged caches, the cache is just fine, but the TLB must be thread-aware. That can be done by adding the single bit to each entry that tells us which of the two threads is this mapping for. In that way, indexing into the TLB with a virtual address will find a set in the TLB, that has multiple entries. And we will only be looking at the entries that both match our virtual page number, and also match our thread ID. So we will now send the page number and also the thread ID here. And the TLB needs to match both before it treats this as a TLB hit and produces the physical address here. Now let's look at the relationships between SMT and cache performance. The cache of the core, is shared by all of the SMT threads that are currently active on for example, if you have 2 SMT then the cache is shared by both of the threads. That gives us some good and some bad properties. The good property Is that we get fast data sharing among the threads. When Thread 0 stores something, and then Thread 1 wants to read it, it gets to get a cache shared. So any communication between these threads that happens within a short span of time like we store and then we quickly load in another thread, will go through the cache and have really, really good performance because the load here will always be a cache hit, assuming that the store happened recently. So the communication now no longer goes through shared memory because we have a cache. The communication tends to go through just the cache. But when we have an SMT processor, the cache capacity that we have and its associativity is shared by all of the threads. So if the working set of thread 0 plus the working set of thread 1 minus the part of the working set that they have in common, which will be counted twice if we did this, exceeds the data size, we get what is called cache trashing. Pretty much the data no longer fits in the cache, and we get a lot of cache misses. If each of the working sets would fit individually in the data cache size, then the performance of the SMT can be and usually is significantly worse than doing the threads one at a time. So we were assuming that when we did SMT that we will get some benefits from using resources that would otherwise be underutilized like the issue slots and the reservation stations and so on. But that assumes we are getting similar cash performance. However, if the cash has a given size and threads are written to fit in that cash, then running two such threads if they don't share enough data, might exceed the cash size in which case suddenly we have a lot more misses then we expected and we make payback. In cache misses a lot more than we gain by overlapping execution in the SMT fashion. So let's see if we understand the relationship between SMT and cache behavior. Let's say we have a program named a whose working set is 10 kilobits. And we have programs B and C whose working sets are 4 kilobits and 1 kilobits, respectively. Let's say our processor supports four threads simultaneously SMT fashion and has an 8 kilobyte data level 1 cache. In this case, we should run A, B, and C together because all of them can run simultaneously. A and B together, we could run C simultaneously but we choose to run it after we finish A and B. Or we do A and B for a while, then C for a while and so on. So pretty much, we don't run them simultaneously. We run C separately from the other two. And the next option is, we run A without using the other three SMT ways. Then, we run B and C in two of the four SMT ways. But we don't run A together with the other two. The next option is run A and C together, then B, but don't do B simultaneously using multithreading when we are doing A and C. Or, do them one at a time. Although we could run four at a time, we choose to run one at a time. Let's look at the solution to our SMT and cache relationship quiz. We have three programs. Program A has a working set that doesn't fit in the cache. That means that program A will have a lot of cache misses no matter what we do. Program B alone would fit in the cache. Program C alone would fit in the cache. Programs B and C would fit in the cache, but if we run all three together, then the total working set size is 15KB which doesn't fit in the cache, so all of them get poor cache behavior. So really we should not be running all three together. We should not be running A and B together, because then C gets to run alone so we don't benefit from SMT and B runs with A, so B also has a lot of cache misses. We should run A and then B and C. That way, A which has to get poor cache behavior, gets poor cache behavior alone. But B and C benefit from SMT and fit in the cache, so they still get good cache behavior. So we should do this. Just like we shouldn't run B with A, we shouldn't run C with A, and then running A and then B and then C is not benefiting from SMT although B and C would fit in the cache. So that's why this is the only correct answer. In this lesson we learned about the two approaches to writing parallel programs, message passing and shared memory. In the next lesson, we will learn how to correctly do caching when we have multiple cores that are supposed to share memory. There aren't several key concepts that are used in many ways in computer architecture. One of those concepts is called pipelining and it is used in several ways in pretty much every computer nowadays. You should already know what pipelining is and how it is used in to improve processor performance. So this lesson is here mainly to review pipelining in a way that sets the stage for more advanced topics. So let's see what pipelining is about. So let's say we have discovered oil somewhere very far away and we have a gas station somewhere. And now we take a bucket of oil, give it to somebody to carry. Three days later, they are here and they pour the bucket of oil here. Now we have a quarter of a tank of oil. So we need to do this about four times. It's going to take ten days probably for us to have enough oil to fill the car. So clearly, the guy with the bucket approach is not working well for us. What does work, you have a long pipe and you start pumping some oil into the pipe. The front part of the oil moves through the pipe. Takes it several days to travel and after three days, we get the bucket of oil here. So, what's the big deal? The latency is still three days. The big deal is that after we have filled the first bucket three days after we send the oil, the second bucket of oil comes right after it. So, pretty much, once we fill this pipeline with oil, we pump some oil here, and we keep pumping oil in and the oil keeps coming out. So the rate at which oil is coming out is going to be many buckets per day, although it takes three days for the first bucket to get there. So the idea of pipelining really is that although the latency is long for one unit of oil to get there, we can be having many units of oil in progress so that when one comes out, we can very quickly get the next one and the next one and so on. Pretty much we are not moving buckets one at a time. There is many, many buckets traveling at the same time. So now let's try to apply the idea of pipelining to a processor. In a traditional processor, that is, nowaday is too simple so we only use it to teach students, but it still shows how things work. We have a program counter, we use the program counter, to act as the instruction memory. We get the instruction from there. We look at the instruction to decode it, to figure out which type of instruction it is. Meanwhile we could be reading our registers. Once we have read our registers, we can feed them to the ALU, where we are going to do the add or subtract or an XO or whatever the instruction wants us to do. The decoding logic is going to tell the ALU what to do. Once we get the result of the ALU, we could be done and just write the result to the register, or we could have a load or a store instruction. In which case, what the ALU computed is really the address that we use to access our data memory, and what comes out of data memory is what we end up writing to our registers. And then of course there is stuff to do with branches and so on, but basically we can do one instruction per cycle by starting at the BC, fetching the instruction, accessing the registers, doing the operation, accessing the data memory and then writing the result back to registers. So you can see that the instruction kind of goes through these five phases. We fetch the instruction, we read registers and decode, we use the ALU, we access the memory and finally, we write the register. The time to do this might be something like 20 nanoseconds. So now we can do one instruction every 20 nanoseconds. So how do we apply pipelining to this? Well, the idea is that if here we have our fetch, the code, ALU memory and width part of this processor. And here we show what happens in cycle one. We will fetch some instruction I1. In cycle two instruction I1 moves to decode. In cycle C3 instruction I1 will be doing the aerial operation. In cycle C4 I1 will move to do memory, and then in cycle C5 I1 will write the result. And the next cycle we can begin the instruction two. If we do that, then we really will finish one instruction every 20 nanoseconds. If we apply the idea of pipelining here, in cycle one, we are still fetching instruction I1. In cycle two, we will be decoding instruction I1, but we can begin fetching the instruction I2. When instruction I1 move to do the ALU operation, I2 can move to be decoded and I3 can be fetched. In the fourth cycle, I1 is the memory stage, I2 will be doing the ALU operation, I3 will be here being decoded and I4 will be fetched. And then when I1 is in the last part of the pipeline, I2 will be right after it in the memory state. I3 will be doing the ALU operation. I4 will be decoded, and we will be fetching an instruction I5. So now the idea is that once the I1 leaves the pipeline, very soon after that I2 will leave the pipeline. If we divide this 20 nanosecond into five equal pieces for these stages, each stage will be four nanoseconds. And now, the latency to do I1 is till 20 nanoseconds. So it takes 20 nanoseconds to do an instruction. But the throughput will be one instruction, will finish every four nanoseconds because once I1 leaves, that's how long it takes for I2 to leave, and then we just keep pouring instructions out. So, it's similar to the oil pipeline. It takes a while to fill the pipeline, but once we do, instructions keep pouring out at a very high rate. So, let's now apply the idea of pipelining to laundry and let's do a quiz on that. Let's say we have a washer, a dryer and some sort of a folder device for laundry. And let's say the washer takes 1 hour to process a load of laundry. The dryer takes 1 hour to process a load of laundry, and the folder takes 1 hour to fold a load of laundry. Let's say you have 10 loads of laundry that you want to do. With no pipelining, how many hours does it take to process these 10 loads of laundry through this? And with pipelining, how many hours does it take? Let's look at the solution for our laundry pipelining quiz. We have a washer, dryer and folder. Each takes one hour. We have ten loads of laundry. With no pipelining, one load takes three hours to finish. Only then we begin the next load. So it's going to take 30 hours to do them, because we do them one at a time. Each takes three hours. With pipelining, the first load will take one, two, three hours. So, so far we are no better off. However, once the first load is in the folder, the second load is already in the dryer. When the first load leaves the pipeline after three hours, it takes only one hour to do the folding for the next load. So we have, after the initial three hours, one load leaving every hour. So what we have, really, is three hours for the first load, plus one hour for the remaining loads. Overall, it takes only 12 hours to process the ten loads. As you can see, by planning, you can significantly improve the execution time, when we have many things to do. And, when you have instructions, you usually have a lot of them to do. Now that we have applied pipelining to laundry, let's try to apply it to instructions, again, in the form of a quiz. Let's say that we have a processor with a five stage pipeline. And each stage takes one cycle to do. And let's say we have a program with ten instructions. Assuming that these ten instructions can just flow through the pipeline, so that we don't have to worry about whether they need to talk to each other in sound they just kind of flow. With no pipelining how many cycles does it take to finish these ten instructions in this pipeline, and with pipelining how many cycles does it take? Let's see the solution for our instruction pipelining quiz. We have a five-stage pipeline. Each stage takes one clock cycle. We have ten instructions. If we do instructions one at time with no pipelining, each instruction will need to go through all five stages. Takes five cycles to do that. And then the next instruction can go, and takes five cycles to do that, and so on. So we're going to have five cycles per instruction, times ten instructions. It will be 50 cycles until we are done. With pipelining, remember that the first instruction will take five cycles to go through but the next instruction will be one cycle behind. So it will take one cycle to get the next instruction, one cycle to get the one after that, and so on. So the remaining nine instructions finish every cycle. Overall, it takes 14 cycles. As you can see again, we have a significant speed-up from using pipelining. And just like with laundry, don't forget that it takes some initial number of cycles to fill the pipeline. Only then, we can get an instruction leaving every cycle. So the cycles, per instruction, we spent in the pipe line should be one once we have filled our pipe line. Note that we have building instructions to process in a program usually. So the first few cycles are negligible, we can just ignore them and say that in the steady state we're just finishing one instruction every cycle. Is that true, however? So the reasons why we don't always get the flow rate of one are the initial filling of the pipeline. But even with the initial fill, the CPI will get closer and closer to one as the number of instructions grows, and because we have many, many instructions this is not really a big problem. Our CPI could be set to be one in spite of this. The CPI in the steady state is not one also because of pipeline stalls. Think about, for example, a car production line where the first stage is to install the doors. The second stage is to install front wheels, the next stage is to install the rear wheels and there are many more stages. And let's say that at some point we have a black car here, with the front wheels already installed, a purple car here, waiting to install the front wheels and the green car here, still without the doors. It's installing the doors. Let's say that during the installation of the front wheels here, the machine damages the wheel. So now what happens is when the production line can move on because the rear wheels have been installed on the black car, the black car can move on to the next stage, and it has the rear wheels installed,. But we don't want to move the purple car here because it still doesn't have the front wheels. If we let that happen we'll end up with a car that has no front wheels. So the purple cars will stay in this install front wheels stage for one more cycle to have a new set of wheels installed. Because the purple car didn't move on, the worker over here is idle because there is no car to install the rear wheels in this cycle and because the purple car didn't move on, the green car, whose doors have been installed, cannot move on. So we have an idle stage here, too. Let's say that the wheels are correctly installed this time here. Now the purple car moves on. The next stage of the pipeline, however, gets to do nothing because no car has moved on. The green car will now move here. And we will begin a new car here for installing the doors. So the problem is that when the black car leaves the pipeline there will be one car worth of time where we don't finish car and only then the purple car will come. So every one of these breaks in the pipeline, that stall the pipeline, it cannot move on, will result in not finishing a car, when a car should have been finished. If these stalls happen on a regular basis, the CP,I will not be one, it will be lower than one. For example, if every five cars we have this, then really takes, six cycles to finish five cars. So the CPI is six cycles over five cars, and we end up with a CPI of 1.2 not one. So even the steady state pipeline CPI will be larger than one, meaning we don't finish an instruction every cycle because of pipeline stalls. So we have seen how a stall can happen in a car pipeline because of broken wheels or something. But in a processor pipeline how can it be that we have a stall? So what goes broken with an instruction? So, now let's look at the process of pipeline stalls and how they can happen. Let's say we have a five stage pipeline and this is where we fetch the instruction, then we decode it and read registers, then we use the ALU to compute the result of the instruction, then we access memory. This is a loaded store instruction. And then we write the result of the instruction to a register. This is what happens in cycle one, cycle two, cycle three, etc. Supposed we have a load instruction here in our pipeline. And it's going in to R1 something from memory. While this load is in the ALU stage here computing the address. So it hasn't accessed memory yet. Suppose we have an add instruction that needs to use the R1 and this instruction is already reading registers here so it can compute the result in the next stage, and we have a subsequence instruction here, let's say, another add that adds our 2 to 1 and produces our 3. So now the problem is that this ADD here, is reading the wrong value of the register. So, if we let it proceed further, because in the next cycle, this load will be here and now, it's actually accessing memory. But by then, it's too late. This ADD, if it moves here will use the value of R1 that is the wrong one. It's the value of the R1 before the load actually produced a value and this is where we get a processor stall. Pretty much what needs to happen is, the add needs to stay here until it gets the correct value of R1 from the load. Because the add is stuck here, the next instruction also needs to stay where it is, because it cannot move here. That means that in our pipeline, now we have a bubble, just like we did in the production line. We can have more than one bubble because of a single dependence between instructions. So in this example, next cycle, the load has loaded the value from memory into R1 and is now writing that value to a register. The ADD needs to repeat the read of R1 because in this cycle here, it still didn't get the correct value of R1. Although the load was now loading that value from memory the R1 was not written yet. So, now the load is writing, the ADD repeats the read and because the ADD is still here, the next instruction is still stuck here and that means that we have two cycles worth of stall. In this cycle it could be that for example the load is writing to R1 in the first half of the cycle and this ADD is reading R1 in the second half of the cycle. So next cycle, the ADD can move on. But it could be that the load actually takes a full cycle to write the result, in which case we will have another cycle stalling. So let's suppose that we can do this, that we can write R1 and then read it in the same cycle. If that is the case, the next cycle, which is cycle four, we will have the load leaves the pipeline. The pipeline bubble here moves forward in the pipeline, this pipeline bubble moves forward in the pipeline and now the ad finally moves on. So the next instruction after this side can move on. We can fetch the next instruction, and so on. So as you can see here. A processor pipeline stall creates a problem because normally every cycle after the load leaves, every cycle will have the next instruction leave, but in this case after the load leaves, there are two cycles worth of pipeline bubble that finish before the next instruction. So, really instead of having just one cycle for the load, one cycle for the add and so on to finish, we now have one cycle for the load, 2 cycles worth of not finishing anything and then the ADD and so on and that causes our pipeline to have a CPI of more than 1. If we count the 3 instructions, 1, 2, 3 instructions we'll finish over 5 cycles. Three cycles for them and 2 cycles for the bubble. So really, our CPI is significantly larger than one. A processor pipeline may also need to be flushed. This is an example that we didn't have in our car assembly line. Suppose that the instruction that we fetch here is a jump instruction, next cycle this jump moves here. The problem is we fetched this jump but in this cycle here, we didn't even know what it is. So when it moves here, the next instruction we fetch will be the one that follows this jump in memory. So we will have, for example, a subtract that never should have been fetched because we should have fetched from the location where the jump is going. But at this point where we needed to decide what to fetch, we didn't even know that it's a jump. Meanwhile, we are decoding the jump. So at the end of this cycle, we will know it's a jump, but we may not know where it's going. So now, let's say that here we figure out that this is a jump and where it's going. Meanwhile, our subtract has moved here. Something else has been fetched from the location where we shouldn't have been fetching things at all. And now we figure out that really we should be fetching from somewhere else. So what happens now is called a pipeline flush. What we do is we convert this fetched instructions into pipeline bubbles. And then, as the jump moves on, these two bubbles will also move on. And now we will fetch the correct instruction that should have been fetched right after the jump. So as you can see, a jump in this case, because it caused the pipeline flush, has two bubbles that follow it. And only then we have the next instruction. So if we count for these two instructions, what the CPI is, it's actually four cycles for only the two instructions. So as you can see, these pipeline flushes are going to be an additional source of having a CPI larger than one. So the branch and jump problems that we have seen in the previous segment are really due to what is called a control dependence. Let's say that we have a program like this. We have an add, then we have a branch that compares R1 and R3 and jumps to the label if they are equal. This is the branch equal. Then we have and add and subtract here, and we have a multiply here, and they're followed by. Additional instructions. We say that these instructions have a control dependence on the branch. Whether these instructions should be executed at all or not depends on the result of this branch instruction. Similarly, these instructions. Also have a dependence on the branch. So pretty much once we have a branch, all subsequent instructions that we execute have a control dependence on the branch. This really means that we don't know whether we should be even fetching these instructions until we figure out the branch. So let's see what control dependencies like this do to the CPI of our five-stage pipeline. On average, about 20% of all instructions are branches and jumps. So, about one in five instructions will be a branch or a jump. We also know that slightly more than 50% of branch and jump instructions are what we call taken. Meaning they go somewhere else and don't just continue. Putting these two together means that for half of these instructions we will be fetching the wrong instructions. So our CPI will be one, this is the ideal CPI, plus 10% of the time we waste some number of cycles because of a mispredicted branch. Basically we fetched the wrong instructions. How many cycles we waste? Well, in our five stage pipeline, if the third stage is where we figure out the branches, once we do that, and we determine that we have been wrong about what we fetched, we have to cancel the two instructions that we already fetched. So there will be two cycles of not finishing an instruction. After such a branch, so what we have is basically every instruction spins one cycle and 10% of the time the instruction is followed by two idol cycles, so the CPI overall will be 1.2 in this example. If we have a deeper pipeline that figures out the branches in a later stage, this penalty for guessing the branch wrong will be larger. If we have more branches, or more of them are taken, then this percentage of instructions that result in this penalty will be larger. If we are somehow able to guess the outcomes of branches and fetch the right instructions most of the time, this percentage here could be a lot smaller than this. We will see later a technique called branch prediction that tries to guess more accurately where the branches are going, and that dramatically reduces this percentage. We will also see that modern pipelines tend to be deeper than just five stages which will increase this penalty. The two are related if the pipeline is deeper, and you have more of a penalty. You need better branch prediction to lower this in order to achieve a CPI that is good. Let's do now a quiz on how control dependencies affect the CPI of a pipeline. Let's say that 25% of all of the instructions we execute are taken branches and jumps. Let's say we have a ten-stage pipeline. Let's say that the correct taken address for branches and jumps is computed in the sixth stage of that pipeline. And let's say that everything else flows smoothly so the CPI would be one, if not for the taken branches. The question for you is, what is the actual CPI in light of there being taken branches? Let's look at the solution for our control dependence quiz in a pipeline. We know how many instructions are taken branches. We don't really care that it's a ten stage pipeline. What we care about is that the branches are resolved in the sixth stage and that everything else works smoothly. So the actual CPI will be one per instruction. This happens for branches and all the other instructions. Plus for every taken branch, and that happens 25% of the instructions, we have a penalty of five cycles. Why? Because when we resolve the branch in the sixth stage. They have already been five stages worth of instructions. That we fetched from the wrong place and we have to throw them away. So following a branch like this there will be five cycles in which we don't finish anything, so we spend one cycle per instruction and for 25% of instructions. We spend additional five cycles for that instruction. Basically there are five empty cycles after it. If you multiply this out you get 2.25. As you can see, this CPI is very far from the ideal. Our processor is less than half as fast as it would be without the taken branches. But wait there's more. We also have data dependencies that can cause pipeline stalls. Consider these two instructions, we add R2 and R3, put the result in R1. Then the subtract instruction here takes R1 and uses it to compute R7. We say that the subtract instruction has a data dependence on this add. In fact we can have this data dependence, and there could be another instruction that produces R8, that the subtract also depends on. When we have a data dependence that means that we cannot just do these instructions like they were loads of laundry that have nothing to do with each other. We have to have the add produce R1 before the subtract can use it. This type of data dependence is called a read after write dependence. We read something after it has been written and that order needs to be preserved. This type of dependence is also called flow dependence because the data flows from one instruction to the other. And it also called true dependence because there is really a dependence as in the subtract needs the add to finish before it because the value doesn't exist until the add is finished. So again, this type of dependence is called RAW or read after write, or flow, or true dependence. There are two other types of data dependencies. Let's say that there is, later, a multiply instruction that takes R5 and R6, and produces R1. So that later instructions will use that R1 as the result of the multiplication. So for this dependence we say that's a RAW, flow, or true dependence. There is a dependence between the add and the multiply. The add needs to finish first and then the multiply needs to finish. Because we want the result of the multiply to end up as the final value of R1. If we let the add finish later than R1, produced by the add, will be left in the R1, so subsequent instructions would see that value which would be wrong. This type of dependence is called a write after write, or an output dependence. Basically the final result, or the output for R1, is what's affected and write after write simply because they are both writing and we need to preserve the order of the writes. This write should happen after this write. The third type of dependence is here. The subtract needs to finish reading R1 before multiply can be allowed to overwrite it. This type of dependence is called write after read because the write needs to happen after this read, and it is also called anti-dependence because it sort of reverses the order of the flow dependencies. For reasons we will see later, these two types of dependencies are called false or name dependencies. So we have true dependencies, which are these RAW dependencies or flow dependencies, and we have false or name dependencies that have to do with overwriting a value that somebody has written or read previously, or at least should read or write previously. Now we have a read after write, write after write, and write after read dependence. So, how about a read after read dependence, that's the only one that is missing here. Let's say we have two adds, both of which are reading R3. Read after read is not a dependence because these two instructions don't need to be done in the order of that dependence. Pretty much we can reorder these two instructions, they will still read the correct value of R3, so that is why this is not a dependence. The second instruction doesn't depend on the first one. It can be done before or after it. Now that you have seen what data dependencies look like, let's do a data dependencies quiz to see if you can find data dependencies in the program. Let's say that these are four instructions in our program. Instruction one multiplies R2 and R3, puts the result in R1. Instruction two takes R4 and R1, adds them up, and puts the result in R4. Instruction three multiplies R5 and R6, puts the result in R1. And the subtract here, instruction four, subtracts R1 from R4 and puts the result back in R4. You need to indicate whether there are any dependencies between instruction I1 and I2. For each of these three types of dependencies, is there such a dependence? You need to do that also for I1 to I3, I1 to I4. So is there a dependence from I1 to I4? And from I2 to I3. To help you out, I'm going to tell you that there is a read after write dependence here. That there is no write after read dependence from I1 to I2. And that there is no write after write dependence. I'm also going to tell you that there is no read after write dependence here. But you need to figure out the remaining eight dependencies. Put a check mark in each of these boxes if there is such a dependence between these instructions. Okay, let's look at the solution for our data dependencies quiz. We have these four instructions. And for each type of dependence, we need to figure whether these pairs of instructions have these dependencies. Let's start I1 to I3. We already know that there is no read off the write dependence. That is because I3 doesn't use the R1 value produced by the I1. Right after read, would mean that I3's writing a result into a register that was supposed to be read by I1 which is not true. Write after write would mean that I3 is overwriting the result by I1 and that is true. Now let's go from I1 to I4. I1 is producing R1. I4 is reading R1, but the value that I4 is supposed to read here, is not the one produced by I1. It is the value produced by I3. So in fact, there is no raw dependence from I1 to I4. That would occur only if I4 was reading R1 that was produced by I1, which is not happening. Is there a right after independence? Is I4 over-writing R4 that was supposed to be read by I1? No, because I1 is not reading R4. Finally, is there a write after write dependence? No, for two reasons. One is that I4 is writing to a different register than I1, so there cannot be an output dependence. Another reason is that even if I4 was writing to R1 that would be an output dependence between I3 and I4. So there would be an output dependence from I1 to I3 and then from I3 to I4, but because the right is happening to R4 and here to R1, certainly there is no write after write dependence. Finally, let us look at I2 to I3. Is there a read after write dependence? Is I2 producing a register that is read by I3? No, it is not. Write after read would mean that I3 is overwriting a register that I2 needed to read, and that is true, because I2 needs to read R1 before I3 overwrites it. So we have a write after read dependence. And finally, we do not have a write after write because there are different registers for writing. So this is, again, not selected. So the only check marks you were supposed to put in are this one and this one. Now, let's discuss the relationship between the dependencies we have seen and what we call hazards in our pipeline. A dependence is the property of the program alone. We can just check whether two instructions have a dependence between them without any regard to what the pipeline looks like. In a particular pipeline, some dependencies will cause problems, potentially, and some dependencies can not cause problems no matter what. Let's look at an example of this. Let's say we have the classical five stage pipeline with fetch, decode, ALU, memory, and write stages. And let's consider two instructions like this that have an output dependence. In this particular pipeline, I will show you that this output dependence is never a problem. The ADD will move through this pipeline, until the right stage, and that is when it will update R1. So let's look at the cycle while the ADD is here, and the multiply is following behind it. At this point, the multiplication has been fetched, has been decoded, and read R4 and R5. If you read the correct value, because these are not affected by the ADD. And now, it's just sitting in a MEM stage, because it's not a memory operation. It has the value that it will deposit in R1. When the ADD leaves the pipeline in the next cycle, the multiply will get here and have its write of R1. So, as you can see, these instructions always write to R1 in the correct order. So the fact that there is this output dependence doesn't mean that we have to worry about it in this pipeline. This dependence will be naturally satisfied in this pipeline because the writes by different instructions are always occurring in program order in this pipeline. Now let's consider a write after read dependence. Let's say that there is a subtract instruction here that overwrites R4 that was read by the multiply. At the point where the multiply reads R4, the subtract is being fetched. So the multiply reads the value of R4, that was not affected by the subtract. Later on when the multiply is done, the subtract will finally write to R4. And that again means that the fact that there is a dependence between the read and the write in the consecutive instructions. Is never a problem for our pipeline because the read of R4 in the multiply occurs many cycles before the subtract overrides that value. So again, we have a dependence that cannot become a problem in our pipeline. Now let's look at what happens when there is a true dependence in this pipeline. In this case R4 here should be used by the next instruction. The divide is reading R4 in a cycle where the subtract is here. Clearly, at that time subtract didn't put its R4 value in the register R4. It is actually just computing that value so when the divide reads register four it's reading a stale value for four that existed before the subtract. Several cycles later when the subtract is writing to that register the divide has already computed its value and the next cycle divide will write that bogus value into the register. So pretty much it divide, read the wrong value R4, computed with that wrong value, and then eventually deposits that wrong value into a register. So clearly, this true dependence here is a problem in our pipeline. If we just let things go and flow through the pipeline, we will have the wrong result deposited into R10 by the divide. The situation when our dependence can cause a problem in a pipeline is called a hazard. So a hazard occurs when a dependence results in incorrect execution of one or more instructions. Hazards are a property of both the program, because you cannot have a hazard unless you have a dependence, but also of the pipeline itself. How the dependence is interacting with the pipeline. So for example we have seen that output and anti-dependencies in this particular pipeline cannot become hazardous. But true dependencies can. Now I will show you that not all true dependencies are hazards in this pipeline. In this case, we have seen that a dependence between consecutive instructions creates a hazard in this pipeline. Let's say that the write to R7 occurs in this multiply instruction, and that we now have an XOR that uses R1 and R7. Let's now see whether there is a dependence between the ADD and XOR. Yes, there is. R1 produced by the ADD should be used by the XOR. And there is also dependence between the multiply and the XOR. But let's see if these dependencies are hazards in this particular pipeline. When the XOR is in the decode stage, the divide is in the ALU stage, the subtract is in the MEM stage, and the multiply is in the write stage, and the ADD has left the pipeline. It was writing to R1 in the previous cycle. So as you can see, the dependence between the ADD and the XOR is not a hazard in this pipeline. Pretty much in this pipeline, if there are three or more instructions between the producing and the consuming instruction, then the consuming instruction reads the register after the producing one has already written it. However, the dependents on R7 between the multiply and XOR is potentially a problem in this pipeline, because the XOR is reading R7 in the cycle during which multiply is writing it to the register. So if this write completes near the end of the clock cycle and this read is happening near the beginning of the clock cycle, the XOR will not read the R7 produced by the multiply. In this pipeline we can tell that a true dependence is not a hazard if there is three or more instructions between the producing and consuming instruction. But it is a hazard if there is fewer then three instructions between them. In this case there are only two instructions in between and that is why we still have a hazard. Now let's do a quiz on the relationship between dependencies and hazards. Let's say that we have a three-stage pipeline, where here we are fetching and encoding the instructions. Here, we are reading the register inputs that we need and doing the ALU operation. And here we're accessing the memory and then writing the result to the register. For an ALU operation we just write to the register. Let's say that we have these four instructions The first one is producing R1, and the next three are using it. And I can tell you that these three dependencies are the only dependencies that exist in this program. The question for you is whether there is a dependence from the add to this instruction, and whether that dependence is a hazard. Put a check mark into these six boxes, depending on which one has a dependence, and which one of these dependents is a hazard. Now let's discuss the solution to our dependencies and hazards quiz. First, let's figure out the dependencies. Again, dependencies are a property of the program alone so we don't have to worry about the pipeline yet. Each of these instructions has a dependence on R1 from the add to that instruction. Which of these dependencies or hazards does depend on the pipeline? To check whether the dependence is a hazard, we need to figure out when does the read of the register happen and when does it get written. In the stage where the subtract is reading the register, the add is still writing it, so the subtract may not get the value from the add, so there is a hazard here. In the cycle when the divide is reading the register R1, the subtract is here and the add has left the pipeline, which means the divide is reading the value of R1 that was already written by the add. So there is no hazard here and, of course, the multiply reads the same register one cycle after the divide so it also has no hazard. Now that we have seen that some dependencies can become hazards in our pipeline, now we need to see how to handle these hazards, so that we still get correct execution in our pipeline. because we cannot just allow the pipeline to produce incorrect results. We need to handle only the situations that are hazards. We don't really care about dependencies that can never become hazards. In fact, what we need to do is detect hazard situations, and then either flash dependent instructions from the pipeline, stall dependent instructions in the pipeline, so that the read is delayed and they get the correct value, or even fix values that were read already by the dependent instructions, so that we feed them the right value and they proceed unharmed through the pipeline. We need to use flushes for control dependencies, because the instructions we have in the pipeline after a controlled dependence becomes a hazard, are the wrong instructions. So delaying them or trying to fix the values that they are reading is not going to work. Pretty much, we have to delete these instructions from the pipeline and fetch the correct ones, and then execute them from scratch. So, we need to flush the dependent instructions on a control dependence. If we have a data dependence, we can stall the subtract, so that it stays in the stage where it would read R1. Until, finally, the add deposits that value in the register. So, for hazards caused by data dependencies, we do not have to flush, we can simply stall. For some data dependencies, we can even forward the value of R1 produced by the add, to the subtract in a way that fixes the value read by the subtract. Let's consider our five-stage pipeline and just worry about the stage where we read the registers and the ALU stage. In a cycle where the subtract is reading the registers, the add is in the ALU stage. At the end of this cycle, the subtract has read the wrong register value for R1 because R1 still doesn't have what add will eventually write to it. But the add at the end of this cycle has the value, it's just that it didn't put it in R1 yet. What we now can do is at the beginning of the following cycle, subtract can get the value that the add produced, and that value replaces the value that we read from R1 so that subtract computes with the correct value. So pretty much, if the value exists somewhere in the pipeline, at the point before we actually use the value to compute, we can fix what we have read so that we still compute with the right value. This is called forwarding, and it amounts to pretty much fixing the values read by dependent instructions. Forwarding does not always work. Sometimes the value we need will be produced at the later point in time so at the time we need it to compute, we still don't even have that value in the pipeline. At that time we need to stall instead of forwarding. So basically, for control hazards we need to flush. For some data hazards we can either stall it forward, and we prefer forwarding because it doesn't introduce stalls in the pipeline. And for some data hazards, at least some stalls are necessary. Now that we have seen flush stall and forwarding as a solution for hazards in a pipeline let's do a quiz to see if you know when each should be used. Let us say that we have a five-stage pipeline where the first stage fetches the instruction, the second stage calls the instruction and reads the registers. The third stage, the instruction does the ALU operation and a branch is the result so we know where its going. In the mem stage loads and stores access memory, so at the end of this stage a load will finally have a value. And finally the fifth stage writes the result to the register. Here, we have some hazards. First, there is a control hazard. After the branch we fetch two wrong instructions; before we fetch the correct multiply instruction in this case. So the red ones are the ones that are fetched incorrectly. The question for you is should we flush, stall or forward here? Put a check mark in each of these three boxes depending on which one we do here. Next, we have a data dependence where the multiply is producing the R1 result and the load needs to use that as the memory address. Put a check mark in each of these three boxes depending on whether we need to flush, stall, or forward the value in this case. And finally, the load is producing the value of R1, that this add needs to use, so the question for you is should we flush, stall, or forward in this case? Note that in some cases, more than one of these needs to be used for optimal performance. To help you get started I'm going to tell you that for data dependencies, as we said, we never want to use flushing, so the real question is just should you stall or forward? Okay, let's discuss the solution for our flushes, stalls and forwarding quiz. We have this five stage pipeline. We said already that for control dependencies, we have to use flushes. We have fetched the wrong instructions. Delaying them or trying to fix the values they read is not going to help us. So here we have to flush, and we don't stall or forward. For this state of dependence, at the time when the load is reading the R1 here, the multiply is producing the result here. The multiply has the result at the end of that cycle, and the load will need to compute the address at the beginning of the next cycle. So here, we can forward the value of R1 from the multiply to load, and don't have to stall. Again, we don't want to use flashes here. The final question is whether the load's result of R1, which needs to be fed to the add here, can be forwarded, or do we have to stall? When the add is reading R1 here, the load is here computing the address. It still has not accessed memory. In the next cycle the add is computing using the wrong value for one, while the load is still loading memory. So the load, only at the end of this cycle, has the proper value for one. Whereas the ADD, at the end of that cycle, has already computed the wrong result. So in this case, we have to stall. Interesting enough, we will actually, after that one stall cycle, forward to avoid stalling more. So we here, we have to have a stall, and we can after the stall, do forwarding or just stall more. But this is one of those situations where forwarding can not help you. You have to stall for a data dependence. Now that we have seen how pipelines work and how they are affected by hazards and dependencies, let's talk about how many stages a pipeline should have. In the pipelines we have seen so far, the ideal CPI is one. We will later, in this course, see pipelines where the ideal CPI will be larger than one because they try to do more than one instruction per cycle. But regardless, there is some ideal CPI that the pipeline should be achieving, and it boils down to executing some number of instructions every single cycle. Now let's consider what happens when we add more stages to the pipeline. We get more hazards this way. We have already seen that some of the dependencies caused a number of stalls to flush cycles that depend on how deep the pipeline is; for example, if a branch is resolved in the second or third stage of the pipeline, we only have to flush one or two instructions that we have incorrectly fetched. If the pipeline involves branches in something like 15 cycle, then we will flush 14 instructions when have a branch that we incorrectly resolve. So we have more hazards when we have more stages. And what that does, is it adds more of that CPI penalty. So the CPI increases because of that penalty that we have when we have a hazard. But with more stages, there is less work per stage, which means that our cycle time can be decreased. Less work per stage means that every cycle there is less work to do, which means we can do it in less time, so we can speed up our clock. We already know the iron law, which says that the execution time is equal to the number of instructions times the CPI times the cycle time. If we have the same number of instructions because we're just trying to figure out for the same instruction set whether we should have a processor that has more or fewer stages. If we increase the number of stages, our CPI goes up, which increases execution time, but the cycle time drops, which decreases execution time. So the number of stages needs to be chosen in a way that balances the CPI and the cycle time; you don't want to have to few stages because then your cycle time will be too long, but you don't want to have too many stages because then your CPI will be too high. We can plot the number of stages here and how execution time depends on it, and this is what it looks like. As you increase the number of stages, the execution time drops because you still don't have too many hazards. But you're improving the clock cycle time. At some point, you get fewer benefits from reducing the cycle time, and the CPI increase starts to hurt. Beyond some point, you are no longer shortening the cycle time very much, but you're suffering a lot from hazards that are now causing dependencies that cause many, many cycles of stores or flushes. So the bottom of this curve, for modern processors, would be at be at something like 30 to 40 stages, which would be a very, very deep pipeline. So this would be if you only consider performance. So for performance, we want a pipeline that is 30 to 40 stages deep, but we need to consider power. If we have more stages, we are shortening the cycle time, so we have more cycles per second. And in each of those cycles, we now have more pipeline latches that spend energy latching temporary results. So power quickly increases with the number of stages. If we plot power, it does something like this. So because of performance and power considerations, we want to choose something that is not in this performance optimal point. We want something that is also power efficient, so we end up with 10 to 15 stages in modern processors. So again, this is not because performance is best when you have this many stages. It's because it's the number of stages that balances pretty good performance with relatively low power consumption. Or at least manageable power consumption. We now know how pipelines work and how they need to balance the increasing clock frequency versus the stalls and flushes caused by hazards. Nowadays almost every processor uses pipelining. So, we will use our pipelining knowledge over and over as we look at some of the more advanced topics in the next few lessons. In this lesson we continue to study control hazards. We already know about branch vectors, but we also know that some branches are really difficult to predict, even when we have a sophisticated branch predictor. So, in this lesson, we will see how the compiler can help us completely avoid some of the hard to predict branches Predication, is another way that we can try to deal with control hazards, Unlike branch prediction. when we are really trying to guess which way, the if-then-else or a bloop or something branch will be going, predication, is another way for dealing with control dependencies. Brown friction, Is about guessing, where the program is going., Usually there is no penalty for being correct in branch prediction, pretty much we just keep fetching, as if the branch wasn't even there. Even if the branch is actually taken, we can just keep fetching from the right place and thus avoid any penalty. But the problem with branch prediction, Is that in modern processor, there is a huge penalty if we are wrong in predicting. Keep in mind that modern processors have a deep bite, with lots of stages, before the one in which we figure out the mass prediction, And in each stage, we have a number of instructions that we have been doing. So, in this prediction, results in throwing away all of that work. Usually, at least several tens, of instructions worth of work, So the penalty is usually 0 instructions here, and if we have something like, maybe, a 12 stage pipeline,with four instructions, Per cycle. We're talking about something like 48 possible instructions or so, so let's say about 50 instructions, worth of work wasted. Predication is about doing work, of the both of the directions,so we basically do the work of both the taken and not taken direction of a branch, that way the waste is up to 50% of the work that we have been doing. Because at the position where the branch was supposed to be, about half of the stuff we've been fetching. could have been from one path, half of the stuff was from the other and only half of that work will actually be used, the rest will be thrown away. And then we simply throw away the work from the wrong path, So, what is good about this? It seems like here, if we're correct, We lose nothing, if we are wrong we lose a lot, but we can very accurate prediction. While here, either way, we lose quite a bit. Well, let's see how the branch prediction and something like predication,compare on different types of condition branches we might want to predict. For a loop, usually we want to do branch prediction, Loop branches, the more iterations we have, the more they are predictable. Whereas, with predication, in each iteration of the loop, we will be dividing our work into two. One is do the next iteration, the other one is do the work after the loop, So, if we have 1,000 iterations, very little of the work will end up being done correctly. Simply the branch is diverging too much, When we need to decide whether to stay in the loop or not, we will be starting the work of both of these. One, path goes after the loop, the other one,goes back into the loop and then here we have a decision again to follow that path,or stay in the loop that path, stay in the loop. So, after awhile if we're still doing the work of all of these paths concurrently, a very small fraction, of the work we are doing ends up being the staying in the loop work. And a lot of it has to do with various variants of exiting the loop, in each iteration, So for a loop, we want to predict function call returns, Same thing. Except, predication really makes no sense, because calls and returns. Always go there, so somehow the direction of not going there makes no sense to do so we want to predict. Let's say no we have a large if then else, so what happens we have a decision and based on it we do,this or we do this, Now the question is? Should we try to predict this decision or predicate so that we are doing the work of both and eventually, they will merge and re-continue, and the answer here, is that if we predict, we waste up to 50 instructions, More likely we wasted nothing. If we predicate and these branches, have equal wave then, let's say we have 100 instructions here and 100 instructions here, We will execute 200 instructions, waste 100 of them either way. To somehow, the amount of waste with predication here, Is larger than even the penalty of branch misprediction. Pretty much, if we predict, and we are right, we lose nothing, If we predict, and we are wrong, we lose 50 instructions, Here, we lose 100 instructions. And we cannot be wrong or right. But either iii way, we are losing 100 inst,ructions, So for a large if-then-else it seems like we should be predicting, but let's see, how things look like for a small if-then-else. Let's say that we have a decision to make, and depending on the decision, we have the, Then or the L spot, but let's say each is something like five instructions long, now it becomes interesting. If we do predication, we will be executing, ten instructions here followed by instructions that we will do anyway, so the waste is five instructions either way. Here we're predicting, and if we're right we waste nothing but if we're wrong we waste 50, instructions. So, our comparison is that 100% of the time, we waste five instructions with predications, so we end up with five on average. For instance of this, whereas here,our branch prediction rate determines how often we waste this 50 so it's going to be,lets say 0 point 10 because it's 10%, probability of mis predicting in which case we end up at five two. So if our accuracy of prediction is 90% or better, then in this case we are better off predicting the branch. But if our accuracy is less than 90%, we are better off predicating. If, if then else, is even smaller, then it biases more in favor of predication, and less in favor of branch prediction. So for really small, if then elses, We want to predicate unless we are reasonably sure, that the branch prediction is going to be highly accurate. So before we go into how predication works in hardware, let's first look at the technique called if-conversion, which is how the compiler creates the code that will be executed along both paths. So let's say we have a relatively small if-then-else here, where depending on the condition, x gets either the value from array of i, or array from j, and y gets either incremented or decremented. If-conversion, transforms this code into the work of both paths, followed by a selection of some sort, between the results of the two paths. But there is still a question of how do we do this. If we convert this into some sort of a conditional expression that still branches based on the condition and then does the move of x2 into x, otherwise, does the move of x into x1, then we really haven't done much. Because we converted one branch into another branch and now we have this branch twice. So unless we have some sort of a correlating predictor like, for example, the global history predictor, we will possibly have two missed predictions here. Instead of the one we might have had. So if this is all you can do, then you don't do if-conversion. What we need for if-conversion to work is some sort of a move instruction that will do something like move x1 into x, if a flag is true. So the simplest form of predication supporting hardware will be a conditional move instruction. In the MIPS instruction set, for example, there is a MOVZ instruction that takes two sources and the destination register. And the way this works is that this instructor compares Rt to 0. And if it is 0, then changes Rd to be equal to Rs, otherwise leaves Rd alone. Note that there is no branch here anymore because this is a single instruction. MIPS also has a MOVN instruction that works exactly the same way, except it moves Rs into Rd only if Rt is not 0. So, we would implement our x equals depending on condition x1 or x2 by doing something like using comparisons or whatever, to put the result of the condition into, let's say, R3. Then R1 would be our whatever x1 expression we wanted to implement. R2 would be our x2. And then we will do, MOVN x, R1, R3. MOVZ X, R2, R3. And this puts one of these two into X, depending on whether R3 was true or false. Which means, depending on the condition. The x86 instruction set has a whole set of so called CMOV instructions, so for example it has CMOVZ, CMOVNZ, CMOV greater than etc, where the condition here is determined by the flags. And all of these instructions effectively implement the, if the condition codes correspond to one of these conditions, depending on the instruction, then the destination register gets the value of the source register, otherwise the destination register is not modified. So, you can see how we can implement similar behavior here. Except maybe we don't even need to put the condition in a register because, probably we can just test the condition and then, do a greater than, less than and so on on it. [BLANK_AUDIO] So, now that we have seen how the conditional MOV instructions work, let's do a quiz about how to use them. Let's say that the code we have is this. If R1 is zero, we branch here and increment R3. Otherwise, we stay here and increment R2, and then either way, we go to the end. After if conversion, we will have something like add one to R2 and put the result in R4. Add 1 to R3 and put the result in R5, followed by what instruction in order to properly make this equivalent to this code. So, let's look at the solution to our MOVZ/MOVN QUIZ. We have this code. And we want to do if conversion of it, so that we get a branch-free code that does the same work. If conversion amounts to doing the work of both paths, both the not-taken and the taken paths. And then selecting between the two and correcting the results so that only one of them is active. So, what we have done is, these two instructions are already doing the work of the not-taken path. Is just that instead of in R2, which would permanently modify R2, we're putting the result in R4 and here the taken path R3 and 1 are added, then we put the result in R5 instead of R3. So what now needs to happen is, if this was the correct path to take, then R2 should get the result of R4. Otherwise, R3 should get the result from R5. This is a branch on equal 0, R1. So if R1 is not equal 0, meaning MOVN on R1 used as a condition. We're going to be moving R4 into R2. If the branch is branching, then R1 is zero, and in that case we want to be moving R5 into R3. And this is our if converted code that has four instructions worth of work, but no branches in them. In contrast, the original code has three instructions worth of work, in this path, and two instructions worth of work in this path. But, it has this branch that might be difficult to predict. So now let's look at the performance of MOVZ and MOVN type of if conversion. In the previous quiz we have already seen that branch code like this can be if converted into code like this. Let's suppose that this branch here is perfectly predicted, so all we have to worry about is this one. And let's say that this branch is 80% accurately predicted. And let's say there is a 40 instruction penalty on missed predictions. In that case, we have, here, this costs us four instructions. This costs us three instructions on this path, or two instructions on this path, let's say it's a very difficult to predict branch. So it's not biased, in which case we have 2.5 instructions all the time, plus 20% of the time, we have a 40 instruction panel. This gives us on average 10.5 instructions worth of work on this, versus four here, so obviously, that if converted code is more optimal. In fact, let's do a quiz on MOV Z and MOV N performance when we do If conversions. So this is the original code. Let's say that this branch here is 50% of the time taken. So half of the time we end up doing this, and half of the time we end up doing this. So we can say then that the if conversion code works better, when the prediction accuracy is below, what? Assuming a 30 instruction misprediction penalty for this branch, and assuming of course that this branch is always perfectly predicted, because it's easy. Now let's look at the solution for our MOVZ MOVN Performance Quiz. The fact that this branch is 50% taken means that on average, we execute half of the time three instructions and half of the time two instructions. So if we perfectly predict this, we will be executing 2.5 instructions on average per execution of this code. In contrast here, we get four instructions, but we cannot have mis-predictions. However, here we have our misprediction rate times a 30 instruction penalty. And if the misprediction is such that these two are equal, then this would be equal to 4. Then, our x, the misprediction rate, needs to be 4 minus 2.5, leaves us with 1.5 instructions worth of advantage here. And 30 instructions worth of disadvantage if we mispredict. So in order for them to balance, the misprediction rate needs to be 0.05, which amounts to 5%. So pretty much, if the prediction accuracy is below 95%, then if converted code is better. If the prediction accuracy in this case is above 95%, then the original code is better. If the misprediction accuracy is exactly 95%, then the two have equal performance. So, to summarize, the approach where we have the conditional move instructions, we need the compiler support to do an if conversion. If the compiler doesn't do anything, the code will be just left with a branch. So this is not a fully backwards-compatible approach in that if we add the conditional move instructions to the ISA, we're only allowing the compiler to take advantage of that. But old code will still not use these conditional instructions. This approach will remove hard to predict branches. So if we have a hard to predict branch, for example, through profiling we determine that a particular branch is difficult to predict, we can perform if conversion to get rid of it. If conversion typically requires use of more registers than the original code. Because we need to keep all the results from both of the paths, and also, more instructions will be executed as a result of if conversion because we're executing instructions from both paths, not just one. And also because we need to add the conditional move instructions to select the actual results, the ones we want to keep, from among the results that we have generated from both paths. Some of these are necessary in order to do if conversion. For example, the compiler support is okay, we need that. It will remove the conditional branches that are hard to predict, so that, we want to stay the way it is. But the need for more registers, for example. Can we do something about that? More instructions are executed because we execute both paths. This again is okay, that's how we do if conversion, but do we really have to have additional instructions just to select the results. To get rid of this and this problem, we can change our ISA so that every instruction is now conditional. Every instruction computes its result and then writes it or doesn't write it to the destination register, depending on whether a condition is true or not. This approach is called full predication. And it requires extensive support in the instruction set. So let's compare what we need for the conditional moves and what we need for full predication. So for the move instructions, if they're conditional, we need a separate opcode that tells us this is a conditional instruction. Usually, we have a separate opcode for each particular condition. So we need a number of new opcodes. For full predication we use the same instructions that we use without conditions. We add condition bits for full predication. However, we add condition bits to every instruction. So every instructor word contains some bits that tell us what is the condition for writing to the destination register. For example, the itanium instructions that has full predication. And this is what an add instruction would look like. It specifies the two source registers, the destination registers, that this is an add, and it has a field within the instruction to specify the so-called qualifying predicate. That is where the condition for the link, the write or the r1 register will be found in. The titanium instruction has 41 bits, where the least significant six bits specify what the qualifying predicate is. The predicates in the itanium are actually small one bit registers, so that you can do a condition check and store the result in this 1-bit register. And then really the six bits here tell you which of the 64-bit conditional registers are you going to use to determine whether to write to R1. So let's say we have this code where again R1 is compared to 0, and we branch here if it's equal to 0. If it's not equal to 0, we do this and branch out. If it is equal to 0, we do this and continue. We have seen that this code results in four instruction's worth of execution, when we have the conditionals MOVs. Two instructions that actually do the work of ADDIs. And two instructions that conditionally write results back to R2 and R3. With full predication, a la the Itanium, we get the condition check is done using the predicate set instruction that compares equal zero on R1 and sets P1 and P2 accordingly. The predicates are always set opposite of each other so that if R1 is zero P1 is set to true and P2 to false. If R1 is not equal to 0 then P1 is set to false and P2 is set to true. Now we can simply do the add i of 1 plus r2 into r2. But we will predicate it with p2, so that it only gets done if r1 was not equal to 0. And then, this instruction becomes exactly what it was here. Except that now it's predicated so that it only gets done if r1 is equal to 0. So as you can see with this, after we do this initial instruction that simply sets the predicates, any code that was here simply gets predicated with one of the conditions. The opposite condition predicates the other branch's code. And that means that first of all we can use the same registers that we used before. We don't need the additional R4 and R5 that we had in the previous example. And second, we still do have the work of both parts. But there is no additional work to now move the results into the registers, the type of destination registers. So the overhead here is kind of similar to what we had with branches. With branches, we needed to have the branch, and then the code from either pass. Here we have the condition set, followed from the code from both paths. So the only really overhead that we have in terms of executing more instructions, comes from doing the work of both paths instead of just one. But there are no additional instructions to select between the results. Let us see if we can do some full predication of our own. Suppose that this is the original code where we check if R2 is equal to 0. If it is we jump here and decrement our R1 by 1. If R2 is not 0, then we continue increment R1 by 1 and then go out. So after if conversion with full predication we get this. First we set predicates P1 and P2 according to whether R2 is equal to 0. And remember that this one will be set. If R2 is equal to 0, and this one is set if R2 is not equal to 0. So they are opposite predicates. This one is set if R2 is equal to 0. The actual question for you is what goes into the predicate for the next instruction. And the predicate for the next instruction after that. And then, the two instructions are add R1, R1, 1 and add R1, R1 minus 1. So what are the predicates here? Also, if we assume that this code gets us a CPI of 0.5 without mispredictions. And that the misprediction penalty is ten cycles. Then we should do the if conversion if the branch equals 0 here is predicted below what accuracy. So doing conversion if branch equals 0 prediction is less than what percentage correct. Let us look at the solution to our full predication quiz. We have this branch based if, then, else code. We're trying to if convert it using full predication. P1 here is set when R2 is zero. When R2 is zero, we branch to the else part. So this is what gets done if R2 is zero. That means that. We do this when P1 is true. And then of course we do this when it's not true. The next part that we want to do is when should we do the if conversions. So what needs to be the accuracy here. Let's foresee how many instructions we have in each of these. Here we have three instructions with a CPI of 0.5. This always takes us 1.5 cycles, assuming that there are other things that can be done in the rest of the cycle. Here, when this branch is taken, we do two instructions. When this branch is not taken, we do three instructions. So let's split the difference and say that on average we do 2.5 instructions. At the CPI of 0.5. That means that without a misprediction, we get 1.25 cycles. A misprediction adds ten cycles. These two perform equally when the cost of mispredictions here. Is 0.25 cycles per branch. So pretty much, the difference between between these two is 0.25 cycles. So on average, we can lose this much to get this to be equal. The penalty for a single misprediction is 10 cycles. If we can lose only 0.25 cycles per branch. That means that only one in 40 branches can be mis-predicted. one in 40 is 2.5%. That means that our correct prediction needs to be at least 97.5% to not do this conversion. If the correct prediction rate is less that this then we a better off with if conversion. In this lesson, we discussed if conversion, where we convert difficult-to-predict branches into code that is longer but has no branches. We also learned about special instructions that we need for if conversion to work. Now that we are up to speed with how control hazards are handled, we need to learn how to handle data hazards while keeping a good instruction flow in our pipeline. We have seen that the processor can improve performance a lot, barely ordering instructions. But things are not that rosy in the instruction reorder land. In real programs we can have exceptions. Such as divide by 0, that can be messed up if we don't execute instructions exactly in program order. This lesson explains how to clean up the reordering mess when exceptions do happen So now we're going to talk about exceptions in out of order execution. Which, as we said, is part of what's wrong with Tomasulo's algorithm as far as modern processors are concerned. Let's look at these four instructions and how they execute. This first instruction is going to issue in the first cycle. And let's say we just are able to issue everything like this. This instruction will dispatch in the second cycle, let us say it takes 40 cycles for a divide to finish. So it is going to write the result in cycle 42. Meanwhile, this load could be dispatched in cycle three and maybe finish in, let's say cycle 15. And then, this instruction is waiting for the result of the load. So it will really dispatch in, let's say, cycle 14, and maybe be done, let's say, in cycle 19. And then, this subtract is waiting for F2 and F6, so it can also be dispatched in cycle 14. And maybe finish in cycle 15. Let's say that F6 is 0, so we get a divide by 0 order. And let's say that is detected in cycle 40. Some are towards the end of the divide. Let's say we ignore that. We have a divide by 0. So what should happen in this processor is the program counter for this instruction should be saved. We should jump to an exception handler. And then, if there is, exception handler comes back, we should be back to this instruction, and kind of start executing from here. However, what really happens is, in cycle 40, we have already executed these instructions and written the results. This is the only one that really hasn't been done. So F10 hasn't been undated. If we now go to the exception handler and then come back. This instruction will use the F0 produced by this instruction. So this instruction can now never get a correct result anymore even if we fix F6. For example, you could put something like a very small value here. Just to avoid a divide by 0 error. This can also happen for example if a load has a page fault. We page the page back from the disk, go back to executing the load. But by that time, instructions after the load might have executed and there is no way to proceed. So this problem of having exceptions precisely. Is what is considered to be the major drawback of Tomasulo's algorithm as it was implemented Another thing that is a problem with Tomasulo's Algorithm is when we try to do branch mispredictions. So, how do we recover from a branch misprediction? So, say we have a program like this. Where we have a divide, you take something like 40 cycles and once we get that result, we can resolve this branch over here. So, this branch can be predicted but it can take a long time to realize that it's been mispredicted. So let's say, meanwhile, we have fetched this instruction, because a branch should have been taken. We have fetched this instruction, and this instruction is already done. In that case, R3 is already written. Once we realize that we have mispredicted this branch, possibly 40 to 50 cycles later, we should behave like we have never executed this instruction, and instead start fetching instructions from the label that the branch is pointing to, but that becomes impossible because R3 by that time has already been updated. So instead of having the old value of R3, now we have the new value of R3 when we jump to the label. So again, the problem is very similar to precise exceptions. In that, pretty much, an instruction can complete and write to registers before preceding instructions have fully been verified, and before we even know that this instruction should have been executed. A final problem occurs because of so-called phantom exceptions. So what we can have is, let's say there is a divide here. Let's say that we have mispredicted this branch. Let's say that this instruction doesn't execute too soon and so on. But the problem is, if there is a divide here and it generates an exception, that the exception will be triggered even though this instruction was never supposed to be executed. So when we come back, we have now a problem of, basically we have already started exception processing for this. We have, you know, done everything for this exception by the time we realize that it's too late. So again, we have to somehow take care of not having exceptions until we are sure that they should happen. So let's see how should we do the correct out of order for the execution. We should execute out of order. We can broadcast out of order. But we should deposit values to registers in order. This is needed because if we deposit register values out of order. If subsequently we discover that one of the earlier, the instruction shouldn't have been done. Like, it has an exception or it's a branch misprediction then this instruction has already deposited it and it shouldn't have. So if we deposit in-order, this cannot happen. We will basically deposit results in order so once we get to deposit. That means that all the previous instructions have finished successfully and everything is fine. In Tomasulo's algorithm this is not happening in order. This is happening out of order which is why we have a problem. So what we need is a structure called reorder buffer. Even after issue, it remembers the program order. And it keeps the results of instructions until they're safe to write to registers. So basically, instead of writing to registers whenever the result is produced, it would end up in the reorder buffer. And then, we will go in order through the reorder buffer and deposit results into the registers. And once the result is deposited to the register, only then the instruction is considered fully done. Now that we know that we need the reorder buffer to hold the results of instructions until they're, ready to finish in program order, let's see what it looks like. The Reorder Buffer is a table of entries. In each entry we keep at least three fields. One says, what the value produced by the instruction is. This is the value that we are. Not writing to the register. When the instruction finishes, it will now write it's value to the reorder buffer when it broadcasts. We need a bit here to tell us whether we actually have a valid value here or not. Because we will give a Reorder Buffer entry to the instruction before. It actually writes the result. And finally in the end we have to write this value to an actual register, and for that we need to know when we do such a write, when we move this value from the Reordered Buffer to our register, what register are we going to write to? . This is a Reordered Buffer that can hold up to eight instructions. And we keep instructions in the Reorder Buffer in program order. So we need two pointers, one pointer tells us when we issue the next instruction where does it go. Another one tells us when we finally complete in program order the next instructions. Which instruction is it going to be? So in this particular Reorder Buffer, the instructions that are actually valid are starting from the oldest one, to the newest one. This, the even newer instructions, are going to be added here. And the oldest one will complete from here. So, these five instructions are actually in program order in the Reorder Buffer. Now that we know what the reorder buffer is supposed to look like let's see how it's used. For that we need a reorder buffer with its commit and issue points and we need the rest of the processor that we have already seen. The instruction queue, the reservation stations, registers and the RAT. Let's say that we have this add instruction here in the instruction queue. So now we want to see what happens once we start issuing this all the way to once we eventually commit this instruction. In a process that now has a reorder buffer. So let's see what happens when instruction issue. As before, when we didn't have the reorder buffer, we get a free reservation station. And we put our instruction in it. Now that we do have a reorder buffer we will also get one of those, unlike reservation station, where we just need a free one, it doesn't matter which one it is. Here we need to get the entry that is the next one at the issue point. So, R instruction will go here in the reorder buffer. That of course also means that our issue point will now move and points to the next entry in the reorder buffer. So now we have seven instructions that have been issued and have not yet committed. Another thing that we had to do, when we were issuing instructions into Tomasulo's algorithm, is that the RAT entry for R1. The destination registry of the instruction needed to be pointed to the reservation station for the instruction. We will still need to do this pointing, but instead of the reservation station, now we point the RAT entry to the ROB entry for the instruction. That means that this RAT entry will be now instead of pointing to the reservation station be pointing to this entry here. So overall at the end of the cycle when we issue this instruction, we will occupy reservation station with the values that we found for these registers. We will make the RAT point to the ROB6 entry. And in this entry, we will say that R1 will need to be written for this instruction. And we don't still have the value, so the done bit is 0 here for this instruction. Now this add here might wait for operands to become ready. And that happens exactly as before with Tomasulo's algorithm. We just capture results that we need, and we are ready to dispatch once we have all the operands. Eventually, the add has all the things that it needs to dispatch. And the dispatching works exactly the same as before. So, we check whether the operands are ready. And when they are, we send the instruction for execution. But now we can also free the reservation station as soon as we dispatch. Previously with Tomasulo's algorithm, we needed to wait for the reservation station release all the way until this instruction broadcast its result. That was needed because the reservation station serves as the name for the result. Now that the name for the result is the ROB entry, we no longer need the reservation stations to play the troll. So the reservation station now does only its primary job, which is wait for operands and capture them. And then, figure out which instructions to dispatch. After that the reservation station for the instruction is done so we can free it. So now we add executes and carries with it the tag of ROB6 which is the name for it's result. Eventually it produces a result. We have seen that the ROB changes when we free the reservation stations. Let's see if we understand that. Suppose that we have a situation where an instruction cannot issue because there is no available reservation station for it. When is this more likely to happen? When we have a processor with normal Tomasulo's algorithm, no reorder buffer, and we have 2 add and 2 multiply reservation stations, or when we have a processor that does have a ROB and has the same number of reservation stations. Let's look at the answer to our, when can we free the reservation stations quiz. If the instruction cannot issue because there is no available reservation station for it, that means that all of our reservation stations are busy. With a processor that doesn't have a ROB, we hold on to our reservation stations. From the time when we issue the instruction until the time the instruction brought us a result. So issuing of an instruction once the reservation stations are busy will have to wait for the next instruction to broadcast the result. In contract, in a ROB based processor with the same number of reservation stations, we grab them at the same time, when we issue the instruction. But, we free them as soon as we dispatch the instruction for execution. So we free the reservation stations sooner, thus, the next instruction might find the reservation station free over here. Where over here it was still not available. So this situation is more likely to happen when we do not have the ROB than when we do. Eventually our ADD produces a result, and when that happens we will broadcast it. The broadcast of the result happens exactly as before in Tomasulo algorithm, except that the tag we carry for the result is the ROB entry, not the reservation station number. Because we have already freed the reservation station. So what we do is we now broadcast, the tag of ROB6 with a value, let's say, 15. Just like with Tomasulo's algorithm, this value, and the tag are now, given to the reservation stations. And they normally capture the result based on the tag, as before. This result is now broadcast to the reservation stations. And they capture this result if, if they need it. And those that need the result of this instruction will capture the value 15 just like, we did in Tomasulo's algorithm. So this has not changed. Without the ROB, we would now also capture this result in the register. And update the RAT to point to that register. With the ROB, we don't want to write to the register yet, so we write to the ROB instead. So these broadcast value now goes to the ROB, and we write 15 here and mark the instruction as done. Also. The result of the instruction, is not going to be in the registers. So, we do not update the RAT, to point to the registers. In fact, the RAT is still pointing to the correct place. The result of the instruction is in the ROB now. So, we don't do this here. After broadcasting its result, the instruction would be completed, in Tomasulo's algorithm. With the ROB. We're still not done with it. Yes, we have computed the result and logged the computation as done, but we have not deposited this result in the registers. For that there is an additional step, called commit. To commit this instruction, we must first commit all of the previous instructions. So each cycle, we test whether the next instruction at the commit point is done. In which case, the commit point can move on and commit this instruction. So eventually, the commit point will reach here. And now, we will test whether this instruction is done, find out that it's done, and commit it. The committing of the instruction, consists of actually taking this value, 15, and writing it to the register that the instruction is supposed to write to. In this case R1. So, 15 will be retained here. And also, we need to update the RAT, because it's pointing to the entry ROB6, that entry will be freed after we commit this instruction. So instead of pointing to ROB6, now we can point to R1. So really, the updating of the RAT for writing to the register, has moved from the broadcast time to the commit time, because that's when we are actually updating the registers. And after committing this instruction, the commit point will move, even further, so that the next instruction to commit will be here, and that completes our reorder buffer processor step by step, for one instruction. As you can see, many other things, are happening exactly the same as in Tomasulo's algorithm. Some of the things have changed, like point the RAT to the ROB entry instead of the reservation station. Writing to the ROB, on broadcast instead of the register file. And also we have added, a step here for committing. Also we can free the reservation station on dispatch, not on broadcast. And finally, we need a new step for actually committing the instruction. So, let's look at what kind of structures and what do we have in the hardware with the ROB. So, we still have the instruction queue, from which we dispatch into the reservation stations. We still have the RAT, that can point to a register file, or to the rename name for the instruction until it writes to the register file. But RAT entries that point to, not to register file in, in normal Tomasulo's algorithm would point to the reservation stations. Now, however, we have a new structure called the reorder buffer, or ROB. It has a head and tail pointer, and basically the instructions that are currently in execution are between these two pointers. The head pointer is the pointer to where we are going to put the next instruction that issues. The tail pointer is where we are considering instructions for commit and the RAT entries that are not pointing register file are now pointing to the ROB entries for the instructions that are going to produce its value. So now that we know what the ROB's purpose is, let's do a ROB quiz. So we need the ROB because we want to remember the program order until commit. Because we need a place to temporarily store the instruction's result until commit, to serve as the name or tag for the result of the instruction. To store the source operands until the dispatch of the instruction. And to determine which instruction goes to which execution unit. You should select all of the correct answers here. There could be more than one that are correct. Okay, so let's see the ROB quiz solution. So we do need the ROB to. Remember the program order. That's the only place where the program order is preserved between the issue which is done in program order, and the commit which is also done in program order because things in between are done out of order. So, we definitely need the ROB for that. To temporarily store the instructions result, yes. The ROB holds the instruction's result between the time when it is produced, meaning when we're actually broadcasting it on the bus. And the time when the instruction's result is committed to the register file, so we definitely have that role for the ROB. To service the name or the TAG for the result. Yes. With [UNKNOWN] algorithm the reservation station was playing this role bit with the ROB, the ROB entry is really going to serve this role for an instruction. To store the source operands, until dispatch, know the reservation's station still. Will retain this role, basically the reservation station still stores the source operand until dispatch, so this answer is not correct. And to determine which instruction goes to which execution unit, again ROB doesn't play this role, the rob is usually unified meaning all of the instructions go to the same ROB, they just get different entries in it. We know which execution unit instructions should go to because, different execution units have different reservation stations. So, when we issue an instruction we send it to the right set of reservation stations and that determines what is going to be the execution unit. So, this answer is also not correct. We said we need to ROB in order to have precise exceptions, and also to be able to easily recover from Branch mispredictions. So let's see how that happens. Suppose we have a program that reaches a branch, and suppose that our prediction was that the program will continue here and fetch these three instructions. Although in fact the program branches. So, this three instructions will be executed in our branch misprediction. Let's say what happens in this program, you know [UNKNOWN] and the registers. First, we will issue the LO here, It's output is the R1, so at the beginning of all of these the ROB is empty. The issue and commit point are the same, and. That, let's say that there are no instruction. At the beginning of all of this, let's say that there are no instructions, so the RAT is just pointing to the registers, and the issue and commit point in the ROB are the same. Meaning that the ROB is empty, all of the entries are free. Now let's see what happens when we issue the load. We move the issue point to here. The load produces R1. An R1 here in the RAT is made to point to the ROB 1 entry. When we issue the branch, it produces no output, and thus we don't need to rename anything and the issue pointer moves here. Next what happens, we will issue instructions. That, shouldn't have been issued. But the [UNKNOWN] doesn't know that yet, so it's just going to do it anyway. So this add here writes to R2. And it gets the next ROB entry here. So the R2 entry is going to point to ROB3. And then this multiply here. Is going to write R3. And that R3 is going to ROB4. And then the divide is going to again write to R2. And that means that the answer for R2, in the RAT will be changed from pointing to the add to pointing to the divide. Which is ROB5, and of course after all of this issuing is done, our issue point is now here. And we have five instructions in our prompt. Now suppose that this LO takes a long time to produce a value for example because it's a cash miss. This branch depends on the loads, we've cannot complete until the load is done, and this add also co-depends on the load and it cannot complete until the load is done. But this multiply here can begin because R3 and R4 are not produced by any of these instructions. So this multiply, might produce a value for example 15 and originally it would write it to the registers, and at that point after the branch misprediction, but this multiply can compute its value because R3 and R4 might be ready, they're not produced many of the previous instructions. So, the value of 15 when the multiply finishes, would have been written to the registers, and then, we don't know how to undo the multiplication once we discover that the branch have been mispredicted. In our [INAUDIBLE] base processor, the result of the multiplication goes into its. ROB entry, and it's marked as done. Once this multiply produces a value, the divide can begin. And let's say that the divide is producing a value of 2, and it might also be done before the load returns. So now we have a situation where these two instructions would have updated registers, and done permanent damage. But in a raw based processor, all they did was update the raw [INAUDIBLE]. The registers are still unmodified Eventually this load does complete, and let's say brings us a value of 700, and it's marked as done. Now we're going to do the branch and the add. Meanwhile, we are going to be checking whether we can commit this instruction and we can, because it's marked as done, and it's the next instruction to commit. So we'll commit this instruction. Which will result in moving the commit pointer here, and writing this value of 700 to register R1 and also, making the RAT, now point here. Now, let's suppose that a branch takes longer to figure out than the ADD. So this ADD completes, produces a value of let's say 3, and is done. And that's all that happens here. Again, this result would have been written to the registers, thus permanently changing R2. And that's all that happens. Now, know that this result of 3 would not have been written to the registers even with Thomas [UNKNOWN] algorithm, because the renaming for R2, is pointing not to this instruction. Meaning that we would have kept in the registers the value of 2, that we would have written, for the divide because that's the latest R2. But in this case, we don't do any of that. We just keep the values in the ROB as is. Eventually, we get to resolve this branch and figure out that it's been mispredicted. So now, the branch, will be marked as done. We will now start fetching from the correct instructions but the question is, how do we get rid of the ones, that we had wrong? Well what we do is we annotate here that there has been a misprediction of this branch, and that's all that happens as a result of its, execution. And we don't really fix the misprediction. We continue fetching the wrong instructions because we don't know how to get rid of this, until the commit reaches the point of the branch. So when we commit this branch we realize that, after this branch, we were fetching the wrong instructions. The PC that the branch should have created, is different from the PC we have actually used. At that point what we do is, we commit the branch normally. It doesn't write to any registers so the commit just moves this. But if it's been mispredicted and it was, what we do is we do the recovery before we restart the fetch from the correct place. How do we do this recovery? Well with a raw based processor. At the point where the commit has reached the branch, the registers, contain exactly the values they need to contain, at the point of the branch. All of the instructions prior to the branch have been committed, which means that their updates have been reflected in the registers. And none of the instructions after the branch, have updated the registers. So the registers contain exactly the correct value at the point of the branch. To undo these instructions, what we need to do, is two things. One is, reverse the issuing of these instructions by simply making the ROB empty at these points. So after we have committed the branch, we make the ROB empty by simply putting the issue point in the same place. So we will issue instructions again into these slots. We of course, as we issue them unmark this done bit. Also the RAT now contains the wrong values. But the questions, what are the right values for the RAT? Well because, the registers contain exactly the correct values as of this point. Undoing the damage in the RAT, simply consists of, rewriting whatever is in the RAT so that each RAT entry points directly, to it's corresponding register. So now. It looks like these instructions have never executed or issued for that matter. And, we can begin fetching from the correct place, updating the renaming as a result of that, and updating the issue pointed as a result of that. As you can see, with the ROB based processor, we can do instructions out of order, but we still can't figure out what the correct. Or that should have been, if we did the right thing at the point of the branch. So as you can see, here we have executed these instructions before we figured out the branch, but we are still able to undo this damage. Overall, the recovery with the ROB based processor consists of, making the right entries point to their corresponding register so can, erase all of the renaming. Emptying out whatever is in the ROB after we commit the offending branch instruction. And of course, getting rid of whatever is leftover in the reservation stations and the ALU so, basically free all the reservation stations and stop the ALUs from broadcasting results in the future. So if, one of these instructions is still in the ALUs or reservation stations. It's going to be, just dropped. Now let's see the ROB is fixing our exception handling problems. And remember there are two of those problems. One of the problems occurs because this divide can be delayed, and realise that R2 is 0 much, much later. While the add for example can be quick and be done. So if you remember with [UNKNOWN] algorithm, the add would deposit the result to the destination register of the add long before the divide had the chance that R2 was 0 and that really we should have jumped to the exception handler here and never executed the add. So how does the rob help? So what we do is we treat the exception just like any other result. So basically when we determine that R2 is 0 instead of producing a result for R0, we mark R0 in the rob as, I mean, the result in the rob is now going to be exception instead of a value. When the divide reaches the commit point, at that point the add still hasn't committed, and everything before divide did. So at that point we can just do the, kind of wait for drain, you know, kind of flush everything, including the divide at this point, and jump to exit, exception handler. And now we have a stable state for the handler, which is here. Basically everything before the divide finished. The divide and everything after it didn't finish which is exactly the state that the divide by 0 exception handler should be seeing. Similarly for a load that would here have a page fault we would have the same situation of, when the page fault reaches the commit, we have committed everything before the page fault, and we haven't committed the page fault itself or anything after it. So there is a very nice resume point for the page fault exception handler. When we go back there and load a page from the disk we can jump back to the load of the store and then start executing from here, and because nothing here has already executed, everything is fine. The second problem with exceptions that we had were those phantom exceptions. Basically if we predicted that the branch here is not taken, we would execute this divide and maybe get the exception, like divide by 0. And at the time when we get this exception, maybe the branch is still not resolved. So when we finally resolve the branch it is too late because the divide by 0 has already been triggered, so how does an R-O-B handle this? Well, the result of this instruction is now going to be marked as an exception in it's R-O-B. As the comment reaches the branch at the point or before it, depending on what kind of branch misprediction strategy we have, we will figure out that the branch has been mispredicted and that we really wanted to jump to this label here. At that point the divide and anything over here is not committed. Basically we haven't committed anything after the branch. So we can just cancel this instructions. They never reach commit and thus the exception here is never triggered. So the idea with exception handling is simply, treat the exception just as any other result. And delay the actual handling of exceptions until the instruction that is triggering the exception commits. At that point, we know exactly what the resume point is for the exception handler. And we know that we won't have any phantom exceptions because we would never reach the commit of this divide unless this divide was on the correct part for all the branches. Basically, if there was a misprediction, we would have canceled this divide long before it reaches the commit point. So lets consider the example of this program here where we do an R and then we should branch to the label but instead we mispredicted and go to a divide, and we really should have executed this multiply. As far as the processor is concerned, it could look like we first did this, then we did this, then we finally did this, resolve the branch, undone this, done this. So at some given point in time. The processor sees maybe this as fully executed, but not committed, at that same point in time, only this instruction is percepts committed. So the Programmer sees just this as done. If the program finishes this, until the processor finishes this here. The Programmer is not going to say that is executed. So the Programmer never really sees, for example, this divide. Because by the time the Programmer sees something after the branch execute, the branch completes execution. And at that point we kill the divide. We remove it from the row so the Programmer will finally see the branch execute. But after that. The processor will start fetching from here so the next thing the Programmer sees is this. So the Programmer never really sees execution of, of any wrong path instructions. And on exceptions, the Programmer never sees any instructions other than those that should have been executed. So basically,. This commit is really kind of the officially executed. Whereas the actual execution before we broadcast the result, is simply kind of the internal state of the processor that is maybe not reflected to the outside world, until we are sure that everything is fine with the result So now that we have seen an example of exception handling with ROB, let's do an exceptions with ROB, quiz. So let's say that we have these instructions in program order. So this is the order in which they are supposed to execute, according to the programmer. Let's say, that this is the current status of this instruction. The add, is committed. Meaning, it has exited the pipeline and its results have been committed from the ROB to the register file and so on. The load, is executing. Meaning, it has left the reservation station, it is now doing its thing and the result still hasn't been broadcast, on the bus, and the load has not committed yet. This add, is done, which means that, it got to the reservation station, it exited the reservation station, computed it's result, deposited the result somewhere. But the result is still not committed. Meaning, the add still hasn't left the processor. The divide, is also executing like the load here, meaning it has read these registers, and now, it's slowly computing its result. This add is done, meaning, it has computed its result, but it still hasn't committed. And this add is also done. Let's say, that this divide, when the status of these instructions is like this, has an exception. So while it is executing, we discover, for example that it has been trying to divide by 0. So, this add now has an exception, the question for you is what is the new status of these instructions after this exception has been handled. Meaning, what is the new status at the point to which we can go to the exception handler? So, let's look at our exceptions with ROB quiz solution. At the point, where this divide has an exception, this is what we have. When we go to the exception handler for this divide, what should be happening is that the instructions before it has finished and the instructions after it have not finished, as far as the programmer is concerned. So, what we need to really do, because programmer really sees only what has committed here is the fact that this is done and this is done needs to be undone. So basically, this instruction is flushed from the pipeline. It behaves as if it was not executed at all, this instruction and this instruction, as well. So, pretty much what we do is when we have an exception such as divide by 0, the correct state of the processor should be right before this instruction if we were doing these instructions one at a time. So, what we do is write all back the execution of all these instructions. How do we do that? Well, we have already committed this instruction before, it has to stay committed, we cannot uncommit instructions. This instruction has been executed and this one has been done. What we do, is, we wait for them to commit. So, what's going to happen is, this instruction will eventually commit once is done executing, and then this one was already done, so we can very quickly commit afterwards. Now we reach the point where the divide carries this exception condition into the commit with it so when we try to commit the divide we realize we cannot do that. And that's the point at which we stop committing, flush everything that follows the divide from the pipeline so these instructions now become unexecuted, meaning we haven't even fetched them as far as the program is concerned and this is the point at which we jump to the exception handler. So pretty much at the point where we call in the exception handler, what you have is some instructions have committed, some of them haven't been even fetched. They have been actually fetched but we have discarded them so they behave like they haven't been fetched. And that's the point where the control is transferred to the exception handler instead of this way, so that's very much like a branch misprediction except that it, it happens on an exception. Okay, so let us say that this is our ROB here, this is our Register Allocation Table and this is our set of Registers. And let's suppose that our instructions are R1=R2+R3, R3=R5+R6, R1=ROB1 x R7, it's ROB1 here because we're using R1 [UNKNOWN] this way. And then R1 equals R4 plus R8 and then R2 equals R9 plus ROB2, because it was really using R3 but it got renamed this way. Let's say that all of these instructions have finished and put these results in the ROB, and now it's time to commit them and this is the next instruction that we'll be committing. At that point the RAT entries are going to be. For R1, we need to point to ROB4. Why? Well because, the entry in ROB4 is the latest write to R1 as far as the instructions that we have issued, is concerned. So, you know, this was pointing to the register and then when this one was renamed it made to point to ROB1. And then when this one was renamed, we made that RAT entry point to ROB3 and finally, it, it became ROB4 when this one was renamed and after that, we didn't rename any R1 instructions. R2, the latest rename to it was ROB5 and then for R3, the latest rename of it was ROB2. So, this is what we have in our RAT for now. And let's assume that there are some values in the registers here, they don't matter anymore because they will all be overwritten. So if you remember before we had the ROB, when we were only doing [UNKNOWN] algorithm, and were finishing instructions out of order, with no commit at the end. What was happening is as the result is broadcast we need to see what's in the RAT for that result. And if it says, for example, that this was supposed to be produced by our instruction, meaning, we were the latest rename of this then we would update the Register file. If, for example, the R1 entry said that it shouldn't be asked that broadcast gets the latest result, then we don't update the registers. With the ROB, this changes because now, we have the commit. We will commit instructions. In the correct order, in order of program execution. And each time, we commit an instruction, we will deposits it's result in the register's, no matter what the RAT really says about it. So when we commit ROB1 entry, we're going to take this result of R2 plus R3, whatever that value is. And we will put that value in the registers even though the renaming in the RAT says that this is not the latest value for this register. However we are then going to check whether we are the latest rename and if we are not then we are not going to change the RAT. Basically the, the RAT needs to indicate that the latest renamed value for R1 is still ROB4. The fact that we committed ROB1 doesn't change that. So after doing this, we free this ROB entry and it is now ready to take another instruction. Now, we go to, try to commit ROB2. Well again, it needs to write a result in R3. We take the result of the instruction and we do right it to the Registers. Regardless of what the RAT says about it. But then, we look at the RAT entry for R3, and check whether the RAT entry says that we are the latest rename for this value. And in this case we are. So when we are committing this instruction, if the RAT keeps pointing at the ROB2, it's going to be pointing at an empty entry that will soon be used by some other instruction. So what we need to do is change the RAT. To now point back to the Registers for R3, so we made the RAT just point to the R3 value in the registers no longer, so basically, now if any instruction wants to access R3, when it wants to read operant, it actually needs to get this value, not a value from the ROB. and after we have done this, we can commit this instruction fully, which frees this ROB entry. And then we continue doing this, this instruction is the next one that we will be committing, so what we do is we take this result, we put it in R1, so we overwrite what was in R1 before, we write whatever this value is, we then check whether for R1 we are the latest rename and we are not. We are Rob3, this has Rob4,. So we leave the RAT entry alone, and we can just free this entry from the ROB. Now's a good time to kind of stop, and figure out, you know, why are we doing this? Why are we depositing values in the Register that we know are going to be overwritten soon? Well we are doing it because at any given point in time if we need to stop there and take an exception like for example now if. We want to take an exception, what will happen is, we can just drain the ROB, just, just delete everything that is still in there, we can make the entire RAT just point to the registers, meaning we will just change the center to point R1, R2, R3, and the content that we already have now in the register file, is exactly the content as of this point. So if we were not updating the registers for every instruction, then we would not be able to do this. The Registers would not be up to date at this point and we would not be able to take an exception by just clearing everything from the RAT and ROB. The fact that we did update it gives us the ability to do this. Pretty much the Registers are always up to date. As of the commit point, which means at any given time we can just stop at that point, cancel everything from here, make this just point directly to the Register file. And at that point, we are ready to fetch instructions from the exception handler. Now, let's say that, that doesn't happen. What we can do then, is just let's commit these instructions just to kind of illustrate once more what happens. So, R1 equals R4 plus R8 is our next thing to commit. We write that to R1, overwriting this value. We check whether we are the last rename for R1, we are. So we make the RAT entry point directly to R1, basically from now on, if somebody wants to. Read R1, they will end up reading this, not some ROB entry and now we are ready to fully free this ROB entry and finally we can commit this by again writing this result, checking whether we have the latest rename. Yes we are. We point them directly to R1. And we finally free this ROB entry. And, if we no longer have any instructions in the ROB, then we have a state exactly as it should look when we have no instructions in the ROB. Meaning the entire RAT is pointing directly to the registers. The registers are all up to date, as of what was committed latest. And so on. When your doing this, be mindful of the fact again that the values are copied to Registers as we commit regardless of what the RAT says. But the RAT is updated on commit only if we are actually changing the renaming from our ROB entry to the registers. If the re-namings in the RAT is saying something else like some other ROB entries still be pointed to here. Then we'll leave it alone because the RAT needs to continue, to point to the latest value wherever that is So, this is the state of the processor. We have some instructions fetched. This is the very first one that we will be trying to issue. This is our reservation stations, our register file with the values, the RAT that says where the values are currently and it's pointing to the register file for now. The ROB that is empty for now. This is a little table that we will be writing what happens in which cycle. And then, this is kind of the legend for our reservation stations and ROBs because we need to know, which field to put which value in. So in cycle 1, what's going to happen is, we're going to try to issue this first instruction. To do that, we need. An available reservation station and an available ROB entry, we have both, so we are ready to go. It's a divide, and we're going to write the divides kind of operation here. It's writing to R2, so we're going to note that in the destination register over here. We're going to write the destination tag in the reservation station, that's going to be ROB1 now. Remember that. The value we broadcast the result with, basically the tag that everybody's trying to compare, is now going to be the ROB entry number, not the reservation station number. That's what allows us to free reservation stations very quickly. The R3 and R4 are going to be looked up here. We will find that it's 45 and 5, and write it here. And then, we are going to rename R2, so that it points to ROB1 from now on. And then because we said that we will be able dispatch in the very same cycle and begin execution in the next cycle, we have issued this instruction Cycle 1. It's going to begin executing in Cycle 2 because both of its operands have values ready to go. Now in Cycle 2 this instruction begins executing, so the reservation station becomes free. Before I free the reservation station I'm going to note that this instruction is dividing 45 and 5, so the result that is going to eventually end up in ROB is going to be 9, 45 over 5. This instruction is not done so technically this value lands here only when it's done but I'm going to note this is what's going to happen because, I want to know what that value will be. And, with that, this reservation station becomes free. So now what's going to happen, this is a divide. It's going to take 40 cycles to do things so in cycle 42, which is the 41st cycle since we began executing. We tried to write its result. That's going to be a long time in the future. Meanwhile, in Cycle 2 we're going to try to do, the rest of the things. Can we dispatch something else, well nothing to dispatch? Can we issue something. Well the multiplication has a reservation station and ROB entry ready to be obtained, so we can do that. It's a MUL of R1. The tag for it is now going to be ROB2. R5 and R6, are 3 and 4. So we have the values, we don't have to wait for anything. And R1 is a result. Now we need to remember that R1 from now on is mapped to ROB2. Basically, if anybody from now on wants to ru, use R1, it really needs to wait for the result of instruction in ROB2. So this instruction has issued in Cycle 2. It's gone up this batch, also in Cycle 2 because it's ready to go. So in Cycle 3, it's going to begin executing. And, with that, we have moved to cycle 3. As we go to cycle 3, again we can free this reservation station. The instruction has been dispatched. So, in cycle 3 we have, this reservation station free again. Don't forget to do that because you might run out of reservation stations pretty much surely otherwise. So in cycle 3, we are going to try to issue this instruction here, it's an ad. So we check that we have this type of reservation station. Do we have ROB entries, yes we do have both. So what we going to do is we going to issue this instruction. It's an add, it tries to R3, R3 is now renamed to Rob3. We mark here that we are issuing this add and we put this add here. The name of the result is Rob3, because we just put this instruction here. We check R7 and R8. They are renamed to. Just the Register file, so we can just take those values of 1 and 2 and this instruction is ready to dispatch,which means it will begin executing the next cycle. It takes only one cycle to execute so in, cycle 5, it's actually going to try to write the result. And, now that we have successfully renamed this instruction and so on, we don't have anything else to do in cycle 3 basically. These two instructions. I'm not going to write anything on the broadcast box yet. So with that we can move on to cycle 4. In cycle 4 the add begins execution and at the beginning of cycle 4, we already have this entry free because we have this patch in the previous cycles. So now we're going to try to issue this instruction here we have our free reservation station. We have free ROB entries so it will be able to ensure in cycle 4, it's going to take his ROB entry. It will write to R1 so we remember that. We take also a reservation station. We say that we're going to be broadcasting ROB4 when we do this. now what we do is, for this instruction we're going to look at the RAT before we update it to see where would we find our Registers that we're going to use as samples. We are using R1 and R3. R1 and R3 map to ROB2 and ROB3. So we actually mark here ROB2 and ROB3. And we don't have values yet, so this instruction actually will have to wait. It will not be able to dispatch now because it's actually waiting for, this is just ROB2 and ROB3, I'm not writing it out because, it would be too crowded. And after we have read these from the RAT, we are ready to update the RAT. R1 now needs to point to ROB4. Be careful when you update the RAT. You first need to check whether you find your register inputs, and then you rename for your outputs. Because if one of your inputs gets overwritten with your output, you first need to use the values of the RAT before you rename. Otherwise, you would, the instruction would be. Waiting for it's own result which will never happen. So now R1 begins to point at ROB4 because that's going to be the latest R1 that we, we're going to produce. And this instruction will not be able to execute here. It's actually going to execute once both of these operands are ready. So this basically finishes cycle 4. And we are ready to see what happens in cycle 5 In cycle 5, two things will happen. One, this instruction will write its result. And two, I'll try to issue the next instruction if I can. We can do that in either or. Let's say that we do the issuing first this time. This is a subtract in ROB5, writing to R4. R1 and R5 are ROB4 and 3. So I write the value of 3 here, and ROB4 here. And then, what I do is I mark that Register 4 is from now on going to be pointing to ROB5, because that's where I put this instruction. I have issued this instruction in cycle 5. And with that, I have completed the issue of this instruction, but it is not ready to execute because it still has one of its operands, is not available and we need to wait for it. Another thing again that happens in cycle 5 is that we need to take care of the writing of the result for the ROB3 instruction. So what happens is, this instruction writes the result of 3, with a tab of ROB3. Now this 3 is really written here and the instruction is marked as done. Also, the value of 3 is broadcast under the name of ROB3, so that the reservation stations can try to capture it. So what's going to happen is there is a match here. This one is waiting for ROB 3. It's going to capture a value of 3 and no longer wait for ROB 3. So this is what happens in cycle 5, as far as broadcasting of this result is concerned. Note that the RAT and everything is not updated yet. That will happen at commit. So now, in cycle 5, we have finished our issuing here, we have broadcast our result, we are ready to move on to cycle 6. Neither of these instructions is ready to actually dispatch in cycle 5, so nothing will begin executing in cycle 6 because both of them still wait for something else. So in cycle 6, what happens is, as far as issuing is concerned, we're going to try to issue this instruction. As far as execution is concerned, these two cannot still execute because they're still waiting for something. As far as writing of results is concerned, the instruction 3 has finished writing the result and has been marked as done. These two still have a while to go before they're going to write their results. Because we're in cycle 6, the next thing here is going to happen in cycle 13. So in cycle 6, pretty much the only thing that happens is these two instructions continue to execute and we try to issue an instruction. We're going to issue this one by grabbing a reservation station and a ROB entry. We're going to be writing to R1. We are now named ROB6. R4 and R2 are coming from ROB5 and ROB1, so neither of those is available yet. And then, we're going to mark that R1 from now on is pointing to ROB6. So with this we have issued this instruction. Now, we have written the result of this instruction, so technically this instruction by itself could commit in cycle 6. However, remember that an instruction cannot commit until all previous instructions have committed. So all of this instruction would commit in 6 if everything else was committed already. It will actually not commit until at least cycle 43 or 4 or 5. So pretty much, this instruction sits here as done. Others can use its result but it cannot really commit until we commit the instructions 1 and 2. So in cycle 6, this is all that happens. There is no commit here. And now we move to cycles 7, in which case, we have nothing to issue anymore, so issuing will not be an issue anymore. We cannot execute anything until we see some results being broadcast. So, really, the next cycle when something interesting will happen, is cycle 13, in which, we will have another write of our results. So let's just kind of fast forward through to that. So in cycle 13, what happens is this ROB2 instruction finally writes its result. This result is 12. We mark now this instruction as done and the value of 12 will be broadcast under the name of ROB2. So now these instructions here are waiting for their inputs and this one here matches that I'm waiting for ROB2 value, so it captures it here. This is the first operand that we're waiting for. The second operand was already there since awhile ago. So this instruction, which is the ROB4 instruction is no longer waiting for anything. It can actually dispatch in this cycle 13 and in cycle 14, the fourth instruction begins executing. None of the other instructions are waiting for ROB2, so this concludes what happens when we broadcast the value in cycle 13 for the ROB2 instruction. What now happens is again, in cycle 14, we begin executing this multiplication. And because it has begun executing in this cycle, this reservation station is already free. It was freed when we dispatched in the previous cycle. So I'm going to note that this multiplication of 12 and 13, which gives us 36 is going to end up in ROB4, eventually here, but it's not going to be done until 10 cycles from cycle 14. So, it's going to be in cycle 24 that we try to write the result. These two instructions still are waiting for ROB4 and ROB5 instructions and this happens beyond cycle 14. So in cycles 15, 16, etc. Nothing really happens. In fact, nothing will happen until we have another broadcasted for result and the results we will broadcast eventually going to be in 42 and 24. 24 comes first, so let's see what happens in cycle 24. In cycle 24, the fourth structure will broadcast the result. This is the multiplication here. So what's going to happen is the value of 36 is going to be broadcast under the name of ROB4. This instruction is waiting for ROB4, so it's going to capture the value of 36 and mark that we are no longer waiting for this operand. This instruction actually, now it doesn't wait for anything anymore. So in cycle 25, the subtract is going to start executing. Meanwhile, our ROB4 instruction gets marked as done. Basically, this value or 36 that was broadcast gets kind of deposited here and marked as done in the ROB and this kind of concludes what happens with this multiplication instruction as far as precommit stuff is concerned. It cannot commit, yet again, because we need all of these to commit before it can commit. So, it's still going to be at least cycle 43 before we consider committing anything. In the cycle 24, however, the ROB5 instruction will be dispatched and it will begin again executing in cycle 25. So let's just move to cycle 25. So we begin executing this instruction. It's a subtraction of 36 minus 3. That's, our result is going to be 33. And it takes only 1 cycle to do adds and subtracts. So this result will be broadcast in cycle 26. So now, the next thing that will happen is either this or this will broadcast. Cycle 26 is the next cycle. So let's see what happens then. In cycle 26, the subtract broadcast a value of 33. Under the name of ROB5. So we mark it as done. 33 gets put here and 33 under the name ROB5 gets broadcast. And this instruction here is waiting for. That value. So we get 33 here, are no longer waiting for it. Oh, yes, and because this instruction has been dispatched during cycle 24. In cycle 25 and after was this reservation station has really been free. So this instruction, although it got now 33, still is not ready to dispatch, because it's still waiting for ROB1, so it cannot go. Okay? So in cycle 27,. Nothing happens. 28, nothing happens. In fact, the first time when something does happen from now is going to be cycle 42 when the result of ROB1 is brokers. So ledger's going to fast forward there. So let's say we are now in cycle 42, this divide finally brokers the value of nine, under the name ROB1. What happens is. When that happens, is we put the 9 here, we mark it as done and the 9 with ROB1 tag is broadcast on the Bus. And all the instructions that are waiting for that is going to, are going to capture that value. The only that really needs anything is this and it captures the value of 9 and is no longer waiting for anything. So in cycle 43, finally this ROB6 instruction, is going to begin execution. So with that let's move to Cycle 43. In cycle 43, we begin execution of this instruction. It's an add 33 and 9 so we end up with a 42 value here. This reservation station is now free. This is an add, so it's going to finish in cycle 44. There is another thing that happens in cycle 43, which is, all this time, we have been waiting to commit this instruction, because none of the other ones can commit until it commits. Finally, we have been in cycle 42 to write the results, so in cycle 43 we can actually commit this instruction, it's been marked as done. So pretty much the commit logic is kind of staring at this entry and waiting to see a cycle where, this entry has a check mark next to it. So what happens is finally we commit this in cycle 43. Now that this instruction commits in 43, what's going to happen is several things. First, the ROB entry for this instruction will be freed, but before we free the ROB entry, we need to update the register files, so what's going to happen is we take this 9, and we're going to write it to register R2. So register R2 is now going to be written as 9. Next, we're going to look at the RAT entry for R2, and see whether it's pointing to our ROB entry. Yes it is. So from now on, ROB1 will be free after this cycle. What we have to do, is make this R2 entry point back to the register file. You know, so the value of 9, that since recently has been, basically pointing to, you know, like. R2 is here. Now, it's no longer going to be here, so we need to make it point to R2 is here. I just kind of, deleted this, but this really means you can find it in the register file. So now really, we are ready to delete this stuff here. And now finally. The divide has fully committed. Okay? So now, if we had an exception or something, basically the divide looks like it's done. All of the other instructions really are done, except for the last one. But they're not considered done as far as the program order is concerned. Basically the commit didn't get there yet. So now let's move the cycle 44 and see what happens. In cycle 44, we will be broadcasting the sixth instruction's result. So this value 42 gets deposited in ROB6. We mark that as none. We also broadcast the value 42 under the name ROB6, but nobody's waiting for anything, so that kind of ends the broadcasting of that value. Another thing that happens in cycle 44 is we're going to look at this, instruction here and see whether we can commit, and yes it can. We look at it because is the next instruction to commit, and yes it can commit because it's done. so in cycle 44 we commit this. The way we commit instructions again is take the value of 12, put it in register R1. Next, look at the ROB entry for R1, and see whether it's pointing to us. It is not, so we don't update the RAT to point here. Basically, the RAT still needs to point here. Because that's still the most recent value for R1 that we will have. And then we can free this ROB entry, and the commit point moves here. So next cycle, we will be looking at this to commit. In cycle 45. We will try to commit this instruction. It's done, so we can. we take 3, put it in R3. We check the ROB entry for R3. It does point to us. So we need to make RAT entry for R3 point back to the register 5. Pretty much from now on, instructions that want to read R3 are actually going to find it in the register file. And now that we have done that, we can free this ROB entry. Now we're ready to move to Cycle 46. In Cycle 46 we will try to commit this instruction. And it's ready to commit so we can. We commit it by writing 36 into R1. And we check the RAT entry for R1. Is it pointing to ROB 4? It is not, so you don't change the RAT. And we have free this ROB entry. This concludes what has happened in cycle 46. In cycle 47 we will try to commit this instruction. It's done, so it's ready to commit. 47 is when we commit it. We will deposit 33 into R4. We will check the right entry for R4. Does it point to ROB5? Yes it does. Now it needs to point to the registry file. And we free this ROB entry. Now we move to cycle 48, where we try to commit this instruction. It's done so it can commit. We deposit 42 into R1, so the final value of R1 is going to be 42. We look at the, RAT entry for R1. Is it pointing to ROB6? Yes, it is. So finally, the RAT entry is made to point to the register file. So only now, instructions that want R1 are going to start looking at the architecture register file. All the time until now, they would have found the value in the ROB, or they would have had to wait for the value. With this, we can finally free this ROB entry, and this concludes our example. So, pretty much the [UNKNOWN] instruction is going to commit in cycle 14 Now that we have seen how the ROB based processor is supposed to work, let's do a ROB quiz. This is actually going to be a series of quizzes. We will have this state of the processor, with these instructions. Note that the add now takes 1 cycle, multiplication 3 cycles, and divide takes 10 cycles and the assumptions about dispatching and execution and so on are the same as before. So, basically, even instruction captures results in one cycle, then it can dispatch in that cycle and begin executing in the next cycle, or if the instruction is issued in one cycle and it has operands ready to go. Then it can dispatch in that same cycle so that it begins executing in the very next cycle. But the last cycle of execution will complete before we broadcast the results. So, if an add begins executing in some cycle X, because it takes one cycle to do that, it's going to be able to broadcast the result in cycle X plus 1. The initial value of the registers are these. The cycle is one. Obviously, nothing will happen except for an issuing of an instruction. So the first question for you is, we will try to issue this instruction, What is going to be the content of the RAT entry for R2? So put the value here. Okay, so this is the first part of the ROB Quiz Solution. What happens in this first cycle is, again, that this instruction issues. So we will have this instruction issuing in cycle one. The issuing is done by taking the instruction, putting it in the reservation station and the ROB entry. The destination registries R2. The instruction's name is ROB1 from now on. And R3 and R4 we get from R3 and R4. And then if you remember we rename R2 to point to the ROB entry. So the correct answer here is what ends up in the right here is, we need to put ROB1 there. Let's continue our ongoing quiz. This instruction has issued. We have issued it in cycle 1. It begins executing in cycle 2 because these operands are ready. The result is going to be 20 over 5, so the result is going to be 4. And that result will become available in 2 plus 10. That result will be broadcast in cycle 12. And this makes us ready to begin cycle 2. At the beginning of cycle 2, this reservation station is already freed. And we will try to issue this instruction. That instruction will be issued by finding a reservation station and a ROB entry. We will write R1. R5 and R6 are going to come from the register file, are going to be four and two. The result will broadcast under the name of ROB2 and we have issued this instruction in cycle 2. In cycle 2 also, the execution of this instruct, of the first instruction has begun. In cycle 3, we will issue the next instruction and begin executing this one. So what happens in cycle 4 is we have issued this instruction, it's operands were ready already, so at the beginning of cycle 4, this reservation station is free. That instruction has begun execution. And, we will consider now issuing this multiplication here. The question for you regarding this multiplication is, what is going to be here, here, here, and here? So what is going to be in the reservation station for the multiplication once we have issued it. And we do issue it in the fourth cycle. And also, which RAT entry, put the register number, is going to be modified as a result of this? So which RAT entry, is it R1, R2, R3, or which one? So this is our continuing solution for this multipart quiz. We have begun issuing instruction MUL 4, so the question is now, what goes into the fields of the reservation station? And also, which RAT entries going to modified? The modification of the RAT entry is going to be easy because it's going to be the RAT entry for R1. So the correct answer here is R1. The RAT entry for R1 is going to be modified, and we modified with ROB4. Before we do that, however, we need to read out what is the RAT content for R1 and R2. And we find that it's ROB2 and ROB1. So, the answer here is going to be that, this reservation station entry for the multiplication is going to have two tags, for R1, ROB2, for R2, ROB1. And it's not going to have either of the values. We would only put something here if we could actually get their actual value from wherever it is. But because we need to wait for ROB2 to get the value and we need to wait for ROB1 to get that value, which simply leave these empty. So that concludes the correct answering for this part of the quiz. So now that we are done with the work for Cycle Four let's see what happens in Cycle Five. Two things will happen in Cycle Five. One is we are going to try to issue the next instruction. Another thing is, we're going to try to see if we can dispatch an instruction, write the result, or comment. Obviously we can issue this instruction so let's do that. So, the question for you is do we dispatch an instruction. Right here, which instruction gets dispatched? Leave it empty if none of them gets dispatched. Another question for you is, do we write any results in this cycle? If yes, write which instruction gets to write the results. If no, just leave this empty and obviously we won't be committing any instructions. The name of the instruction here should be the, really the ROB entry. So if it's for example the ROB1 instruction that gets dispatched you should write ROB1 here. Okay, so let's look at the solution for this part of the quiz. Do we dispatch an instruction in cycle five? No we do not. All of these instructions are waiting for something to be broadcast. So until we actually broadcast some results, and these instructions can capture the values, they will not be able to dispatch. Both of them are waiting for something. So this should be empty. Do we write any results? Yes. The instruction in ROB3 is an add. It began executing in cycle four. It takes one cycle to execute. So in cycle five it will write its result. So the correct answer here is ROB3. This is which instruction writes its result in cycle five. As a result of this right result. In cycle five, we mark this ROB3 entry as done. The value of three is broadcast with the tag of ROB3. It gets captured here because this instruction is waiting for ROB3. So in fact, because we can dispatch this instruction in the same cycle, in cycle five. This subtract does dispatch. So the instruction in ROB5 will dispatch, and it will begin executing in cycle six. So this is somewhat of a trick question because you needed to first figure out that this write result happens, and as a result of this we enabled the dispatch of this instruction. So it's kind of interesting in that in this cycle this instruction has been issued. Captured a result and dispatched. Okay. So we have dispatch instruction in ROB5. It's value that it will produce is minus 1. That value will be broadcast in cycle seven because it begin executed at six. But before we get to that in cycle six we are going to issue this instruction. In cycle six we have finished our multiplication here. It was executing in cycles three, four, and five. Took three cycles. So in cycle six it also broadcast the result. So the result of the multiplication is broadcast as an eight under the name of ROB2 and this is marked as done. ROB2 value is captured. This 8 is going to be captured here, because we were waiting for ROB2. Here we are not waiting for ROB2 so everything is fine. In cycle seven, the ROB5 instruction, broadcast the result of minus 1. This minus 1 gets captured here because it matches this. And this instruction is done. In cycle seven, as we have seen, we have written this result, captured it here, we have marked this instruction as done, we have nothing left to issue. So the question for you is in what cycle do we dispatch this instruction. Okay let's look at the solution for this. So the question is in what cycle is this instruction dispatched? Well, that instruction is waiting for ROB1. ROB1 is going to be broadcasting cycle 12, value four will be broadcast then and at that point, we will also dispatch this. Remember that when we capture a value and that's the last value, that's the cycle in which we dispatch and we begin executing in the following cycle. So, the cycle in which we dispatch, the cycle 12 because that's when we captured the result, and this instruction, the ROB4 instruction, begins executing in cycle 13. Okay, so in cycle 13 we have seen that this instruction begins executing and also the rob six instruction was dispatched because it captured the same result. And begins executing, and it will eventually go towards the value of three. The question for you is in addition to basically this instructions capturing values in this batching, does anything else happen in cycle 13? Right here, what happens, issue, execution, write of result or commit, just write one letter. And the number of the ROB entry here. So for example, if you think that instruction ROB two writes result, write W two, here. The solution for this is that in cycle 13, we have already issued all the instructions so no issue will happen. We have already begun execution of all these instructions, so the correct answer obviously, will not be that something else executes in 13 because we have already begun execution. We have already seen that this begin executing in 13. But, that's not the question. The writing of the results, neither of this can write the result in 13, and all the other results have already been written. For commit however, this result has been written in 12. This is the very first instruction that we are going to try to commit, so this instruction will actually commit in cycle 13. So the correct answer is C for commit, one because this is the instruction that commits. So this is also what happens in cycle 13. So we have seen that in cycle 13 this instruction commits. It commits by depositing the value of four into register R2. Looking at the ROB entry for R2 to see whether it points to it, and it does. And thus we make the entry in the RAT point to the register file, and freeing the ROB entry. Now we will consider what happens in cycle 14. This instruction would write the result in cycle 16. This instruction is an add, it executes in 13 and writes the result in 14. The question for you is, does the RAT, any entry change? So for example, if you believe that the RAT entry for R1 is going to change, the write R1 here. So which RAT entry changes as the result of writing this value for ROB6 instruction? The solution for this is that as a result of writing this result, nothing changes in the RAT. Remember that write result doesn't update the RAT, commit does. So we only update the RAT when we issue an instruction to but the new renaming in here. And when we commit the instruction, to possibly remove that remaining if it, if it's still pointing to us. So the correct answer is really nothing. So this instruction simply gets marked done. We then broadcast the value of three under the name of ROB6, but nobody's waiting for it. That's all that happens for writing this value. In cycle 14 we will also commit this instruction. The question for you, which entry in the architectural register file changes, if any. Leave this blank if none of them change, otherwise write the R1, R2, et cetera, depending on which one changes. And if it does change, that entry becomes what value? The solution for this is that when we commit the ROB2 instruction we're going to take the value of 8 and put it in R1. So the correct answer here is R1 and the value is 8. And then we're going to check whether R1 mapping maps to ROB2. It doesn't. It still points to ROB6. So we just freed this reservation station. In cycle 15, none of the results is broadcast. But we will be looking at this instruction to commit it and we can commit it. So we will update our processor accordingly. Three goes in R3. The mapping for R3, we check if it's ROB3, yes it is. We make the RAT point do this three, and we can free this Rob end. The final question of the quiz is when does the last instruction commit. So, what will be in this entry here Okay, so we have been asked when will this instruction commit. We already know that this one will broadcast the result in cycle 16, that means it cannot commit in the next cycle. So, basically in cycle 15 we have committed this, in cycle 16 we cannot commit this because the broadcast is happening in 16, we can only commit it at 17. Next cycle 18, we will check if this can commit. Yes it can because it has already broadcasted its result and it's done. And finally this becomes 19. So the correct answer here is 19. You can see that, to determine this we didn't actually have to figure out what really happens here. But let's do that for the sake of exercise. So in cycle 17, this gets marked down. 32 gets deposited in register 1. We check the mapping for R1. Is it ROB4, no. We leave the RAT alone, and free this ROB entry. In cycle 18, we can commit this. We put minus 1 in R4. We check the entry for R4. Is it pointing to ROB5? It is. We changed the RAT to point to this R4 and we free the ROB entry. And finally in cycle 19, we commit this instruction, we put the value of R3 into R1, we then check the ROB entry for R1, is it pointing to ROB6? Yes it is. Make it point to here, and we can free this ROB entry. And this concludes really what happens, in this whole program as far as, you know, cycles one through 19 are concerned. Okay. So now that we have seen a little example of how the ROB-based processor works, let's look at a ROB timing example. So this is how we can figure out what happens when without actually tracking everything, like in the previous, very busy video. Here we have some instructions. Here we have what latencies of various units are. And now we're just going to track when instructions are issued, when they begin executing, when do they write the result, broadcast it, and when do they commit. And here we will put some comments that kind of, you know, help us tell what happened when and why. This time, let's make it a bit different from before. First, our processor will free our reservation station when the result is broadcast, not when we actually dispatch the instruction. This can happen for example, because a processor is doing something speculative and it wants to keep the instruction until it's really sure that you know, the instructions should have executed. So some of the processors actually do this. So, let's try an example with this. The processor can still issue, capture a result and dispatch the instruction in the same cycle. But the execution will again happen in the following cycle. This is just like the previous processors. And again, we have the add take one cycle, the multiplication takes ten cycles and divide takes 40 cycles. So let's see. Well, in cycle one, we will try to issue the first instruction and nothing else can happen because we do not have any other instructions in the processor. This instruction, of course, being the first one that we know does not depend on anything, so it will being executing in the second cycle. It is a divide instructions, it takes 40 cycles to do it. So, 40 cycles after this one, we will try to write the result, and that happens in cycle 42. And commit will happen in the following cycle, because this is the very first instruction, so we can just say 43 here. Now let's go to this multiplication. Once we have some instructions in the processor, we have to start worrying about reservation stations and ROB entries and so on, and we haven't said how many ROB entries we have, but let's say we have a lot of ROB entries. However, we still have two divide multiply reservation stations and three add subtract reservations stations. So now, one of our reservation stations for divide and multiply is busy between cycles one and 42. There is, however, another one so this multiplication can be issued in the following cycle. So it will issue in cycle two. When can it execute? Well, it uses R5 and R6. This doesn't use R3, so it can actually execute in the very following cycle. So we will star executing it here, takes ten cycles, so it will broadcast the result in cycle 13. And when will we commit? Well, we cannot commit in cycle 14. Now, we have to wait for both the previous instruction to commit N for us to have a result, so really we will commit in cycle 44. Now let's look at this add here. It's using R7 and R8, so it will be executing right after it issues. We have ADDand subtract reservation stations, remember we have three separate reservation stations for that. So we will issue in cycle three, start execution in cycle four, takes only one cycle so we will write the result in cycle five, and now when do we commit this instruction in cycle 45. Now things become a little bit more interesting because this multiplication. Wants to issue in cycle four. Remember we are issuing instructions in order, we have reached this instruction in cycle four, we want to issue it. However, we need a multiplication and divide reservation station for that. And we free reservation stations when we broadcast, not when we dispatch this processor. So we already have in cycle two both of the reservation stations used. The first available one will become at cycle 13. So basically we only cycle 14 we can actually issue this instruction. So this instruction was waiting to issue, but couldn't because there was no free reservation station. So let's kind of note this. So this is just kind of to note that basically we have been delayed in our issue because of the needing the reservation station. We're using results of R1 and R3. So we have to wait for the multiplication and ADD before we can execute. But those are results having broadcast in cycles 13 and five, so in cycle 15, which is when we can try to execute, we can. If we manage to issue in cycle four, we will still have to wait until cycle 14 here because of cycle 13 here. So pretty much, our result will be broadcast in cycle 13. We will be able to dispatch only in 13, we would be executing in 14. Because we also needed to wait for a reservation station, we couldn't issue until cycle 14, so we can only execute in cycle 15. It's a multiplication instruction, so it's going to try to broadcast the result in cycle 25 and we will be able to commit it in cycle 46. Now let's look at this attract instruction. If it's going to try to issue in cycle 15 we have to issue in order. So although we could have issued this instruction in cycle four as far as, you know, dependencies are concerned, because we couldn't have issue this instruction in cycle three, four, five if everything was okay. And there was a free reservation station at the time here. But remember, we have to issue instructions in order. Because of that, this will issue in cycle 15, and we will now grab another reservation station in the ADD, subtracting. So now what happens is we need to figure out when do we execute? We need R1 and R5, the R1 comes from this multiplication. The R5 is already available because none of them are producing it. The R1 here becomes available after this result is written. So in cycle 25, we grab the value we need. And then in cycle 26, we begin executing. It's a subtract. It's going to finish in cycle 27. And it's going to commit in cycle 47, not because of this, but because of this. And finally let's look at this ADD here. It's going to issue in cycle 16. Execution will begin when R4 and R2 are available. R4 is available in cycle 27, but R2 is available only cycle 42. So we will dispatch in cycle 42 and begin executing in cycle 43. We will write our result one cycle after that. And we will commit in cycle 48. And that concludes our discussion. Now, here you want to make a note, for example, that the issue has been delayed because of reservation stations. And for these instructions, for example here, you may want to note that execution has been delayed because of R2. Just so that if you, on an exam or something, if you slightly mess this up, if you made these notes at least whoever is grading this will be able to follow, you know, what went wrong and you know, what was your real mistake, and what was just a consequence of that mistake. Now let's do an ROB Timing quiz. So here is again a sequence of instructions and we want to know when they issue, execute, write, and commit. And we want some comments here. Now let's say that an add takes one cycle, multiplication takes two cycles, and divide takes four cycles. And in this processor, we can broadcast one add or subtract result from the add subtract unit. And one multiplication divide result per cycle. So if both the add unit and the multiply unit have a result available they can actually broadcast them in the same cycle. And this processor can also commit up to two instructions per cycle. So two instructions can commit in a cycle if both of them can commit. And we now free our reservation station at dispatch. So let's look at what should be happening here. We know that this instruction issues in cycle one. Begin execution in cycle two. It will begin execution in cycle two. And then four cycles later in cycle six, it's going to write the result. And it's going to commit in cycle seven. So the question for you is for this multiplication. When does it issue execute, write and commit? You can write your answers here. So pretty much what should go here, you should write here, here, here and here. So this is the solution for our ROB timing quiz. The first part, we were asked when does this multiplication instruction issue, execute, write, and commit. This instruction will issue in the second cycle. The reason for that is that we have still two multiply divide reservation stations. This is using the first one, we can use the second one here so we can actually issue in the second cycle. So the correct answer here is two for issue. For execution, we are using R5 and R6. That doesn't depend on R2, so we can actually begin execution right after we issue in cycle three. The multiplication will take two cycles to execute so it's executing in cycles three and four. It's going to try and write the result in cycle five. And now when does it commit? It commits when it has both written the result and when all previous instructions have committed. The previous instruction commit is, had happening later. So, we will commit in cycle eight, right after the first instruction has committed. Okay, continuing our ROB Timing Quiz, we have seen that the multiplication now is going to execute write, commit, exec cycles, three, five, eight. Now let's see what happens with the add and multiplication here. So the question for you is, when does the add and multiply issue? Note that they don't need to issue in the same cycle. In fact, they won't because we can only issue one instruction per cycle. So the question is when does the add issue and when does the multiply issue? Write your answers here for the issue. And also when do they commit? Write your answer here for commit for the add, and write your answer here for the commit for the multiply. Okay, let's work on the solution for our ROB timing quiz this time. So the question is, when does the add and multiply issue and when do they commit. For issue, the add will try to issue in the third cycle, if we have a reservation station available, and we do, because we are currently using both of the multiply dividers reservation station, but the add 1 is free. So the add issues in cycle three. And the next instruction, multiplication, we'll try to issue in cycle 4. The question is whether we have a free reservation station. Indeed we do, because we have begun executing the multiplication in cycle 3 here. That has freed one of the two reservation stations we were holding. So, pretty much, at cycle 4 both of the multiplied divided reservation stations are actually available so we can issue in cycle four. So I'll put this here, so we have 3 and 4. When does this execute? Because, to figure out the commit, we actually have to figure out also when do they execute and write the results? The add D uses R7 and R8. Those are not written by any of the previous instructions so the add can actually begin execution cycle 4. And because it takes only one cycle it will try to write the result in cycle 5. Now the multiplication is already writing the result in cycle 5. But we said that we can't broadcast one add result and one multiply result in the same cycle. This is a multiply, this is a divide, so they can actually go in the same cycle, so this will broadcast the result, also in cycle 5, now let's see what happens with the multiplication here. It issues in cycle 4, it uses R1 and R2. So it actually cannot execute until both of those results have been written by instructions, divide and multiply here. So we have to wait for the later of these two, which is cycle 6. That's when we dispatch, we can begin execution in cycle 7. This is a multiplication, these two cycles. So, in cycles 7 and 8, it is executing in cycle 9, it will write its result. Now we are ready to figure out the commit. We said that we can commit up to two instructions per cycle. In cycle 8 we are committing the multiplication here. The question is can we commit another instruction the same cycle, because normally we could if the instruction is done? And indeed it is because this instruction has been ready to commit since cycle 5, all it was waiting for is for the commit point to reach there. So in cycle 8 here we will commit it, thus the correct answer for the commit here is eight. And now finally, we need to figure out, when does the multiplication commit. Well, it cannot commit in cycle 9, because it doesn't have a result written yet, then. But it will try to commit in cycle 10, and by that time, previous instructions have committed, we have a result. So indeed, it commits in cycle 10. So we can write 10 here. Continuing our Rob Timing Quiz. We have two instructions left to go. The question for you is just, when does the last instruction commit? Okay let's work on the solution for our ROB timing quiz. The final question was, when does the ADD here, the last instruction commit? To do that we have to figure out, what happens with the last two instructions. So can we issue the subtract in cycle 5? Yes we can. Only one of the, add subtract reservation stations is used at that time. In cycle 5 it actually will be freed so from cycle 6 we actually have all the reservations in add subtract available, but for sure one is available in cycle 5. And then the third one, we can use it in cycle 6. In fact, by that time, this one is also available, so we, we can just issue these instructions. The subtract here, uses R2 and R5. R5 is not produced by any of the previous instructions but R2 is. So, we actually have to wait until cycle 7 before we can. Proceed to execution. Now, because in cycle 7, other instructions are also beginning execution, we have to figure out can we actually run this subtract now or do we have to wait for the next cycle. So we want to run it in cycle 7, but remember, this is a multiplication. This is a subtract, so they're using different execution units and they can begin in the same cycle. If another, for example add or subtract was beginning at seven, then we would actually have to wait until eight. But because these are different units we can begin at seven, And we're going to try to broadcast our result at eight. Nobody else is trying to broadcast at eight, so we will succeed at that. And finally, for the add here, it's waiting for R4 and R3. R4 is produced by this subtract. R3 is produced here. So we need to wait for cycles five and eight and then begin in cycle nine. We will try to broadcast in cycle ten. And now the question is when do they commit? . Remember that we can commit up to two instructions per cycle. So what's going to happen. Is in cycle 10. We're going to also try to commit to subtract. We only committing this multiplication so far. We can commit two instructions. We'll try to commit to the next instruction. So in cycle 10 we will actually commit this. And now interestingly enough, this add is done just in time to write to commit to cycle 11. So it's going to commit to cycle 11. So, the final answer for the final part of the quiz is that the commit is happening in cycle 11, here. Now that you have seen how the, ROB based processor works, when we have reservation stations for separate units separately, let's talk about unified reservation stations. So, this is what we had so far. We had three reservation stations for the add unit, and we had two for the multiply unit. This also does subtracts, this also does divides. And we have seen it in at least one of our examples that we can run out of these reservation stations while we still have plenty of these left. However, we still couldn't issue instructions. Basically, once we cannot issue instruction because it needs one of these reservation stations, because issue needs to happen in order. We cannot issue a new this, either. So until these reservation stations become available, we cannot really use the available ones here. Note that, these reservation stations are exactly the same, except that this one is feeding this unit, and this one is feeding this unit. But the logic in them is exactly the same. They're monitoring all of the results that are broadcast, capturing results, and then going to execution units. So to improve our ability to use the expensive reservation stations. We can go with the so called unified reservation stations approach. Where all of the reservation stations are in one big array. Now, when we need to issue, we need to grab one of these reservation stations. But it doesn't matter which unit it is for. Basically all of the reservation stations are equal. So the benefit of this is that, as long as there are any available reservation stations, we can issue instructions. The drawback here is that the logic for dispatching instructions into these execution units becomes a little bit more complication. Here, for issue into the multiplication we only need it to see whether one of these is available, and if, one or more or available, select one. Meanwhile, here we're selecting one of these three. And sending it to the add you need. And if more than one are available we have to figure out just which one of the three. Here however, every cycle we need to select one of these, for the add and one of these for the multiply, so we need to actually consider which one of these are adds to go here, which one of these are multiplies to go here, and then every cycle we need to select one of the adds and one of the multiplies to go there. So, the logic for. Dispatching becomes more complicated here because we have more reservation stations to look at. And we also for each unit we need to look at reservation stations that just have wrong things for that unit, so we have to ignore that. But again, reservation stations are very, very expensive. So, usually processes will use some variant of this organization rather than this one. Basically we don't have each unit. Having its own reservation station's typically a group of units who have a reservation station so that we can benefit from, our reservation stations if we have them. We have seen a RAW based processor, but this processor has not been a superscalar processor. It was only issuing one instruction per cycle, and even though we have seen a variant in one of the quizzes that commits up to two instructions per cycle, the bottom link will still be the issue. So now let's look at a real superscalar processor. What does it have to do? First, it needs to fetch more than one instruction per cycle. When we are really fetching more than one instructions worth of memory per cycle. So if the instructions are for example four bites and we want to fetch two, our fetch really needs to supply us with eight bits every cycle. Then, we need to decode more than one instruction per cycle. This amounts to basically having more than one decoder. So, every cycle the first decoder will be looking at the first instruction, while the second one will be looking at the second instruction we fetched, etc. Next, we have to issue more than one instruction per cycle. Remember that we need to issue instructions in order. So every cycle, we will consider the next instruction that should issue for issuing. And if it can issue, we will also consider the next instruction, and if that one can issue, we will consider the next one, and so on. So for example, if you were trying to issue up to three instructions per cycle, they still need to be three consecutive instructions, and if one of them cannot issue, then the following ones cannot issue either. We want to be able to dispatch more than one instruction per cycle. We have seen that if we have more than one execution unit, we actually can dispatch one instruction to each of the units. Effectively, we will already able to dispatch more than one instruction per cycle. Now, the problem usually is that you have many more adds than multiplies for example. So if you really want to finish more than one instruction per cycle and do that over many cycles, probably we would have to have something like two add units and a multiplication unit so that even if we have only adds to do we would be able to issue and dispatch two per cycle on average. Then we need to be able to broadcast more than one result per cycle. This involves not having only two broadcast passes or three broadcast passes but also every reservation station has to compare it's tags that it's waiting for to every one of the passes because in any given cycle we could have any of the passes producing a result. So, this immensely complicates the reservation station. Basically the cost of each reservation station is proportional to the number of broadcasts we have to monitor. If we had one, let's say that's some cost, having three broadcasting sets is going to be at least three times as costly. And finally our commit logic needs to be capable of committing more than one instruction per cycle. We have already seen in one of our quizzes that this is very similar to issuing in that for committing, we consider the first instruction that is not committed for commit. It that one can't commit, we consider the next one to commit in the same cycle, and so on. So, it's not only that we have to consider three instructions for commit and check whether they're done, we also have to make sure that we don't commit for example, the third instruction we are considering unless both of the previous ones are committed. And among these we really have to be worried about the weakest link. Meaning if we can fetch up to let's say four instructions per cycle, decode up to four instructions per cycle, issue only one per cycle, dispatch up to four, broadcast up to four, and commit up to four, we will still be limited by the issue of only one. So on average we can never produce more than one instruction finishing per cycle and committing, because simply they have to go through issue one at a time. If all of these are very, very large, and one of them is limited to let's say three, then again, we will not be able to average more than three instructions per cycle, no matter how well the programming is managed in this organization. So, pretty much this is kind of like a pipe where the width of the pipe in each of the stage, is determining the flow of the entire pipe pretty much. The narrowest place in the pipe is what limits the flow of the instructions through it. One of the things we also should discuss is the terminology confusion about the out of order processors and how we name different parts of them. We have been calling the stages of the execution, issue was when we obtained the reservation station and the A.R.B. entry. Dispatch was when the instruction was selected for execution, and commit was when the instruction has committed. This terminology was introduced in the original [UNKNOWN] algorithm and then commit was introduced in the first paper that talked about [UNKNOWN]. So most of the academics are going to use, this terminology, so most of the research papers that are written will use something similar to this. However, some companies and some of the papers, will for issue use the term, not only issue, but also, allocate because, this is when we allocated resources for the instructions. And so pretty much we allocate and then [UNKNOWN] and so on. So they're going to call this allocating, sort of issue. And, to confuse the issue even further, some call the issue part, dispatch. So we dispatch the instruction to reservation stations and the ROB entries. Similarly, what we have been calling dispatch, some will call execute. Some will call it issue, so we issue the instruction from a reservation station to execution, as well as our usual dispatch, which is we dispatch the instruction to execution. And finally, the commit is most often called commit, but sometimes, it's been called complete, which introduces some confusion because actually the instruction completes execution. Well before it commits, it's just that now, the term complete will refer to commit if that is the nomenclature that they are using. And then, commit is also often called retire. So we retire the instruction from the processor. And because a lot of papers have been written by academics, especially graduate students, commit has also been called graduate. You can guess why. Finally, let us discuss in arrow B based out of order execution processor, what really is out of order here? So this is our processor pipeline, lets say, this is the fetch stage, there is a decode stage, we issue, then we dispatch and then we have execution, and then the right of the result which is the broadcast we have and then finally the commit. So in this pipeline, if this was an out of order processor, keep in mind that not all of these stages are processing instructions out of order. In fact, from fetch up to issue, we are processing instructions in order. We have to fetch them in the order that they should appear in the program. We then decode them in the correct order, and we fetch them if you remember in order. This ensures that any dependencies we figure out here are the dependencies that are compatible with the original program order. Now that, we have issued instructions to reservation stations and they're waiting for execution, they're going to execute in, the order of data dependencies, not necessarily in program order. So the execution and writing of results, are happening out of order. And then, again, commit needs to do instructions in the proper program order because if you remember the order of committing is really the order in which we tell the outside world that we are finishing instructions. So, this has to happen in order to give the appearance that we're executing instructions in program order. So, when we have an out of order processor, it really is going many stages in order and only the execution and broadcast and things in between are happening out of order. So the dispatching, the execution, the, the writing of results, everything else is still in order. In contrast, what we would call an in-order processor, just as these in order too. But even an out of order proser does a lot of things in order. So let's do an in order versus out of order quiz. So let's say that these are the stages of our processor. We have fetch, decode, issue instructions, then we dispatch them to execution. We have two stages of execution, so everything takes two cycles to execute, followed by a broadcast of the result. Then we have a commit stage. And then we have a separate stage after commit in which we release the ROB entry. The question for you is to mark which of these stages are happening in program order and which one of these are out of order in a out of order processor? Let's look at our in order versus out of order quiz solution. We said already that we have to fetch the code and issue instructions in order. Dispatching is happening in the order of dependencies not necessarily totally in program order so this is out of order. Execution then happens out of order. We broadcast results. When we finish execution out of order. But then commit has to happen in order, and then because we commit in order, the rest of the pipeline after commit, if there is anything will be also in order. So this is what happens in order and what happens out of order in an out of order processor. If this were an in order processor, these would also be here. In this lesson, we'll discuss the reorder buffer, which lets us correctly handle exceptions and branch mispredictions in out of order processors. This is very important for actually making real programs work. So, all high performance processors today, include such a reorder buffer. In this lesson, we will discuss storage systems such as hard drives and how they're connected to the rest of the computer. This will help us understand why they are so much slower but can store so much more data than the main memory. So let's talk about the role of the storage in a computer system. It keeps all the files, the programs, the data, all the settings, the operating system, everything. And also, our virtual memory is implemented using storage because we cannot fit all the data that the application actually wants to access to physical memory. Some of those pages are actually on disk and when the program accesses them, they're loaded into the physical memory. But meanwhile, they're sitting in our storage, for example, on our disk. So storage is important, not only to keep data, but also to extend our virtual memory so that we think we have more memory than we actually have. For both uses of storage, we care about performance, mainly throughput and latency. Throughput is how many bytes per second can we get out of our, for example, disk. Latency, once we request a page of data, when are we going to get it back? And then storage, this is improving, but not as quickly as the processor speed is improving, whereas the latency is improving, but very slowly. The reduction in storage latency is even slower than the reduction in DRAM latency. So processor are getting faster and faster. DRAM is getting faster but lagging behind processors and becoming slower and slower, relative to the processors, and then storage is falling further behind DRAM itself. In addition to performance, we are also really concerned with reliability. If our processor fails, our system temporarily doesn't work until we replace the processor, but then we can boot up the system and it works again. But if our disk fails, we lose our programs, data settings, etc., which is much, much worse than just a processor failing. So we are much more worried about the reliability of our storage than we are about most of the other elements of our computer system. And finally, the types of storage that we can use are very diverse. We can have magnetic disks, our traditional hard drives, optical disks such as CDs and DVDs, tape for backup, flash drives that are much faster than most of the other ones, and so on. So let's talk about magnetic disks, which today usually means a hard disk, or a hard drive. But the so-called floppy disks are also magnetic disks, and work very much along the same lines. It's just that today we mostly have hard drives, and few people even remember floppy drives. A magnetic disk has a spindle to which we can attach so-called platters. All of the platters are attached to the same spindle and rotate at the same speed, let's say in this direction. There is a motor here that really drives the spindle to rotate and that causes all the platters to rotate at the same speed. If we look at the single platter it has a surface covered with magnetic material on one side and also on another. So this drive that has three platters usually will have total of six surfaces. The data bits are on all the surfaces. We access this data by having a magnetic head attached to an arm and another one for the lower surface. And then all of these heads are attached to a head assembly which can move all the heads in unison. So if this head is here, this head will be in the same position on another platter, and so on. We don't individually move the heads, we move them all at the same time. So because the platter is rotating and the head is staying in place usually, the head will be able to access this circle on the surface of the disk. Each head will be able to access the circle on its own surface, and all of these are going to be at the same distance from the spindle. This is called a track. All of the tracks at the same distance from the spindle form what is called a cylinder. The cylinder simply consists of one track from each surface where the tracks in the cylinders are those that can be accessed by different heads at the same time. So for example here, there are six heads. When they stay in this position and all the platters rotate, they're able to access the six racks of that cylinder. The way we access different tracks on the disk is by moving the head so that it comes closer or gets further from the spindle. And that way we can access the entire area of each surface. So the data, naturally, will be organized into bits that are positioned along the track, and then another track will have different bits and so on. If you look at the surface from above, and this is now the spindle, this is would be one track and this would be another. And finally on one track we don't store data continuously through the track because usually a lot of bits are on a single track, instead the data along one track is divided into sectors. And a sector will be the smallest unit that we can actually read. Now if the disk rotates in this direction, and the head is currently here, as the disk rotates, different bits of the sector are going to be under the head. So the sector will have some preamble here that is a recognizable bit pattern that tells the head that this is the beginning of a sector. Then we have the appropriate number of the bits for the sector that actually hold the data, followed usually by some checksum and other information that is needed to possibly correct errors in the sector. And then another sector would just have the same things and so on. So when the head assembly moves to a particular cylinder, then the heads start listening for the beginning of the sector. Once they see the beginning of the sector and see which sector it is, they know where they are in the whole track. So the disk capacity can be computed as the number of platters times 2. This will really be the number of surfaces times how many tracks we have per surface. Or expressed in another way how many cylinders do we have on these disks times how many sectors do we have per track times how many bytes of data do we have in each sector. Usually we have a small number of platters, such as one, two, three, maybe four. We have thousands of tracks per surface, tens to hundreds sectors per track, and something like a kilobyte or maybe a half of kilobyte of bytes per sector. So sectors are usually like kilobytish in size. We have about 100, let's say, sectors per track, maybe a little bit more, and we have thousands of tracks per surface. Obviously, the whole thing needs to be very thin so these platters are really close to each other. It needs to be, let's say, two and a half inches wide total, including the head assembly and so on. So the platters are only a couple of inches in diameter and because we have so many tracks per surface that means that the tracks are spaced very, very close to each other and the head assembly needs to be very, very precise in how it positions heads. So let's see how do we access the magnetic disks and how long does it take to get to our data? We will assume that the disk is already spinning. If the disk is not spinning it takes possibly several seconds to actually spin up the disks. So most of the time if you're accessing the data frequently, the disk would just keep spinning. So if the disk is spinning then the axis time consists of the seek time, which is the time it takes to move the head assembly to the correct cylinder for our data. So we know where the data is, we know which cylinder, which sector, which surface. We first have to get the heads so that they are above the correct cylinder. So at that point, one of the heads will be above the track where our data really is. Next, there is the rotational latency. After we have moved the heads, they're somewhere along the track, but not necessarily, and usually not, above the very sector that we are interested in. So the rotational latency is about waiting for the start of our sector to finally get under the head that will read it. Note that the head is actually sitting in one place at this point, but the platter is rotating so different parts of the track are getting under the head and the rotational latency's how long until the beginning of our sector gets under the head. Next we have the data read which is, we read until the end to the sector is seen by the head. Again the data read really depends on how fast a disk is spinning and how many sectors we have per track. For example, if the whole track is just one sector, then the data read will be a whole rotation of the disk. If we have ten sectors per track, then the data read will be one-tenth of a rotation. Then there is some controller time for the disk controller to actually check the check sound, determine that the sector is okay, and so on. And the IO bus time which is once the disk drive itself and its controller have the data, how long does it take to actually get that data to the main memory? Now note that unlike a cache or a DRAM where we could actually be doing multiple axises potentially at the same time, on a magnetic disk once we seek to one track we cannot really be reading other tracks. So really what happens is accesses to the disk are happening one at a time. We go to a track, possibly read several sectors, but we cannot access another track until we are done with this one and move to the next track. So all of this is the latency to do one disk access and often times the latency of a disk request made by the operating system includes this, plus a significant queuing delay because we first have to wait for previous accesses to finish doing this and only then our access can move the heads to the correct position and do a read or a write. So, let's see if we can figure out the disk access time. Suppose we have a disk that has a 1000 cylinders and 10 sectors per track. Suppose that the head assemble starts out at cylinder 0, at one end of the disk, and the cylinders are of course numbered 0 through 999. Suppose that the speed at which the head assembly can move is 10 microseconds per cylinder. So it's moving at linear speed. And the disk rotates 100 times per second. Suppose we have a perfect controller in the bus, or very, very fast relative to this. And suppose that there is no previous request so we don't have to worry about queuing. So the question for you is what is the average time to read a randomly chosen byte from this disk, where every bite on the disc has the same chance of being requested? Let's look at the solution to our disk access time quiz. We can ignore queuing, so our read of a randomly chosen byte begins by seeking there. The head assembly is at one end of the disk. A randomly chosen byte will be at the cylinder where we are, or 999 cylinders away, or anywhere in between with equal probability. So on average, we will be seeking 500 cylinders away, times 10 microseconds per cylinder. So our seek will take us five milliseconds. At that point, our head is in a random place on the correct track. Now we need to wait for the beginning of the correct sector to get under the head. And just like when we were doing the seek, the rotational latency will be on average, half of a rotation. Sometimes we will be lucky and land just at the very beginning of the sector we want. Sometimes we will be unlucky and land just past the beginning of the sector we want. So, we will not be able to read that sector, even though we are technically right above the data we want to read. We will have to wait for the beginning of the sector to get back which is a whole rotation. So, on average, we will be having half a rotation. Each rotation is a hundredth of a second or 10 milliseconds. So we end up adding another five milliseconds to our read time. Next, we need to read our sector. There are ten sectors per track. So the read will take 1 10th of a rotation, which will be one millisecond because we already determined that a rotation will be ten milliseconds. So in total, it will take 11 milliseconds to complete a disk access. So, let's look at some of the trends for magnetic disks over the years. The capacity of magnetic disks, how much data fits on a single disk, has actually been improving exponentially at the rate of about two per one or two years for a while. So things are getting better. We get more and more data per magnetic disk. The seek time has consistently been five to ten milliseconds over a number of years, with very slow improvement. The only things that really improve the seek time is if we either make a faster motor that can still move the head assembly very precisely, which is very hard to do. Or if we make the platter smaller so that we traverse a smaller distance. So there has been some recent improvement in that we have gone from five inch diameter disks to three and a half, to more recently two and a half and even smaller. So there has been some improvement in seek time, mainly because the disks have shrunk. The speed of rotation for disks has actually been improving. It went from 5,000 rotations per minute to 10,000 rotations per minute, and even beyond that. So that part of the magnetic disk speed has been improving. So this type of improvement requires improvement in what material is the platter built out of. Another factor that has been affecting this is the noise. Disks that rotate faster, tend to both noisier and also create a higher a pitch sound, which is less pleasant for people. And this type of improvement is also relatively slow. And the speed of this controllers, and the buses over which we transfer data has been improving at an okay rate. So really the controller and the bus are becoming a smaller and smaller fraction of the overall disk access time. So the disk access time is pretty much dominated by the seek time and to a lesser extent now, the rotational latency. And now you can see why disks have been lagging in improving their latency relative to memory and processors. This is because seek time requires better mechanics, and rotational latency also requires better materials and better mechanical properties. Both of which are not really subject to something like Moore's Law that we get for processors. Now let's briefly talk optical disks. An optical disk is very similar to a hard disk in that it has a platter that rotates and we store the bits on the surface of the disk. The difference is that, instead of there being a head that needs to be placed very close to the disk surface, we can shoot a laser to the surface and the reflection from the material here will tell us whether we have a zero or a one. But many of the other properties of the optical disks are very similar to hard disks. They have tracks and so on. Unlike hard drives that tends to be closed units, so you don't just insert a new hard drive into the machine, and you don't carry one with you usually, optical drives don't need the laser to get that close to the surface so the smudges or dust are less of a problem for it. Basically if there is a speck of dust here, it doesn't scratch the laser as it rotates, but a speck of dust will scratch the magnetic head as it rotates and probably glue itself to the head and then the head no longer works. So hard drives have much more of a problem with dirt, so they need to be an enclosed case. Because optical disks suffer less from this problem, we often see them in the form of CDs and DVDs that we can carry around and even though they get dirty, we can still read data from them. But as a result of them being portable, they need to be standardized so that we can put a CD or a DVD into any drive that we want. And this standardization helps improve portability and their usefulness, but it limits the rate of improvement, because the technology improves but then that technology needs to make it through a standards process, where companies agree that something will be a standard and only then make it into products. So really, the technology can improve relatively rapidly. But we don't see any of that progress until a new standard is decided on at which point products are released. In contrast, with hard drives, the whole enclosure needs to obey some standards about how to connect to the rest of the computer, but what happens inside the enclosure can be the latest and greatest technology in magnetic disks. So usually optical disks are used for pretty much carrying the data around, storing music, movies and so on. Let's briefly talk about magnetic tape, which is usually used for backup purposes or so-called secondary storage, which means that the magnetic tape is not the first device on which you look for your data. You first try to find your data on the hard drive. And only if it's not there, you go to secondary storage. Either because the hard drive was lost and you need to restore and back-up or because this data was not used for a long, long time, so it was stored on tapes and put away somewhere. Tapes can have large capacity and are usually replaceable. Meaning you just bring a reel of tape and you attach to the tape machine. But their access is fundamentally sequential. We need to seek along the tape until we find the point we are interested in. So tapes are very good for things like back-up restoring back-ups and so on when you're writing things sequentially for a long time or reading them sequentially for a long time. But tapes are not very good, for example, as a virtual memory or something like that. Tapes are slowly dying out, mainly because they have a low production volume because not many people actually use tape. So the cost of tapes is not dropping as rapidly as the cost of disks, which have much more mass production, so their cost gets cheaper over time. And at this point, it's cheaper to buy an entire hard drive together with all the motors and the head assembly and a built-in electronics for it, than to buy some tape and the machine that will read it. So people for example, use USB drives which is pretty much an entire hard disk just packaged with a USB interface, as a method of backup storage and so on. So let's see if we understood the difference between the disks and the tape. If we want to read a gigabyte file from start to end it is okay to use disk or tape, or is it okay to use both as far as performance is concerned? But if we want to read just the first and just the last byte of a one gigabyte file that will work reasonably well with a disk, with a tape, with neither of them, or with both of them. And if we want to make a cat happy, we would give it a hard drive or a reel of magnetic tape. Let's discuss the answer to our disks and tape quiz. Tape can only efficiently support sequential axises. We are reading a large file from start to end, which is a sequential axis so it's okay to use tape. However, the disk will not do too bad on that either. If, however, we want to read just the first and last byte of a large file for tape we pretty much have to fast forward through the entire file on tape. So tape is not going to do too well, whereas on disk we will seek to where the first and then where the last byte is and just read it. So disk will still work reasonably well here because the disk is a so-called random access medium meaning we give it an address and it kind of goes there. With a tape, it takes a lot longer to get to any single position because we need to really go past all the other positions. And finally, a cat would much rather play with a reel of tape than with a hard drive and there are thousands of YouTube videos that prove this point. Cats like stringy things much more than disky things, especially if the disky things are enclosed in a metal, rectangular box. So we have seen that hard drives do not really benefit from Moore's Law. They don't get much faster and also, although their capacity is improving, their speed is very, very, very slow. So one question would be RAMs, especially DRAM, are benefiting from Moore's Law in terms of capacity. So their capacity is growing about as fast as the capacity of hard drives, but they're way faster than hard drives. Hard drive access times are about, let's say, ten milliseconds, whereas RAM access times are well under a microsecond. So we're talking about at least three orders of magnitude speed difference. So the reason we normally do not use memory for storage is that the hard drives are about 100 times cheaper per gigabyte. However, DRAM has many times better latency than hard drives. So people came up with this idea of solid-state disk, SSD, which really is not a disk at all. It is just solid-state, meaning electronic storage, but because it's intended as disk replacement they call it disk. One approach to build a solid-state disk would be to use DRAM and our controller that will refresh it and then have a battery so that we can keep the DRAM content refreshed and not lose it. This is very fast. It's actually very reliable, more reliable than the hard drives, but it is extremely expensive. And it is not good for archiving data because eventually the battery will run out, and then we lose our content, whereas a hard drive can be stored for a really long time. Today there is another option, which is flash memory. So flash is fabricated in similar technology to the one that is used for making DRAMs and processors. And it uses transistors to store data so it benefits from Moore's law. It consumes very little power because unlike a disk that spins all the time even when you're not using it, flash, while it's idle, consumes very, very little power. And also it doesn't need to move any heads or anything. It's still very fast, way faster than disks, although it is slower than DRAM, and it has smaller capacity than disks because for flash we are still expressing their sizes in gigabytes, whereas disks easily go into the terabytes. And one good thing about flash, as compared to the DRAM and battery, is that flash keeps the data alive without power. So you can store something in a flash, forget about it for several years, and you will probably still be able to read it, whereas here, your battery will almost certainly be dry after a couple of years. So one very popular approach would be to combine the magnetic disks with Flash. The magnetic disk is very cheap per gigabyte compared to flash. We know how to build huge capacity magnetic disks in a relatively small package, but it's very power hungry. It spends and consumes power even when not accessing data. And it's very slow because it has to mechanically move things around in order to access data. And another disadvantage of magnetic disks is that they are sensitive to impact. Like if they fall down and hit the ground, while they are spinning, because their heads are then likely to scratch the surface of the disk and permanently damage the whole track possibly, and maybe the head itself. Flash on the other hand, is fast, power efficient, and has no moving parts. So if it falls down, it might get damaged, but it's very unlikely. So the idea is to have both of them, and use the flash effectively as a cache for the disk. That way most of the data is on the disk, but the data we're frequently accessing is on the flash where it's fast, it can be accessed without moving parts, and it can be accessed very efficiently. Now of course the disk would still be spinning if we occasionally access it, but if we have enough flash to access data in it for even minutes at a time, we can actually power down the disk and spin it down, and only really access it once we have a flash miss. And because both of these can keep their data without power, if we turn off power suddenly, the data in the flash will still survive, and so will the data on the disk. Unlike for example, if we use a magnetic disk with a DRAM cache to keep the most recently accessed data. So let's see if we can compare the flash disk and the combination of the two. Let's say that we have a user who will play a game for two hours, and the game issues reads to 2 GB of data and writes to another 10 MB of data. And then after these two hours the user switches to watching a movie for two hours where throughout this two hours we will read 1 GB sequentially. And then the user does this three more times. If we have a disk that can read 100 MBs a second, if we read sequentially, and read or write 1 MB per second, if we do it randomly, and if we have flash that can access 1GB per second either way. What is the total access time for all the data here, if we use disk? What is the total access time with the disk in this whole, 4 times 4 hours? What is the total access time if we use pure flash? And what is the total access time if we have a disk and a 4 GB flash cache that is initially empty? Lets look at the solution with our storage comparison quiz. The total access time with the disk will be when we play a game, we will read two gigabytes at 100 megabytes a second, so we will spend 20 seconds reading these two gigabytes. And we will also spend ten seconds writing to the ten megabytes. So we will spend 30 seconds here and then a sequential read of the one gigabyte here will be another ten seconds. So over the four hours we spent 40 seconds reading and writing you, using the disk, we repeat that four times. And we get 40 seconds times four, equals 160 seconds. With flash, we get one gigabyte per second so we will get two seconds and change and another one second. So we will get 3.01 seconds times 4. We get 12.04 seconds. Which is way better than the disk. But if all we have is flash, we have to buy a lot of flash, and we probably paid thousands of dollars. So, now let's say what happens with the disk and the four gigabytes of flash. The first time we read these two gigabytes and another ten megabytes, we will read them in 30 seconds, because we are accessing our disk, but then we move them to the flash. The next read is going to be the one gigabyte again in ten seconds but we move them to the flash. So we spend 40 seconds accessing the disk in the first round of this, and we spend 3.01 seconds copying this data to the flash. After that, we're having the flash access time. So all together we have 40 seconds of disk access time, plus 1 4th of this for copying to the flash, and 3 4ths of this for actually reading off the flash the same as here, for a total of 52.04 seconds. So although it's slower than pure flash, it's still much faster than pure disk. And this is about as costly as a single disk, because four gigabytes of flash are not going to cost us that much. So now that we have seen what storage devices look like, let's talk about how they and other I/O devices can be connected to the system. We usually connect I/O devices using some sort of a standardized I/O bus. It needs to have a standard because we want to be able to connect for example, hard drives from one manufacturer to a computer built by another one. And then possibly even replace the hard drive with another one from another manufacturer. Or even move our existing hard drive from one system to a new one we are buying so that we can access our data without having to spend a lot of time copying it. Because these buses have standards they have a limited rate of improvement, and often this improvement goes in steps. So all of the technology might go up in a relatively smooth fashion. This is where a new standard gets adopted by manufacturers so things suddenly improve. But then for a while you just have the same buses even though the technology is actually improving. And instead of having just one type of bus that connects to all I/O devices, typically we have a hierarchy of buses. You have a mezzanine bus such as PCI Express. It will be pretty fast and short so that it can be this fast, and it can be used to directly connect fast devices to the processor such as for example graphics. Then we will connect storages buses to it. For example, the SATA or SCSI for hard drives, both of these are used usually to connect storages devices. These are somewhat specialized for storage. And a SATA controller will act as a PCI Express device on the mezzanine bus. But the controller itself actually controls a SATA bus to which we now connect devices. So the reason why we have these and don't directly connect our hard drives to the PCI Express, is that although these, for example, serial ATA buses are slower, there is also less change in their standards. And there need not be that much change because these devices are much slower than what the PCI Express can provide. So, we don't really need all the speed of PCI Express. In exchange, we get to keep our hard drive standards for a while longer while PCI Express can improve a little bit more rapidly. All we have to do is devise new controllers that connect these storage busses to the newest PCI Express that we have. And then we can have a USB hub for example, also attached to a PCI Express bus. The USB hub itself provides a USB bus connection for USB devices that are even slower than for example, SATA. But again, their standard lives for a lot longer, so we can use a lot of different devices relatively cheaply and connect them to this. So overall we can have is a PCI Express bus to which we can attach the processor, the graphics, the I/O bus control for disks, for USB. And these in turn provide buses to which we can attach disks here, and also some devices here. So the closer we get to the processor, the more important it is to keep improving speeds, because, because some of the devices, like graphics, need that speed. But for storage, and for many of the standard USB devices, we really are much more interested in standardization so that we can exchange these. So that we can interchange these devices and not have them tied to a particular technology and have to change them every year or two. So, now we know how storage devices work and how they're hooked up to the processor, but what happens when they fail? Failures and how to avoid them are the topic for our next lesson. In this lesson we will learn how to provide efficient synchronization among cores and threads, that are trying to work together on the same program. So let's look at one example of why we may need synchronization. So let's say we have two threads and what they are doing is counting occurrences of different letters in a letter. The first thread, let's call it thread A, is counting in the first half of the document. So it's going to count occurrences of each letter in the first half of the document. And the second thread, let's call that thread B, is counting in the second half of the document, and then they will put these counts together somehow. So, the code might look like this. A thread will load a letter, load from memory the count for that letter, add one to that count, and then store the new count back into memory. While thread A is doing this, thread B might be doing this. It loads a letter from its own part of the array. Note that R1 and R2 are different in these threads. This is kind of the pointer to where I got in this part of the array, this is the pointer where this thread got in its part of the array. So this thread is doing exactly the same work as this one, and as long as the letters they're processing are different this is going to work. This thread is incrementing one part of the count array, this one is incrementing something else in the count array. But what if this thread sees letter A. And this thread, at the same time, sees another letter A in its own part of the array. Then what happens is they both try to load the same element from the count array. Let's say the count was 15, so this one will load 15 and this one will load 15. Now we add one and we get 16, and we do the same here. Finally, we store 16 to memory, and here we also store 16 to memory. So now there were two occurrences of letter A, but the value in the array ends up being 16, which is only one more than the original 15. So after both of these threads are done updating the counter, the counter should have been 17, so clearly this is incorrect behavior. What we need is really that this happens and then this happens, or the other way around, but these should not be happening simultaneously because then we can get the incorrect result. If for example, this happens first, we load, we get 15. Increment store 16. Now when we load this here, cache coherence ensures that we load 16 and we end up with 17 here which is a correct result. Similarly if we did this first it would produce 16 in memory, then we would load that in increment to 17. So no matter which one of these happens first we should get the correct result, but we don't get the correct if they happen in an interleaved fashion. They should happen one at a time, I did this and this, or all of this and then all of this. So these things that have to execute kind of one at a time are called atomic or critical sections. So we have one critical section here and another one here. They can actually be the same code, it's just that both threads are currently in the same piece of code. But as you can see, if we have a section that should behave as a critical session, meaning, we do this without interference from any other thread. If we just let it happen, it's not going to be done correctly. So what we need is synchronization. We need additional code that ensures that only one thread gets to do this at any given time. The type of synchronization we use for atomic sections is called mutual exclusion or lock. So this is really an example of where a lock will be needed. What we really need to do is say something like, I'm entering, don't come in here. Then do this, then say, I'm leaving, now you can come in. And when this one is done, this one can then say, I'm entering, I'm leaving. But if this thread comes and wants to continue past this point while the first thread is already in the critical section, we should wait here. So what we want here really is, before we get into the critical section, qe need to lock, a lock. Let's call that count lock that corresponds to a given element of disarray. So count lock of L is a lock that corresponds to count of L. If this lock currently is open, then we will enter. If this lock is currently locked, then we will spin there until it becomes open. Once we have grabbed the lock, we can enter here. The other thread will do the same. So the first one of them that succeeds in grabbing the lock will enter this critical section. The other one will now realize that this count lock has been locked and will have to wait there. Once we leave the critical section, we will unlock the lock. And now, if this one is waiting here at that point it sees that the lock is now available and starts executing the critical section, and then releases the lock. Note that having the lock and unlock enforces mutual exclusion of this code. We do this alone. And we do this alone. But it doesn't impose a particular order between them. So, whichever of them comes first to this lock will execute its critical section first. So we can either have this happen first, and then this happen second. Or we can have this happens first, and this happens second. What the lock is really preventing is interleaving when these are happening simultaneously. But it doesn't prevent, for example, this second thread doing this first and only then the first thread gets to do it. So let's see if we can figure out what a lock variable really is. Suppose we have a code like this. This code is similar to the previous one, it's just written in C, so we have lock, the lock that corresponds to element L, increment account for L, unlock. The question for you is, what is count lock of L? Is it just another location in shared memory, a location is some sort of special synchronization memory, or a special variable that doesn't really have a memory address at all? Let's look at the answer to our lock variable quiz. In this code we are using the lock, CountLock and it's an array of lock that has elements. So we are indexing into the array and getting that one element of the array and that's our lock that we lock to access count of L. And then we unlock that lock. So the question was, what is really this lock? Is it just another location in shared memory? So, CountLock of L really is just a memory word. Or is it a location in some sort of special synchronization memory? So, there is a sort of special memory where this array of locks is residing. Or is it that locks are special variables that we can refer to like this in code, but really they don't have memory addresses whatsoever? The correct answer here is that a lock is just another location in shared memory. So, a lock is a variable just like any other variable. It has a memory address. We can load to get its value, we can store to change its value and so on. So, now we will see what that variable looks like and what the lock and unlock functions are supposed to look like. So let's look at what lock synchronization looks like. So we said that a lock, or mutex is just a location in memory. So let's say that we are using integers, to represent the lock structure. So really, a lock is just a number. We initialize a lock variable. So you pass a lock variable to initialize to let's say a lock_init function. And what it's going to do is set the lock. To an unlocked value, which, let's say that's 0. So pretty much lock initialization is just write 0, to that memory location that represents the lock. Zero means unlocked in this case. And now the lock function, the function that tries to acquire the lock and make sure that we're alone in doing that. We'll do something like this. It will spin, as long as the lock var is 1, which is the lock value and once it sees that lock var is 0 not 1, it's going to put 1 in that value, to mark it as locked, and then we can enter the critical section because others will now see our 1 and spin there, and then to unlock, we will just, set the lock variable back to being free, once we do this. Coherence insures that whatever is waiting here, now sees a 0 and acquires the log themselves, so we let them go, into the critical section. But this log function doesn't really work. Suppose you have two threads, the purple, and the green that come to this point at the same time. The log was initialized to 0, both of them read the log for the first time, see 0. Both of them exit this loop because the lock var is not 1, so both of them come here, write 1 to the lock var, and then leave this lock function, and now both of them are in the critical section, which is what we were trying to prevent. What we really need to correctly implement the lock function, is that the looking at lock var and seeing 0. And the writing to Lock var of 1, needs to be atomic, which means we really need to do this in a critical section of its own to make sure that when we see a 0, we end up writing 1 before anybody else gets to see a 0. Or at least, when we see a 0 we try to write 1, but we know if we were the one to write 1 or if somebody did it before us. We should not write 1 if somebody succeeded in writing 1 before us. So we have some sort of a paradox here, in that, we need locks here, in order to implement the lock function. So how do we ever support a lock function? Because in order to have a working lock function we really need to have a lock and an unlock here. So let's see how we can implement the lock function. Our lock function needs to do something like this. We need some sort of a magical lock that works without using locks. That protects a critical section in which we check if the lock variable is zero. We put one in the lock variable. And we leave. Because we have just successfully acquired the lock. If the lock variable is one, we repeat all of this, and we need to let others also be checking, so really we need to insert the unlock magic here and here. So basically we're just trying to do this check. And if it's 0, setting to 1 automatically with each other using some sort of magic. Obviously there is no magic. So if there is no magic, what to we do? There are several ways to get the effect that lock magic would give us. One way is to use what's called Lamport's Bakery algorithm or some other algorithm that actually is able to use normal old loading stored instructions to replace lock magic, but these algorithms tends to be relatively complicated. And just as simple a lock function will take 10s of instructions to do. So doing this is expensive, it makes the lock function slow. The other option is to use some sort of special atomic read and write instructions. So there will be an instruction that does this check and set to 1, or something similar to that, that would allow us to in one instruction do this without needing the magic. So let's see if we can figure out what kind of atomic instruction do we need. So we want to implement locks easily. And for that, we need what type of instruction? We need a load that reads memory, or we need a store that writes memory, or we need an instruction that both reads and writes memory, or we need an instruction that doesn't access memory at all. So what do we need in order to implement locks relatively easily without resorting to complicated algorithms? Let's look at the answer to our atomic instruction quiz which asks, what do we need to implement locks easily. An instruction that, loads memory, stores to memory, or one that reads and writes memory, or one that doesn't access memory at all. We already, saw that when either instruction does something like this, without interference from others. So we the instruction that atomically does this to operations. One is a read, one is a write. So an instruction to just read's memory, will not give us what we want. It's capable of doing this, but it's not going to let us do this. A store, might be able to do this, but it can not first check the log variable to see if it's available. So what we need really is an instruction that both reads, and then writes the memory. And certainly we don't need instruction that doesn't access memory because the lock var is a memory location. So the correct answer is this. We said that to implement locks easily, we need atomic instructions that are going to both read and write memory. There are three main types of such instructions. The first type is an atomic exchange instruction, something like this. The parameters for this instruction look like the parameters for a load or a store, but what an exchange does, it does both a load and a store at the same time, so that it swaps the content of R1 with the contents of the memory location that it, the address is. So a load would read this and put it in R1. A store would take what's in R1 and put it here. An exchange swaps the contents of the two. So the register gets what was in the memory location. And the memory location gets what was in the register. If we have an atomic exchange, what we can do is put 1 into R1 and then repeatedly as long as R1 is equal to 1, exchange R1 with that lock variable. So this is the idea, R1 being 1, we are exchanging our R1 with a lock variable. We start out with 1 in R1. When we swap them, if the memory location for the lock had a zero, it becomes one in the same instruction that gets us a zero. If we do get a zero, we exit this loop. And the lock variable has become 1. If however the lock was already occupied, then we swap 1 with 1. R1 stays 1, so we keep looping. So until we succeed in obtaining a zero, we will loop here just as before, but also as we're getting our zero, we are automatically setting the lock variable to one, so nobody else can also observe a zero. Pretty much if we get a zero in our R1, that means that everybody who'll try to do this at the same time will get our one. If somebody else did this before us, then we would be getting their one in our R1. So the fact that we are reading zero here means that we are the ones who acquired the lock. So atomic exchange gives us this read and write at the same time behavior that we want to implement our locks and as you can see the lock function can now be very simple. The drawback of the atomic exchange operation is that it keeps writing to the memory location all the time, even while the lock is busy, while the lock has been locked by somebody else. So as long as we're doing this, we will be these writes even though we are just getting a one all the time. So the second main type of atomic instructions is a whole family of things that can be classified as test and then write. So, the idea is that we test the location and if it satisfies some conditions, then we are going to write, but we don't write all the time. So let's say we have some sort of a test and store instruction which behaves like this. We read the memory location that we are addressing, and if that location is zero then we store R1 into it. And in that case, R1 becomes 1, but if the location wasn't o ,then we don't store anything to that location. We just return a 0. So the idea here is that we are testing the whether the lock is free. If it is free, then we try to write a 1 in it. If it's not free, then we simply continue. There is no point in trying to write to a lock that we know is not free. So we're avoiding the writes that happen in the exchange, while the lock is actually not free. So this behaves very similarly to the exchange, except that it only does the write if the lock has the unlock state. So let's see if we can use the test and write family of instructions to implement logs. The instruction we will be using will be the test-and-set instruction and its parameters are a register and a memory address, usually in the form of offset plus a register. What test-and-set does is very similar to our test and write. In that it checks the memory location at this address. If the memory location is zero, we put a one there. And we set our one to one. If the memory location was not zero, then we just set our one to zero without writing to memory. If we have such a test-and-set instruction, the log function for a mutex lockvar can be implemented by setting our R1 to zero. And then, as long as R1 is equal to zero doing what? Let's look at the solution to our test and set quiz. We have a test and set instruction that checks the memory location at the address defined in the instruction. If that location is 0, then we write a 1 there and set the register to 1. If the location is not 0, then we just set the register to 0 without writing to the memory location. And we want to use this to implement a lock function. The way we do that is, we set R1 to 0, and then, as long as R1 is equal to 0, we test and set lockvar and get the result of that instruction in R1. So, what happens is, our R1 starts out at 0, so we will enter this first iteration of the loop. If the test and set finds the lock busy, it's going to read lockvar, see that it's not 0, return 0. And R1 stays 0, and we keep looping. If, however, the test and set finds the lockvar to be free, that means its memory location is 0, then we do this. We set lockvar to 1, thus changing the state of the lock to locked, and we return 1 in R1, so that this loop will exit. So, we will only exit this lock function once we acquire the lock, and we will wait here as long as the lock is not available. So, coming back to our atomic instructions, we have seen that we can have an atomic exchange, where the problem was that it keeps writing to the lockvar even when it's not available. Our test and write class of instructions, is trying to avoid that. By testing the memory location, and only writing to it if they find a zero there. And we might implement a log, with a test and store, for example, instruction, we will implement a log by setting R1 to 1, then testing log variance storing R1 there. And the result we get from this test and store, let's say 0 in R1 means that the lockvar was occupied. In which case, we will just repeat this, try this again and again. But the idea is that, as long as the lock is occupied, this test and store is only reading the lock variable, it is not writing to it. This is good, because remember coherence. If you keep writing, you keep invalidating all the other copies. So, everybody that is waiting, keeps generating bus traffic because they're all trying to write, and thus invalidating each other. In contrast the test and store, everybody that is waiting while the lock is occupied, simply gets to share the lock variable. They are all having it in the shared state and simply ro, and simply iterating on their cached copy. Once the lock has been freed, those shared copies are invalidated, because the unlock, writes to the lock variable. At this point, everybody sees the new value and tries to grab the lock, so there's really only communication on the lock, when the lock becomes free, until somebody acquires it again. So this test and write approach, solves the problem of continuously writing to the lock variable, but doesn't really solve the problem of, this is a really strange instruction. It's neither a store nor a load. It would be ideal for processor design, if we could do something that is very much like a load, and very much like a store, yet behaves this way. And that brings us to the third type of atomic instructions, which are really now two instructions. The special load, and a special store, that are somehow linked together, to create an atomic instruction. So lets look at the load linked/store conditional instructions, which usually we call LL/SC instruction. We have them because atomic reading and writing in the same instruction even if we just do the write, if the read has tested positive for something. Is really bad for pipelining your processor. Let's look at the classical five stage pipeline where we fetch the code and read registers. Do the area operation. This is where compute the address for memory instructions. Then we have a memory axis stage and finally, we write the result to a register, if a register result exists in the instruction. Loads will be fetched, decoded and read the register, compute the address using that register, access memory to get the value from that address and then write the result to a register. An atomic read/write instruction, such as a swap, or test and set, or test and write, would compute the address and read registers. And then when it gets to the memory stage, it cannot do everything it needs to do in one access to memory. because loads and stores do either a read or a write. A read/write instruction needs to do both and that cannot be done in the same cycle for the same memory location without really complicating the memory stage a lot. So just for the sake of atomic read/write instructions, we would need to add memory 2 stage, possibly memory 3, so that they can read the memory, do some checking and then finally write the memory. So we would be adding to our pipeline several stages, possibly, just for these instructions, but remember that every instruction flows through the pipeline the same way. So if we add these two stages for these instructions we really have added two stages for all instructions, which is not very good. because these are actually not going to execute that frequently to justify this. So we will separate the atomic read and write into two instructions. The first will be a load linked instruction. Its purpose is to implement the read part of the atomic operation. What it does is it behaves just like a normal load. It just reads from a memory location and puts the value in a register. But in addition to that, it saves the address from which it loaded into a special link register. So that's the only thing that it does that is different from a normal load. The store instruction that we will use to implement the right part of the atomic read/write operation is called a store conditional. What it does is to check first if the address that it computes is the same as the one in the link register. If the address is the same, then it does a normal store to that memory location, and returns a 1 in it's register. If the address that the store computes is not the same as the one in the link register, then it just returns 0 in the the register without storing anything. These two are behaving like a single atomic operation because of the link register. So let's see in more detail how that works. So remember that we need the load length and the store conditional to behave like a single atomic instruction so that writes by other processors cannot come in between them without us knowing. Well this is how it happens. We do a load length from a lock variable to R1, and then we do a store conditional of R2 into the lock variable. And what we need is that if the load link finds the lock available and we try to do a store conditional, then the store conditional should only succeed if the lock is still available at the time we try that. If somebody else has written into the lock, then store conditional needs to fail. The key for that is that if we snoop a write to lockvar at any time, we put a 0 in the link register. So if the load link loads the lock variable, and we see that it's free and we try to do a store conditional, but somebody beats us to it, then the store will fail because it will observe that the link register doesn't match the address of the lockvar. So the load link/store conditional really are relying on the coherence where we observe write by others to make sure that if the store conditional succeeds, it can guarantee that nobody else grabbed a lock between when we checked the lock and when we think we grabbed it, really by writing a busy value into it. Now note that the load link/store conditional are atomic by themselves. We don't need actually to use a lock to make this atomic. So some of the critical sections that would just lock something, increment a variable and then unlock the lock, can now directly be implemented using load link and store conditional on the variable itself. By load linking the variable itself into R1, incrementing R1, store conditional of R1 back into the variable. And now we need to check if the R1 as a result of the store conditional became a 0, in which case somebody else was trying to do the same thing, and we didn't succeed in doing this atomically, in which case we just try again. So eventually we will succeed in this. We will load, and we will manage to store it before somebody else beats us to it. So if another thread is doing the exact same thing here, let's say we both load. Now both of us link to the variable, we both increment, we both stores. Whoever does the first store will succeed. Whoever does the second store is going to fail because by that time their link is broken by the first store. So that's how, for example, we can do atomic increments and all sorts of other things that involve only one memory location. So very simple critical sections like this can actually be directly implemented with load link/store conditional without even needing locks anymore. We can use load link and store conditional directly on the variable itself, and we no longer need an actual lock around it. For more complicated critical sections, such as those that access multiple variables, however, load link/store conditional doesn't work well. But we can still use load link/store conditional to actually implement the lock variable here without needing complicated algorithms. So let's see if we can use load links to our conditional to implement a lock function. The lock function takes a reference to variable lockvar, and then, in assembler, and then it will do something like this. Put 1 in R1, load the current value of lockvar into R2 using a load link, then store R1 into lockvar using store conditional, so the idea is that we load the current value of the lock and then we store a 1 there. The question for you is what do we put here to complete this lock function in a way that works correctly? That is, if we leave this function we know that we have acquired the lock and we are the only ones that did so. The first option for where the question mark is, is simply to do a branch on branch on equals 0, R1, trylock. The second choice here is branch on equals 0, R2, trylock. The third choice is to do branch on not equal 0, R2, trylock. And then branch on equals 0, link, trylock. The fourth option is branch on not equal 0, R2, trylock, and then branch on equals 0, R1, trylock. And the fifth, final option is to do branch on not equal 0 in R2 to trylock. Followed by branch on not equal 0, link, trylock. Select which one of these five is the correct thing to put here. So let's look at the solution to our load link store conditional lock implementation quiz. This is what this code is trying to do. It's putting one in R1 and then it's loading the current value of the lockvar into R2 and then trying to put a one into the lockvar. All of these choices are repeating the try to require the lock under some conditions. So the question is what are the correct conditions for retrying the lock? Well, there are two conditions that need to be checked. One is did we actually see a lock that was free? The lock is free when we read a zero. In that case, we don't go back. We actually saw a lock that was free. If the lock was busy, we go back. So if R2 is zero, then we actually don't go back to try lock. That means that we are actually acquiring the lock successfully so far. So this is definitely not a correct solution. We need to check R2 for not equal zero. Meaning, we found a nonzero value here. That means that the lock was busy and we retry. So one of these three are going to be the correct answer, because they include the correct check for whether the lock was available. The second thing that we need to check is if we found the lock available, which means we are past this point. We need to check, whether we successfully stored R1 without somebody else beating us to it. We successfully store R1, if R1 after the store conditional has a value of one. So we fail to store R1 successfully if R1 is equal to zero. That means that this is really the correct solution. Note that these two are checking the link register directly and we said that the link registry is a hidden register. It can really only be accessed implicitly through load-link and store-conditional instructions. There is no instruction that can directly read the link registers, so that's why these two are not correct choices. So in this lock function, basically what we are doing is if we saw the lock that was free and we managed to store R1 into it without anybody interfering, then we acquire the lock. If we saw the lock was busy, we try again. If the lock was free, but somebody beat us to it, then again, we try again. So now let's consider how different lock implementations interact with the coherence protocol and what performance do they achieve. Let's look at three cores, 0, 1, and 2. Let's first try to do this with atomic exchange instructions. Core 0 let's say, tries to grab the lock first. And it succeeds. It does an atomic exchange, gets that the lock was free. The lock var is now going to be in the modified state in the cache of core 0. And it's going to not be present in core 1 and core 2's caches. Now eventually core 0, which grabbed the lock, will unlock by writing 0 to the lock var. But the question is what happens meanwhile on core 1 and core 2? Both core 1 and core 2 might want to also enter the same critical section. Let's say core 1 tries first. He does an exchange of R1 and lock var, see that lock var is one. So, it didn't grab the lock, but because there was a write to lock var, there is a transfer of lock var into this cache, where it's now modified. And now lock var is invalid in core 0's cache. Let's say that core 0 now does it's own exchange. Does the same thing. Tries to grab the lock, sees that it's one, will try again. But because it did the write to lockvar, now it grabs the block in the modified state and core 1 gets it in invalid state and core 2 stays invalid. And now, as long as the lock is busy, these two will be spinning in that loop. So what's going to happen is this block will move many many times between their caches, each time causing communication on the shared bus. And each time spending a lot of power. None of this activity here will actually succeed in grabbing the lock until the lockvar becomes 0. So at some point, let's say that core 1 was the last one to grab the lock at the point where lock lockvar is written to 0. This write will cause yet another movement of the block, it becomes modified in core 0's cache now. And now, whoever tries to grab the lock first will succeed by obtaining the block, writing to it, but this time grabbing the lock. So, if core one now succeeds in grabbing the lock after it became available, core two continues this activity further. So, there is a lot of this going on as long as anybody's waiting on a lock. This is very power hungry because these movements of data between caches are going to cost us a lot of energy per transfer. And also, this activity here keeps the interconnect between the course busy, for example, the shared bus. So the one core that is actually doing useful work here while this is going on. If it has a cache miss will now be slower in handling this cache miss because it has to wait for the busy bus that these two are keeping busy. So not only is it power hungry to do this, it also by busy waiting in such an active way we're slowing down the useful work that is the only thing that can really result in us actually getting the lock eventually. So we have seen that performing these atomic read write operations on the lock repeatedly even while waiting for the lock is inefficient in terms of energy and it actually slows down the thread that is in the critical section. We want to make this more efficient and one way of doing that is to do a so-called test and atomic op approach in the locking function. So the original approach to using for example, an atomic exchange is to set R1 to 1, and as long as R1 is 1, try to swap between lock var, eventually we see a 0 and we exit. But as we now know, this keeps invalidating everybody else who is waiting on the lock, so we are getting a lot of cache misses while waiting. And active waiting like this is not only spending energy on the bus transfers all the time, but also keeps the bus busy so that useful work in other trays that are not busy waiting, you slow down. So this is kind of like a repeated atomic op approach. To make it more efficient, what we do is we do the same type of looping here, but we don't immediately try to swap. What we do first is use normal loads to wait for lockvar to become zero. So the idea is that were using normal loads while lockvar is busy. If we observe that its free, we try to grab it by doing an atomic exchange. We cannot try to grab it using the normal store, that's why we have to do the exchange. Bu the idea is that there is no point in trying to do the exchange unless the lock is actually free and the purpose of the exchange now is solely to ensure that only one of us grabs the lock once it becomes free. But the busy waiting while the lock is continuously free is done using normal reads. So now if we have core zero, one, and two, and core zero succeeds in doing an exchange, and gets the lockvar in the modified state. Core one will now reach this point and read lockvar. See that it's one. It's done using a normal load, so here we're moved to the own state, here we are moved to the shared state. And there is a cache to cache transfer here. Now we will keep doing this, but we'll keep getting cache hits so we no longer access the bus while waiting. Similarly core two will load the lockvar. It will also get the value and have it in the shared state. And now it also gets cached hits while waiting. So as long as core zero keeps the lock busy here, these two cores are no longer generating bus traffic. So core zero basically gets the entire coherence bus to itself so it can very quickly service cache misses. And cache hits here are much more efficient than bus accesses. And this is going to continue until core zero releases the lock, at which point, there is an invalidation, and these two will read again, see a value of one, try to compete for it using the exchange and so on. But those bursts of activity really happen only when they're actually trying to acquire the lock. As long as the lock is really busy, they just keep using normal loads in this loop. So this is much more energy efficient. So let's see if we understood how test and atomic-op logs work. Remember our original code for load links for conditional locks which loads the previous state of the lock, tries to put one into the lock and then checks whether we observe the unlock state of the lock and whether we successfully grab the lock. So if we apply the test an automatic approach to this. Amazingly we do that by just rearranging these instructions. The first two instructions are exactly the same. And then in what order do we use the remaining three instructions? Instead of writing out the instruction, just say which of these is instruction A, B, and C. Let's look at the solution for our test and atomic-op quiz using load links to our conditional instructions. So we said that the first to instructions of the new test and atomic op approach are the same. Put 1 in R1 and then load from Lockvar to R2. The idea of test and atomic-op is not to try the right until we see that the lock is free. So before trying the store we should check whether the lock is free. That is done by using the B instruction here. So the correct answer should have been just B here. And I'm going to write it out to show you how it works. So [INAUDIBLE] is that if we load a busy value, meaning a value of 1, then R2 is not equal to 0 and we're just going to repeat this. So, as long as the lock is busy, we just keep doing the load link. The load link without the corresponding store conditional behaves just like a normal load. Yes, it's setting the link register every time, but we're not checking it so it's just the same as if we didn't. But things get more interesting once we actually see an available value of the lock. Then we try to do a store conditional. So now we should use the A instruction. So we have seen that the lock is free. Now we try to get it. And after this we need to check using instruction C whether we actually got it. So if R1 is equal to 0, that means that our store conditional didn't succeed. We go back to trying to acquire the lock again. So we have seen that the lock is free but between our load link and our store conditional somebody else managed to write to the log so we need to retry the whole thing. So after our load link we check whether the log was free. If so, we try to store conditional. If we succeed, then we got the lock. So the waiting for the lock to become free is done using the same load link that will be part of our atomic operation once we add the store conditional when we see that the lock is free. Now let's see if we can figure out what the unlock needs to do. So the unlock function here does what? Option one, use a normal store to write zero to lockvar. Option two, load link, see if we loaded one and then store conditional. And option three, we need additional atomic instructions in our ISA to do this. Let's look at the solution to our unlock quiz where we were asked really what does unlock do. Does it just store zero to lockvar, does it, check whether the lockvar is one and then do a store conditional atomically to make it zero? Or do we need additional atomic instructions, that are going to somehow, unlock the lock? And the answer is that. Unlike the lock function, where we needed to both, check whether lock is available and, make it busy, in one go, the unlock function, is done only by the thread that actually has the lock. So when we get to unlock we know that the lock is busy, because we made it busy. All we have to do is just, make the lock free. We can do that by writing zero in any way we want. So we chose the simplest way of just doing a normal store to the lock variable. So we don't need to atomically test whether it's one and then, make it zero. And we definitely don't need additional atomic instructions. Another type synchronization that we often need in programs, is called barrier synchronization. This often occurs when there is a parallel section where several threads are doing something independent of each other, for example each thread is adding up the numbers in its own part of a shared array. Eventually, we want to know the total sum. So what needs to happen is we need to make sure that everybody has finished their own adding up, and that's what the barrier is. And only then somebody, for example, thread 0, can actually form the total sum and, for example, print it out. We don't want it to print the sum before we know that everybody's contributions to the sum are complete. Another thing that can happen is that we are computing the sum in this parallel way, and then all of the threads need to use this sum. So again, all of them need to wait for the sum to be certainly done. And only then can they proceed to actually use the sum. If thread 0 starts using the sum before making sure that other threads are done computing the sum, then what happens is we are using the sum that may not include all of the numbers that are in the array. Same for any other thread. So the barrier pretty much is a global wait that ensures that all threads have entered the barrier before any of them can proceed past the barrier. So the barrier synchronization ensures that all threads that need to arrive to the barrier have arrived before any can leave the barrier. A barrier implementation can have two variables. A counter that counts how many threads have arrived and the flag that gets set when the counter finally reaches N. So what each tread arriving to the barrier needs to do is increment the counter, check if the counter is equal to the number of threads that need to arrive. If it is equal, set the flag. If it's not equal, that means we are not the last arriving thread. Then we spin, waiting on the flag to finally become set. Of course, things are not as simple. So let's look at the relatively simple implementation of a barrier in a program. So this is our barrier implementation. Our counter is a shared variable, and everybody's trying to increment it when they arrive to the variable. Multiple threads could arrive to the variable at about the same time. So we have to protect the counter using a lock. So here what we are really doing is counting the arrivals. Once we are done incrementing the counter we can check if the counter is equal to the total. If it is, that means that we are either the last thread. Or the last thread has arrived. Note that the two are not necessarily the same. We may be able to increment the counter, but we are not the last thread. But, by the time we get to check the total, the last thread has arrived and increments the counter. So, we read the counter that is not the same as what we left it with. Either way, if the counter is equal to the total. Then we reset the counter, because it needs to be re initialized so that next time we enter the same barrier variable we have the counted zero. And we set the release flag to one so that the threats who are waiting find out that the barrier can now be released. If the count is not equal to the total, then we're going to spin here waiting for somebody else to find that the count is equal to the total end release. After we are done going through one instance of this barrier, the release will be one. Before we can reuse this barrier, the release needs to be set at zero and that's the purpose of this line here. If we are the first thread to arrive, then we're going to set release to zero. So the idea is now that as we enter the barrier, the first thread that arrives, and know that this is done in a critical section, so exactly one thread will see this as zero, it sets release to zero. That thread then increments the count, it becomes one, and goes on here. That thread, and all the other threads, now see release as zero and the only way it becomes one is if count reaches the total. So what happens is the first thread might actually be delayed and read the count after everybody's done in which case it's going to do this. Or it might go here and spin but either way, release becoming one means that somebody got to this point. Which is when the barrier should be released. One or more threads that arrive towards the end are going to do this, which will reset the count so that next time we enter we see count zero. And release is set to one which ensures that those threads that didn't end up here are going to see release as one. Note that the meaning of this spin,release equals one, is actually that we wait for release to be one. It turns out that this barrier code is not entirely correct. If two threads are synchronizing on this barrier, the total will be two. And the barrier actually works the first time. So the thread one reaches the barrier. And so does thread two. Now they synchronize on this barrier, and that will work. But if we try to continue work, and then synchronize on the same barrier variable again, we would expect that this setting of the count to zero, and is setting of the release to zero, to work but it doesn't always work. So this is not an entirely correct implementation of a barrier that can be used more than once. So the simple barrier operation doesn't work entirely. And we look now at why it doesn't work. Let's consider only two cores. Core 0, which we will show in blue color what it's doing. And core 1, which we will show in this color what it's doing. What they're both doing initially is doing something before they arrive to the barrier. And let's say that Core 0 arrives here first. So what Core 0 does, is resets release to 0, increments the count to 1, unlocks the lock, checks whether the count is equal to 2, and starts spinning, waiting for the release to become 1. So now, release is still 0 because we just reset it. And core 0 is going to to stay here spinning. So, what core 0 does is it essentially does a load word of release, sees 0, and will check again. And it keeps doing that. Now, let's see what happens when core 1 arrives. Normally, what should happen is, core 1 arrives, doesn't do this because count is now 1. Increments the count, exits the critical section. Sees that the count is equal to 2, resets the count, sets the release to 1, and now core 1 proceeds past the barrier. What should happen now is core 0 should see this release that we just set to 1. And also proceed past the barrier. So, what should happen is really that we set the release to 1 here and core 0 sees that. Exits the barrier and from then, everything will actually be correct. But let's say that core 0 has been checking the release, however, when the store happens, core 0 is delayed. For example, it's executing some sort of an interrupt function so for a while it's suspended in this place in the code and cannot read the release fast enough. In that case, we don't have this, and core 0 is still stuck here. In a correct barrier implementation, core 0 will eventually, at some point, it will return from the interrupt, read the release that is 1, and proceed. However, what happens if core 0 is waiting here while core 1, after releasing and exiting the barrier, does very little work and then comes back to the barrier again? It sees that the count is 0, because it reset it to 0 when it was releasing the barrier. And sets the release to 1. And now it writes 0 to the release variable and now, even if core 0 checks the release here and sees what it is, it's going to see the 0. So, core 0 is going to proceed to wait here for a very long time. Meanwhile, core 1 increments the count to 1. Checks if it's equal to 2 for the second iteration of the barrier. It's not 2, so core 1 is now going to spin waiting for somebody to release the second instance of the barrier while core 0 is going to be waiting there, in the first instance of the barrier, and never being released. So the only thing that can really core 1 now is if core 0 somehow left and entered the second instance of the barrier. The only thing that can release core 0, however, is if somebody comes and releases here, but that's not going to be core 2. So, now we have a deadlock situation. Let's see if we understood what the problem is with the simple barrier. If we have thread zero, that creates a thread one. They both do some work. They use a barrier to synchronize, and then immediately after that, thread one ends and thread zero is the only one that continues. Does the simple barrier work for this code: yes, or no? Let's look at the answer to our simple barrier quiz where we had this program where both threads are going to synchronize on a single barrier and then thread one finishes and then thread zero continues alone. The question is does a simple barrier implementation work for that? And the answer is yes, it does work. If we only do this barrier once, then the first thread that enters this critical section will set the release to zero and increment the count to one. The second thread that reaches this will increment the count to two. If the first thread ends up waiting for release, the second one will still see that the count is two, reset the count, set the release and eventually due to coherence. Remember that coherence guarantees that when we read the release eventually we'll see it, it's just that we can be arbitrarily delayed. But no matter how long we are delayed eventually we see it, and there is no way to get stuck here. So the thread that does the releasing leaves the barrier, that release leaves the barrier. Or it might happen that both of them execute this in very close proximity to each other, both of them now see the new count of two, and then both of them release and exit the barrier. Either way, nobody gets stuck. The reason why the simpler barrier is not entirely correct is not that it doesn't work on the first try, it's that it's not reusable. So the problem occurs if we try to use this barrier again after the first time we use it. In which case, somebody might be stuck still here and we reset this so that they never leave the first instance while everybody else is in the second instance. But unless we have that second instance of the barrier, this barrier works. So in this particular case if we are sure that we are going to use this barrier only once in the entire run of the program then this barrier does work correctly. So how do we implement reusable barriers correctly? This is how. The idea here is that the value for releasing the barrier will not be the same for all instances of the barrier. Even instances when release becomes zero, all instances are going to release when release becomes one. So the idea is that we never really need to reinitialize the release. We just flip the release. Each thread now has a localSense, which is kind of what's the release that we should get in order to get out? So if we have 2 threads, 0 and 1, let's say that they started with localSense being 0. Now they're going to get here and they're both going to figure out that the localSense is 1. localSense is local to each thread, so each of them just sets a local variable independently. So, this is not going to interfere with each other. Now, one of the thread will enter the counter lock, increment the count, and here if the count is equal to the total, they will reset the count, and set the release to localSense. So it sets the release to what we are waiting for, in this case, let's say thread one releases it to one. That means that thread 0, when it incremented account, didn't see the total at 2, it saw it at 1, and proceeded to exit here and enters this wait here. It's waiting now for the release to become what it thinks localSense is which is 1. So it's going to spin until somebody puts 1 into the release. Eventually, thread 1 does that. So now let's say that thread 0 is stuck here, that was the problem with the previous implementation of the barrier. Let's say that we just don't read it fast enough. So we want to see a 1, but we don't check it fast enough. Thread 1, now, proceeds to exit, finishes the work and comes back to the second instance of the barrier. It now flips its own localSense to 0. Increments that count to 1, because we reset the count when we were releasing the first instance, checks the count, sees that it is 1, doesn't do this, unlocks the lock, and enters the spin here. But thread 1 is now checking whether the release has become 0, because it flipped the localSense. Now it's waiting for this value for the release, so they're waiting for different value. Note that the release is still the same as thread 1 left it in the first instance. So eventually thread 0 will check and see that the release has become 1, at which point it will leave. Eventually it's going to come here, change its local sense to 0, increment the count, see that it's 2, reset the count, set the release to 0 this time, and now thread 1 will be released. So by flipping the value of the release instead of reinitializing it in each iteration of the loop, we avoid the problem that we had. So this barrier is reusable. We can synchronize on the same barrier over and over without risking that lock situations. In this lesson, we have learned that synchronization operations such as locks and barriers are needed to coordinate the activities among multiple threads and that special atomic instructions are needed to implement locks efficiently. In the next lesson, we will see that synchronization can be messed up when a processor can reorder loads and stores, and we will learn how to guard against that. In this lesson we will learn about VLIW, and explicitly parallel processors, such as Intel's Itanium. Unlike their out of order cousins, these processors do not try to identify ILP on their own, instead, they just do what the compiler tells them to do. In previous lessons we have seen, that a Superscalar processor will try to execute more than one instruction per cycle. The VLIW processors, are trying to achieve the same amount of work per cycle but in a different way. So let's compare the two approaches and see how the VLIW processors differ from the out of order, and in order Superscalar processors that we have been already seeing. So what we will do is compare the out of order Superscalar processor. The in-order Superscalar processor, and the very long instruction word, or the VLIW processor according to several things. First, how many instructions per cycle do we try to do. An N issue out-of-order Superscalar processor is trying to do up to N instructions per cycle. And in-order Superscalar processor, is also trying to do up to N instructions per cycle. A VLIW processor is different in that it tries to execute 1 large instruction per cycle, but that one large instruction does the same work as N normal instructions would do, so really we're trying to do the same work per cycle. But the Superscalar processors are trying to do more than one simple instruction per cycle, where as [UNKNOWN] VLIW processor does one large instruction that really does the same work as N of these; but it needs to be given a single instruction to do per cycle. In terms of, how do we find independent instructions that we can do in a cycle, the out-of-order Superscaler processor finds its N instructions, by looking at much than this number of instructions in its instruction window. So it fetches the code is, and then kind of has a window of more than n instructions. And among these it's trying to find the N, that it needs to execute. In contrast, an in-order Superscalar processor is trying to execute up to N instructions per cycle by looking at only the next N instructions in program order. So it doesn't maintain this huge instruction window among which it is trying to find N instructions, instead it just tries to see whether the next N instructions or some subset of them can be executed in program order in parallel. A VLIW processor doesn't. Even try to find independent instructions. It simply tries to do the next large instruction. So it behaves like a non-Superscalar processor that executes instructions in order. It's just that each of these instructions is really, describing a lot of work. In terms of hardware cost, this. Looking at many instructions, in order to find the N that we are looking for in an out-of-order Superscalar processor is really expensive. The in-order Superscalar processor, does a lot less work and needs a lot less hardware to find the instructions. So it's less expensive in terms of harder costs than they're out-of-order Superscalar cousins. The VLIW processor reduces that cost even further, so it's even less expensive than an in-order Superscalar processor for the same amount of work per cycle, assuming we can find that work because we are now even simpler in finding that work. And that brings us to our last point, which is does the processor need help from the compiler? In an out-of-order Superscalar processor, it does pretty well, even without any compiler help. But the compiler can help improve the program performance. An in-order Superscalar processor is more dependent on the compiler. If the compiler doesn't do anything to put independent instructions. Consecutively than an in-order Superscalar processor will not have good performance, it performance will lack significantly. The performance that an out-of-order Superscalar processor with the same issue with would be able to achieve. So an in-order Superscalar processor really needs help from the compiler, without such help it is going to perform significantly worse. Then, an out-of-order Superscalar processor would. And finally, VLIW processor completely depends on the compiler, to produce performance. If we don' have a good compiler, a VLIW processor will fail miserably as far as performance is concerned. So as we can see among this three, the out-of-order Superscalar processor is trying to do, kind of like a hardware intensive. I don't need much help from anybody, but it sacrifices hardware to do that. An in-order Superscalar processor tries to reduce the hardware cost, but still can execute the same program that an out-of-order Superscalar processor could. And in some programs, it really needs compiler help in order to achieve good performance. And finally, a VLIW processor goes to the extreme of completely depending on the compiler to provide and express the parallelism it needs to do work. But in exchange, it gives us much lower cost. So let's do another superscalar versus VLIW comparison as a quiz. Let's say we have an out of order superscalar processor and a VLIW processor. Let's say the out of order superscalar processor executes thirty two bit instructions while the VLIW executes one hundred twenty eight bit instructions. Where each VLIW instruction specifies four operations. Each of these operations is similar to what one of these instructions on a superscalar browser can do. If we have a program for the other superscalar that is 4000 bytes in length, we can say the the VLIW program size will be between how many, and how many bytes. So this is our superscalar versus VLIW quiz solution. We have an [INAUDIBLE] superscalar with a 32 bit instruction. And we have a VLIW processor, where it really packages up to four such instructions into a 128 bit instruction. If we manage to package four instructions into one of these. Then the size is the same, four of these are equal to the size of one of these. So the program size, at best, will be the same, 4000 bytes. However, it can happen for example that all of these instructions depend on each other. In which case we can specify only one of them in each one of these. We have to fill the rest of the 128 bit instruction with three no ops, simply because there are no two instructions from this program that can actually fit in the same instruction here, because they depend on each other. And all we can put in a VLIW instruction are operations. That are independent of each other. So, what could happen is that we have a program that is really four times larger, which is 16000 bytes. So for the VLIW let's discuss the good and the bad sides of it. Among the good things is that the compiler does the hard work. Because the compiler is run only once and then we run the program many times the compiler can have plenty of time to figure out a good schedule. In contrast, in an out of order processor we have very little time to come up with a decent schedule because any time we spend figuring out the schedule is the time we're adding to the program execution time. The VLIW processor typically will have simpler hardware than a comparable out of order processor. A VLIW processor can be more efficient than an out of order processor because the hardware had less to do per executed instruction. And the VLIW processors typically work really well on loops and so called regular code, where regular code is basically things like sweeping to arrays, multiplying matrices, and other things that a compiler can easily figure out and then schedule well around it. The bad side of VLIW are that latencies of instructions are not always the same. The compiler has to assume some sort of latency in order to schedule code well, but sometimes the latencies cannot be determined exactly. So for example, we can plan for a load to have a cache hit latency but then when it has a cache miss suddenly there is a much longer latency than the compiler planned for. Second, many applications are irregular, almost every application that does a lot of decision making, like for example A.I. applications, applications that work on pointers or pointer intensive structures and so on, are very hard for the compiler to figure out. And finally there is the code bloat issue. We have already seen in our quiz, that the code for a VAW can be much larger than the code for a normal, out of order processor. Because we are inserting a lot of NOPs to prevent things that are dependent on each other from being in the same large instruction. We have seen some aspects of VLIW. Now lets discuss VLI backward compatibility as a quiz. Let's say we have a simple VLIW processor. It has a 64-bit that specifies two operations, so it's roughly equivalent to two normal instructions, and let's say that this processor fetches, decodes and executes, etc, one of the 64-bit instructions per cycle. Now, we want to build a better version of this processor. It can do four operations per cycle. So, it will need to fetch the code and execute two of those 64-bit instructions per cycle. The question for you is, is this new processor a VLIW processor? The possible answers for you are yes it is, or no it's not really a VLIW processor. Okay, so we have our VLIW backward compatibility quiz. Let's work on a solution. We had a simple VLIW that had 64-bit instructions and we were doing one, one of those at a time and each of those were equivalent to two normal operations. And now we want to have a VLIW processor with four operations per cycle, and the way we want to do that, while maintaining backward compatibility with the original version, is to fetch the code and execute two of those 64 bit instructions per cycle. The questions is, is this new processor a VLIW processor? The correct answer here is no, not really. Here's why. In a real VLIW processor, the compiler tells us which operations can be done in parallel by putting them in the same instruction, and then we try to do only one of those instructions per cycle. Thus, every cycle we are guaranteed that we're going independent operations. When we run programs compiled for this processor. The compiler only got on these, that the two operations in the same instruction are in parallel. In fact, if we have something like this, where we have a data dependence, the compiler will put these operations into two separate VLIW instructions. And put NOPs for the second operation to ensure that we first do this, and then we do this. If this processor now is trying to fetch the code and execute two such 64-bit instructions per cycle, it might easily fetch these two and try to do them in the same cycle, thus violating this dependence. So, this type of processor is not a true VLIW processor because it would have to check for dependencies which makes it similar to a normal superscalar processor. We have already seen that a VLIW instruction really specifies several operations. Each of which operations would have taken a normal instruction in a normal super scalar processor. The instructions set for a VLIW processor typically has all the normal ISA opcodes, so each of the VLIW instructions can typically do whatever normal instructions would have been able to do in and out of normal processor. A typical VLIW processor will also have support for full predication, or at least very extensive predication support. This is because it relies on the compiler to expose parallelism. One of the ways the compiler does that is through scheduling instructions, so we really want the compiler to be able to predicate and thus expose more opportunities for instruction scheduling. A VLIW ISA also, typically, includes a lot of architectural registers. The reason for this is at that a lot of the scheduling optimizations require use of additional registers. We have already seen some examples of this in the compiler support for IOP lesson. What we have seen, that sometimes we have to put somethings in other registers, when we're rearranging instructions. So to help the compiler do instruction scheduling, we need more registers than usual and we really, really want the compiler to do a god job here so we will provide a lot of registers to it. Another frequently seen type of ISA support for compiler work are the branch hints, where the compiler can specify to the hardware what it thinks the branches will do. This helps the branch vectors. And we also often see some sort of compaction mechanism for VLIW instructions. For example, if we have a four operation instruction, and we have something like this, we do some operation and then the next operation needs to be here and thus we have to put NOPs in here. The actual VLIW instruction might include some sort of a stop date for every instruction, in which case we can put OP1 here, then mark this as a stop, and then we can put OP2, and OP3, like this, mark this as a stop, and even squeeze in maybe something else here. So, the idea is that instead of having a lot of NOPs in our instruction, now the processor fetches this instruction, checks for the stop bits, and in the first cycle does this, in the second cycle does up to the next stop and so on. So, this helps a lot in reducing the number of NOPs and thus, the code bloat So let's discuss some examples of VLIW processor. The most famous example of how to probably not do VLIW is Intel's Itanium processors. It has tons of ISA features that help the compiler schedule code well, etcetera. As a result, the hardware of the processor became very complicated. It no longer needed to check for dependencies between instructions, but it had so many other bells and whistles to do this that the hardware is probably the most complicated Intel ever belt. And yet fundamentally this is still a VLIW processor, so it's still not doing great on your regular code. Another example of where VLIW is often used is DSP processors. What DSP stands for, digital signal processing. Digital signal processing does a lot of floating point work, typically in a very regular loop with a very small amount of work within each iteration and lots of, lots of iterations. So typically, these types of processors on these types of codes get excellent performance and are also very energy efficient, because they don't spend much power on figuring out dependencies and so on. So these are examples of when we try to use VLIW for all types of codes, we run into problems. But if we find the type of codes where compilers can do well, suddenly VLIW looks like a very, very good choice for that. So let's do a VLIW target market quiz. So what type of application is the VLIW best for? An application that adds up many numbers, and application that figures out the best path in some sort of maze, or an application that often counts elements in a linked list? This is a VLIW target market quiz solution. So, the question was, what is VLIW best for, among these three options? We already know that VLIW is best for things where the compiler can figure the program out. Adding many numbers together is a very easy thing for the compiler to figure out. Typically, we are talking about a small loop with very well controlled dependencies, and so on. In contrast, figuring out the best path in a maze typically involves a lot of decisions, and then we will have to do a lot of if-conversions and so on, so it's going to be relatively inefficient, because a lot of the instructions that you do are going to be predicated out. And then counting elements in a linked list is going to have probably a lot of load instructions. Some of them will miss, some of them are not. So our compilers scheduling will be greatly effected by the cache misses and so on. So this is probably the best scenario for a VLIW. Followed probably by something like this and it's unlikely to do well on something like this. In this lesson, we have discussed VLIW processors, and their itanium cousins. We have learned the pros and cons of these processors, and also some of the reasons for the itaniums high expectations and eventual demise. This lesson concludes the first part of my high-performance computer architecture course, where we have learned how the modern processor core works. In the next part of the course, we will learn how to get good performance even though the main memory is really slow, and way too small to fit the programs we want to run. After that, the final part of the course is about putting multiple processor cores together to get even more performance and energy efficiency. In this lesson we will be discussing the support computer architectures provide for virtual memory. This is an important topic because this makes modern operating systems a lot more efficient. Before we go into how virtual memory works, let's first see why do we even want to have virtual memory? And the short answer is that we want to have virtual memory because the programmer and the hardware have different views of what memory means. In the hardware view, the machine has some memory modules with an actual amount of memory that can be accessed by the real processor. Let's say that this module is 2 gigabytes and this module is 2 gigabytes. So what the hardware really has is 4 gigabytes of memory, and the address is for this 4 gigabytes are going to be 0 through 4 gigabytes. In contrast, the programmer's view of memory is that it's a huge array with address that go from 0 to a very, very large number if it's a 64 bit machine. This is many, many gigabytes. Obviously, this is much more than with the machine really has. Some of this memory might be reserved for the system. Some of this memory contains the actual program instructions. Some of it is for static data. Then we have the memory for the heap, which is where malloc finds its memory, and this heap can grow. And then some of it, usually very high in the memory address space, is the stack. And that typically grows this way. But the programmer doesn't really want to be bothered by how much space is there between the heap and the stack. What they want to do is simply malloc, or push things on the stack, and they never run out of memory. And in a 64 bit of rest space, chances are they won't actually run out of memory. But the problem is the actual amount of memory that they're using might be less, or it might be much more than the actual memory that the machine has here. Things are actually much more complicated because we typically don't run only run one program on a machine, we actually simultaneously having multiple programs run. You might be having something that browses files here, an MP3 player playing in the background, and a word processor here. Each of these sees the address space that begins at 0 and ends at the very large address. We don't want to write our programs so that this needs to run as the first program in the system. This needs to run as the second program in the system. You want to write the program as if it was the only program in the system and then just run multiple programs like that. But that means that this program's idea of memory is it thinks it has all of this memory. This program similarly thinks it has all of this memory. So as far as these two programs are concerned, for example, their code might well be at the same address in each program, although in fact they each have different instructions at those addresses. So virtual memory is a way of reconciling how the programmers view memory and how the hardware actually needs to view memory in order to work. Let's see if we understood what virtual memory is about. Let's say we have a computer with 16 active applications. Each with a 32-bit address space meaning, that the application generates 32-bit addresses, so it sees 4 GB of memory potentially. The question for you is, what does the system actually have as far as memory is concerned? Two 2 GB memory modules, four 4 GB memory modules, eight 8GB memory modules, one 16 GB memory module. So like all the answers here that are possible. That is, all of the answers that might work with this, should be checked. Let's look at the solution to our virtual memory quiz. We said that the computer has 16 active applications, each with a 32-bit address space. So each potentially sees a four gigabyte memory. What is the real memory in the system? It turns out that all of these are potentially correct. We can have a system that has only two 2-gigabyte memory modules. So it really has a total of 4 gigabytes physical memory. Yet it can run 16 applications, each of which thinks that it has 4 gigabytes of memory. We can also have more memory, even more memory, a different combination of memory and so on. So pretty much what the applications think they have in terms of memory and what the system really has can be completely decoupled by virtual memory. So let's look at how the processor sees the memory. The processor sees what we call, physical memory, which is the memory contained in the actual memory modules, that we bought and put in the system. The amount of that memory sometimes, even lower, than 4GB. It is almost never 4GB per process that we have in the system, because there can be tens or even hundreds of processes in a modern operating system. And it is never 16 exabytes per process. This is how much memory we need, if we have 2 to the 64th memory locations, which is what you would have with a 64-bit address. So if each process in a 64-bit machine thinks it has this much memory, there is just no way we can have enough actual memory for that. So we conclude that the amount of physical memory we have is usually less than what all the programs can access. If all the programs access all the memory they possibly can, they will access much, much more than the memory we actually have. The addresses that the processor uses for the physical memory have a one to one mapping to the bytes or words in the physical memory. So a given address always goes to the same physical location. And the physical location always has exactly one physical address. Now let's see what the program's view of memory is. The program sees a huge amount of memory, and usually some contiguous regions of this memory are actually used by the program. And there is an enormous region in the middle, between the heap and the stack, that the program will never access unless the heap grows that way. But the heap is usually small relative to how much space we have here, because it's a huge, huge amount of memory. So what happens is the program thinks it has a lot of memory, but in reality most of that memory, it will never access. And this is what we call virtual memory. So the program virtually has a lot of memory, but in practice there isn't that much memory. Another program has its own virtual memory. Let's say it's a small program. So it's using less memory, and less for the stack, so it has more memory here that it's not really using. But the idea that the program has about this memory is that it can always use more. So this is the virtual memory of one program, this is the virtual memory of another program, they might be running at the same time. Let's say this is our physical memory. We need to figue out what happens when this program generates an address that should access this memory location. How do we find where in here do we actually go? And if this program accesses a memory location with the same address, how do we figure out where does it go? It might go to a different place in the physical memory. For example, when these programs have nothing to do with each other. It might go to the same place in the physical memory. For example, if these programs are sharing data. In fact, data sharing among programs is not constrained so that you have to put the data in the same virtual address. You could easily allocate this here, and then let this program share it, but as far as this program is concerned it's here. It still needs to map to the same location. So how do we reconcile these different possibilities? So when a program generates a virtual address, for example, does a load or a store to it. The processor needs to access some physical address. The question is how does the map what the program is trying to access to what really should be accessed? This mapping would be really hard, if every byte of virtual memory could map to an arbitrary byte in the physical memory. Then we would basically have to have a huge table of mappings and we don't want to do that, because that table would be very large and consume a lot of our memory. So instead, the program's memory is divided into equal size chunks called pages. A typical page size is 4 kilobytes, so this would be page 0 in this program, page 1 in this program, page 2, 3, 4, etc. So pretty much the first 4 kilobytes will be page 0, the next 4 kilobytes will be page 1 and so on. Note that each page is aligned to the page size. So the page, for example is 4 kilobytes and it begins at the 4 kilobyte boundary. So this is our virtual memory. Our physical memory is divided into slots that can hold pages and these slots are called frames. So if you remember your caches, pretty much the physical memory behaves like a cache for the virtual memory in that it has a certain number of places where we can put pages. So a page is kind of like a memory block and the frame is kind of like a cache line, except that this is the actual memory we have and this is the memory that the program just thinks it has, but we never actually have that memory. So now the operating system creates a mapping, where it decides which pages in the program will map to which frames. We can do something like this, for example. So page 0, for example, the operating system decides to put it in frame 0. Page 1 can be in frame 2, page 2 can be in frame 3. If we have another process, it has its own page 0, page 1, etc. And these pages might map to different frames. If we want this page 1 and this page 1 to be the same, because these 2 processors need to share memory, then we can map them to the same frame. And now when this processor writes to its memory and this processor reads to the same address, they actually are accessing the same memory location. So this process reads what this process wrote. But for page 0, for example, they have different memory there. So this process sees its own content for page 0 and this process sees its own content for page 0. So who decides how to do this mapping here and here? And the answer is the operating system decides and the actual mapping mechanism is called a page table. It's a table that says, for each page in a process, where is that page really mapped in the physical memory? This is the page table for this process. Another process with its own idea of virtual memory will have a different page table that says, for each page here, where does it go here. So lets see if we understood the relationship between page size and frames in memory. Lets say we have system whos physical memory's only two gigabytes. The virtual memory same page process is four gigabytes, the page size in this system is four kilobytes. The question for you is how many page frames do we have in this system. And how many entries does each page table have? An entry is what keeps the mapping for a single page to where it is in physical memory. Let's look at the solution to our page size quiz. Our physical memory's 2 GB. Our page size is 4 kB. How many page frames we have? Well, a page frame is a place in physical memory where a page might fit. So, the number of frames is simply 2 GB, the physical memory size, divided by the page size. The frame is the same size as a page because it's basically a place for a page to be in. When we are doing things like this, usually it's a lot easier to think about this as what power of two is it. So, 2 GB is 2 to the 31st bytes. 4 kB is 2 to the 12th bytes, so we end up with 2 to the 19th frames, so the answer here is 2 to the 19th. How many entries do we have in each page table? We need one entry in a page table for each page in the virtual memory. So the answer here is 4 GB divided by 4 kB, and that is 2 to the 20th, or one mega entry as they would say. Note that we need a separate page table for each process. Each process is 4 GB of virtual memory. So each process in our system will need a page table that is this large, 2 to the 20th or slightly more than one million entries. So now that we know that each application saves a lot of memory. And there is much less physical memory than all the applications actually can access. The question is where is the missing memory? So let's say we have one application with four pages. Another application with let's say four pages. A physical memory with let's say four physical frames. Let's say that each of these applications is actually using all of its pages. Because there are only four. In this application there is a four entry page table. That for every page tells us which frame contains it and it could be any frame in memory. This application also has some of its pages in physical memory but obviously there is not enough physical memory for all of these pages. So the question is where are the rest of them. And the answer is, they're on the hard disk. So some of these pages are not actually in memory, they're actually stored on the hard drive in the system. These pages can not be directly accessed by the processor because the processor can only really access through loads and stores in the physical memory. So if the processor ever tries to access one of these they need to be brought from the disk before they can be accessed. We will shortly see how that works, but for now just remember that because the virtual memory of all the applications can significantly exceed the size of the physical memory, some of the memory that the applications think they have is on the disk, not really in memory at all. So now that we know that the program generates virtual addresses but the processor needs physical addresses to access the actual memory, let's see how the processor does the virtual to physical address translation. So the program generates a virtual address. This is what the load, for example, will compute as its address. The processor divides this address into the page offset part which tells us where in the page we are. The virtual page number which tells us which page in the process are we talking about. If we have a 4 kilobyte page 12 bits are telling us where in the page we are, and the remaining bits are telling us what the page number is. So let's consider the address FC51908B, which is a 32 bit address. The least significant 12 bits are telling us where in the 4 kilobyte page we are, so that's the page offset. The more significant bits are telling us what the virtual page number is. Now what happens is we take the virtual page number and use it as an index into the page table. In this case, the index into the page table will be FC159, so we find the entry with that index. What that entry tells us is what the frame number is that corresponds to this virtual page number, that is where did we put that page in the actual physical memory. Let's say that the frame number in this entry is 00152. We take this physical frame number and put it together with the page offset into a physical address. And that is what we use to actually access the physical memory. In this example, the physical address is composed of the frame number and the page offset. So this is a 32 bit physical address that we will use to access the memory. Note that the page offset is present in both the virtual and the physical address. So from the virtual address, the least significant bits that corresponds to the page offset do not change when we translate into the physical address. But the virtual page number does change because it gets translated into the frame number. Now that we have seen an address translation, let's do an address translation quiz. Suppose that we have a process with a page table that only has four entries that contain these frame numbers. Suppose that this machine has a 16-bit virtual address base, and a 20-bit physical address space. That is, the virtual address is a 16-bit address. But the physical address is a 20-bit address. Then, given this 16-bit virtual address, what is the physical address it translates into? And given this virtual address, what physical address does it translate into? Let's look at the solution to our address translation quiz. We have a 16 bit virtual address space and a four entry page table, that means that the most significant two bits of the 16 bit address are the page number, the remaining 14 bits are going to be the page offset. So this 16 bit address can be written like this in binary. F0F0. The most significant two bits are really the page number, which corresponds to this entry in the page table, because this is three, so we need to look at this entry in the page table. In this particular address we have 001F. These two bits correspond to this entry in the page table. So now we can write out the physical address that corresponds to this virtual address. It is going to contain the frame number from entry number three in the page table. Which is going to be 1 7. Concatenated with the page offset which is going to be 1 1 etc. And we can know write it out it's going to be 0F0F0. And then we have F and this is five. It's a 20 bit address so it will have five hex digits. Similarly here we are looking at entry number zero here, so it's going to be 1F, followed by the page offset. So it's going to have 00 and then the remaining digits are going to be the same. So it's going to have F10 and then C7. C because it's 1100 and 7 because it's 0111. So this is the physical address for this particular virtual address. What we have discussed so far are the so-called flat page tables, where for every page number, there is an entry in the table. Let's talk about the size of such a flat page table. It has one entry per page in the entire virtual address space. Even for pages in the address space that the program never actually accesses, such as those pages that sit between the heap and the stack. The entry in such a page table contains the frame number, plus a few bits that tell us if the page is accessible. For example, the protection bits or a bit that tells us that the page is not really in physical memory at all. So the entry in the page table is typically similar in size to the actual physical address. The entry needs to contain the frame number, which is most of the bits in the physical address. It doesn't need to contain the page offset that the physical address has, but it needs to contain some extra bits. So overall, if we have a 32 bit physical address, probably the entry size in the page table will be 32 bits. If we have a 64 bit physical address, probably the entry size will be 64 bits. So the overall size of a page table is the overall size of the virtual memory of a process divided by the page size. This is how many entries we have in the page table, times the size of a page table entry. Let's look at an example where the virtual memory size is 4 gigabytes for a process, the page size is 4 kilobytes, and the size of an entry is 4 bytes. In that case, we get a 4 megabyte page table per process. Now note, that a process might actually be using less than four megabytes of actual virtual memory, because most of the pages are unused in such a process. Yet, the page table still needs to be four megabytes. So one problem with the flat page tables, is that the page table is reasonably large, even if the process uses very little of its available virtual memory. The other problem with the flat page table is that if the virtual memory is a 64 bit address space, so if the virtual memory's 2 to the 64th bytes, then we are talking about a page table that is four billion times this, which is much larger than the memory we actually have. So the page table like this simply cannot fit in memory if we have a large virtual address space, and today's processors have such a large space. So we need to reorganize the page table so that it doesn't occupy this much space. So let's see if we can compute the size of the flat page table. Let's say that the page table needs to have 8 bytes per entry. For example, because it has a 64-bit physical address space. Let's say that the page size is 4 kB. Let's say that the system only has two processes that are active. Let's say that the system only really has 2 GB of physical memory. Let's say that the programs are creating 32-bit virtual addresses. Let's say that the process number one is using only 1 MB of it's available 32-bit address space. And let's say that process number two is using 1 GB of its virtual memory. It could use up to 4 GB but it's only using 1 GB. The question for you is, what is the total size of all the page tables in the system, in megabytes? Let's look at the solution to our flat page table size quiz. We are asked what the total size of the page tables in the system is. There are two processes in the system, so there will be two page tables. We need to add up their sizes. The page size is 4 kilobytes. The applications have a 32-bit virtual address space. So we have 2 processes times 2 to the 32nd bytes of address space divided by 2 to the 12 bytes in a page. This is how many pages we have times the size of an entry is 8 bytes, which gives us two times eight is 16 x 2 to the 20th. 2 to the 20th bytes is a megabyte, So we have 16 megabytes. Note that it didn't matter how much memory the processes are actually using. The flat page table needs to have an entry for every possible page in the virtual address space, not only for the pages we are actually using. Also, it doesn't matter how much physical memory we actually have. What matters is what the size of the entry is. So for example, this entry was designed to accommodate up to a 64-bit physical address, even though the system actually has much less memory. Why would you do this? Well, because you want to build the system such that it can expand its memory if necessary, for example, by adding more memory modules. So now that we have seen that flat page tables can be quite large, even for 32-bit address spaces, and are too big to even fit in memory for 64-bit address spaces, let's talk about multi-level page tables, which is how we reduce the size of page tables when we have a large address space. So, the problem with flat page tables is that their size is proportional to the address space, meaning, how much memory can the application possibly address, not how much memory it is actually using. For a 32-bit address space, we have seen that we have a reasonably sized page table. So, for 32-bit virtual addresses, the flat page table size is several megabytes. Note that we can have a very small application that only uses a couple of kilobytes of memory, yet it still needs a page table of this size, because the page table size has nothing to do with how much memory we are using, it has to do with how much memory can we possibly address. For 64-bit virtual addresses, a flat page table would simply be too big. The page table would be many many gigabytes in size, so we simply cannot use it because it wouldn't fit in the amount of physical memory we actually have. So multi-level page tables are designed to try to overcome both of the problems. We want the overall size of the page table to be proportional to how much memory we are using and we want our page table to work even for 64-bit virtual addresses, as long as the program is actually not using the entire 64-bit virtual address space, which no program really does. The idea of why multi-level page tables work has to do with how the virtual address space is usually used by the applications. Recall that the application thinks it can address memory with 64-bit addresses, so it has this many potential memory locations it can access. But in reality the application only allocates memory like this. At the beginning of the virtual memory, there is the code, the static variables, and the heap, and at the top of the virtual address space is the stack. So, we're really talking about two contiguous regions of virtual memory that are actually used by the application. And even if these two are several gigabytes each, in a 64-bit address space, there will by many, many terabytes of unused address space here. But in a flat page table, we had to have a page table entry for each page in this huge space here. So a multi-level page table, combines the idea of a flat page table, where we use some bits from the virtual address to index the table. This works really well in hardware. Hardware really likes using some bits to index some sort of an array, but we want to avoid the use of table entries that correspond to this. Meaning, we want to have a pretty normal page table like thing here, and here, but somehow not have anything for this so that we can simply omit those page table entries. Now that we have the idea for what we want, let's see how the multi-level page table is actually organized. So, the multi-level page table still looks at the virtual address in terms of page offset and page number, but instead of using the entire page number to access one huge table, it partitions the page number into the so-called inner page number, and the outer page number. The outer page number tells us which part of the larger page table we would be using. The inner part tells us, which specific entry in that large page table we would be using, instead of having the large page table, the outer page number is now used to index into the outer page table. And each entry in the outer page table, tells us where to find the so called inner page table. Once we find the inner page table. We take the inner page number, and use it to index into the inner page table. And what we find there, is the actual frame number to access. Another entry in the outer page table will point to a different inner page table. So pretty much what we have is the outer page table tells us which of the small inner page tables to use. And the inner page table tells us in where each of the page tables we need to look at. So it seems like this is just a more complicated way to access the page table, but still for every possible page number, we will have a frame number in one of the inner page tables. The total size of the inner page tables, will be the size of the original flat page table. And we still have added the outer page table, which we didn't have before. So, pretty much, we still have the same number of entries total here. And we have added something. So, where are the savings? To understand how the multi-level page table saves space, let's look at an example of a simple and small two-level page table. In this example, we will use a very small virtual address where we have an offset and the page number, and each of these is 4 bits. So using a flat page table, we would need 2 to the 4th entries in it, one for each possible page number. So the entries would be numbered 0 through 15. And we would index with our page number here, and get our frame number. With the two-level page table, let's say we split this 4-bit page number into two 2-bit pieces. So the same virtual address still has a 4-bit page number that we split into two, 2-bit pieces. The outer part of the page number is used to index into a four-entry outer page table, and each entry in this outer page table can point to a four-entry inner page table. So in total, we have fourinner page tables. Once we find the pointer to the correct one, we can use the inner part of the page number to tell us which entry to use and that gives us our frame number. So as indicated previously, the total size of all the inner page tables here is the same as the original size of the flat page table here. We had 16 entries here, we have 4 times 4 entries here, plus we added the space for the outer page table. So where are the savings? The savings are in that if there is an unused large part of the address space, such that all of these entries do not correspond to physical memory, then we don't need this and we can mark here in the outer page table that the entry's not necessary, and then we can eliminate this inner page table. In a larger address space the outer page table will have many pointers, most of which will point to inner page tables like this, so we eliminate them. So we end up having one reasonably large outer page table, but one that we can fit in memory. And a small number of inner page tables, each of which is reasonably sized, and fits in memory. But we don't have most of the inner page tables, so we save a lot of space. So now, let's talk about how big the two-level page-table gets. Say we have a 32-bit address space and a 4 kilobyte page. Let's say we're using a 1024-entry outer page table and a 1024 entry inner tables. There can be many of them, up to 1024 of them. And let's say that the page table entry's 8 bytes. Assuming that our page table entry is 8 bytes and that we have a program that uses virtual memory at the beginning of memory from addresses 0 through this, and that the top of the memory from address this to the end, let's look at what the flat page table size will be and what our two-level page-table size will be, with this split between the inner and outer page-tables. The flat page table size will have one entry for each possible page. So we have 2 to the 32nd pages, 2 to the 12 bytes in each page. So we have 2 to the 20th entries. And the total size will be 8 times this, which ends up being 8 megabytes. For the 2-level page table, we need to figure out how many inner page tables do we have here and here. Our address is now composed of a 12 bit offset, to index into the 4 kilobyte page, a 10-bit outer page number and a 10-bit inner page number. They need to add up to 32, which they do. So we know that we need to have the outer page table, and it's size will be 2 to the 10th entries times 8 bytes per entry here, which ends up being 8 kilobytes for the outer page table. Now the question is, which of these entries need to point to actual inner page tables and which ones can simply say that there is no inner page table? To figure that out, we need to see which of the inner page entries do we need to actually point of pages of memory and which ones can simply be empty. Thus we will decompose these addresses. This part of the address is the page offset. The upper part is the page number and we need to split this 20 bit page number into 10-bit groups. So let's just write it out. We have 0000, these are the two zeros. Another hex digit for 0, the 1, and the 0. This is the outer page number, this is the inner page number. As you can see, all of the addresses in this range have the same outer page number. We start at 0, we end at 0. So only the entry number 0 here needs to point to an actual inner page table. For this address range, the page number goes from FFFF0 to FFFFF. These page numbers also have the same outer page number. It's all 1s. So only the last outer page table entry needs to point to an actual inner page table. Note that there are a 1,024 entries here, only 2 of which point to inner page tables. So, there is a 1,022 entries that simply say nothing. So we have 2 inner page table times, what's the size of one of them? Well the size is 1024 entries times 8 bytes per entry, which gives us a total of 16 kilobytes. Adding the size of the outer page table, we get only 24 kilobytes. So the size of a 2-level page table for this application is only 24 kilobytes compared to 8 megabytes for the flat page table. And this is why multilevel page-tables are almost exclusively using today's processors especially when we have a 64 bit address space, because for a 64 address space, the flat page table size would be too large to fit in memory. But the multi level table will fit mostly because the outer most page table will have many empty entries. So we will need very few of these inner page tables. So now that we have seen a two level page table, let's see if we can work with a four level page table. This time let's assume we have a 64 bit virtual address space. A 64 kilobyte page size. An eight byte page table entry. And we have a relatively large program that is only using addresses zero through four gigabytes from this huge address space. The flat page table size is very large. The question for you is it's a power of two, what power of two? So it's two to what? And if you have a four level page table where the page number is split equally so the bits of the page number are divided into four equal groups to access these page tables. This page table uses how many kB of memory? So let's look at the solution to our 4-level page table quiz. We have 2 to the 64th bytes of address space. We have 2 to the 16th bytes in each page. We get that there are 2 to the 48th pages in our address space. For our flat page table, the page table has 2 to the 48th entries, times the size of each entry, is 2 to the 3rd. So the overall size is 2 to the 51st bytes. So the answer is here, 51. This is an enormous page table, many, many terabytes of page table. Now let's see what the four level page table looks like. The page number in this page table is split across four levels. So the page number is split into four pieces of 12 bits. The easiest way to figure out how large the overall page table is, is to figure out how many innermost page tables we have. 4 GB worth of memory is 2 to the 32nd bytes, which is 2 to the 16th pages of this size. So we have 2 to the 16th pages. One innermost page table has 2 to the 12th entries, so it covers 2 to the 12th pages. To cover 2 to the 16th consecutive pages, we need 2 to the 4th of these inner most page tables. So, now we know that we have 16 innermost page tables. How many page tables at the next level we need to point to them? Well, each table at that entry can also have 2 to the 12th entries, and this is only 2 to the 4th. So, we need only one. So this is the page table situation we have. The outermost page table has 2 to the 12th, or 4,960 entries. But only one of them, the first one, actually has a page table at the next level. That page table at the next level also has 2 to the 12th entries. But only one of them, the first one, points to the next level of page table. That next level page table has 16 entries that point to innermost page tables. So overall the number of these page tables is 1 here, 1 here, 1 here, and 16 of these, for a total of 19 of them. The size of each of these small page tables is the number of entries, 2 to the 12th times the size of an entry, 2 to the 3rd. So the size of each one of these is 2 to the 15th bytes, which corresponds to 32 kilobytes. So overall we have 19 page tables at various levels, each of which is 32 kilobytes. So the solution here is 19 x 32. So the final answer is 608 kilobytes. As you can see, this is only 608 kilobytes, less than a megabyte. Compare that to many many terabytes of page table for a flat page table organization. This is why all 64 bit X86 processors, nowadays, use multi-level page tables of at least three levels. So we have seen in most of our examples that we have used pages of four kilobytes, and in the most recent quiz we have used larger pages. So how do we choose the page size that we're going to support in our processor? So comparing smaller pages to larger pages, larger pages are good because they will result in a smaller page table. The page table needs to have one entry for each page that we are using in the case of multi-level pages. So, if pages are larger, we would have fewer entries in our page table. Smaller pages will suffer from large page tables. So it seems like larger pages are good, but we can only give memory to applications in units of pages. So if you have a very large page, we will suffer from what is called internal fragmentation. Internal fragmentation occurs when the application requests some amount of space, let's say this much, but we can only give it space in terms of pages, so we end up giving it, for example, two pages, where this space in this page is not really used by the application. That means that whenever this page is actually in memory, it is going to be using actual physical memory, although in fact only this much of it is being used by the application, so this much physical memory is going to be wasted whenever this page is in memory. When the page is on disk, this is also wasted because we store the whole page on disk, but this is maybe less of a problem because the disk is usually much larger than the physical memory. So how do we choose the page size? Well, obviously, we want smaller page tables but we don't want to suffer from too much internal fragmentation, so, just like with the block sizing caches, we want to reach a compromise. And it turns out that the good compromises are at page sizes from a few kB to a few MB. That is why x86 processors use a 4kB page. At the time when they were designed, the bias was slightly more toward the smaller pages because we were so worried about wasting memory. Nowadays, waste of memory even with megabyte sized pages is relatively small compared to the overall memory size, so if we were designing a processor nowadays, probably the page size would be closer to a megabyte than to a 4kB page. So now that we have seen how paging and V->P translation works, let's talk about the memory access time when we have this translation. So our program does something like this. It says load R1 from the address computed by adding 4 to what is in R2. This, when computed, will be the virtual address that we want to access in the program. So to do a load/store, the processor has to compute the virtual address. This is quick. Usually, it's just addition of two numbers that are already in the processor. One is a constant fetch with instruction. The other one is the register that is inside the processor. Next, the presser has to compute the page number, which is extremely quick because it just means that we take some bits from the virtual address. And without any address translation, the next thing that the processor will do is access the cache, and sometimes if there is a cache miss, the actual memory. So again, these two are extremely quick. Most of the time will be spent accessing the cache, and occasionally, we will spend more cycles because we need to access the memory. With V->P Translation, after computing the page number, the processor has to compute the physical address of the page table entry where the translation for the page number is. This is done by adding the page number that we computed to the beginning address of the page table. Then we read the actual page table entry, and compute the physical address by combining the physical frame number from the page table entry with the page offset from the virtual address. So how fast is this? Well, this is fast, because it just means adding the page number to some sort of address. This is fast, because it only means combining two numbers that you already have. But how fast is this? And for that, we need to ask ourselves, so where is the page table? Is it inside the processor so it can be quickly accessed, or is it in memory? We have seen the page table can be fairly large. Yes, for a multilevel page table we can have it be less than a megabyte for some programs, but in theory it can be extremely large. So, we cannot guarantee that the page table for even one process fits on the processor chip, which means that the page tables really need to be in memory. And that means, that for each load or store, we now need to have a memory access, which is just as slow as when we have a cache miss that we were trying to avoid before we got to page tables. Things are even worse, because for a multi-level page table, for example, a 4-level page table, we would need to do this multiple times. For example, 4x. Compute the physical address of the outermost page table, read that entry, compute the physical address of the next page table, go ahead and do that four times. So we need four memory accesses to read the four page table entries until we get the actual translation, after which we can quickly access the cache. So somehow, we have a situation where the virtual to physical address translation is costing us more than the memory access is that we try to avoid by having the caches. So to see the impact of virtual to physical translation and performance let's do a quiz about that. In a processor the data single cycle to compute a virtual address for a load or store instruction. Needs one cycle to access the cache on a hit. Takes 10 cycles to access memory. For example when we have a cache miss. Has a 90% hit rate for data it is trying to accesses. And the page table entries it's going to need to access cannot be in the cache, so we need to access memory directly for them. So how many cycles do we need for our load instructions such as this, if we are using a three level page table? Let's look at the solution to our virtual to physical Translation speed Quiz. We were asked, how many cycles do we need for our load instruction if we use 3-level page table? We need 1 cycle to compute the virtual address. We need 10 cycles to access the outermost page table. 10 cycles to access the next level of page table, 10 cycles to access the inner-most page table. Because neither of those can be cached. Plus, we need to do the actual cache access and maybe the memory. So, overall, we need 1 + 3 levels of page tables, times memory latency, plus one cycle to have a hit in our cache, plus, 10% of the time, we have. A miss penalty to access the memory. This ends up being 33 cycles, 30 of which are to access the page tables. Let us repeat the previous quiz, except that now, the page table entries can be cached, just like data, and have the same hit rate, like data. Let's look at the solution to our latest virtual to physical translation quiz, where we had the memory access times and the hit rates for the cache, and we were assuming that the three-level page table now can be cached just like data with the same hit rate. Now we have one cycle to compute the virtual address, followed by, for each level of the page table, three times, we need to do one cycle to access the cache plus 10% of the time for each of those. We will have a miss and go to memory to fetch the page table entry. And then after we do that, We can access the data, which takes one cycle plus 10% of the time, it takes ten more cycles to access memory. So now we have nine cycles altogether, six of which are used by virtual to physical translation. And only three for the instruction itself and the memory access, including the misses. So we can see that the virtual to physical translation is expensive, even if we allow page table entries to be treated as data and cached. So to speed up virtual to physical translation, the processor includes a structure called the translation look-aside buffer or TLB. The TLB is a cache for translations. So now you would ask yourself, well, how is the TLB better than just using the cache itself? Well, first, the cache is relatively big. It could store a lot of translations, if we only held translations there, but we don't. Most of the stuff is in the cache is data. One translation in the cache covers a whole page worth of data. So, the TLB that caches only translations can be extremely small, which means extremely fast. Let's look, for example, at the program that accesses 16 kilobytes of data. For that to work well we need about 16 kilobytes of cache. But this is only four pages if we have a four kilobyte page. So the TLB that corresponds to that only needs four entries to cover the same amount of memory. And with only four entries we can make the TLB extremely, extremely fast, much less than a cycle for each translation. Next we have seen that in a multi level page table we access our cache for each level of the page table because we need to look up all those entries in order to find the final translation. In contrast, the TLB only stores the final translation. Even if you have a four level page table, eventually we find what a given page number maps to and we just store the frame number in the TLB, not the intermediate page stable entries that we have been using to finally come up with a translation. So in our cache we will have a 4-level page table, 4 accesses to do the translation. In the TLB, we do this in 1 access. So now a load or a store just needs to form the address, access the TLB to find the translation, and then access the cache to get the data. If we have both the TLB hit and the cache hit, this can be done in one or two cycles. But what if we have a TLB miss? What if we do not find a translation in the TLB? In that case we need to perform the translation using the page table, or page tables if we have a multiple level page table. And then put that translation in our TLB so that it can be used later when we access the same page again. But who's going to do these two things? Should it be the operating system? So when we have a TLB miss, we go back into the operating system and let it figure out what the page tables have, using some sort of software? Or, should the processor automatically read the page table, or page tables, and then update the TLB, without any operating system intervention? Which one of these two do you think is correct? To answer a question, both are okay options. This option is called software TLB mishandling and it has the advantage of letting the operating system use any sort of page-table that it wants because the hardware doesn't really need to access the page-table. The hardware simply has the TLB. And the job of the operating system is to put the correct translations into the TLB. But you can use any organization for the page table it wants. So the operating system, for example, may not even have the page table in form of a table. It can use a binary tree, or a hash table, or anything else it wants. The second option, where the processor automatically reads page tables and updates the TLB is called hardware TLB miss handling. In this case, the page tables need to be in a form that is easily accessible by the hardware. So we will have a flat or a multi-level page table like we have seen already in our previous videos. This approach does require more hardware, because the hardware now needs to actually be able to access the page tables and sequence the accesses to the multiple levels and so on, but it is faster than software handling. This is treated kind of like a cache miss almost, whereas here we need to execute a little program to fill the TLB. So because this is faster and because hardware is cheap these days, most high performance processors like x86 will use this approach of hardware TLB mishandling. But some embedded processors will use the software TLB mishandling because they're concerned about the hardware cost. So they want to be as simple as possible. And also because in this embedded processors, TLB misses occur frequently because they are running more regular applications. So now that we have seen what the TLB size should be, let's see if we understood it well. Suppose we have a processor with a 32 kB cache and a 64 byte block size. Suppose we also have virtual to physical address translation for a 4kB page size. The question for you is, how many TLB entries do we need if we want to have similar miss rates in this cache and in our TLB? The choices you have are 8 to 512 entries, 64 to 32,000, fewer than 64 entries, or more than 256 entries. Choose the best answer here. Let's look at a solution to our TLB size quiz. We have a cache that is 32 kB in size and has a 64 B block size. We have a 4 kB page size. So the question is how many TLB entries do we need to have a similar hit rate and miss rate to our cache. Well, if the processor is just accessing up to 32 kB of memory, then our TLB needs to cover the same amount of memory that the cache does, and that means we need eight pages. Now it looks like this would be the best answer. However, if the processor is not accessing all of the data in each of the pages, then we would need more pages to cover the same amount of memory because in the cache with a 64 B block size, we can fit 512 different blocks, and they can be spread around the whole memory. In contrast, with eight entries in the TLB, we can only be accessing eight different pages, while these 500 blocks theoretically could be in 512 different pages. So our TLB, in this case, needs to have up to 512 different pages in order to have hits for the same data that the cache does. In reality, the number of pages we need for the same hit rate will be somewhere in between these two. So this is the best answer. Because TLB's kind of like a cache. Let's talk about how it is organized in terms of cache properties. First, easy direct map of fully associative or somewhere in between. Well, it is small, so it is already very fast. So it tends to be fully or highly associative. Usually we don't have direct map TLBs because that would sacrifice the hit rate for speed, but because it's so small, it's already very fast so we don't really need that. How about the TLB size? We want it to hit at least as often as our cache, so we want it to cover more memory than the cache. So we usually want our TLB to be somewhere about 64 to 512 entries. What if we need more TLB entries in order to get the hit rate that we want? Well, you don't want it to be too large because then it's going to become slow. In that case, you want to have a two-level TLB. The first level, or level one TLB, will be the small and fast one that hits in a single cycle. But if you have a miss there, we have a larger level two TLB. It has a hit time that is several cycles, so it's significantly slower than L1. But still way faster than going through memory and doing the translation. And it is large. It can be several thousand entries because with several thousand entries we can still support relatively good hit times, but not a single cycle time. Now that we have seen how TLB's work, let's do a quiz on TLB performance. Suppose we have a program with a 1 MB array that it reads one byte at a time from start to end and this start to end read happens ten times. So ten times we sweep through the array reading it. Suppose also that no other memory, other than this 1 MB array, is accessed during this time. Suppose that the processor has a 4kB page size, and a level one TLB that has 128 entries, and a level two TLB with 1,024 entries. And the TLBs are initially empty, they have no translations cached when we begin the first read. The array, here, is page aligned, meaning the array begins at the beginning of a page and the TLBs are direct mapped. The question for you is, in the level one TLB, we have how many hits and how many misses, and in the level two TLB how many hits and how many misses? Let's discuss the solution to our TLB performance quiz. We have a one megabyte array that we are reading one byte at a time. One megabyte is 2 to the 20th bytes. Our page is 4 kilobytes, which is 2 to the 12th. So there are 256 pages in this array. Now what happens is, once you have the first access, you will have a level one TLB miss. A level two will be missed and the translation will be generated and placed in both TLB's. Next, when we access the second byte, we have a TLB hit because that byte is in the same page as the previous one. So after the first miss we will have hits for the remainder of the four kilobyte page. In the first sweep we will have 256 L1 TLB misses, cause that's how many pages we accessed, and we will have 4095. This is one less than four kilobytes, times 256. They'll be hits at L1. After this first sweep, the Level 1 TLB contains the entries for the second half of the array, because the array needs 256 pages, we only have 128- entry. The level two TLB, however, contains the mappings for the entire array because it's still has enough elements to hold them. So what's going to happen after the first sweep, is the level one TLB will continue doing this 256 hits and this many misses each time we sweep, but the level two TLB will start hitting all the time. So what we have is ten times two-fifty-six misses here and ten times 4095 times 256 hits here. Now the question is for L2 what do we have? We know that we will have 256 misses, that is because during the first sweep we have no mappings for any of the 256 pages. After the first sweep We will have the mapping for all the pages so we will only have hits, but the question is how many of them? Well know that we only access the level two TLB if we have a miss in the level one TLB. So the number of hits here will be this minus this, so for nine sweeps we will be having these Misses go here and big hits. So this is the answer to this quiz. Now we know how virtual memory works. This is important in its own right and we will also need it for our next lesson, which is about advanced caching optimizations.