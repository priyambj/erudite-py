{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"2df22341-4830-4312-8387-85f8ecbd81a5\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\") {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  var js_urls = ['https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.js', 'https://cdn.pydata.org/bokeh/release/bokeh-compiler-0.12.0.min.js'];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      Bokeh.$(\"#2df22341-4830-4312-8387-85f8ecbd81a5\").text(\"BokehJS successfully loaded\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.0.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.0.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i](window.Bokeh);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import *\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from termcolor import colored\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense, Flatten, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.engine import Input\n",
    "from keras.layers import Merge\n",
    "from keras.layers import merge\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.regularizers import l2, activity_l2, activity_l1, l1l2, l1\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from tqdm import tqdm, tqdm_pandas\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([               u'id',  u'course_shortname',               u'url',\n",
      "             u'concept_tag',             u'title',          u'subtitle',\n",
      "             u'description', u'short_description',             u'venue',\n",
      "              u'slide_text',   u'transcript_text',              u'tags'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>course_shortname</th>\n",
       "      <th>url</th>\n",
       "      <th>concept_tag</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>description</th>\n",
       "      <th>short_description</th>\n",
       "      <th>venue</th>\n",
       "      <th>slide_text</th>\n",
       "      <th>transcript_text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4ffSHNYEeWIfhKr_WcYsQ</td>\n",
       "      <td>reproducible-res</td>\n",
       "      <td>http://www.coursera.org/learn/reproducible-res...</td>\n",
       "      <td>reproducibility</td>\n",
       "      <td>Reproducible Research</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This course focuses on the concepts and tools ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coursera</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[reproducibility, project, r, in_depth, video_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-N44X0IJEeWpogr5ZO8qxQ</td>\n",
       "      <td>ml-clust-retrieve</td>\n",
       "      <td>http://www.coursera.org/learn/ml-clustering-an...</td>\n",
       "      <td>natural_language_processing</td>\n",
       "      <td>Machine Learning: Clustering &amp; Retrieval</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Case Studies: Finding Similar Documents\\n\\nA r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coursera</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[natural_language_processing, text_data, high_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id   course_shortname  \\\n",
       "0  -4ffSHNYEeWIfhKr_WcYsQ   reproducible-res   \n",
       "1  -N44X0IJEeWpogr5ZO8qxQ  ml-clust-retrieve   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://www.coursera.org/learn/reproducible-res...   \n",
       "1  http://www.coursera.org/learn/ml-clustering-an...   \n",
       "\n",
       "                   concept_tag                                     title  \\\n",
       "0              reproducibility                     Reproducible Research   \n",
       "1  natural_language_processing  Machine Learning: Clustering & Retrieval   \n",
       "\n",
       "  subtitle                                        description  \\\n",
       "0      NaN  This course focuses on the concepts and tools ...   \n",
       "1      NaN  Case Studies: Finding Similar Documents\\n\\nA r...   \n",
       "\n",
       "  short_description     venue slide_text transcript_text  \\\n",
       "0               NaN  Coursera        NaN             NaN   \n",
       "1               NaN  Coursera        NaN             NaN   \n",
       "\n",
       "                                                tags  \n",
       "0  [reproducibility, project, r, in_depth, video_...  \n",
       "1  [natural_language_processing, text_data, high_...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('../data/tagged_resources.df')\n",
    "print(df.columns)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Digitizer:\n",
    "    def __init__(self):\n",
    "        self.word_dict = defaultdict(lambda: len(self.word_dict))\n",
    "        self.word_dict[0] = ''\n",
    "    def series_digitizer(self, text_series, max_len=-1):\n",
    "        digitzed_ser = text_series.apply(self.digitize, args=(max_len,)).astype('object')\n",
    "        max_len = digitzed_ser.apply(len).max()\n",
    "        digitzed_ser = digitzed_ser.apply(lambda x: x if len(x) == max_len else x + [0] * (max_len - len(x)))\n",
    "        return digitzed_ser\n",
    "        \n",
    "    def digitize(self, text, max_len=-1):\n",
    "        if max_len < 0:\n",
    "            return [self.word_dict[i] for i in text.split()]\n",
    "        else:\n",
    "            return [self.word_dict[i] for i in text.split()[:max_len]]\n",
    "    def num_words(self):\n",
    "        return len(self.word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "allowed_chars = set(string.ascii_lowercase) | {' '}\n",
    "en_stopwords = set(stopwords.words(\"english\"))\n",
    "def text_cleanup(text):\n",
    "    if isinstance(text, float):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    #text = ' '.join((stemmer.stem(i.decode('utf-8')) for i in text.split()))\n",
    "    if text == 'nan':\n",
    "        text = ''\n",
    "    text = ''.join([i if i in allowed_chars else ' ' for i in text])\n",
    "    text = ' '.join([i for i in text.split() if i not in en_stopwords])\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([               u'id',  u'course_shortname',               u'url',\n",
       "             u'concept_tag',             u'title',          u'subtitle',\n",
       "             u'description', u'short_description',             u'venue',\n",
       "              u'slide_text',   u'transcript_text',              u'tags'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title 71\n",
      "description 2104\n",
      "transcript_text 0\n",
      "num distinct words: 2708\n"
     ]
    }
   ],
   "source": [
    "digiz = Digitizer()\n",
    "max_words = 10000\n",
    "text_series = list()\n",
    "for i in ['title', 'description', 'transcript_text']:\n",
    "    ser = df[i].copy()\n",
    "    ser = ser.apply(text_cleanup)\n",
    "    print(i, ser.apply(len).max())\n",
    "    text_series.append(digiz.series_digitizer(ser, max_len=max_words))\n",
    "print('num distinct words:', digiz.num_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "1    [3, 4, 5, 6, 0, 0, 0, 0, 0, 0]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_series[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [107, 238, 45, 46, 151, 239, 240, 21, 241, 1, ...\n",
       "1    [116, 117, 288, 289, 290, 291, 292, 293, 294, ...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_series[1].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tags'] = df['tags'].apply(lambda x: list(filter(lambda y: y not in {'in_depth', 'mooc', 'video_lecture'}, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n",
      "148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 107, 238, 45, 4...\n",
       "1      [3, 4, 5, 6, 0, 0, 0, 0, 0, 0, 116, 117, 288, ...\n",
       "2      [7, 8, 9, 0, 0, 0, 0, 0, 0, 0, 107, 365, 121, ...\n",
       "62     [10, 11, 12, 0, 0, 0, 0, 0, 0, 0, 391, 392, 39...\n",
       "63     [13, 14, 15, 0, 0, 0, 0, 0, 0, 0, 510, 21, 442...\n",
       "149    [16, 17, 18, 19, 20, 0, 0, 0, 0, 0, 394, 536, ...\n",
       "150    [21, 22, 23, 0, 0, 0, 0, 0, 0, 0, 23, 64, 550,...\n",
       "184    [24, 21, 25, 0, 0, 0, 0, 0, 0, 0, 21, 552, 564...\n",
       "185    [3, 4, 26, 0, 0, 0, 0, 0, 0, 0, 116, 144, 582,...\n",
       "186    [27, 28, 29, 30, 0, 0, 0, 0, 0, 0, 107, 629, 6...\n",
       "Name: digitized_text, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = text_series[0].copy()\n",
    "for i in text_series[1:]:\n",
    "    all_text += i\n",
    "df['digitized_text'] = all_text\n",
    "max_len = df['digitized_text'].apply(len).max()\n",
    "print(max_len)\n",
    "df['digitized_text'] = df['digitized_text'].apply(lambda x: np.array(x))\n",
    "print(len(df))\n",
    "df['digitized_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_class_vec(tag_series):\n",
    "    tag2num = defaultdict(lambda : len(tag2num))\n",
    "    tag_dig = tag_series.apply(lambda x: [tag2num[i] for i in set(x)])\n",
    "    len_class_vec = len(tag2num)\n",
    "    def tagdig2vec(x):\n",
    "        v = np.zeros(len_class_vec)\n",
    "        v[x] = 1.\n",
    "        return v\n",
    "    tag_dig = tag_dig.apply(tagdig2vec)\n",
    "    return tag_dig, {j:i for i,j in tag2num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering\n"
     ]
    }
   ],
   "source": [
    "df['tagvec'], num2tag = create_class_vec(df['tags'])\n",
    "df['tagvec'].head(2)\n",
    "len_class_vec = len(df['tagvec'].iloc[0])\n",
    "print(num2tag[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#words: 2708\n",
      "seq len: 286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 107, 238, 45, 4...\n",
       "1    [3, 4, 5, 6, 0, 0, 0, 0, 0, 0, 116, 117, 288, ...\n",
       "Name: digitized_text, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = digiz.num_words()\n",
    "print('#words:', n_words)\n",
    "print('seq len:', max_len)\n",
    "df['digitized_text'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tagvec    [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "tags                 [reproducibility, project, r, project]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['tagvec', 'tags']].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = np.vstack(df['digitized_text'].values), np.vstack(df['tagvec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_nn(n_words, input_len):   \n",
    "    dim = 100\n",
    "    emb = Embedding(input_dim=n_words, output_dim=dim, mask_zero=True)\n",
    "    c = Input(shape=(input_len,), dtype='int32', name='course')\n",
    "    #dropout = Dropout(0.5)\n",
    "    c_e = emb(c)\n",
    "    c_l = LSTM(128)(c_e)\n",
    "    c_d = Dense(len_class_vec, activation='sigmoid')(c_l)\n",
    "    model = Model(c, c_d)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "course (InputLayer)              (None, 286)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)         (None, 286, 3)        8124        course[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                   (None, 10)            560         embedding_13[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 71)            781         lstm_13[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 9465\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nn = get_nn(n_words, max_len)\n",
    "print(nn.summary())\n",
    "#embedded_text = nn.predict(df['digitized_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 133 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "133/133 [==============================] - 1s - loss: 16.1584 - acc: 0.0301 - val_loss: 15.5945 - val_acc: 0.1333\n",
      "Epoch 2/100\n",
      "133/133 [==============================] - 1s - loss: 16.0465 - acc: 0.0150 - val_loss: 15.4860 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "133/133 [==============================] - 1s - loss: 15.7742 - acc: 0.0075 - val_loss: 15.3430 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "133/133 [==============================] - 1s - loss: 15.4565 - acc: 0.0075 - val_loss: 15.1875 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "133/133 [==============================] - 1s - loss: 15.0429 - acc: 0.0075 - val_loss: 15.0065 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "133/133 [==============================] - 1s - loss: 14.5722 - acc: 0.0075 - val_loss: 14.8616 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "133/133 [==============================] - 1s - loss: 14.1679 - acc: 0.0677 - val_loss: 14.8331 - val_acc: 0.0667\n",
      "Epoch 8/100\n",
      "133/133 [==============================] - 1s - loss: 13.8905 - acc: 0.1128 - val_loss: 14.8708 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "133/133 [==============================] - 1s - loss: 13.7268 - acc: 0.0226 - val_loss: 14.9502 - val_acc: 0.0667\n",
      "Epoch 10/100\n",
      "133/133 [==============================] - 1s - loss: 13.6222 - acc: 0.0301 - val_loss: 15.0485 - val_acc: 0.0667\n",
      "Epoch 11/100\n",
      "133/133 [==============================] - 1s - loss: 13.5603 - acc: 0.0301 - val_loss: 15.1347 - val_acc: 0.0667\n",
      "Epoch 12/100\n",
      "133/133 [==============================] - 1s - loss: 13.5242 - acc: 0.0301 - val_loss: 15.2232 - val_acc: 0.0667\n",
      "Epoch 13/100\n",
      "133/133 [==============================] - 1s - loss: 13.4850 - acc: 0.0301 - val_loss: 15.2815 - val_acc: 0.0667\n",
      "Epoch 14/100\n",
      "133/133 [==============================] - 1s - loss: 13.4545 - acc: 0.0301 - val_loss: 15.3674 - val_acc: 0.0667\n",
      "Epoch 15/100\n",
      "133/133 [==============================] - 1s - loss: 13.4264 - acc: 0.0301 - val_loss: 15.4298 - val_acc: 0.0667\n",
      "Epoch 16/100\n",
      "133/133 [==============================] - 1s - loss: 13.4116 - acc: 0.0301 - val_loss: 15.5061 - val_acc: 0.0667\n",
      "Epoch 17/100\n",
      "133/133 [==============================] - 1s - loss: 13.3910 - acc: 0.0301 - val_loss: 15.5776 - val_acc: 0.0667\n",
      "Epoch 18/100\n",
      "133/133 [==============================] - 1s - loss: 13.3934 - acc: 0.0376 - val_loss: 15.6135 - val_acc: 0.0667\n",
      "Epoch 19/100\n",
      "133/133 [==============================] - 1s - loss: 13.3766 - acc: 0.0301 - val_loss: 15.6771 - val_acc: 0.0667\n",
      "Epoch 20/100\n",
      "133/133 [==============================] - 1s - loss: 13.3646 - acc: 0.0301 - val_loss: 15.6901 - val_acc: 0.0667\n",
      "Epoch 21/100\n",
      "133/133 [==============================] - 1s - loss: 13.3529 - acc: 0.0301 - val_loss: 15.7512 - val_acc: 0.0667\n",
      "Epoch 22/100\n",
      "133/133 [==============================] - 1s - loss: 13.3452 - acc: 0.0301 - val_loss: 15.7776 - val_acc: 0.0667\n",
      "Epoch 23/100\n",
      "133/133 [==============================] - 1s - loss: 13.3437 - acc: 0.0301 - val_loss: 15.8244 - val_acc: 0.0667\n",
      "Epoch 24/100\n",
      "133/133 [==============================] - 1s - loss: 13.3285 - acc: 0.0376 - val_loss: 15.8691 - val_acc: 0.0667\n",
      "Epoch 25/100\n",
      "133/133 [==============================] - 1s - loss: 13.3242 - acc: 0.0376 - val_loss: 15.9254 - val_acc: 0.0667\n",
      "Epoch 26/100\n",
      "133/133 [==============================] - 1s - loss: 13.3180 - acc: 0.0376 - val_loss: 15.9363 - val_acc: 0.0667\n",
      "Epoch 27/100\n",
      "133/133 [==============================] - 1s - loss: 13.3058 - acc: 0.0301 - val_loss: 15.9774 - val_acc: 0.0667\n",
      "Epoch 28/100\n",
      "133/133 [==============================] - 1s - loss: 13.3022 - acc: 0.0376 - val_loss: 16.0145 - val_acc: 0.0667\n",
      "Epoch 29/100\n",
      "133/133 [==============================] - 1s - loss: 13.2970 - acc: 0.0301 - val_loss: 16.0255 - val_acc: 0.0667\n",
      "Epoch 30/100\n",
      "133/133 [==============================] - 1s - loss: 13.2938 - acc: 0.0376 - val_loss: 16.0376 - val_acc: 0.0667\n",
      "Epoch 31/100\n",
      "133/133 [==============================] - 1s - loss: 13.2799 - acc: 0.0376 - val_loss: 16.0576 - val_acc: 0.0667\n",
      "Epoch 32/100\n",
      "133/133 [==============================] - 1s - loss: 13.2662 - acc: 0.0376 - val_loss: 16.0869 - val_acc: 0.0667\n",
      "Epoch 33/100\n",
      "133/133 [==============================] - 1s - loss: 13.2587 - acc: 0.0376 - val_loss: 16.1268 - val_acc: 0.0667\n",
      "Epoch 34/100\n",
      "133/133 [==============================] - 1s - loss: 13.2483 - acc: 0.0376 - val_loss: 16.1434 - val_acc: 0.0667\n",
      "Epoch 35/100\n",
      "133/133 [==============================] - 1s - loss: 13.2424 - acc: 0.0376 - val_loss: 16.1629 - val_acc: 0.0667\n",
      "Epoch 36/100\n",
      "133/133 [==============================] - 1s - loss: 13.2302 - acc: 0.0376 - val_loss: 16.2076 - val_acc: 0.0667\n",
      "Epoch 37/100\n",
      "133/133 [==============================] - 1s - loss: 13.2223 - acc: 0.0376 - val_loss: 16.1734 - val_acc: 0.0667\n",
      "Epoch 38/100\n",
      "133/133 [==============================] - 1s - loss: 13.2100 - acc: 0.0451 - val_loss: 16.1793 - val_acc: 0.0667\n",
      "Epoch 39/100\n",
      "133/133 [==============================] - 1s - loss: 13.1957 - acc: 0.0451 - val_loss: 16.0098 - val_acc: 0.0667\n",
      "Epoch 40/100\n",
      "133/133 [==============================] - 1s - loss: 13.1811 - acc: 0.0451 - val_loss: 15.8083 - val_acc: 0.0667\n",
      "Epoch 41/100\n",
      "133/133 [==============================] - 1s - loss: 14.0413 - acc: 0.0677 - val_loss: 15.1540 - val_acc: 0.2000\n",
      "Epoch 42/100\n",
      "133/133 [==============================] - 1s - loss: 14.3401 - acc: 0.0902 - val_loss: 15.0303 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "133/133 [==============================] - 1s - loss: 14.3660 - acc: 0.1128 - val_loss: 15.0914 - val_acc: 0.1333\n",
      "Epoch 44/100\n",
      "133/133 [==============================] - 1s - loss: 13.9860 - acc: 0.0827 - val_loss: 15.0909 - val_acc: 0.1333\n",
      "Epoch 45/100\n",
      "133/133 [==============================] - 1s - loss: 13.6012 - acc: 0.1278 - val_loss: 15.1932 - val_acc: 0.0667\n",
      "Epoch 46/100\n",
      "133/133 [==============================] - 1s - loss: 13.5165 - acc: 0.1203 - val_loss: 15.3083 - val_acc: 0.1333\n",
      "Epoch 47/100\n",
      "133/133 [==============================] - 1s - loss: 13.3094 - acc: 0.0902 - val_loss: 15.3473 - val_acc: 0.1333\n",
      "Epoch 48/100\n",
      "133/133 [==============================] - 1s - loss: 13.1948 - acc: 0.0977 - val_loss: 15.3284 - val_acc: 0.1333\n",
      "Epoch 49/100\n",
      "133/133 [==============================] - 1s - loss: 13.0763 - acc: 0.0752 - val_loss: 15.3256 - val_acc: 0.1333\n",
      "Epoch 50/100\n",
      "133/133 [==============================] - 1s - loss: 13.1698 - acc: 0.0451 - val_loss: 15.0411 - val_acc: 0.0667\n",
      "Epoch 51/100\n",
      "133/133 [==============================] - 1s - loss: 13.0957 - acc: 0.0526 - val_loss: 15.2520 - val_acc: 0.1333\n",
      "Epoch 52/100\n",
      "133/133 [==============================] - 1s - loss: 13.0474 - acc: 0.0526 - val_loss: 15.8233 - val_acc: 0.0667\n",
      "Epoch 53/100\n",
      "133/133 [==============================] - 1s - loss: 13.2118 - acc: 0.0526 - val_loss: 16.1991 - val_acc: 0.0667\n",
      "Epoch 54/100\n",
      "133/133 [==============================] - 1s - loss: 13.2017 - acc: 0.0526 - val_loss: 16.1010 - val_acc: 0.0667\n",
      "Epoch 55/100\n",
      "133/133 [==============================] - 1s - loss: 13.1386 - acc: 0.0602 - val_loss: 16.0228 - val_acc: 0.1333\n",
      "Epoch 56/100\n",
      "128/133 [===========================>..] - ETA: 0s - loss: 13.0980 - acc: 0.0625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-d70c923ceef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m               verbose=1)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1106\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1108\u001b[1;33m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[0;32m   1109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[0;32m    824\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda2/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/home/services/.theano/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.12-64/scan_perform/mod.cpp:6489)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda2/lib/python2.7/site-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mvalue_zeros\u001b[1;34m(self, shape)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mvalue_zeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m         \"\"\"\n\u001b[0;32m    632\u001b[0m         \u001b[0mCreate\u001b[0m \u001b[0man\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mof\u001b[0m \u001b[1;36m0\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = nn.fit(X, y,\n",
    "              batch_size=8,\n",
    "              validation_split=.1,\n",
    "              shuffle=True,\n",
    "              nb_epoch=100, \n",
    "              verbose=1)\n",
    "\n",
    "data = zip(hist.history['loss'], hist.history['val_loss'])\n",
    "hist_df = pd.DataFrame(columns=['train', 'val'], data=data[1:])\n",
    "hist_df.plot(y=['train', 'val'], secondary_y=['val'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_vec_pred = nn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pred_to_tag(pred, th=.5):\n",
    "    tags = list()\n",
    "    idx = range(len(pred[0]))\n",
    "    for i in pred:\n",
    "        filt_preds = list(filter(lambda x: x[0] > th, zip(i, idx)))\n",
    "        if len(filt_preds) > 0:\n",
    "            tag_vals, tag_nums = zip(*filt_preds)\n",
    "            pred_tags = map(lambda x: num2tag[x], tag_nums)\n",
    "            tags.append(list(zip(pred_tags, tag_vals)))\n",
    "        else:\n",
    "            tags.append([tuple()])\n",
    "        \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[()], [()], [()], [()], [()]]\n"
     ]
    }
   ],
   "source": [
    "pred_tags = pred_to_tag(tag_vec_pred)\n",
    "print(pred_tags[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['pred'] = 0\n",
    "df['pred'] = df['pred'].astype('object')\n",
    "df['pred'] = pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducible Research\n",
      "Machine Learning: Clustering & Retrieval\n",
      "Introduction to Genomic Technologies\n",
      "Econometrics: Methods and Applications\n",
      "Fundamentals of Quantitative Modeling\n",
      "Measuring Causal Effects in the Social Sciences\n",
      "Data Science Capstone\n",
      "Developing Data Products\n",
      "Machine Learning: Classification\n",
      "Hadoop Platform and Application Framework\n",
      "Practical Predictive Analytics: Models and Methods\n",
      "Forensic Accounting and Fraud Examination\n",
      "Python for Genomic Data Science\n",
      "Customer Analytics\n",
      "Data Management and Visualization\n",
      "Business Intelligence Concepts, Tools, and Applications\n",
      "Modeling Risk and Realities\n",
      "Building a Data Science Team\n",
      "Machine Learning With Big Data\n",
      "Interprofessional Healthcare Informatics \n",
      "Statistics for Genomic Data Science\n",
      "Decision-Making and Scenarios\n",
      "Design and Build a Data Warehouse for Business Intelligence Implementation\n",
      "Data Science at Scale - Capstone Project\n",
      "Data Analysis Tools\n",
      "Machine Learning: Recommender Systems & Dimensionality Reduction\n",
      "Graph Analytics for Big Data\n",
      "Managing Big Data with MySQL\n",
      "Machine Learning\n",
      "Algorithms for DNA Sequencing\n",
      "Practical Machine Learning\n",
      "Designing, Running, and Analyzing Experiments\n",
      "Getting and Cleaning Data\n",
      "Business Metrics for Data-Driven Companies\n",
      "Introduction to Big Data\n",
      "Operations Analytics\n",
      "Data Science in Real Life\n",
      "Capstone: Geospatial Analysis\n",
      "Business Analytics Capstone\n",
      "Introduction to Big Data Analytics\n",
      "Qualitative Research Methods\n",
      "Python Data Structures\n",
      "Increasing Real Estate Management Profits: Harnessing Data Analytics\n",
      "Data Visualization and Communication with Tableau\n",
      "R Programming\n",
      "Machine Learning Capstone: An Intelligent Application with Deep Learning\n",
      "Mastering Data Analysis in Excel\n",
      "Experimentation for Improvement\n",
      "Data Analysis and Interpretation Capstone\n",
      "A Crash Course in Data Science\n",
      "Executive Data Science Capstone\n",
      "Principles of fMRI 1\n",
      "Managing Data Analysis\n",
      "Introduction to Search Engine Optimization\n",
      "Regression Modeling in Practice\n",
      "Case studies in business analytics with ACCENTURE\n",
      "Statistical Inference\n",
      "Basic Statistics\n",
      "Genomic Data Science with Galaxy\n",
      "Regression Models\n",
      "Digital Analytics for Marketing Professionals: Marketing Analytics in Practice\n",
      "Foundations of strategic business analytics\n",
      "Intro to Computer Science\n",
      "Using Databases with Python\n",
      "Exploratory Data Analysis\n",
      "The Data Scientist’s Toolbox\n",
      "Inferential Statistics\n",
      "Bioconductor for Genomic Data Science\n",
      "Machine Learning: Regression\n",
      "Big Data - Capstone Project\n",
      "Capstone: Analyzing (Social) Network Data\n",
      "Introduction to People Analytics\n",
      "Capstone:  Create Value from Open Data\n",
      "Database Management Essentials\n",
      "Machine Learning Foundations: A Case Study Approach\n",
      "Accounting Analytics\n",
      "Capstone: Retrieving, Processing, and Visualizing Data with Python\n",
      "Understanding Clinical Research: Behind the Statistics\n",
      "Machine Learning for Data Analysis\n",
      "Intro to Statistics\n",
      "Communicating Data Science Results\n",
      "The Importance of Listening\n",
      "Introduction to Spreadsheets and Models\n",
      "Introduction to Recommender Systems\n",
      "Mathematical Biostatistics Boot Camp 2\n",
      "Wharton Business and Financial Modeling Capstone\n",
      "Data Wrangling with MongoDB\n",
      "Intro to Machine Learning\n",
      "Intro to Data Analysis\n",
      "Intro to Inferential Statistics\n",
      "A/B Testing\n",
      "Intro to Data Science\n",
      "Real-Time Analytics with Apache Storm\n",
      "Data Visualization and D3.js\n",
      "Intro to Hadoop and MapReduce\n",
      "Data Analysis with R\n",
      "Deep Learning\n",
      "Intro to Descriptive Statistics\n",
      "Model Building and Validation\n",
      "Command Line Tools for Genomic Data Science\n",
      "Probability\n",
      "Introduction to Data Science\n",
      "Introduction to Natural Language Processing\n",
      "Core Concepts in Data Analysis\n",
      "Preparing for the AP* Statistics Exam\n",
      "Web Intelligence and Big Data\n",
      "Process Mining: Data science in Action\n",
      "Machine Learning\n",
      "Discrete Optimization\n",
      "Teaching Statistical Thinking: Part 1 Descriptive Statistics\n",
      "Passion Driven Statistics\n",
      "The Caltech-JPL Summer School on Big Data Analytics\n",
      "Introduction to Statistics for the Social Sciences\n",
      "Coding the Matrix: Linear Algebra through Computer Science Applications\n",
      "Linear and Discrete Optimization\n",
      "Natural Language Processing\n",
      "Pattern Discovery in Data Mining\n",
      "Text Retrieval and Search Engines\n",
      "Cluster Analysis in Data Mining\n",
      "Text Mining and Analytics\n",
      "Data Visualization\n",
      "Big Data Analytics for Healthcare\n",
      "Healthcare Data Visualization\n",
      "Statistical Reasoning for Public Health 2: Regression Methods\n",
      "Statistics One\n",
      "Applied Regression Analysis\n",
      "Applied Logistic Regression\n",
      "Big Data Science with the BD2K-LINCS Data Coordination and Integration Center\n",
      "Practical Learning Analytics\n",
      "Probabilistic Graphical Models\n",
      "Understanding China, 1700-2000: A Data Analytic Approach, Part 2\n",
      "Statistics: Making Sense of Data\n",
      "Social Network Analysis\n",
      "Social and Economic Networks:  Models and Analysis\n",
      "Case-Based Introduction to Biostatistics\n",
      "Data Analysis and Statistical Inference \n",
      "Natural Language Processing\n",
      "Discrete Inference and Learning in Artificial Vision\n",
      "Neural Networks for Machine Learning\n",
      "Statistical Reasoning for Public Health 1: Estimation, Inference, & Interpretation\n",
      "Mining Massive Datasets\n",
      "Artificial Intelligence Planning\n",
      "Computational Methods for Data Analysis\n",
      "Relational Database Support for Data Warehouses\n",
      "Marketing Analytics\n",
      "People Analytics\n",
      "Data Warehouse Concepts, Design, and Data Integration\n",
      "Data Manipulation at Scale: Systems and Algorithms\n"
     ]
    }
   ],
   "source": [
    "for ridx, (title, tags, pred) in df[['title', 'tags', 'pred']].iterrows():\n",
    "    print(title)\n",
    "    try:\n",
    "        pred_dict = {i:j for i,j in pred}\n",
    "    except ValueError:\n",
    "        continue\n",
    "    overlap = set(tags) & set(pred_dict.keys())\n",
    "    print('\\ttrue positives')\n",
    "    for i in sorted(overlap):\n",
    "        print('\\t\\t', i, pred_dict[i])\n",
    "    print('\\tfalse positives')\n",
    "    for i in sorted(set(pred_dict.keys()) - set(tags)):\n",
    "        print('\\t\\t', i, pred_dict[i])\n",
    "    print('\\tfalse negatives')\n",
    "    for i in sorted(set(tags) - set(pred_dict.keys())):\n",
    "        print('\\t\\t', i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
